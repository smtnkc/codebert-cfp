code,label,prediction
"def remove_old_message():
    
    global g_old_messages_to_remove
    global g_dict_changed

    # extract old java ignored messages to be removed in old_message_dict
    old_message_dict = extract_message_to_dict(g_old_messages_to_remove)

    if old_message_dict:
        g_dict_changed = True
        update_message_dict(old_message_dict,2)",1,3
"def open_stream(stream):
    
    global stream_fd

    # Attempts to open the stream
    try:
        stream_fd = stream.open()
    except StreamError as err:
        raise StreamError(""Could not open stream: {0}"".format(err))

    # Read 8192 bytes before proceeding to check for errors.
    # This is to avoid opening the output unnecessarily.
    try:
        log.debug(""Pre-buffering 8192 bytes"")
        prebuffer = stream_fd.read(8192)
    except IOError as err:
        stream_fd.close()
        raise StreamError(""Failed to read data from stream: {0}"".format(err))

    if not prebuffer:
        stream_fd.close()
        raise StreamError(""No data returned from stream"")

    return stream_fd, prebuffer",1,1
"def read_json(filename, mode='r'):
    
    with open(filename, mode) as filey:
        data = json.load(filey)
    return data",1,1
"def get_sample_trim(p1_data, p2_data):
    

    sample_ranges = [trim_range(x) for x in [p1_data, p2_data]]

    # Get the optimal trim position for 5' end
    optimal_5trim = max([x[0] for x in sample_ranges])
    # Get optimal trim position for 3' end
    optimal_3trim = min([x[1] for x in sample_ranges])

    return optimal_5trim, optimal_3trim",1,3
"def get_data(self):
        
        with self as gr:
            while True:
                try:
                    yield gr.read_next_data_block_int8()
                except Exception as e:
                    print(""File depleted"")
                    yield None, None, None",1,1
"def load(fp, **kwargs):
    
    _load_arg_defaults(kwargs)
    if not PY2:
        fp = _wrap_reader_for_text(fp, kwargs.pop('encoding', None) or 'utf-8')
    return _json.load(fp, **kwargs)",1,1
"def _loop_cmd(self):
        
        self._cmd('LOOP', 1)
        raw = self.port.read(LoopStruct.size)  # read data
        log_raw('read', raw)
        return raw",1,1
"def __line_gen(self):
        
        while True:
            line = self.__buffer.readline()
            if not line:
                self.__recv()
                continue
            yield line",1,1
"def __buf_gen(self, length=0):
        
        while True:
            buf = self.__buffer.read(length)
            if not buf:
                self.__recv()
                continue
            yield buf",1,1
"def read (self, size = -1):         # File-like object.
        

        if size == 0:
            return self._empty_buffer
        if size < 0:
            self.expect (self.delimiter) # delimiter default is EOF
            return self.before

        # I could have done this more directly by not using expect(), but
        # I deliberately decided to couple read() to expect() so that
        # I would catch any bugs early and ensure consistant behavior.
        # It's a little less efficient, but there is less for me to
        # worry about if I have to later modify read() or expect().
        # Note, it's OK if size==-1 in the regex. That just means it
        # will never match anything in which case we stop only on EOF.
        if self._buffer_type is bytes:
            pat = (u'.{%d}' % size).encode('ascii')
        else:
            pat = u'.{%d}' % size
        cre = re.compile(pat, re.DOTALL)
        index = self.expect ([cre, self.delimiter]) # delimiter default is EOF
        if index == 0:
            return self.after ### self.before should be ''. Should I assert this?
        return self.before",1,1
"def textFileStream(self, directory):
        
        return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())",1,1
"def binaryRecordsStream(self, directory, recordLength):
        
        return DStream(self._jssc.binaryRecordsStream(directory, recordLength), self,
                       NoOpSerializer())",1,1
"def read_nonblocking(self, size=1, timeout=None):
        

        try:
            s = os.read(self.child_fd, size)
        except OSError as err:
            if err.args[0] == errno.EIO:
                # Linux-style EOF
                self.flag_eof = True
                raise EOF('End Of File (EOF). Exception style platform.')
            raise
        if s == b'':
            # BSD-style EOF
            self.flag_eof = True
            raise EOF('End Of File (EOF). Empty string style platform.')

        s = self._decoder.decode(s, final=False)
        self._log(s, 'read')
        return s",1,1
"def read(self, size=-1):
        

        if size == 0:
            return self.string_type()
        if size < 0:
            # delimiter default is EOF
            self.expect(self.delimiter)
            return self.before

        # I could have done this more directly by not using expect(), but
        # I deliberately decided to couple read() to expect() so that
        # I would catch any bugs early and ensure consistent behavior.
        # It's a little less efficient, but there is less for me to
        # worry about if I have to later modify read() or expect().
        # Note, it's OK if size==-1 in the regex. That just means it
        # will never match anything in which case we stop only on EOF.
        cre = re.compile(self._coerce_expect_string('.{%d}' % size), re.DOTALL)
        # delimiter default is EOF
        index = self.expect([cre, self.delimiter])
        if index == 0:
            ### FIXME self.before should be ''. Should I assert this?
            return self.after
        return self.before",1,1
"def read_until_close(self) -> Awaitable[bytes]:
        
        future = self._start_read()
        if self.closed():
            self._finish_read(self._read_buffer_size, False)
            return future
        self._read_until_close = True
        try:
            self._try_inline_read()
        except:
            future.add_done_callback(lambda f: f.exception())
            raise
        return future",1,1
"async def release(self) -> None:
        
        if self._at_eof:
            return
        while not self._at_eof:
            await self.read_chunk(self.chunk_size)",1,1
"def _read_payload(socket, payload_size):
    

    remaining = payload_size
    while remaining > 0:

        # Try and read as much as possible
        data = read(socket, remaining)
        if data is None:
            # ``read`` will terminate with an empty string. This is just a transient state where we didn't get any data
            continue

        if len(data) == 0:  # pylint: disable=C1801
            # Empty string. Socket does not have any more data. We are done here even if we haven't read full payload
            break

        remaining -= len(data)
        yield data",1,1
"def encode(self, tags, encoding, values_to_sub):
        

        for tag in tags:
            if tags[tag].get(encoding) != ""None"":
                if tags[tag].get(encoding) == ""url"":
                    values_to_sub[tag] = self.url_encode(values_to_sub[tag])
                if tags[tag].get(encoding) == ""base64"":
                    values_to_sub[tag] = self.base64_utf_encode(values_to_sub[tag])
        return values_to_sub",1,0
"def compress_file(fh_, compresslevel=9, chunk_size=1048576):
    
    try:
        bytes_read = int(chunk_size)
        if bytes_read != chunk_size:
            raise ValueError
    except ValueError:
        raise ValueError('chunk_size must be an integer')
    try:
        while bytes_read == chunk_size:
            buf = BytesIO()
            with open_fileobj(buf, 'wb', compresslevel) as ogz:
                try:
                    bytes_read = ogz.write(fh_.read(chunk_size))
                except AttributeError:
                    # Open the file and re-attempt the read
                    fh_ = salt.utils.files.fopen(fh_, 'rb')
                    bytes_read = ogz.write(fh_.read(chunk_size))
            yield buf.getvalue()
    finally:
        try:
            fh_.close()
        except AttributeError:
            pass",1,1
"def read_file(fh_, chunk_size=1048576):
    
    try:
        if chunk_size != int(chunk_size):
            raise ValueError
    except ValueError:
        raise ValueError('chunk_size must be an integer')
    try:
        while True:
            try:
                chunk = fh_.read(chunk_size)
            except AttributeError:
                # Open the file and re-attempt the read
                fh_ = salt.utils.files.fopen(fh_, 'rb')  # pylint: disable=W8470
                chunk = fh_.read(chunk_size)
            if not chunk:
                break
            yield chunk
    finally:
        try:
            fh_.close()
        except AttributeError:
            pass",1,1
"def _get_raw_data(subset):
  
  raw_data = _LOADED.get(subset)
  if raw_data is not None:
    return raw_data, _LOADED[""vocab""]
  else:
    train_data, valid_data, test_data, vocab = ptb_reader.ptb_raw_data(
        FLAGS.data_path)
    _LOADED.update({
        ""train"": np.array(train_data),
        ""valid"": np.array(valid_data),
        ""test"": np.array(test_data),
        ""vocab"": vocab
    })
    return _LOADED[subset], vocab",1,1
"def _GetChunkForReading(self, chunk):
    
    try:
      return self.chunk_cache.Get(chunk)
    except KeyError:
      pass

    # We don't have this chunk already cached. The most common read
    # access pattern is contiguous reading so since we have to go to
    # the data store already, we read ahead to reduce round trips.

    missing_chunks = []
    for chunk_number in range(chunk, chunk + 10):
      if chunk_number not in self.chunk_cache:
        missing_chunks.append(chunk_number)

    self._ReadChunks(missing_chunks)
    # This should work now - otherwise we just give up.
    try:
      return self.chunk_cache.Get(chunk)
    except KeyError:
      raise aff4.ChunkNotFoundError(""Cannot open chunk %s"" % chunk)",1,1
"def output_thread(log, stdout, stderr, timeout_event, is_alive, quit,
        stop_output_event):
    

    poller = Poller()
    if stdout is not None:
        poller.register_read(stdout)
    if stderr is not None:
        poller.register_read(stderr)

    # this is our poll loop for polling stdout or stderr that is ready to
    # be read and processed.  if one of those streamreaders indicate that it
    # is done altogether being read from, we remove it from our list of
    # things to poll.  when no more things are left to poll, we leave this
    # loop and clean up
    while poller:
        changed = no_interrupt(poller.poll, 0.1)
        for f, events in changed:
            if events & (POLLER_EVENT_READ | POLLER_EVENT_HUP):
                log.debug(""%r ready to be read from"", f)
                done = f.read()
                if done:
                    poller.unregister(f)
            elif events & POLLER_EVENT_ERROR:
                # for some reason, we have to just ignore streams that have had an
                # error.  i'm not exactly sure why, but don't remove this until we
                # figure that out, and create a test for it
                pass

        if timeout_event and timeout_event.is_set():
            break

        if stop_output_event.is_set():
            break

    # we need to wait until the process is guaranteed dead before closing our
    # outputs, otherwise SIGPIPE
    alive, _ = is_alive()
    while alive:
        quit.wait(1)
        alive, _ = is_alive()

    if stdout:
        stdout.close()

    if stderr:
        stderr.close()",1,1
"def open_fifo_write(path, data):
    
    os.mkfifo(path, stat.S_IRUSR | stat.S_IWUSR)
    threading.Thread(target=lambda p, d: open(p, 'wb').write(d),
                     args=(path, data)).start()",1,0
"def get_reads(sam, \
        contigs = False, mismatches = False, mm_option = False, \
        sort_sam = True, req_map = False, region = False, sbuffer = False):
    
    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])
    if sort_sam is True:
        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])
        if sam != '-':
            if os.path.exists(mapping) is False:
                os.system(""\
                    sort -k1 --buffer-size=%sG -T %s -o %s %s\
                    "" % (sbuffer, tempdir, mapping, sam))
        else:
            mapping = 'stdin-sam.sorted.sam'
            p = Popen(""sort -k1 --buffer-size=%sG -T %s -o %s"" \
                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True)
            p.communicate()
        mapping = open(mapping)
    else:
        if sam == '-':
            mapping = sys.stdin
        else:
            mapping = open(sam)
    for read in reads_from_mapping(mapping, contigs, mismatches, mm_option, req_map, region):
        yield read",1,1
"def create_map_from_file(self, data_filename):
        
        
        op_filename = data_filename + '.rule'
        
        dataset = mod_datatable.DataTable(data_filename, ',')
        dataset.load_to_array()
        l_map = self.generate_map_from_dataset(dataset)
        with open(op_filename, 'w') as f:
            f.write('# rules file autogenerated by mapper.py v0.1\n')
            f.write('filename:source=' + data_filename + '\n')
            f.write('filename:rule=' + op_filename + '\n\n')
            for row in l_map:
                #print('ROW = ' , row)
                if type(row) is str:
                    f.write(row + '\n')
                else:
                    for v in row:
                        f.write(v)",1,0
"def value(self):
        

        value = getattr(self.instrument, self.probe_name)
        self.buffer.append(value)

        return value",1,2
"def alignment_to_reads(
    sam_merged,
    output_dir,
    parameters=DEFAULT_PARAMETERS,
    save_memory=True,
    *bin_fasta
):
    

    #   Just in case file objects are sent as input
    def get_file_string(file_thing):
        try:
            file_string = file_thing.name
        except AttributeError:
            file_string = str(file_thing)
        return file_string

    #   Global set of chunks against which reads are required to
    #   map - we store them in a tuple that keeps track of the
    #   original bin each chunk came from so we can reattribute the reads later

    bin_chunks = set()
    for bin_file in bin_fasta:
        for record in SeqIO.parse(bin_file, ""fasta""):
            bin_chunks.add((get_file_string(bin_file), record.id))

    chunk_size = int(parameters[""chunk_size""])

    mapq_threshold = int(parameters[""mapq_threshold""])

    def read_name(read):
        return read.query_name.split()[0]

    #   Since reading a huge BAM file can take up a
    #   lot of time and resources, we only do it once
    #   but that requires opening fastq files for writing
    #   as matching reads get detected along the
    #   bam and keeping track of which ones are
    #   currently open.

    def get_base_name(bin_file):

        base_name = ""."".join(os.path.basename(bin_file).split(""."")[:-1])

        output_path = os.path.join(
            output_dir, ""{}.readnames"".format(base_name)
        )

        return output_path

    if save_memory:
        opened_files = dict()
    else:
        read_names = dict()

    with pysam.AlignmentFile(sam_merged, ""rb"") as alignment_merged_handle:

        for (my_read_name, alignment_pool) in itertools.groupby(
            alignment_merged_handle, read_name
        ):

            for my_alignment in alignment_pool:

                relative_position = my_alignment.reference_start
                contig_name = my_alignment.reference_name

                chunk_position = relative_position // chunk_size

                # The 'chunk name' is used to detect macthing positions
                chunk_name = ""{}_{}"".format(contig_name, chunk_position)

                # But such matching positions have to map acceptably
                quality_test = my_alignment.mapping_quality > mapq_threshold

                for bin_file in bin_fasta:
                    chunk_tuple = (bin_file, chunk_name)
                    if chunk_tuple in bin_chunks and quality_test:
                        if save_memory:
                            output_path = get_base_name(bin_file)
                            try:
                                output_handle = opened_files[bin_file]
                            except KeyError:
                                output_handle = open(output_path, ""w"")
                                opened_files[bin_file] = output_handle

                            output_handle.write(""@{}\n"".format(my_read_name))

                        else:
                            try:
                                read_names[my_read_name].append(bin_file)
                            except KeyError:
                                read_names[my_read_name] = [bin_file]

    for file_handle in opened_files.values():
        file_handle.close()
    #   Return unpaired file names for pair_unpaired_reads() to process
    if save_memory:
        return opened_files.keys()
    else:
        return read_names",1,1
"def url_read_text(url, verbose=True):
    r
    data = url_read(url, verbose)
    text = data.decode('utf8')
    return text",1,1
"def load_data_and_build(self, filename, delimiter="",""):
        
        data = np.genfromtxt(
            filename, dtype=float, delimiter=delimiter, names=True
        )
        data = data.view(np.float64).reshape(data.shape + (-1,))

        X = data[:, 0:-1]
        Y = data[:, -1]

        self.build(X=X, Y=Y)",1,1
"def readline(self, timeout):
        
        buf = self.__remainder
        while not linefeed_byte in buf:
            buf += self._read_timeout(timeout)
        n = buf.index(linefeed_byte)
        self.__remainder = buf[n + 1:]
        buf = buf[:n]
        if (len(buf) > 0) and (buf[-1] == cr_byte_value):
            buf = buf[:-1]
        return u(buf)",1,1
"def bait(self):
        
        with progressbar(self.metadata) as bar:
            for sample in bar:
                if sample.general.bestassemblyfile != 'NA':
                    # Only need to perform baiting on FASTQ files
                    if sample[self.analysistype].filetype == 'fastq':
                        # Make the system call - allow for single- or paired-end reads
                        if len(sample.general.fastqfiles) == 2:
                            # Create the command to run the baiting - ref: primer file, k: shortest primer length
                            # in1, in2: paired inputs, hdist: number of mismatches, interleaved: use interleaved output
                            # outm: single, zipped output file of reads that match the target file
                            sample[self.analysistype].bbdukcmd = \
                                'bbduk.sh ref={primerfile} k={klength} in1={forward} in2={reverse} ' \
                                'hdist={mismatches} threads={threads} interleaved=t outm={outfile}' \
                                .format(primerfile=self.formattedprimers,
                                        klength=self.klength,
                                        forward=sample.general.trimmedcorrectedfastqfiles[0],
                                        reverse=sample.general.trimmedcorrectedfastqfiles[1],
                                        mismatches=self.mismatches,
                                        threads=str(self.cpus),
                                        outfile=sample[self.analysistype].baitedfastq)
                        else:
                            sample[self.analysistype].bbdukcmd = \
                                'bbduk.sh ref={primerfile} k={klength} in={fastq} hdist={mismatches} ' \
                                'threads={threads} interleaved=t outm={outfile}' \
                                .format(primerfile=self.formattedprimers,
                                        klength=self.klength,
                                        fastq=sample.general.trimmedcorrectedfastqfiles[0],
                                        mismatches=self.mismatches,
                                        threads=str(self.cpus),
                                        outfile=sample[self.analysistype].baitedfastq)
                        # Run the system call (if necessary)
                        if not os.path.isfile(sample[self.analysistype].baitedfastq):
                            run_subprocess(sample[self.analysistype].bbdukcmd)",1,1
"def run(self):
        
        while (self._run_thread):
            (mode, version, packet) = self._read_packet()
            if (mode == PROP_PACKET_SENSE):
                self._decode_sense_packet(version, packet)
            elif (mode == PROP_PACKET_CTRL):
                self._decode_ctrl_packet(version, packet)",1,3
"def _mq_callback(self, message):
        
        while threading.active_count() > settings.settings['bash_runnable_count'] + self.initial_thread_count:
            time.sleep(0.1)

        t = BashRunnable(self.logger, message, self.consumer, self.performance_tracker)
        t.daemon = True
        t.start()",1,2
"def geocode(self):
        

        submit_set = []
        data_map = {}

        for address, o in self.gen:
            submit_set.append(address)
            data_map[address] = o

            if len(submit_set) >= self.submit_size:
                results = self._send(submit_set)
                submit_set = []

                for k, result in results.items():
                    o = data_map[k]
                    yield (k, result, o)

        if len(submit_set) > 0:
            results = self._send(submit_set)
            # submit_set = []

            for k, result in results.items():
                o = data_map[k]
                yield (k, result, o)",1,2
"def combineReads(filename, sequences, readClass=DNARead,
                 upperCase=False, idPrefix='command-line-read-'):
    
    # Read sequences from a FASTA file, if given.
    if filename:
        reads = FastaReads(filename, readClass=readClass, upperCase=upperCase)
    else:
        reads = Reads()

    # Add any individually specified subject sequences.
    if sequences:
        for count, sequence in enumerate(sequences, start=1):
            # Try splitting the sequence on its last space and using the
            # first part of the split as the read id. If there's no space,
            # assign a generic id.
            parts = sequence.rsplit(' ', 1)
            if len(parts) == 2:
                readId, sequence = parts
            else:
                readId = '%s%d' % (idPrefix, count)
            if upperCase:
                sequence = sequence.upper()
            read = readClass(readId, sequence)
            reads.add(read)

    return reads",1,1
"def lines(fp):
    
    if fp.fileno() == sys.stdin.fileno():
        close = True

        try: # Python 3
            fp = open(fp.fileno(), mode='r', buffering=BUF_LINEBUFFERED, errors='replace')
            decode = False
        except TypeError:
            fp = os.fdopen(fp.fileno(), 'rU', BUF_LINEBUFFERED)
            decode = True

    else:
        close = False

        try:
            # only decode if the fp doesn't already have an encoding
            decode = (fp.encoding != UTF8)
        except AttributeError:
            # fp has been opened in binary mode
            decode = True

    try:
        while 1:
            l = fp.readline()
            if l:
                if decode:
                    l = l.decode(UTF8, 'replace')
                yield l
            else:
                break
    finally:
        if close:
            fp.close()",1,1
"def _folder_to_dict(self, path):
        
        res = {}
        for key in os.listdir(path):
            if key.startswith('.'):
                continue
            key_path = os.path.join(path, key)
            if os.path.isfile(key_path):
                val = open(key_path).read()
                key = key.split('.')[0]
                res[key] = val
            else:
                res[key] = self._folder_to_dict(key_path)
        return res",1,1
"def read_csv(path, delim=','):
    

    fd = _try_open_file(path, 'r',
                        'The first argument must be a pathname or an object that supports readline() method')

    data = []
    line = fd.readline()
    while line != """":
        if line[0] != '#' and not empty_line_p(line):
            data.append(parse_line(line, delim))
        line = fd.readline()

    _try_close_file(fd, path)
    return data",1,1
"def read_str(delim=',', *lines):
    

    data = []
    for line in lines:
        com = parse_line(line, delim)
        data.append(com)
    return data",1,1
"def read(env_file="".env""):
    
    try:
        with open(env_file) as f:
            content = f.read()

    except IOError:
        content = ''

    for line in content.splitlines():

        m1 = re.match(r'\A([A-Za-z_0-9]+)=(.*)\Z', line)
        if m1:
            key, val = m1.group(1), m1.group(2)
            m2 = re.match(r""\A'(.*)'\Z"", val)
            if m2:
                val = m2.group(1)
            m3 = re.match(r'\A""(.*)""\Z', val)
            if m3:
                val = re.sub(r'\\(.)', r'\1', m3.group(1))
            os.environ.setdefault(key, val)",1,1
"def load(text, match=None):
    
    if text is None: return None
    text = text.strip()
    if len(text) == 0: return None
    nametable = {
        'namespaces': [],
        'names': {}
    }
    root = XML(text)
    items = [root] if match is None else root.findall(match)
    count = len(items)
    if count == 0:
        return None
    elif count == 1:
        return load_root(items[0], nametable)
    else:
        return [load_root(item, nametable) for item in items]",1,1
"def read_indexed(i,flds=None,sclr=None,
                 gzip='guess', dir='.', vector_norms=True,
                 keep_xs=False,gettime=False):
    
    fldsp4  = '{}/flds{}.p4'.format(dir,i);
    sclrp4  = '{}/sclr{}.p4'.format(dir,i);
    fldsgz  = fldsp4 + '.gz';
    sclrgz  = sclrp4 + '.gz';
    if gzip == 'guess':
        fldsname = fldsp4 if os.path.exists(fldsp4) else fldsgz
        sclrname = sclrp4 if os.path.exists(sclrp4) else sclrgz
    else:
        fldsname = fldsgz if gzip else fldsp4;
        sclrname = sclrgz if gzip else sclrp4;
    if not (flds or sclr):
        raise ValueError(""Must specify flds or sclr to read."");
    elif flds is not None and sclr is not None:
        sd,srt=read(sclrname,
                    var=sclr,first_sort=True, gzip='guess',
                    keep_xs=keep_xs);
        fd=read(fldsname,
                var=flds, sort=srt, gzip='guess',
                keep_xs=keep_xs);
        ret = dict(sd=sd,fd=fd);
        ret.update({k:sd[k] for k in sd});
        ret.update({k:fd[k] for k in fd});
        if vector_norms:
            ret.update({k:vector_norm(ret,k) for k in flds})
        if gettime:
            ret['t'] = get_header(sclrname,gzip='guess')['timestamp'];
    else:
        if flds:
            var = flds;
            name= fldsname;
        else:
            var = sclr;
            name= sclrname;
        ret,_ = read(name,var=var,first_sort=True,gzip='guess');
        if flds and vector_norms:
            ret.update({k:vector_norm(ret,k) for k in flds})
        if gettime:
            ret['t'] = get_header(name,gzip='guess')['timestamp'];
    return ret;",1,1
"def load_iter(self, key, chunksize=None, noexpire=None):
        
        return chunk_iterator(self.load_fd(key, noexpire=noexpire),
                              chunksize=chunksize)",1,1
"def _update_config_sets(self,directory,files=None):
        
        if not self._connectToFlickr():
            print(""%s - Couldn't connect to flickr""%(directory))
            return False

        # Load sets from SET_FILE
        _sets=self._load_sets(directory)

        # Connect to flickr and get dicionary of photosets
        psets=self._getphotosets()

        db = self._loadDB(directory)

        # To create a set, one needs to pass it the primary
        # photo to use, let's open the DB and load the first
        # photo
        primary_pid=db[db.keys()[0]]['photoid']

        # Loop through all sets, create if it doesn't exist
        for myset in _sets:
            if myset not in psets:
                logger.info('set [%s] not in flickr sets, will create set'%(myset))
                self._createphotoset(myset,primary_pid)

        # Now reaload photosets from flickr
        psets=self._getphotosets()

        # --- Load DB of photos, and update them all with new tags
        for fn in db:
            # --- If file list provided, skip files not in the list
            if files and fn not in files:
                continue

            pid=db[fn]['photoid']

            # Get all the photosets this photo belongs to
            psets_for_photo=self._getphotosets_forphoto(pid)

            for myset in _sets:
                if myset in psets_for_photo:
                    logger.debug(""%s - Already in photoset [%s] - skipping""%(fn,myset))
                    continue
                logger.info(""%s [flickr] Adding to set [%s]"" %(fn,myset))
                psid=psets[myset]['id']
                logger.debug(""%s - Adding to photoset %s""%(fn,psid))
                resp=self.flickr.photosets_addPhoto(photoset_id=psid,photo_id=pid)
                if resp.attrib['stat']!='ok':
                    logger.error(""%s - flickr: photos_addPhoto failed with status: %s"",\
                            resp.attrib['stat']);
                    return False

            # Go through all sets flickr says this photo belongs to and
            # remove from those sets if they don't appear in SET_FILE
            for pset in psets_for_photo:
                if pset not in _sets:
                    psid=psets[pset]['id']
                    logger.info(""%s [flickr] Removing from set [%s]"" %(fn,pset))
                    logger.debug(""%s - Removing from photoset %s""%(fn,psid))
                    resp=self.flickr.photosets_removePhoto(photoset_id=psid,photo_id=pid)
                    if resp.attrib['stat']!='ok':
                        logger.error(""%s - flickr: photossets_removePhoto failed with status: %s"",\
                                resp.attrib['stat']);
                        return False

        return True",1,1
"def _update_config_tags(self,directory,files=None):
        
        if not self._connectToFlickr():
            print(""%s - Couldn't connect to flickr""%(directory))
            return False

        logger.debug(""Updating tags in %s""%(directory))

        _tags=self._load_tags(directory)

        # --- Load DB of photos, and update them all with new tags
        db = self._loadDB(directory)
        for fn in db:
            # --- If file list provided, skip files not in the list
            if files and fn not in files:
                logger.debug('%s [flickr] Skipping, tag update',fn)
                continue

            logger.info(""%s [flickr] Updating tags [%s]"" %(fn,_tags))
            pid=db[fn]['photoid']
            resp=self.flickr.photos_setTags(photo_id=pid,tags=_tags)
            if resp.attrib['stat']!='ok':
                logger.error(""%s - flickr: photos_setTags failed with status: %s"",\
                        resp.attrib['stat']);
                return False
            else:
                return True
        return False",1,3
"def _calc_result(self, results, calc):
        
        lg = logging.getLogger(""%s.%s"" % (self.ln, inspect.stack()[0][3]))
        lg.setLevel(self.log_level)
        # if the calculation is empty exit
        if calc is None:
            return results
        lg.debug(""calc %s"", calc)
        # perform concatenation
        hits = results.get('hits',{}).get('hits',[])
        for item in hits:
            lg.debug(""\n*** item:\n%s"", pp.pformat(item))
            if ""+"" in calc:
                calc_parts = calc.split(""+"")
                calc_str = """"
                for i, part in enumerate(calc_parts):
                    if '""' in part:
                        calc_parts[i] = part.replace('""','')
                    else:
                        if part.startswith(""_""):
                            calc_parts[i] = item.get(part)
                        else:
                            calc_parts[i] = Dot(item['_source']).get(part)
                lg.debug("" calc result: %s"", """".join(calc_parts))
                item['_source']['__calc'] = """".join(calc_parts)
        lg.debug(""calc %s"", calc)
        return results",1,3
"def _main_loop(self):
        
        self.logger.debug(""Processing messages"")
        old_time = 0
        while True:
            self._process_messages()
            if self.settings['STATS_DUMP'] != 0:
                new_time = int(old_div(time.time(), self.settings['STATS_DUMP']))
                # only log every X seconds
                if new_time != old_time:
                    self._dump_stats()
                    old_time = new_time

            self._report_self()
            time.sleep(self.settings['SLEEP_TIME'])",1,2
"def load(text, match=None):
    
    if text is None: return None
    text = text.strip()
    if len(text) == 0: return None
    nametable = {
        'namespaces': [],
        'names': {}
    }

    # Convert to unicode encoding in only python 2 for xml parser
    if(sys.version_info < (3, 0, 0) and isinstance(text, unicode)):
        text = text.encode('utf-8')

    root = XML(text)
    items = [root] if match is None else root.findall(match)
    count = len(items)
    if count == 0: 
        return None
    elif count == 1: 
        return load_root(items[0], nametable)
    else:
        return [load_root(item, nametable) for item in items]",1,1
"def downsample(f1, f2, data, N, quick=False):
    
    if quick:
        rand_records = range(N)
    else:
        records = sum(1 for _ in open(f1)) / 4
        N = records if N > records else N
        rand_records = sorted(random.sample(xrange(records), N))

    fh1 = open_possible_gzip(f1)
    fh2 = open_possible_gzip(f2) if f2 else None
    outf1 = os.path.splitext(f1)[0] + "".subset"" + os.path.splitext(f1)[1]
    outf2 = os.path.splitext(f2)[0] + "".subset"" + os.path.splitext(f2)[1] if f2 else None

    if utils.file_exists(outf1):
        if not outf2:
            return outf1, outf2
        elif utils.file_exists(outf2):
            return outf1, outf2

    out_files = (outf1, outf2) if outf2 else (outf1)

    with file_transaction(out_files) as tx_out_files:
        if isinstance(tx_out_files, six.string_types):
            tx_out_f1 = tx_out_files
        else:
            tx_out_f1, tx_out_f2 = tx_out_files
        sub1 = open_possible_gzip(tx_out_f1, ""w"")
        sub2 = open_possible_gzip(tx_out_f2, ""w"") if outf2 else None
        rec_no = - 1
        for rr in rand_records:
            while rec_no < rr:
                rec_no += 1
                for i in range(4): fh1.readline()
                if fh2:
                    for i in range(4): fh2.readline()
            for i in range(4):
                sub1.write(fh1.readline())
                if sub2:
                    sub2.write(fh2.readline())
            rec_no += 1
        fh1.close()
        sub1.close()
        if f2:
            fh2.close()
            sub2.close()

    return outf1, outf2",1,0
"def _can_use_mem(fastq_file, data, read_min_size=None):
    
    min_size = 70
    if read_min_size and read_min_size >= min_size:
        return True
    thresh = 0.75
    tocheck = 5000
    shorter = 0
    for count, size in fastq_size_output(fastq_file, tocheck):
        if int(size) < min_size:
            shorter += int(count)
    return (float(shorter) / float(tocheck)) <= thresh",1,1
"def _expected_reads(run_info_file):
    
    reads = []
    if os.path.exists(run_info_file):
        tree = ElementTree()
        tree.parse(run_info_file)
        read_elem = tree.find(""Run/Reads"")
        reads = read_elem.findall(""Read"")
    return len(reads)",1,1
"def total_reads_from_grabix(in_file):
    
    gbi_file = _get_grabix_index(in_file)
    if gbi_file:
        with open(gbi_file) as in_handle:
            next(in_handle)  # throw away
            num_lines = int(next(in_handle).strip())
        assert num_lines % 4 == 0, ""Expected lines to be multiple of 4""
        return num_lines // 4
    else:
        return 0",1,1
"def _gatk_extract_reads_cl(data, region, prep_params, tmp_dir):
    
    args = [""PrintReads"",
            ""-L"", region_to_gatk(region),
            ""-R"", dd.get_ref_file(data),
            ""-I"", data[""work_bam""]]
    # GATK3 back compatibility, need to specify analysis type
    if ""gatk4"" in dd.get_tools_off(data):
        args = [""--analysis_type""] + args
    runner = broad.runner_from_config(data[""config""])
    return runner.cl_gatk(args, tmp_dir)",1,1
"def _collect(self, lines):
        
        for tag,value,unit in re.findall(
                '([\.A-Za-z]+)\s+%s\s+([A-Za-z]+)?' %
                self._num_regex,lines):
            tag = tag.lower()
            unit = unit.lower()
            if tag == ""latticeconstant"":
                self._tags['latticeconstantunit'] = unit.capitalize()
                if unit == 'ang':
                    self._tags[tag] = float(value) / Bohr
                elif unit == 'bohr':
                    self._tags[tag] = float(value)
                else:
                    raise ValueError('Unknown LatticeConstant unit: {}'.format(unit))

        for tag,value in re.findall('([\.A-Za-z]+)[ \t]+([a-zA-Z]+)',lines):
            tag = tag.replace('_','').lower()
            if tag == ""atomiccoordinatesformat"":
                self._tags[tag] = value.strip().lower() 

        #check if the necessary tags are present
        self.check_present('atomiccoordinatesformat')
        acell = self._tags['latticeconstant']

        #capture the blocks
        blocks = re.findall(
            '%block\s+([A-Za-z_]+)\s((?:.+\n)+?(?=(?:\s+)?%endblock))',
            lines, re.MULTILINE)
        for tag,block in blocks:
            tag = tag.replace('_','').lower()
            if   tag == ""chemicalspecieslabel"":
                lines = block.split('\n')[:-1]
                self._tags[""atomicnumbers""] = dict([map(int,species.split()[:2])
                                                    for species in lines])
                self._tags[tag] = dict(
                    [(lambda x: (x[2],int(x[0])))(species.split())
                     for species in lines])
            elif tag == ""latticevectors"":
                self._tags[tag] = [[ float(v)*acell for v in vector.split()]
                                   for vector in block.split('\n')[:3]]
            elif tag == ""atomiccoordinatesandatomicspecies"":
                lines = block.split('\n')[:-1]
                self._tags[""atomiccoordinates""] = [
                    [float(x)  for x in atom.split()[:3]] for atom in lines]
                self._tags[""atomicspecies""] = [int(atom.split()[3])
                                               for atom in lines]
       
        #check if the block are present
        self.check_present(""atomicspecies"")
        self.check_present(""atomiccoordinates"")
        self.check_present(""latticevectors"")
        self.check_present(""chemicalspecieslabel"")
            
        #translate the atomicspecies to atomic numbers
        self._tags[""atomicnumbers""] = [self._tags[""atomicnumbers""][atype]
                                       for atype in self._tags[""atomicspecies""]]",1,3
"def load(cls, filename, project=None, delim=' | '):
        r
        if project is None:
            project = ws.new_project()

        fname = cls._parse_filename(filename, ext='csv')
        a = pd.read_table(filepath_or_buffer=fname,
                          sep=',',
                          skipinitialspace=True,
                          index_col=False,
                          true_values=['T', 't', 'True', 'true', 'TRUE'],
                          false_values=['F', 'f', 'False', 'false', 'FALSE'])

        dct = {}
        # First parse through all the items and re-merge columns
        keys = sorted(list(a.keys()))
        for item in keys:
            m = re.search(r'\[.\]', item)  # The dot '.' is a wildcard
            if m:  # m is None if pattern not found, otherwise merge cols
                pname = re.split(r'\[.\]', item)[0]  # Get base propname
                # Find all other keys with same base propname
                merge_keys = [k for k in a.keys() if k.startswith(pname)]
                # Rerieve and remove arrays with same base propname
                merge_cols = [a.pop(k) for k in merge_keys]
                # Merge arrays into multi-column array and store in DataFrame
                dct[pname] = sp.vstack(merge_cols).T
                # Remove key from list of keys
                [keys.pop(keys.index(k)) for k in keys if k.startswith(pname)]
            else:
                dct[item] = sp.array(a.pop(item))

        project = Dict.from_dict(dct, project=project, delim=delim)

        return project",1,1
"def _CreateLineReader(self, file_object):
    
    # The Python 2 csv module reads bytes and the Python 3 csv module Unicode
    # reads strings.
    if py2to3.PY_3:
      line_reader = text_file.TextFile(
          file_object, encoding=self._encoding, end_of_line=self._end_of_line)

      # pylint: disable=protected-access
      maximum_read_buffer_size = line_reader._MAXIMUM_READ_BUFFER_SIZE

    else:
      line_reader = line_reader_file.BinaryLineReader(
          file_object, end_of_line=self._end_of_line)

      maximum_read_buffer_size = line_reader.MAXIMUM_READ_BUFFER_SIZE

    # Line length is one less than the maximum read buffer size so that we
    # tell if there's a line that doesn't end at the end before the end of
    # the file.
    if self._maximum_line_length > maximum_read_buffer_size:
      self._maximum_line_length = maximum_read_buffer_size - 1

    # If we specifically define a number of lines we should skip, do that here.
    for _ in range(0, self.NUMBER_OF_HEADER_LINES):
      line_reader.readline(self._maximum_line_length)
    return line_reader",1,1
"def make_inputs(input_file: Optional[str],
                translator: inference.Translator,
                input_is_json: bool,
                input_factors: Optional[List[str]] = None) -> Generator[inference.TranslatorInput, None, None]:
    
    if input_file is None:
        check_condition(input_factors is None, ""Translating from STDIN, not expecting any factor files."")
        for sentence_id, line in enumerate(sys.stdin, 1):
            if input_is_json:
                yield inference.make_input_from_json_string(sentence_id=sentence_id,
                                                            json_string=line,
                                                            translator=translator)
            else:
                yield inference.make_input_from_factored_string(sentence_id=sentence_id,
                                                                factored_string=line,
                                                                translator=translator)
    else:
        input_factors = [] if input_factors is None else input_factors
        inputs = [input_file] + input_factors
        if not input_is_json:
            check_condition(translator.num_source_factors == len(inputs),
                            ""Model(s) require %d factors, but %d given (through --input and --input-factors)."" % (
                                translator.num_source_factors, len(inputs)))
        with ExitStack() as exit_stack:
            streams = [exit_stack.enter_context(data_io.smart_open(i)) for i in inputs]
            for sentence_id, inputs in enumerate(zip(*streams), 1):
                if input_is_json:
                    yield inference.make_input_from_json_string(sentence_id=sentence_id,
                                                                json_string=inputs[0],
                                                                translator=translator)
                else:
                    yield inference.make_input_from_multiple_strings(sentence_id=sentence_id, strings=list(inputs))",1,1
"def listdir(self, relpath):
    

    path = self._realpath(relpath)
    if not path.endswith('/'):
      raise self.NotADirException(self.rev, relpath)

    if path[0] == '/' or path.startswith('../'):
      return os.listdir(path)

    tree = self._read_tree(path[:-1])
    return list(tree.keys())",1,1
"def read_full(data, size):
    
    default_read_size = 32768 # 32KiB per read operation.
    chunk = io.BytesIO()
    chunk_size = 0

    while chunk_size < size:
        read_size = default_read_size
        if (size - chunk_size) < default_read_size:
            read_size = size - chunk_size
        current_data = data.read(read_size)
        if not current_data or len(current_data) == 0:
            break
        chunk.write(current_data)
        chunk_size+= len(current_data)

    return chunk.getvalue()",1,1
"def _get_response(self, connection):
        

        response_header = self._receive(connection, 13)
        logger.debug('Response header: %s', response_header)

        if (not response_header.startswith(b'ZBXD\x01') or
                len(response_header) != 13):
            logger.debug('Zabbix return not valid response.')
            result = False
        else:
            response_len = struct.unpack('<Q', response_header[5:])[0]
            response_body = connection.recv(response_len)
            result = json.loads(response_body.decode(""utf-8""))
            logger.debug('Data received: %s', result)

        try:
            connection.close()
        except Exception as err:
            pass

        return result",1,3
"def get_length(self, byte_stream):
        
        length = struct.unpack(""!i"", byte_stream.read(4))[0]
        log.debug(""4 bytes delimited part length: %d"" % length)
        return length",1,1
"def _read_generated_broker_id(meta_properties_path):
    
    try:
        with open(meta_properties_path, 'r') as f:
            broker_id = _parse_meta_properties_file(f)
    except IOError:
        raise IOError(
            ""Cannot open meta.properties file: {path}""
            .format(path=meta_properties_path),
        )
    except ValueError:
        raise ValueError(""Broker id not valid"")

    if broker_id is None:
        raise ValueError(""Autogenerated broker id missing from data directory"")

    return broker_id",1,1
"def _data_flow_chain(self):
        
        if self.data_producer is None:
            return []

        res = []
        ds = self.data_producer
        while not ds.is_reader:
            res.append(ds)
            ds = ds.data_producer
        res.append(ds)
        res = res[::-1]
        return res",1,1
"def _cfgs_to_read(self):
        
        # use these files to extend/overwrite the conf_values.
        # Last red file always overwrites existing values!
        cfg = Config.DEFAULT_CONFIG_FILE_NAME
        filenames = [
            self.default_config_file,
            cfg,  # conf_values in current directory
            os.path.join(os.path.expanduser('~' + os.path.sep), cfg),  # config in user dir
            '.pyemma.cfg',
        ]

        # look for user defined files
        if self.cfg_dir:
            from glob import glob
            filenames.extend(glob(self.cfg_dir + os.path.sep + ""*.cfg""))
        return filenames",1,1
"def _main():
    
    if argv[1:]:
        stdout.write(_CLI_HELP)
        raise SystemExit()
    for line in stdin:
        try:
            message = loads(line)
        except ValueError:
            stdout.write(""Not JSON: {}\n\n"".format(line.rstrip(b""\n"")))
            continue
        if REQUIRED_FIELDS - set(message.keys()):
            stdout.write(
                ""Not an Eliot message: {}\n\n"".format(line.rstrip(b""\n"")))
            continue
        result = pretty_format(message) + ""\n""
        if PY2:
            result = result.encode(""utf-8"")
        stdout.write(result)",1,0
"def spawn_reader_writer(get_data_fn, put_data_fn):
    
    def _reader_thread():
        while True:
            out = get_data_fn()
            put_data_fn(out)
            if not out:
                # EOF.
                # We've passed this on so things farther down the pipeline will
                # know to shut down.
                break

    t = threading.Thread(target=_reader_thread)
    t.daemon = True
    t.start()
    return t",1,1
"def read(filename):
    
    formula = []
    for line in open(filename, 'r'):
        line = line.strip()
        if line[0] == ""#"":
            continue
        lit = line.split("":-"")
        if len(lit) == 1:
            posvar = lit[0]
            negvars = []
        else:
            assert len(lit) == 2
            posvar = lit[0].strip()
            if posvar == '':
                posvar = None
            negvars = lit[1].split(',')
            for i in range(len(negvars)):
                negvars[i] = negvars[i].strip()
        formula.append((posvar, negvars))
    return formula",1,1
"def _read_cmap(self):
        

        try:
            i = 0
            colormap = {0: (0, 0, 0)}
            with open(settings.COLORMAP) as cmap:
                lines = cmap.readlines()
                for line in lines:
                    if i == 0 and 'mode = ' in line:
                        i = 1
                        maxval = float(line.replace('mode = ', ''))
                    elif i > 0:
                        str = line.split()
                        if str == []:  # when there are empty lines at the end of the file
                            break
                        colormap.update(
                            {
                                i: (int(round(float(str[0]) * 255 / maxval)),
                                    int(round(float(str[1]) * 255 / maxval)),
                                    int(round(float(str[2]) * 255 / maxval)))
                            }
                        )
                        i += 1
        except IOError:
            pass

        self.cmap = {k: v[:4] for k, v in colormap.items()}",1,1
"def peek_pointers_in_registers(self, peekSize = 16, context = None):
        
        peekable_registers = (
            'Eax', 'Ebx', 'Ecx', 'Edx', 'Esi', 'Edi', 'Ebp'
        )
        if not context:
            context = self.get_context(win32.CONTEXT_CONTROL | \
                                       win32.CONTEXT_INTEGER)
        aProcess    = self.get_process()
        data        = dict()
        for (reg_name, reg_value) in compat.iteritems(context):
            if reg_name not in peekable_registers:
                continue
##            if reg_name == 'Ebp':
##                stack_begin, stack_end = self.get_stack_range()
##                print hex(stack_end), hex(reg_value), hex(stack_begin)
##                if stack_begin and stack_end and stack_end < stack_begin and \
##                   stack_begin <= reg_value <= stack_end:
##                      continue
            reg_data = aProcess.peek(reg_value, peekSize)
            if reg_data:
                data[reg_name] = reg_data
        return data",1,3
"def peek_pointers_in_data(self, data, peekSize = 16, peekStep = 1):
        
        aProcess = self.get_process()
        return aProcess.peek_pointers_in_data(data, peekSize, peekStep)",1,3
"def peek_pointers_in_data(self, data, peekSize = 16, peekStep = 1):
        
        result = dict()
        ptrSize = win32.sizeof(win32.LPVOID)
        if ptrSize == 4:
            ptrFmt = '<L'
        else:
            ptrFmt = '<Q'
        if len(data) > 0:
            for i in compat.xrange(0, len(data), peekStep):
                packed          = data[i:i+ptrSize]
                if len(packed) == ptrSize:
                    address     = struct.unpack(ptrFmt, packed)[0]
##                    if not address & (~0xFFFF): continue
                    peek_data   = self.peek(address, peekSize)
                    if peek_data:
                        result[i] = peek_data
        return result",1,1
"def _all_reads_from_contig(self, contig, fout):
        
        sam_reader = pysam.Samfile(self.bam, ""rb"")
        for read in sam_reader.fetch(contig):
            print(mapping.aligned_read_to_read(read, ignore_quality=not self.fastq_out), file=fout)",1,1
"def paste_file(conn, filename, **kwargs):
    
    with open(filename, 'r') as fp:
        return _run(conn, None, fp.read(), {'no_prompt': True}, **kwargs)",1,1
"def add_commands_from_file(self, filename, autoprompt=True):
        
        if autoprompt:
            deco = self._create_autoprompt_handler
        else:
            deco = None
        self.commands.add_from_file(filename, deco)",1,1
"def add_from_file(self, filename, handler_decorator=None):
        
        args = {}
        execfile(filename, args)
        commands = args.get('commands')
        if commands is None:
            raise Exception(filename + ' has no variable named ""commands""')
        elif not hasattr(commands, '__iter__'):
            raise Exception(filename + ': ""commands"" is not iterable')
        for key, handler in commands:
            if handler_decorator:
                handler = handler_decorator(handler)
            self.add(key, handler)",1,1
"def from_template(filename, **kwargs):
    
    with open(filename) as fp:
        return from_template_string(fp.read(), **kwargs)",1,1
"def populate_entries(self):
        
        try:
            with open(self.hosts_path, 'r') as hosts_file:
                hosts_entries = [line for line in hosts_file]
                for hosts_entry in hosts_entries:
                    entry_type = HostsEntry.get_entry_type(hosts_entry)
                    if entry_type == ""comment"":
                        hosts_entry = hosts_entry.replace(""\r"", """")
                        hosts_entry = hosts_entry.replace(""\n"", """")
                        self.entries.append(HostsEntry(entry_type=""comment"",
                                                       comment=hosts_entry))
                    elif entry_type == ""blank"":
                        self.entries.append(HostsEntry(entry_type=""blank""))
                    elif entry_type in (""ipv4"", ""ipv6""):
                        chunked_entry = hosts_entry.split()
                        stripped_name_list = [name.strip() for name in chunked_entry[1:]]

                        self.entries.append(
                            HostsEntry(
                                entry_type=entry_type,
                                address=chunked_entry[0].strip(),
                                names=stripped_name_list))
        except IOError:
            return {'result': 'failed',
                    'message': 'Cannot read: {0}.'.format(self.hosts_path)}",1,1
"def get_parser(output_formats=[], conf=None):
    

    def _conf_or_default(key, value):
        return value if conf is None else conf.get_or_set(key, value)

    parser = ArgumentParser(description='crate shell')
    parser.add_argument('-v', '--verbose', action='count',
                        dest='verbose', default=_conf_or_default('verbosity', 0),
                        help='print debug information to STDOUT')
    parser.add_argument('-A', '--no-autocomplete', action='store_false',
                        dest='autocomplete',
                        default=_conf_or_default('autocomplete', True),
                        help='disable SQL keywords autocompletion')
    parser.add_argument('-a', '--autocapitalize', action='store_true',
                        dest='autocapitalize',
                        default=False,
                        help='enable automatic capitalization of SQL keywords while typing')
    parser.add_argument('-U', '--username', type=str, metavar='USERNAME',
                        help='Authenticate as USERNAME.')
    parser.add_argument('-W', '--password', action='store_true',
                        dest='force_passwd_prompt', default=_conf_or_default('force_passwd_prompt', False),
                        help='force a password prompt')
    parser.add_argument('--schema', type=str,
                        help='default schema for statements if schema is not explicitly stated in queries')
    parser.add_argument('--history', type=str, metavar='FILENAME',
                        help='Use FILENAME as a history file', default=HISTORY_PATH)
    parser.add_argument('--config', type=str, metavar='FILENAME',
                        help='use FILENAME as a configuration file', default=CONFIG_PATH)

    group = parser.add_mutually_exclusive_group()
    group.add_argument('-c', '--command', type=str, metavar='STATEMENT',
                       help='Execute the STATEMENT and exit.')
    group.add_argument('--sysinfo', action='store_true', default=False,
                       help='print system and cluster information')

    parser.add_argument('--hosts', type=str, nargs='*',
                        default=_conf_or_default('hosts', ['localhost:4200']),
                        help='connect to HOSTS.', metavar='HOSTS')
    parser.add_argument('--verify-ssl', type=boolean, default=True,
                        help='force the verification of the server SSL certificate')
    parser.add_argument('--cert-file', type=file_with_permissions, metavar='FILENAME',
                        help='use FILENAME as the client certificate file')
    parser.add_argument('--key-file', type=file_with_permissions, metavar='FILENAME',
                        help='Use FILENAME as the client certificate key file')
    parser.add_argument('--ca-cert-file', type=file_with_permissions, metavar='FILENAME',
                        help='use FILENAME as the CA certificate file')
    parser.add_argument('--format', type=str,
                        default=_conf_or_default('format', 'tabular'),
                        choices=output_formats, metavar='FORMAT',
                        help='the output FORMAT of the SQL response')
    parser.add_argument('--version', action='store_true', default=False,
                        help='print the Crash version and exit')

    return parser",1,0
"def build_csv_transforming_training_input_fn(schema,
                                             features,
                                             stats,
                                             analysis_output_dir,
                                             raw_data_file_pattern,
                                             training_batch_size,
                                             num_epochs=None,
                                             randomize_input=False,
                                             min_after_dequeue=1,
                                             reader_num_threads=1,
                                             allow_smaller_final_batch=True):
  

  def raw_training_input_fn():
    

    if isinstance(raw_data_file_pattern, six.string_types):
      filepath_list = [raw_data_file_pattern]
    else:
      filepath_list = raw_data_file_pattern

    files = []
    for path in filepath_list:
      files.extend(file_io.get_matching_files(path))

    filename_queue = tf.train.string_input_producer(
        files, num_epochs=num_epochs, shuffle=randomize_input)

    csv_id, csv_lines = tf.TextLineReader().read_up_to(filename_queue, training_batch_size)

    queue_capacity = (reader_num_threads + 3) * training_batch_size + min_after_dequeue
    if randomize_input:
      _, batch_csv_lines = tf.train.shuffle_batch(
          tensors=[csv_id, csv_lines],
          batch_size=training_batch_size,
          capacity=queue_capacity,
          min_after_dequeue=min_after_dequeue,
          enqueue_many=True,
          num_threads=reader_num_threads,
          allow_smaller_final_batch=allow_smaller_final_batch)

    else:
      _, batch_csv_lines = tf.train.batch(
          tensors=[csv_id, csv_lines],
          batch_size=training_batch_size,
          capacity=queue_capacity,
          enqueue_many=True,
          num_threads=reader_num_threads,
          allow_smaller_final_batch=allow_smaller_final_batch)

    csv_header, record_defaults = csv_header_and_defaults(features, schema, stats, keep_target=True)
    parsed_tensors = tf.decode_csv(batch_csv_lines, record_defaults, name='csv_to_tensors')
    raw_features = dict(zip(csv_header, parsed_tensors))

    transform_fn = make_preprocessing_fn(analysis_output_dir, features, keep_target=True)
    transformed_tensors = transform_fn(raw_features)

    # Expand the dims of non-sparse tensors. This is needed by tf.learn.
    transformed_features = {}
    for k, v in six.iteritems(transformed_tensors):
      if isinstance(v, tf.Tensor) and v.get_shape().ndims == 1:
        transformed_features[k] = tf.expand_dims(v, -1)
      else:
        transformed_features[k] = v

    # image_feature_engineering does not need to be called as images are not
    # supported in raw csv for training.

    # Remove the target tensor, and return it directly
    target_name = get_target_name(features)
    if not target_name or target_name not in transformed_features:
      raise ValueError('Cannot find target transform in features')

    transformed_target = transformed_features.pop(target_name)

    return transformed_features, transformed_target

  return raw_training_input_fn",1,1
"def upload(self, filename, input, packethook=None, timeout=SOCK_TIMEOUT):
        
        self.context = TftpContextClientUpload(self.host,
                                               self.iport,
                                               filename,
                                               input,
                                               self.options,
                                               packethook,
                                               timeout,
                                               localip = self.localip)
        self.context.start()
        # Upload happens here
        self.context.end()

        metrics = self.context.metrics

        log.info('')
        log.info(""Upload complete."")
        if metrics.duration == 0:
            log.info(""Duration too short, rate undetermined"")
        else:
            log.info(""Uploaded %d bytes in %.2f seconds"" % (metrics.bytes, metrics.duration))
            log.info(""Average rate: %.2f kbps"" % metrics.kbps)
        log.info(""%.2f bytes in resent data"" % metrics.resent_bytes)
        log.info(""Resent %d packets"" % metrics.dupcount)",1,2
"def read(self, ulBuffer, pDst, unBytes):
        

        fn = self.function_table.read
        punRead = c_uint32()
        result = fn(ulBuffer, pDst, unBytes, byref(punRead))
        return result, punRead.value",1,1
"def _make_read_func(file_obj):
    
    @ffi.callback(""cairo_read_func_t"", error=constants.STATUS_READ_ERROR)
    def read_func(_closure, data, length):
        string = file_obj.read(length)
        if len(string) < length:  # EOF too early
            return constants.STATUS_READ_ERROR
        ffi.buffer(data, length)[:len(string)] = string
        return constants.STATUS_SUCCESS
    return read_func",1,1
"def read(self):
        
        if not self.ready_to_read():
            return None

        data = self._read()
        if data is None:
            return None

        return self._parse_message(data)",1,1
"def read_index_iter(self):
        

        with fopen(self.vpk_path, 'rb') as f:
            f.seek(self.header_length)

            while True:
                if self.version > 0 and f.tell() > self.tree_length + self.header_length:
                    raise ValueError(""Error parsing index (out of bounds)"")

                ext = _read_cstring(f)
                if ext == '':
                    break

                while True:
                    path = _read_cstring(f)
                    if path == '':
                        break
                    if path != ' ':
                        path = os.path.join(path, '')
                    else:
                        path = ''

                    while True:
                        name = _read_cstring(f)
                        if name == '':
                            break

                        (crc32,
                         preload_length,
                         archive_index,
                         archive_offset,
                         file_length,
                         suffix,
                         ) = metadata = list(struct.unpack(""IHHIIH"", f.read(18)))

                        if suffix != 0xffff:
                            raise ValueError(""Error while parsing index"")

                        if archive_index == 0x7fff:
                            metadata[3] = self.header_length + self.tree_length + archive_offset

                        metadata = (f.read(preload_length),) + tuple(metadata[:-1])

                        yield path + name + '.' + ext, metadata",1,1
"def _read_mz(self, mz_offset, mz_len, mz_enc_len):
        
        self.ibd.seek(mz_offset)
        data = self.ibd.read(mz_enc_len)
        self.ibd.seek(0, 2)
        data = self.mz_compression.decompress(data)
        return tuple(np.fromstring(data, dtype=self.mz_dtype))",1,1
"def read1(self, size=-1):
        
        # Usually, read1() calls _fp.read() at most once. However, sometimes
        # this does not give enough data for the decompressor to make progress.
        # In this case we make multiple reads, to avoid returning b"""".
        self._check_can_read()
        if size is None:
            #This is not needed on Python 3 where the comparison to zero
            #will fail with a TypeError. 
            raise TypeError(""Read size should be an integer, not None"")
        if (size == 0 or self._mode == _MODE_READ_EOF or
            not self._fill_buffer()):
            return b""""
        if 0 < size < len(self._buffer):
            data = self._buffer[:size]
            self._buffer = self._buffer[size:]
        else:
            data = self._buffer
            self._buffer = None
        self._pos += len(data)
        return data",1,1
"def read_yaml_config(filename, check=True, osreplace=True, exit=True):
    
    location = filename
    if location is not None:
        location = path_expand(location)

    if not os.path.exists(location) and not check:
        return None

    if check and os.path.exists(location):

        # test for tab in yaml file
        if check_file_for_tabs(location):
            log.error(""The file {0} contains tabs. yaml ""
                      ""Files are not allowed to contain tabs"".format(location))
            sys.exit()
        result = None
        try:

            if osreplace:
                result = open(location, 'r').read()
                t = Template(result)
                result = t.substitute(os.environ)

                # data = yaml.safe_load(result)
                data = ordered_load(result, yaml.SafeLoader)
            else:
                f = open(location, ""r"")

                # data = yaml.safe_load(f)

                data = ordered_load(result, yaml.SafeLoader)
                f.close()

            return data
        except Exception as e:
            log.error(
                ""The file {0} fails with a yaml read error"".format(filename))
            Error.traceback(e)
            sys.exit()

    else:
        log.error(""The file {0} does not exist."".format(filename))
        if exit:
            sys.exit()

    return None",1,1
"def readall_fast(stm, size):
    
    buf, offset = stm.read_fast(size)
    if len(buf) - offset < size:
        # slow case
        buf = buf[offset:]
        buf += stm.recv(size - len(buf))
        return buf, 0
    return buf, offset",1,1
"def _get_method_params(self):
        
        caller = sys._getframe(2)
        var_names = list(caller.f_code.co_varnames)
        caller_locals = caller.f_locals

        var_names.remove('self')
        kwargs = {key: value for key, value in caller_locals.items()
                  if key in var_names and value is not None}
        return kwargs",1,3
"def magic_read(self,infile):
        
#        print ""calling magic_read(self, infile)"", infile
        hold,magic_data,magic_record,magic_keys=[],[],{},[]
        try:
            f=open(infile,""r"")
        except:
            return [],'bad_file'
        d = f.readline()[:-1].strip('\n')
        if d[0]==""s"" or d[1]==""s"":
            delim='space'
        elif d[0]==""t"" or d[1]==""t"":
            delim='tab'
        else:
            print('error reading ', infile)
            sys.exit()
        if delim=='space':file_type=d.split()[1]
        if delim=='tab':file_type=d.split('\t')[1]
        if file_type=='delimited':
            if delim=='space':file_type=d.split()[2]
            if delim=='tab':file_type=d.split('\t')[2]
        if delim=='space':line =f.readline()[:-1].split()
        if delim=='tab':line =f.readline()[:-1].split('\t')
        for key in line:
            magic_keys.append(key)
        lines=f.readlines()
        for line in lines[:-1]:
            line.replace('\n','')
            if delim=='space':rec=line[:-1].split()
            if delim=='tab':rec=line[:-1].split('\t')
            hold.append(rec)
        line = lines[-1].replace('\n','')
        if delim=='space':rec=line[:-1].split()
        if delim=='tab':rec=line.split('\t')
        hold.append(rec)
        for rec in hold:
            magic_record={}
            if len(magic_keys) != len(rec):

                print(""Warning: Uneven record lengths detected: "")
                #print magic_keys
                #print rec
            for k in range(len(rec)):
               magic_record[magic_keys[k]]=rec[k].strip('\n')
            magic_data.append(magic_record)
        magictype=file_type.lower().split(""_"")
        Types=['er','magic','pmag','rmag']
        if magictype in Types:file_type=file_type.lower()
#        print ""magic data from magic_read:""
#        print str(magic_data)[:500] + ""...""
#        print ""file_type"", file_type
        return magic_data,file_type",1,1
"def read_criteria_file(self, criteria_file_name=None):
        
#        import pdb; pdb.set_trace()
        acceptance_criteria = pmag.initialize_acceptance_criteria()
        if self.data_model == 3:
            if criteria_file_name == None:
                criteria_file_name = ""criteria.txt""
            contribution = cb.Contribution(self.WD, read_tables=[
                                           'criteria'], custom_filenames={'criteria': criteria_file_name})
            if 'criteria' in contribution.tables:
                crit_container = contribution.tables['criteria']
                crit_data = crit_container.df
                crit_data = crit_data.to_dict('records')
                for crit in crit_data:
                    m2_name = map_magic.convert_direction_criteria(
                        'magic2', crit['table_column'])
                    if m2_name != """":
                        try:
                            if crit['criterion_value'] == 'True':
                                acceptance_criteria[m2_name]['value'] = 1
                            else:
                                acceptance_criteria[m2_name]['value'] = 0
                            acceptance_criteria[m2_name]['value'] = float(
                                crit['criterion_value'])
                        except ValueError:
                            self.user_warning(""%s is not a valid comparitor for %s, skipping this criteria"" % (
                                str(crit['criterion_value']), m2_name))
                            continue
                        acceptance_criteria[m2_name]['pmag_criteria_code'] = crit['criterion']
            return acceptance_criteria
        else:
            if criteria_file_name == None:
                criteria_file_name = ""pmag_criteria.txt""
            try:
                acceptance_criteria = pmag.read_criteria_from_file(
                    os.path.join(self.WD, criteria_file_name), acceptance_criteria)
            except (IOError, OSError) as e:
                self.user_warning(""File %s not found in directory %s aborting opperation"" % (
                    criteria_file_name, self.WD))
            return acceptance_criteria",1,1
"def get_data(self):
        
        # ------------------------------------------------
        # Read magic measurement file and sort to blocks
        # ------------------------------------------------

        # All meas data information is stored in Data[secimen]={}
        Data = {}
        Data_hierarchy = {}
        Data_hierarchy['study'] = {}
        Data_hierarchy['locations'] = {}
        Data_hierarchy['sites'] = {}
        Data_hierarchy['samples'] = {}
        Data_hierarchy['specimens'] = {}
        Data_hierarchy['sample_of_specimen'] = {}
        Data_hierarchy['site_of_specimen'] = {}
        Data_hierarchy['site_of_sample'] = {}
        Data_hierarchy['location_of_site'] = {}
        Data_hierarchy['location_of_specimen'] = {}
        Data_hierarchy['study_of_specimen'] = {}
        Data_hierarchy['expedition_name_of_specimen'] = {}

        if self.data_model == 3:

            if 'measurements' not in self.con.tables:
                self.user_warning(
                    ""Measurement data file is empty and the GUI cannot start, aborting"")
                return Data, Data_hierarchy

            if self.con.tables['measurements'].df.empty:
                self.user_warning(
                    ""Measurement data file is empty and the GUI cannot start, aborting"")
                return Data, Data_hierarchy

            # extract specimen data from measurements table
            if not len(self.spec_data):
                specs = self.con.tables['measurements'].df['specimen'].unique()
                df = pd.DataFrame(index=specs, columns=['specimen'])
                df.index.name = 'specimen_name'
                df['specimen'] = specs
                self.con.tables['specimens'].df = df
                self.spec_data = df

            if not len(self.spec_data):
                self.user_warning(
                    ""Measurement data file does not seem to have specimen data and the GUI cannot start, aborting"")
                return Data, Data_hierarchy


            if 'sample' not in self.spec_data.columns or 'sample' not in self.samp_data.columns:
                if 'specimen' not in self.spec_data.columns:
                    self.spec_data['specimen'] = self.con.tables['measurements'].df['specimen']
                    self.spec_data.set_index('specimen', inplace=True)
                    self.spec_data['specimen'] = self.spec_data.index

                ui_dialog = demag_dialogs.user_input(
                    self, [""# of characters to remove""], heading=""Sample data could not be found attempting to generate sample names by removing characters from specimen names"")
                self.show_dlg(ui_dialog)
                ui_data = ui_dialog.get_values()
                try:
                    samp_ncr = int(ui_data[1][""# of characters to remove""])
                except ValueError:
                    self.user_warning(
                        ""Invalid input, specimen names will be used for sample names instead"")
                    samp_ncr = 0
                self.spec_data['sample'] = [x[:-samp_ncr]
                                            for x in self.spec_data['specimen']]

                self.samp_data['sample'] = self.spec_data['sample']
                self.samp_data.set_index('sample', inplace=True)
                self.samp_data['sample'] = self.samp_data.index

            if 'site' not in self.samp_data.columns or 'site' not in self.site_data.columns:
                ui_dialog = demag_dialogs.user_input(
                    self, [""# of characters to remove"", ""site delimiter""], heading=""No Site Data found attempting to create site names from specimen names"")
                self.show_dlg(ui_dialog)
                ui_data = ui_dialog.get_values()
                try:
                    site_ncr = int(ui_data[1][""# of characters to remove""])
                    self.samp_data['site'] = [x[:-site_ncr]
                                              for x in self.spec_data['specimen']]
                except ValueError:
                    try:
                        sd = ui_data[1][""site delimiter""]
                        self.samp_data['site'] = [
                            x.split(sd)[0] for x in self.spec_data['specimen']]
                    except ValueError:
                        self.samp_data['site'] = [
                            x for x in self.spec_data['specimen']]

                self.site_data['site'] = self.samp_data['site']
                self.site_data.drop_duplicates(inplace=True)
                self.site_data.set_index('site', inplace=True)
                self.site_data['site'] = self.site_data.index

            if 'location' not in self.site_data.columns or 'location' not in self.loc_data.columns:
                ui_dialog = demag_dialogs.user_input(
                    self, [""location name for all sites""], heading=""No Location found"")
                self.show_dlg(ui_dialog)
                ui_data = ui_dialog.get_values()
                self.site_data['location'] = ui_data[1][""location name for all sites""]

                self.loc_data['location'] = self.site_data['location']
                self.loc_data.drop_duplicates(inplace=True)
                self.loc_data.set_index('location', inplace=True)
                self.loc_data['location'] = self.loc_data.index

            # add data to other dataframes
            self.con.propagate_location_to_measurements()
            self.con.propagate_location_to_specimens()

            # get measurement data from contribution object
            meas_container = self.con.tables['measurements']
            meas_data3_0 = meas_container.df

            meas_data3_0.replace({'specimen': {nan: 'unknown'}, 'sample': {nan: 'unknown'}, 'site': {
                                 nan: 'unknown'}, 'location': {nan: 'unknown'}}, inplace=True)
            meas_data3_0['specimen'] = meas_data3_0['specimen'].apply(str)
            meas_data3_0['sample'] = meas_data3_0['sample'].apply(str)
            meas_data3_0['site'] = meas_data3_0['site'].apply(str)
            meas_data3_0['location'] = meas_data3_0['location'].apply(str)

            # do some filtering
#            if 'location' in meas_data3_0.columns:
#                if any(meas_data3_0['location'].isnull()):
#                    print(""-W- Some measurements are missing location data, and will not be used"")
#                meas_data3_0 = meas_data3_0[meas_data3_0['location'].notnull()]
# meas_data3_0.replace({'location':float('nan')},'unknown',inplace=True)
#            if 'site' in meas_data3_0.columns:
#                if any(meas_data3_0['site'].isnull()):
#                    print(""-W- Some measurements are missing site data, and will not be used"")
#                meas_data3_0 = meas_data3_0[meas_data3_0['site'].notnull()]
# meas_data3_0.replace({'site':float('nan')},'unknown',inplace=True)
#            if 'sample' in meas_data3_0.columns:
#                if any(meas_data3_0['sample'].isnull()):
#                    print(""-W- Some measurements are missing sample data, and will not be used"")
#                meas_data3_0 = meas_data3_0[meas_data3_0['sample'].notnull()]
# meas_data3_0.replace({'sample':float('nan')},'unknown',inplace=True)
#            if 'specimen' in meas_data3_0.columns:
#                missing = meas_data3_0[meas_data3_0['specimen'].isnull()]
#                if len(missing):
#                    print(""-W- {} measurements are missing specimen data, and will not be used"".format(missing))
#                meas_data3_0 = meas_data3_0[meas_data3_0['specimen'].notnull()]
# meas_data3_0.replace({'specimen':float('nan')},'unknown',inplace=True)

#            col_names = ['specimen', 'sample', 'site', 'location']
#            for col_name in col_names:
#                if col_name in meas_data3_0.columns:
#                    pruned = meas_data3_0[meas_data3_0[col_name].apply(cb.not_null)]
#                    num_missing = len(meas_data3_0) - len(pruned)
#                    if num_missing:
#                        msg = ""{} measurements cannot be associated with a {} and will be excluded\nTry using Pmag GUI (step 3) to make sure you have provided the full chain from specimen to location."".format(num_missing, col_name)
#                        pw.simple_warning(msg)
#                        print(""-W- {} measurements are missing {} data and will be excluded"".format(num_missing, col_name))
#                        meas_data3_0 = pruned
            Mkeys = ['magn_moment', 'magn_volume', 'magn_mass']
# fish out all the relavent data
            meas_data3_0 = meas_data3_0[meas_data3_0['method_codes'].str.contains(
                'LT-NO|LT-AF-Z|LT-T-Z|LT-M-Z|LT-LT-Z') == True]
            if not len(meas_data3_0):
                self.user_warning(""Your measurements table contains none of the required method codes to run Demag GUI: [LT-NO, LT-AF-Z, LT-T-Z, LT-M-Z, LT-LT-Z]"")
                return {}, {}
# now convert back to 2.5  changing only those keys that are necessary for thellier_gui
            meas_con_dict = map_magic.get_thellier_gui_meas_mapping(
                meas_data3_0, output=2)
            intensity_col = cb.get_intensity_col(meas_data3_0)
            if not intensity_col:
                self.user_warning(""Your measurements table must have one of the following columns to run Demag GUI: 'magn_moment', 'magn_volume', 'magn_mass',or 'magn_uncal'"")
                return {}, {}

            print('-I- Using {} for intensity'.format(intensity_col))
            self.intensity_col = meas_con_dict[intensity_col]
            meas_data2_5 = meas_data3_0.rename(columns=meas_con_dict)
            # make a list of dictionaries to maintain backward compatibility
            mag_meas_data = meas_data2_5.to_dict(""records"")

        else:  # data model 2.5
            try:
                print((""-I- Read magic file %s"" % self.magic_file))
            except ValueError:
                self.magic_measurement = self.choose_meas_file()
                print((""-I- Read magic file %s"" % self.magic_file))
            mag_meas_data, file_type = pmag.magic_read(self.magic_file)
            if file_type != ""magic_measurements"":
                self.user_warning(""You have selected data model 2.5, but your measurements file is either not in 2.5, or is not a measurements file.\n{} has file type: {}"".format(self.magic_file, file_type))
                return {}, {}


        self.mag_meas_data = self.merge_pmag_recs(mag_meas_data)

        # get list of unique specimen names with measurement data
        CurrRec = []
        sids = pmag.get_specs(self.mag_meas_data)  # specimen ID's
        for s in sids:
            if s not in list(Data.keys()):
                Data[s] = {}
                Data[s]['zijdblock'] = []
                Data[s]['zijdblock_geo'] = []
                Data[s]['zijdblock_tilt'] = []
                Data[s]['zijdblock_lab_treatments'] = []
                Data[s]['pars'] = {}
                Data[s]['csds'] = []
                Data[s]['zijdblock_steps'] = []
                Data[s]['measurement_flag'] = []  # a list of points 'g' or 'b'
                # index in original magic_measurements.txt
                Data[s]['mag_meas_data_index'] = []
                Data[s]['measurement_names'] = []

        prev_s = None
        cnt = -1
        # list of excluded lab protocols. copied from pmag.find_dmag_rec(s,data)
        self.excluded_methods = [""LP-AN-ARM"", ""LP-AN-TRM"", ""LP-ARM-AFD"",
                                 ""LP-ARM2-AFD"", ""LP-TRM-AFD"", ""LP-TRM"", ""LP-TRM-TD"", ""LP-X"", ""LP-PI-ARM""]
        self.included_methods = [
            ""LT-NO"", ""LT-AF-Z"", ""LT-T-Z"", ""LT-M-Z"", ""LT-LT-Z""]
#        self.mag_meas_data.sort(key=meas_key)
        # asiigned default values for NRM
        if len(self.mag_meas_data) > 0 and self.intensity_col in list(self.mag_meas_data[0].keys()):
            NRM = float(self.mag_meas_data[0][self.intensity_col])
        for rec in self.mag_meas_data:
            # if ""measurement_number"" in rec.keys() and str(rec['measurement_number']) == '1' and ""magic_method_codes"" in rec.keys() and ""LT-NO"" not in rec[""magic_method_codes""].split(':'):
            #    NRM = 1 #not really sure how to handle this case but assume that data is already normalized
            cnt += 1  # index counter
            s = rec[""er_specimen_name""]
            if ""er_sample_name"" in list(rec.keys()):
                sample = rec[""er_sample_name""]
            else:
                sample = ''
            if ""er_site_name"" in list(rec.keys()):
                site = rec[""er_site_name""]
            else:
                site = ''
            if ""er_location_name"" in list(rec.keys()):
                location = rec[""er_location_name""]
            else:
                location = ''
            expedition_name = """"
            if ""er_expedition_name"" in list(rec.keys()):
                expedition_name = rec[""er_expedition_name""]

            methods = rec[""magic_method_codes""].replace(
                "" "", """").strip(""\n"").split("":"")
            LP_methods = []
            LT_methods = []

            for k in ['zdata', 'zdata_geo', 'zdata_tilt', 'vector_diffs']:
                if k not in Data[s]:
                    Data[s][k] = []

            for i in range(len(methods)):
                methods[i] = methods[i].strip()
            if 'measurement_flag' not in list(rec.keys()):
                rec['measurement_flag'] = 'g'
            SKIP = True
            lab_treatment = """"
            for meth in methods:
                if 'DIR' in meth:
                    SKIP = False
                if meth in self.included_methods:
                    lab_treatment = meth
                    SKIP = False
                if ""LP"" in meth:
                    LP_methods.append(meth)
            for meth in self.excluded_methods:
                if meth in methods:
                    SKIP = True
                    break
            if SKIP:
                continue
            tr, LPcode, measurement_step_unit = """", """", """"
            if ""LT-NO"" in methods:
                tr = 0
                if prev_s != s and self.intensity_col in rec:
                    try:
                        NRM = float(rec[self.intensity_col])
                    except ValueError:
                        NRM = 1
                for method in methods:
                    if ""AF"" in method:
                        LPcode = ""LP-DIR-AF""
                        measurement_step_unit = ""mT""
                    if ""TRM"" in method:
                        LPcode = ""LP-DIR-T""
                        measurement_step_unit = ""C""
            elif ""LT-AF-Z"" in methods:
                try:
                    tr = float(rec[""treatment_ac_field""])*1e3  # (mT)
                except ValueError:
                    print((""Could not convert ac field for measurement, was given %s, skipping"" %
                           rec[""treatment_ac_field""]))
                    continue
                measurement_step_unit = ""mT""  # in magic its T in GUI its mT
                LPcode = ""LP-DIR-AF""
            elif ""LT-T-Z"" in methods or ""LT-LT-Z"" in methods:
                try:
                    tr = float(rec[""treatment_temp""])-273.  # celsius
                except ValueError:
                    print(
                        (""Could not convert temperature for measurement, was given %s, skipping"" % rec[""treatment_temp""]))
                    continue
                measurement_step_unit = ""C""  # in magic its K in GUI its C
                LPcode = ""LP-DIR-T""
            elif ""LT-M-Z"" in methods:
                # temporary for microwave
                tr = float(rec[""measurement_number""])
            else:
                # attempt to determine from treatment data
                if all(im not in methods for im in self.included_methods):
                    if 'treatment_temp' in list(rec.keys()) and not str(rec['treatment_temp']).isalpha() and rec['treatment_temp'] != '' and float(rec['treatment_temp']) > 0:
                        tr = float(rec[""treatment_temp""])-273.  # celsius
                        measurement_step_unit = ""C""  # in magic its K in GUI its C
                        LPcode = ""LP-DIR-T""
                    elif 'treatment_ac_field' in list(rec.keys()) and not str(rec['treatment_ac_field']).isalpha() and rec['treatment_ac_field'] != '' and float(rec['treatment_ac_field']) > 0:
                        tr = float(rec[""treatment_ac_field""])*1e3  # (mT)
                        measurement_step_unit = ""mT""  # in magic its T in GUI its mT
                        LPcode = ""LP-DIR-AF""
                    else:
                        tr = 0
                        if prev_s != s and self.intensity_col in rec:
                            try:
                                NRM = float(rec[self.intensity_col])
                            except ValueError:
                                NRM = 1
                        for method in methods:
                            if ""AF"" in method:
                                LPcode = ""LP-DIR-AF""
                                measurement_step_unit = ""mT""
                            if ""TRM"" in method:
                                LPcode = ""LP-DIR-T""
                                measurement_step_unit = ""C""
                else:
                    tr = float(rec[""measurement_number""])
            if prev_s != s and len(Data[s]['zijdblock']) > 0:
                NRM = Data[s]['zijdblock'][0][3]

            ZI = 0
            if tr != """":
                Data[s]['mag_meas_data_index'].append(
                    cnt)  # magic_measurement file intex
                if not int(self.data_model) == 2:
                    try:
                        Data[s]['measurement_names'].append(rec['measurement'])
                    except KeyError:
                        Data[s]['measurement_names'].append(rec['measurement_number'])
                Data[s]['zijdblock_lab_treatments'].append(lab_treatment)
                if measurement_step_unit != """":
                    if 'measurement_step_unit' in list(Data[s].keys()):
                        if measurement_step_unit not in Data[s]['measurement_step_unit'].split("":""):
                            Data[s]['measurement_step_unit'] = Data[s]['measurement_step_unit'] + \
                                "":""+measurement_step_unit
                    else:
                        Data[s]['measurement_step_unit'] = measurement_step_unit
                dec, inc, inten = """", """", """"
                if ""measurement_dec"" in list(rec.keys()) and cb.not_null(rec[""measurement_dec""], False):
                    dec = float(rec[""measurement_dec""])
                else:
                    continue
                if ""measurement_inc"" in list(rec.keys()) and cb.not_null(rec[""measurement_inc""], False):
                    inc = float(rec[""measurement_inc""])
                else:
                    continue
                if self.intensity_col in list(rec.keys()) and cb.not_null(rec[self.intensity_col], False):
                    intensity = float(rec[self.intensity_col])
                else:
                    intensity = 1.  # just assume a normal vector
                if 'magic_instrument_codes' not in list(rec.keys()):
                    rec['magic_instrument_codes'] = ''
                if 'measurement_csd' in list(rec.keys()):
                    csd = str(rec['measurement_csd'])
                else:
                    csd = ''
                Data[s]['zijdblock'].append(
                    [tr, dec, inc, intensity, ZI, rec['measurement_flag'], rec['magic_instrument_codes']])
                Data[s]['csds'].append(csd)
                DIR = [dec, inc, intensity/NRM]
                cart = pmag.dir2cart(DIR)
                Data[s]['zdata'].append(array([cart[0], cart[1], cart[2]]))

                if 'magic_experiment_name' in list(Data[s].keys()) and Data[s]['magic_experiment_name'] != rec[""magic_experiment_name""]:
                    print((""-E- ERROR: specimen %s has more than one demagnetization experiment name. You need to merge them to one experiment-name?"" % (s)))
                if float(tr) == 0 or float(tr) == 273:
                    Data[s]['zijdblock_steps'].append(""0"")
                elif measurement_step_unit == ""C"":
                    Data[s]['zijdblock_steps'].append(
                        ""%.0f%s"" % (tr, measurement_step_unit))
                else:
                    Data[s]['zijdblock_steps'].append(
                        ""%.1f%s"" % (tr, measurement_step_unit))
                # --------------
                if 'magic_experiment_name' in list(rec.keys()):
                    Data[s]['magic_experiment_name'] = rec[""magic_experiment_name""]
                if ""magic_instrument_codes"" in list(rec.keys()):
                    Data[s]['magic_instrument_codes'] = rec['magic_instrument_codes']
                Data[s][""magic_method_codes""] = LPcode

                # --------------
                # """"good"" or ""bad"" data
                # --------------

                flag = 'g'
                if 'measurement_flag' in list(rec.keys()):
                    if str(rec[""measurement_flag""]) == 'b':
                        flag = 'b'
                Data[s]['measurement_flag'].append(flag)

                # gegraphic coordinates
                try:
                    sample_azimuth = float(
                        self.Data_info[""er_samples""][sample]['sample_azimuth'])
                    sample_dip = float(
                        self.Data_info[""er_samples""][sample]['sample_dip'])
                    d_geo, i_geo = pmag.dogeo(
                        dec, inc, sample_azimuth, sample_dip)
                    # if d_geo or i_geo is null, we can't do geographic coordinates
                    # otherwise, go ahead
                    if not any([np.isnan(val) for val in [d_geo, i_geo]]):
                        Data[s]['zijdblock_geo'].append(
                            [tr, d_geo, i_geo, intensity, ZI, rec['measurement_flag'], rec['magic_instrument_codes']])
                        DIR = [d_geo, i_geo, intensity/NRM]
                        cart = pmag.dir2cart(DIR)
                        Data[s]['zdata_geo'].append([cart[0], cart[1], cart[2]])
                except (IOError, KeyError, ValueError, TypeError) as e:
                    pass
                #                    if prev_s != s:
                #                        print( ""-W- can't find sample_azimuth,sample_dip for sample %s""%sample)

                # tilt-corrected coordinates
                try:
                    sample_bed_dip_direction = float(
                        self.Data_info[""er_samples""][sample]['sample_bed_dip_direction'])
                    sample_bed_dip = float(
                        self.Data_info[""er_samples""][sample]['sample_bed_dip'])
                    d_tilt, i_tilt = pmag.dotilt(
                        d_geo, i_geo, sample_bed_dip_direction, sample_bed_dip)
                    Data[s]['zijdblock_tilt'].append(
                        [tr, d_tilt, i_tilt, intensity, ZI, rec['measurement_flag'], rec['magic_instrument_codes']])
                    DIR = [d_tilt, i_tilt, intensity/NRM]
                    cart = pmag.dir2cart(DIR)
                    Data[s]['zdata_tilt'].append([cart[0], cart[1], cart[2]])
                except (IOError, KeyError, TypeError, ValueError, UnboundLocalError) as e:
                    pass
                #                    if prev_s != s:
                #                        printd(""-W- can't find tilt-corrected data for sample %s""%sample)

                if len(Data[s]['zdata']) > 1:
                    Data[s]['vector_diffs'].append(
                        sqrt(sum((array(Data[s]['zdata'][-2])-array(Data[s]['zdata'][-1]))**2)))

            # ---------------------
            # hierarchy is determined from magic_measurements.txt
            # ---------------------

            if sample not in list(Data_hierarchy['samples'].keys()):
                Data_hierarchy['samples'][sample] = {}
                Data_hierarchy['samples'][sample]['specimens'] = []

            if site not in list(Data_hierarchy['sites'].keys()):
                Data_hierarchy['sites'][site] = {}
                Data_hierarchy['sites'][site]['samples'] = []
                Data_hierarchy['sites'][site]['specimens'] = []

            if location not in list(Data_hierarchy['locations'].keys()):
                Data_hierarchy['locations'][location] = {}
                Data_hierarchy['locations'][location]['sites'] = []
                Data_hierarchy['locations'][location]['samples'] = []
                Data_hierarchy['locations'][location]['specimens'] = []

            if 'this study' not in list(Data_hierarchy['study'].keys()):
                Data_hierarchy['study']['this study'] = {}
                Data_hierarchy['study']['this study']['sites'] = []
                Data_hierarchy['study']['this study']['samples'] = []
                Data_hierarchy['study']['this study']['specimens'] = []

            if s not in Data_hierarchy['samples'][sample]['specimens']:
                Data_hierarchy['samples'][sample]['specimens'].append(s)

            if s not in Data_hierarchy['sites'][site]['specimens']:
                Data_hierarchy['sites'][site]['specimens'].append(s)

            if s not in Data_hierarchy['locations'][location]['specimens']:
                Data_hierarchy['locations'][location]['specimens'].append(s)

            if s not in Data_hierarchy['study']['this study']['specimens']:
                Data_hierarchy['study']['this study']['specimens'].append(s)

            if sample not in Data_hierarchy['sites'][site]['samples']:
                Data_hierarchy['sites'][site]['samples'].append(sample)

            if sample not in Data_hierarchy['locations'][location]['samples']:
                Data_hierarchy['locations'][location]['samples'].append(sample)

            if sample not in Data_hierarchy['study']['this study']['samples']:
                Data_hierarchy['study']['this study']['samples'].append(sample)

            if site not in Data_hierarchy['locations'][location]['sites']:
                Data_hierarchy['locations'][location]['sites'].append(site)

            if site not in Data_hierarchy['study']['this study']['sites']:
                Data_hierarchy['study']['this study']['sites'].append(site)

            # Data_hierarchy['specimens'][s]=sample
            Data_hierarchy['sample_of_specimen'][s] = sample
            Data_hierarchy['site_of_specimen'][s] = site
            Data_hierarchy['site_of_sample'][sample] = site
            Data_hierarchy['location_of_site'][site] = location
            Data_hierarchy['location_of_specimen'][s] = location
            if expedition_name != """":
                Data_hierarchy['expedition_name_of_specimen'][s] = expedition_name
            prev_s = s

        print(""-I- done sorting meas data"")
        self.specimens = list(Data.keys())

        for s in self.specimens:
            if len(Data[s]['zdata']) > 0:
                Data[s]['vector_diffs'].append(
                    sqrt(sum(array(Data[s]['zdata'][-1])**2)))  # last vector of the vds
            vds = sum(Data[s]['vector_diffs'])  # vds calculation
            Data[s]['vector_diffs'] = array(Data[s]['vector_diffs'])
            Data[s]['vds'] = vds
            Data[s]['zdata'] = array(Data[s]['zdata'])
            Data[s]['zdata_geo'] = array(Data[s]['zdata_geo'])
            Data[s]['zdata_tilt'] = array(Data[s]['zdata_tilt'])
        return(Data, Data_hierarchy)",1,1
"def read_magic_file(self, path, sort_by_this_name):
        
        DATA = {}
        try:
            with open(path, 'r') as finput:
                lines = list(finput.readlines()[1:])
        except FileNotFoundError:
            return []
        # fin=open(path,'r')
        # fin.readline()
        line = lines[0]
        header = line.strip('\n').split('\t')
        error_strings = []
        for line in lines[1:]:
            tmp_data = {}
            tmp_line = line.strip('\n').split('\t')
            for i in range(len(tmp_line)):
                tmp_data[header[i]] = tmp_line[i]
            if tmp_data[sort_by_this_name] in list(DATA.keys()):
                error_string = ""-E- ERROR: magic file %s has more than one line for %s %s"" % (
                    path, sort_by_this_name, tmp_data[sort_by_this_name])
                # only print each error message once
                if error_string not in error_strings:
                    print(error_string)
                    error_strings.append(error_string)
            DATA[tmp_data[sort_by_this_name]] = tmp_data
        # fin.close()
        finput.close()
        return(DATA)",1,1
"def get_requirements(lookup=None):
    

    if lookup == None:
        lookup = get_lookup()

    install_requires = []
    for module in lookup['INSTALL_REQUIRES']:
        module_name = module[0]
        module_meta = module[1]
        if ""exact_version"" in module_meta:
            dependency = ""%s==%s"" %(module_name,module_meta['exact_version'])
        elif ""min_version"" in module_meta:
            if module_meta['min_version'] == None:
                dependency = module_name
            else:
                dependency = ""%s>=%s"" %(module_name,module_meta['min_version'])
        install_requires.append(dependency)
    return install_requires",1,3
"def readcols(infile, cols=None):
    
    if _is_str_none(infile) is None:
        return None

    if infile.endswith('.fits'):
        outarr = read_FITS_cols(infile, cols=cols)
    else:
        outarr = read_ASCII_cols(infile, cols=cols)
    return outarr",1,1
"def _writeloop(self, consumer):
        
        while True:
            bytes = self._inputFile.read(self._readSize)
            if not bytes:
                self._inputFile.close()
                break
            consumer.write(bytes)
            yield None",1,0
"def init_widget(self):
        
        super(AndroidView, self).init_widget()

        # Initialize the widget by updating only the members that
        # have read expressions declared. This saves a lot of time and
        # simplifies widget initialization code
        for k, v in self.get_declared_items():
            handler = getattr(self, 'set_'+k, None)
            if handler:
                handler(v)",1,2
"def _load_gui_setting(self, key, d=None):
        
        # Whether we should pop the value from the dictionary when we set it.
        pop_value = False
        
        # If d is None, assume we're using the lazy load settings.
        if d == None: 
            d = self._lazy_load
            pop_value = True
        
        # If we have a databox, take the header dictionary
        if not type(d) == dict: d = d.headers
        
        # only do this if the key exists
        if key in d:
            
            try:
                # Try to set the value
                eval(key).set_value(d[key])

                # If this fails, perhaps the element does not yet exist
                # For example, TabArea may not have all the tabs created
                # and cannot set the active tab until later.
                # If it's here, it worked, so pop if necessary
                if pop_value: d.pop(key)
           
            except: 
                print(""ERROR: Could not load gui setting ""+repr(key))",1,3
"def _readFile(self, sldir, fileName, sep):
        

        if sldir.endswith(os.sep):
            fileName = str(sldir)+str(fileName)
        else:
            fileName = str(sldir)+os.sep+str(fileName)


        fileLines=[] #list of lines in the file
        header=[]    #list of Header lines
        dataCols=[]  #Dictionary of data column names
        data=[]      #List of Data lists
        cols=[]      #List of column names

        f=open(fileName,'r')
        fileLines=f.readlines()
        i=0
        if self.datatype != 'trajectory':

            while i<len(fileLines):
                if fileLines[i].startswith(self.header_char):
                    tmp=fileLines[i].lstrip(self.header_char)
                    header.append(tmp.strip())
                else:
                    break
                i+=1

            cols=fileLines[i].split(sep)

            tmp=[]
            tmp1=[]
            for j in range(len(cols)):
                tmp1=cols[j].strip()
                if tmp1 !='':
                    tmp.append(tmp1)
            cols=tmp
            i+=1
        else:
            header={}
            while fileLines[i].startswith('#') or '=' in fileLines[i]:
                if fileLines[i].startswith('#') and cols==[]:
                    cols=fileLines[i].strip('#')
                    cols=cols.strip()
                    cols=cols.split()
                elif fileLines[i].startswith('#'):
                    tmp1=fileLines[i].strip('#')
                    tmp1=tmp1.strip()
                    self.headerLines.append(tmp1)
                elif not fileLines[i].startswith('#'):
                    tmp=fileLines[i].split('=')
                    tmp[0]=tmp[0].strip()
                    tmp[1]=tmp[1].strip()
                    if header=={}:
                        header={str(tmp[0]):str(tmp[1])}
                    else:
                        header[str(tmp[0])]=str(tmp[1])
                i+=1
        while i<len(fileLines):
            if fileLines[i].startswith('#'):
                i=i+1
            else:
                tmp=fileLines[i].split()
                for j in range(len(tmp)):
                    tmp[j]=tmp[j].strip()
                data.append(tmp)
                i+=1
        tmp=[]
        tmp1=[]
        for j in range(len(data)):
            for k in range(len(data[j])):
                tmp1=data[j][k].strip()
                if tmp1 !='':
                    tmp.append(tmp1)
            data[j]=tmp
            tmp=[]
        tmp=[]

        for j in range(len(cols)):
            for k in range(len(data)):
                try:
                    a=float(data[k][j])
                    tmp.append(a)
                except ValueError:
                    tmp.append(data[k][j])
                #else:
                 #   tmp.append(float(data[k][j])) # previously tmp.append(float(data[k][j]))
            tmp=array(tmp)

            if j == 0:
                dataCols={cols[j]:tmp}
            else:
                dataCols[cols[j]]=tmp
            tmp=[]

        return header,dataCols",1,1
"def wasb_write(self, log, remote_log_location, append=True):
        
        if append and self.wasb_log_exists(remote_log_location):
            old_log = self.wasb_read(remote_log_location)
            log = '\n'.join([old_log, log]) if old_log else log

        try:
            self.hook.load_string(
                log,
                self.wasb_container,
                remote_log_location,
            )
        except AzureHttpError:
            self.log.exception('Could not write logs to %s',
                               remote_log_location)",0,0
"def gcs_write(self, log, remote_log_location, append=True):
        
        if append:
            try:
                old_log = self.gcs_read(remote_log_location)
                log = '\n'.join([old_log, log]) if old_log else log
            except Exception as e:
                if not hasattr(e, 'resp') or e.resp.get('status') != '404':
                    log = '*** Previous log discarded: {}\n\n'.format(str(e)) + log

        try:
            bkt, blob = self.parse_gcs_url(remote_log_location)
            from tempfile import NamedTemporaryFile
            with NamedTemporaryFile(mode='w+') as tmpfile:
                tmpfile.write(log)
                # Force the file to be flushed, since we're doing the
                # upload from within the file context (it hasn't been
                # closed).
                tmpfile.flush()
                self.hook.upload(bkt, blob, tmpfile.name)
        except Exception as e:
            self.log.error('Could not write logs to %s: %s', remote_log_location, e)",0,0
"def to_csv(
            self,
            hql,
            csv_filepath,
            schema='default',
            delimiter=',',
            lineterminator='\r\n',
            output_header=True,
            fetch_size=1000,
            hive_conf=None):
        

        results_iter = self._get_results(hql, schema,
                                         fetch_size=fetch_size, hive_conf=hive_conf)
        header = next(results_iter)
        message = None

        i = 0
        with open(csv_filepath, 'wb') as f:
            writer = csv.writer(f,
                                delimiter=delimiter,
                                lineterminator=lineterminator,
                                encoding='utf-8')
            try:
                if output_header:
                    self.log.debug('Cursor description is %s', header)
                    writer.writerow([c[0] for c in header])

                for i, row in enumerate(results_iter, 1):
                    writer.writerow(row)
                    if i % fetch_size == 0:
                        self.log.info(""Written %s rows so far."", i)
            except ValueError as exception:
                message = str(exception)

        if message:
            # need to clean up the file first
            os.remove(csv_filepath)
            raise ValueError(message)

        self.log.info(""Done. Loaded a total of %s rows."", i)",0,0
"def s3_write(self, log, remote_log_location, append=True):
        
        if append and self.s3_log_exists(remote_log_location):
            old_log = self.s3_read(remote_log_location)
            log = '\n'.join([old_log, log]) if old_log else log

        try:
            self.hook.load_string(
                log,
                key=remote_log_location,
                replace=True,
                encrypt=configuration.conf.getboolean('core', 'ENCRYPT_S3_LOGS'),
            )
        except Exception:
            self.log.exception('Could not write logs to %s', remote_log_location)",0,0
"def write_java_message(key,val,text_file):
    

    text_file.write(key)
    text_file.write('\n')

    if (len(val[0]) > 0) and (len(val) >= 3):
        for index in range(len(val[0])):
            text_file.write(""Java Message Type: "")
            text_file.write(val[1][index])
            text_file.write('\n')

            text_file.write(""Java Message: "")

            for jmess in val[2][index]:
                text_file.write(jmess)
                text_file.write('\n')

        text_file.write('\n \n')",0,0
"def output_stream(plugin, stream):
    
    global output

    success_open = False
    for i in range(args.retry_open):
        try:
            stream_fd, prebuffer = open_stream(stream)
            success_open = True
            break
        except StreamError as err:
            log.error(""Try {0}/{1}: Could not open stream {2} ({3})"", i + 1, args.retry_open, stream, err)

    if not success_open:
        console.exit(""Could not open stream {0}, tried {1} times, exiting"", stream, args.retry_open)

    output = create_output(plugin)

    try:
        output.open()
    except (IOError, OSError) as err:
        if isinstance(output, PlayerOutput):
            console.exit(""Failed to start player: {0} ({1})"",
                         args.player, err)
        else:
            console.exit(""Failed to open output: {0} ({1})"",
                         args.output, err)

    with closing(output):
        log.debug(""Writing stream to output"")
        read_stream(stream_fd, output, prebuffer)

    return True",0,0
"def run(self, dag):
        
        # Initiate the commutation set
        self.property_set['commutation_set'] = defaultdict(list)

        # Build a dictionary to keep track of the gates on each qubit
        for wire in dag.wires:
            wire_name = ""{0}[{1}]"".format(str(wire[0].name), str(wire[1]))
            self.property_set['commutation_set'][wire_name] = []

        # Add edges to the dictionary for each qubit
        for node in dag.topological_op_nodes():
            for (_, _, edge_data) in dag.edges(node):

                edge_name = edge_data['name']
                self.property_set['commutation_set'][(node, edge_name)] = -1

        for wire in dag.wires:
            wire_name = ""{0}[{1}]"".format(str(wire[0].name), str(wire[1]))

            for current_gate in dag.nodes_on_wire(wire):

                current_comm_set = self.property_set['commutation_set'][wire_name]
                if not current_comm_set:
                    current_comm_set.append([current_gate])

                if current_gate not in current_comm_set[-1]:
                    prev_gate = current_comm_set[-1][-1]
                    if _commute(current_gate, prev_gate):
                        current_comm_set[-1].append(current_gate)

                    else:
                        current_comm_set.append([current_gate])

                temp_len = len(current_comm_set)
                self.property_set['commutation_set'][(current_gate, wire_name)] = temp_len - 1",0,2
"def _report_dependencies_graph(self, sect, _, _dummy):
        
        dep_info = self.stats[""dependencies""]
        if not dep_info or not (
            self.config.import_graph
            or self.config.ext_import_graph
            or self.config.int_import_graph
        ):
            raise EmptyReportError()
        filename = self.config.import_graph
        if filename:
            _make_graph(filename, dep_info, sect, """")
        filename = self.config.ext_import_graph
        if filename:
            _make_graph(filename, self._external_dependencies_info(), sect, ""external "")
        filename = self.config.int_import_graph
        if filename:
            _make_graph(filename, self._internal_dependencies_info(), sect, ""internal "")",0,0
"def format(self, layout, stream=None, encoding=None):
        
        if stream is None:
            stream = sys.stdout
        if not encoding:
            encoding = getattr(stream, ""encoding"", ""UTF-8"")
        self.encoding = encoding or ""UTF-8""
        self.out = stream
        self.begin_format()
        layout.accept(self)
        self.end_format()",0,0
"def write(self, diadefs):
        
        for diagram in diadefs:
            basename = diagram.title.strip().replace("" "", ""_"")
            file_name = ""%s.%s"" % (basename, self.config.output_format)
            self.set_printer(file_name, basename)
            if diagram.TYPE == ""class"":
                self.write_classes(diagram)
            else:
                self.write_packages(diagram)
            self.close_graph()",0,0
"def generate_config(self, stream=None, skipsections=(), encoding=None):
        
        options_by_section = {}
        sections = []
        for provider in self.options_providers:
            for section, options in provider.options_by_section():
                if section is None:
                    section = provider.name
                if section in skipsections:
                    continue
                options = [
                    (n, d, v)
                    for (n, d, v) in options
                    if d.get(""type"") is not None and not d.get(""deprecated"")
                ]
                if not options:
                    continue
                if section not in sections:
                    sections.append(section)
                alloptions = options_by_section.setdefault(section, [])
                alloptions += options
        stream = stream or sys.stdout
        printed = False
        for section in sections:
            if printed:
                print(""\n"", file=stream)
            utils.format_section(
                stream, section.upper(), sorted(options_by_section[section])
            )
            printed = True",0,0
"def begin(script='TEMP3D_default.mlx', file_in=None, mlp_in=None):
    
    script_file = open(script, 'w')
    script_file.write(''.join(['<!DOCTYPE FilterScript>\n',
                               '<FilterScript>\n']))
    script_file.close()

    current_layer = -1
    last_layer = -1
    stl = False

    # Process project files first
    if mlp_in is not None:
        # make a list if it isn't already
        if not isinstance(mlp_in, list):
            mlp_in = [mlp_in]
        for val in mlp_in:
            tree = ET.parse(val)
            #root = tree.getroot()
            for elem in tree.iter(tag='MLMesh'):
                filename = (elem.attrib['filename'])
                current_layer += 1
                last_layer += 1
                # If the mesh file extension is stl, change to that layer and
                # run clean.merge_vert
                if os.path.splitext(filename)[1][1:].strip().lower() == 'stl':
                    layers.change(script, current_layer)
                    clean.merge_vert(script)
                    stl = True

    # Process separate input files next
    if file_in is not None:
        # make a list if it isn't already
        if not isinstance(file_in, list):
            file_in = [file_in]
        for val in file_in:
            current_layer += 1
            last_layer += 1
            # If the mesh file extension is stl, change to that layer and
            # run clean.merge_vert
            if os.path.splitext(val)[1][1:].strip().lower() == 'stl':
                layers.change(script, current_layer)
                clean.merge_vert(script)
                stl = True

    # If some input files were stl, we need to change back to the last layer
    if stl:
        layers.change(script, last_layer)  # Change back to the last layer
    elif last_layer == -1:
        # If no input files are provided, create a dummy file
        # with a single vertex and delete it first in the script.
        # This works around the fact that meshlabserver will
        # not run without an input file.
        file_in = ['TEMP3D.xyz']
        file_in_descriptor = open(file_in[0], 'w')
        file_in_descriptor.write('0 0 0')
        file_in_descriptor.close()
        layers.delete(script)
    return current_layer, last_layer",0,0
"def _write_local_data_files(self, cursor):
        
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}
        for row in cursor:
            row_dict = self.generate_data_dict(row._fields, row)
            s = json.dumps(row_dict).encode('utf-8')
            tmp_file_handle.write(s)

            # Append newline to make dumps BigQuery compatible.
            tmp_file_handle.write(b'\n')

            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle

        return tmp_file_handles",0,0
"def _write_local_schema_file(self, cursor):
        
        schema = []
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)

        for name, type in zip(cursor.column_names, cursor.column_types):
            schema.append(self.generate_schema_dict(name, type))
        json_serialized_schema = json.dumps(schema).encode('utf-8')

        tmp_schema_file_handle.write(json_serialized_schema)
        return {self.schema_filename: tmp_schema_file_handle}",0,0
"def _write_local_data_files(self, cursor):
        
        schema = list(map(lambda schema_tuple: schema_tuple[0].replace(' ', '_'), cursor.description))
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}

        for row in cursor:
            # Convert if needed
            row = map(self.convert_types, row)
            row_dict = dict(zip(schema, row))

            s = json.dumps(row_dict, sort_keys=True)
            s = s.encode('utf-8')
            tmp_file_handle.write(s)

            # Append newline to make dumps BQ compatible
            tmp_file_handle.write(b'\n')

            # Stop if the file exceeds the file size limit
            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle

        return tmp_file_handles",0,0
"def _write_local_data_files(self, cursor):
        
        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))
        tmp_file_handles = {}
        row_no = 0

        def _create_new_file():
            handle = NamedTemporaryFile(delete=True)
            filename = self.filename.format(len(tmp_file_handles))
            tmp_file_handles[filename] = handle
            return handle

        # Don't create a file if there is nothing to write
        if cursor.rowcount > 0:
            tmp_file_handle = _create_new_file()

            for row in cursor:
                # Convert datetime objects to utc seconds, and decimals to floats
                row = map(self.convert_types, row)
                row_dict = dict(zip(schema, row))

                s = json.dumps(row_dict, sort_keys=True).encode('utf-8')
                tmp_file_handle.write(s)

                # Append newline to make dumps BigQuery compatible.
                tmp_file_handle.write(b'\n')

                # Stop if the file exceeds the file size limit.
                if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                    tmp_file_handle = _create_new_file()
                row_no += 1

        self.log.info('Received %s rows over %s files', row_no, len(tmp_file_handles))

        return tmp_file_handles",0,0
"def _write_local_schema_file(self, cursor):
        
        schema = []
        for field in cursor.description:
            # See PEP 249 for details about the description tuple.
            field_name = field[0]
            field_type = self.type_map(field[1])
            field_mode = 'REPEATED' if field[1] in (1009, 1005, 1007,
                                                    1016) else 'NULLABLE'
            schema.append({
                'name': field_name,
                'type': field_type,
                'mode': field_mode,
            })

        self.log.info('Using schema for %s: %s', self.schema_filename, schema)
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)
        s = json.dumps(schema, sort_keys=True).encode('utf-8')
        tmp_schema_file_handle.write(s)
        return {self.schema_filename: tmp_schema_file_handle}",0,0
"def _write_local_data_files(self, cursor):
        
        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))
        col_type_dict = self._get_col_type_dict()
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        if self.export_format == 'csv':
            file_mime_type = 'text/csv'
        else:
            file_mime_type = 'application/json'
        files_to_upload = [{
            'file_name': self.filename.format(file_no),
            'file_handle': tmp_file_handle,
            'file_mime_type': file_mime_type
        }]

        if self.export_format == 'csv':
            csv_writer = self._configure_csv_file(tmp_file_handle, schema)

        for row in cursor:
            # Convert datetime objects to utc seconds, and decimals to floats.
            # Convert binary type object to string encoded with base64.
            row = self._convert_types(schema, col_type_dict, row)

            if self.export_format == 'csv':
                csv_writer.writerow(row)
            else:
                row_dict = dict(zip(schema, row))

                # TODO validate that row isn't > 2MB. BQ enforces a hard row size of 2MB.
                s = json.dumps(row_dict, sort_keys=True).encode('utf-8')
                tmp_file_handle.write(s)

                # Append newline to make dumps BigQuery compatible.
                tmp_file_handle.write(b'\n')

            # Stop if the file exceeds the file size limit.
            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                files_to_upload.append({
                    'file_name': self.filename.format(file_no),
                    'file_handle': tmp_file_handle,
                    'file_mime_type': file_mime_type
                })

                if self.export_format == 'csv':
                    csv_writer = self._configure_csv_file(tmp_file_handle, schema)

        return files_to_upload",0,0
"def write_to(self, out_stream, do_compress=False):
        

        self.update_header()

        if (
                self.vlrs.get(""ExtraBytesVlr"")
                and not self.points_data.extra_dimensions_names
        ):
            logger.error(
                ""Las contains an ExtraBytesVlr, but no extra bytes were found in the point_record, ""
                ""removing the vlr""
            )
            self.vlrs.extract(""ExtraBytesVlr"")

        if do_compress:
            laz_vrl = create_laz_vlr(self.points_data)
            self.vlrs.append(known.LasZipVlr(laz_vrl.data()))
            raw_vlrs = vlrlist.RawVLRList.from_list(self.vlrs)

            self.header.offset_to_point_data = (
                    self.header.size + raw_vlrs.total_size_in_bytes()
            )
            self.header.point_format_id = uncompressed_id_to_compressed(
                self.header.point_format_id
            )
            self.header.number_of_vlr = len(raw_vlrs)

            points_bytes = compress_buffer(
                np.frombuffer(self.points_data.array, np.uint8),
                laz_vrl.schema,
                self.header.offset_to_point_data,
            ).tobytes()

        else:
            raw_vlrs = vlrlist.RawVLRList.from_list(self.vlrs)
            self.header.number_of_vlr = len(raw_vlrs)
            self.header.offset_to_point_data = (
                    self.header.size + raw_vlrs.total_size_in_bytes()
            )
            points_bytes = self.points_data.raw_bytes()

        self.header.write_to(out_stream)
        self._raise_if_not_expected_pos(out_stream, self.header.size)
        raw_vlrs.write_to(out_stream)
        self._raise_if_not_expected_pos(out_stream, self.header.offset_to_point_data)
        out_stream.write(points_bytes)",0,0
"def write_configs(self, project_root):
        

        # Write resources config
        with open(join(project_root, ""resources.config""), ""w"") as fh:
            fh.write(self.resources)

        # Write containers config
        with open(join(project_root, ""containers.config""), ""w"") as fh:
            fh.write(self.containers)

        # Write containers config
        with open(join(project_root, ""params.config""), ""w"") as fh:
            fh.write(self.params)

        # Write manifest config
        with open(join(project_root, ""manifest.config""), ""w"") as fh:
            fh.write(self.manifest)

        # Write user config if not present in the project directory
        if not exists(join(project_root, ""user.config"")):
            with open(join(project_root, ""user.config""), ""w"") as fh:
                fh.write(self.user_config)

        lib_dir = join(project_root, ""lib"")
        if not exists(lib_dir):
            os.makedirs(lib_dir)
        with open(join(lib_dir, ""Helper.groovy""), ""w"") as fh:
            fh.write(self.help)

        # Generate the pipeline DAG
        pipeline_to_json = self.render_pipeline()
        with open(splitext(self.nf_file)[0] + "".html"", ""w"") as fh:
            fh.write(pipeline_to_json)",0,0
"def write_batch_to_file(self, filename='batch_test_default.txt'):
        

        #---------------------------------------------------------#
        # The following is the file which is passed to planarradpy.
        #---------------------------------------------------------#
        self.batch_file = open(str(filename), 'w')

        self.batch_file.write()
        self.batch_file.write(str(self.batch_name))
        self.batch_file.write()
        self.batch_file.write(str(self.saa_values))
        self.batch_file.write()
        self.batch_file.write(str(self.sza_values))
        self.batch_file.write()
        self.batch_file.write(str(self.p_values))
        self.batch_file.write()
        self.batch_file.write(str(self.x_value))
        self.batch_file.write()
        self.batch_file.write(str(self.y_value))
        self.batch_file.write()
        self.batch_file.write(str(self.g_value))
        self.batch_file.write()
        self.batch_file.write(str(self.s_value))
        self.batch_file.write()
        self.batch_file.write(str(self.z_value))
        self.batch_file.write()
        self.batch_file.write(str(self.wavelength_values))
        self.batch_file.write()
        self.batch_file.write(str(self.nb_cpu))
        self.batch_file.write()
        self.batch_file.write(self.executive_path)
        self.batch_file.write()
        self.batch_file.write(str(self.verbose_value))
        self.batch_file.write()
        self.batch_file.write(self.phytoplankton_path)
        self.batch_file.write()
        self.batch_file.write(self.bottom_path)
        self.batch_file.write()
        self.batch_file.write(str(self.report_parameter_value))

        self.batch_file.write()

        self.batch_file.close()

        #-------------------------------------------------------------------#
        # The following is the action to move the file to the good directory.
        #-------------------------------------------------------------------#
        src = './' + filename
        dst = './inputs/batch_files'
        os.system(""mv"" + "" "" + src + "" "" + dst)",0,0
"def lock_connection(cls, conf, dsn, key=None):
        
        with Config(conf, ""w"", key) as c:
            connection = c.get_connection(dsn)
            if not connection:
                raise ConfigurationError(""Unable to lock connection"")
            if dsn is None:
                dsn = c.settings[""connections""][""default""]
            value = ""connections.{}.lock"".format(dsn)
            lock = c.get_value(""connections.{}.lock"".format(dsn), default=0)
            if lock >= 2:
                raise ConnectionLock(dsn)
            lock += 1
            c.set_value(""connections.{}.lock"".format(dsn), lock)
            c.write()",0,0
"def unlock_connection(cls, conf, dsn, key=None):
        
        with Config(conf, ""w"", key) as c:
            connection = c.connections.get(dsn, None)
            if not connection:
                raise ConfigurationError(""Unable to unlock connection"")
            if dsn is None:
                dsn = c.settings[""connections""][""default""]
            if connection.get(""lock"", None) is None:
                raise GiraffeError(""Connection '{}' is not locked."".format(dsn))
            c.unset_value(""connections.{}.lock"".format(dsn))
            c.write()",0,0
"def to_archive(self, writer):
        
        if 'b' not in writer.mode:
            raise GiraffeError(""Archive writer must be in binary mode"")
        writer.write(GIRAFFE_MAGIC)
        writer.write(self.columns.serialize())
        i = 0
        for n, chunk in enumerate(self._fetchall(ROW_ENCODING_RAW), 1):
            writer.write(chunk)
            yield TeradataEncoder.count(chunk)",0,0
"def _writeResponse(self, response):
        
        encoded = dumps(response, default=_default)
        self.transport.write(encoded)",0,0
"def on_batch_end(self, last_loss:Tensor, iteration:int, **kwargs)->None:
        ""Callback function that writes batch end appropriate data to Tensorboard.""
        if iteration == 0: return
        self._update_batches_if_needed()
        if iteration % self.loss_iters == 0: self._write_training_loss(iteration=iteration, last_loss=last_loss)
        if iteration % self.hist_iters == 0: self._write_weight_histograms(iteration=iteration)",0,0
"def on_backward_end(self, iteration:int, **kwargs)->None:
        ""Callback function that writes backward end appropriate data to Tensorboard.""
        if iteration == 0: return
        self._update_batches_if_needed()
        if iteration % self.stats_iters == 0: self._write_model_stats(iteration=iteration)",0,0
"def on_epoch_end(self, last_metrics:MetricsList, iteration:int, **kwargs)->None:
        ""Callback function that writes epoch end appropriate data to Tensorboard.""
        self._write_metrics(iteration=iteration, last_metrics=last_metrics)",0,0
"def on_batch_end(self, iteration:int, **kwargs)->None:
        ""Callback function that writes batch end appropriate data to Tensorboard.""
        super().on_batch_end(iteration=iteration, **kwargs)
        if iteration == 0: return
        if iteration % self.visual_iters == 0: self._write_images(iteration=iteration)",0,0
"def on_backward_end(self, iteration:int, **kwargs)->None:
        ""Callback function that writes backward end appropriate data to Tensorboard.""
        if iteration == 0: return
        self._update_batches_if_needed()
        #TODO:  This could perhaps be implemented as queues of requests instead but that seemed like overkill. 
        # But I'm not the biggest fan of maintaining these boolean flags either... Review pls.
        if iteration % self.stats_iters == 0: self.gen_stats_updated, self.crit_stats_updated = False, False
        if not (self.gen_stats_updated and self.crit_stats_updated): self._write_model_stats(iteration=iteration)",0,0
"def EndVector(self, vectorNumElems):
        

        self.assertNested()
        ## @cond FLATBUFFERS_INTERNAL
        self.nested = False
        ## @endcond
        # we already made space for this, so write without PrependUint32
        self.PlaceUOffsetT(vectorNumElems)
        return self.Offset()",0,0
"def _schema_to_json_file_object(self, schema_list, file_obj):
        
        json.dump(schema_list, file_obj, indent=2, sort_keys=True)",0,0
"def init_from_adversarial_batches_write_to_datastore(self, submissions,
                                                       adv_batches):
    
    # prepare classification batches
    idx = 0
    for s_id in iterkeys(submissions.defenses):
      for adv_id in iterkeys(adv_batches.data):
        class_batch_id = CLASSIFICATION_BATCH_ID_PATTERN.format(idx)
        idx += 1
        self.data[class_batch_id] = {
            'adversarial_batch_id': adv_id,
            'submission_id': s_id,
            'result_path': os.path.join(
                self._round_name,
                CLASSIFICATION_BATCHES_SUBDIR,
                s_id + '_' + adv_id + '.csv')
        }
    # save them to datastore
    client = self._datastore_client
    with client.no_transact_batch() as batch:
      for key, value in iteritems(self.data):
        entity = client.entity(client.key(KIND_CLASSIFICATION_BATCH, key))
        entity.update(value)
        batch.put(entity)",0,0
"def _extract_file(self, tgz, tarinfo, dst_path, buffer_size=10<<20):
    
    src = tgz.extractfile(tarinfo)
    dst = tf_v1.gfile.GFile(dst_path, ""wb"")
    while 1:
      buf = src.read(buffer_size)
      if not buf:
        break
      dst.write(buf)
      self._log_progress(len(buf))
    dst.close()
    src.close()",0,0
"def write_summaries(self, tagged_data, experiment_name, run_name):
    
    logger.debug('Writing summaries for %s tags', len(tagged_data))
    # Connection used as context manager for auto commit/rollback on exit.
    # We still need an explicit BEGIN, because it doesn't do one on enter,
    # it waits until the first DML command - which is totally broken.
    # See: https://stackoverflow.com/a/44448465/1179226
    with self._db:
      self._db.execute('BEGIN TRANSACTION')
      run_id = self._maybe_init_run(experiment_name, run_name)
      tag_to_metadata = {
          tag: tagdata.metadata for tag, tagdata in six.iteritems(tagged_data)
      }
      tag_to_id = self._maybe_init_tags(run_id, tag_to_metadata)
      tensor_values = []
      for tag, tagdata in six.iteritems(tagged_data):
        tag_id = tag_to_id[tag]
        for step, wall_time, tensor_proto in tagdata.values:
          dtype = tensor_proto.dtype
          shape = ','.join(str(d.size) for d in tensor_proto.tensor_shape.dim)
          # Use tensor_proto.tensor_content if it's set, to skip relatively
          # expensive extraction into intermediate ndarray.
          data = self._make_blob(
              tensor_proto.tensor_content or
              tensor_util.make_ndarray(tensor_proto).tobytes())
          tensor_values.append((tag_id, step, wall_time, dtype, shape, data))
      self._db.executemany(
          ,
          tensor_values)",0,0
"def update(self, session, arrays=None, frame=None):
    
    new_config = self._get_config()

    if self._enough_time_has_passed(self.previous_config['FPS']):
      self.visualizer.update(new_config)
      self.last_update_time = time.time()
      final_image = self._update_frame(session, arrays, frame, new_config)
      self._update_recording(final_image, new_config)",0,3
"def set_reboot_required_witnessed():
    
    errcode = -1
    dir_path = os.path.dirname(NILRT_REBOOT_WITNESS_PATH)
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path)
        except OSError as ex:
            raise SaltInvocationError('Error creating {0} (-{1}): {2}'
                                      .format(dir_path, ex.errno, ex.strerror))

        rdict = __salt__['cmd.run_all']('touch {0}'.format(NILRT_REBOOT_WITNESS_PATH))
        errcode = rdict['retcode']

    return errcode == 0",0,0
"def _write_incron_lines(user, lines):
    
    if user == 'system':
        ret = {}
        ret['retcode'] = _write_file(_INCRON_SYSTEM_TAB, 'salt', ''.join(lines))
        return ret
    else:
        path = salt.utils.files.mkstemp()
        with salt.utils.files.fopen(path, 'wb') as fp_:
            fp_.writelines(salt.utils.data.encode(lines))
        if __grains__['os_family'] == 'Solaris' and user != ""root"":
            __salt__['cmd.run']('chown {0} {1}'.format(user, path), python_shell=False)
        ret = __salt__['cmd.run_all'](_get_incron_cmdstr(path), runas=user, python_shell=False)
        os.remove(path)
        return ret",0,0
"def _decrypt_and_extract(self, fname):
        
        with open(fname, ""r"") as fp_in:
            encrypted_data = fp_in.read()

            decrypted_data = self._hilink_decrypt(encrypted_data)

            with open(binwalk.core.common.unique_file_name(fname[:-4], ""dec""), ""w"") as fp_out:
                fp_out.write(decrypted_data)",0,1
"def download_to_stream(self, file_id, destination):
        
        gout = self.open_download_stream(file_id)
        for chunk in gout:
            destination.write(chunk)",0,0
"def fsync(self, **kwargs):
        
        self.admin.command(""fsync"",
                           read_preference=ReadPreference.PRIMARY, **kwargs)",0,0
"def _eval_summary(self, context: MonitorContext, feed_dict: Optional[Dict]=None) -> None:
        

        if self._summary is None:
            raise RuntimeError('TensorBoard monitor task should set the Tensorflow.Summary object')

        if context.session is None:
            raise RuntimeError('To run a TensorBoard monitor task the TF session object'
                               ' must be provided when creating an instance of the Monitor')

        summary = context.session.run(self._summary, feed_dict=feed_dict)
        self._file_writer.add_summary(summary, context.global_step)
        if self._flush_immediately:
            self.flush()",0,0
"def _WriteResponses(self, responses, cursor):
    

    query = (""INSERT IGNORE INTO flow_responses ""
             ""(client_id, flow_id, request_id, response_id, ""
             ""response, status, iterator, timestamp) VALUES "")

    templates = []
    args = []
    for r in responses:
      templates.append(""(%s, %s, %s, %s, %s, %s, %s, NOW(6))"")
      client_id_int = db_utils.ClientIDToInt(r.client_id)
      flow_id_int = db_utils.FlowIDToInt(r.flow_id)

      args.append(client_id_int)
      args.append(flow_id_int)
      args.append(r.request_id)
      args.append(r.response_id)
      if isinstance(r, rdf_flow_objects.FlowResponse):
        args.append(r.SerializeToString())
        args.append("""")
        args.append("""")
      elif isinstance(r, rdf_flow_objects.FlowStatus):
        args.append("""")
        args.append(r.SerializeToString())
        args.append("""")
      elif isinstance(r, rdf_flow_objects.FlowIterator):
        args.append("""")
        args.append("""")
        args.append(r.SerializeToString())
      else:
        # This can't really happen due to db api type checking.
        raise ValueError(""Got unexpected response type: %s %s"" % (type(r), r))

    query += "","".join(templates)
    try:
      cursor.execute(query, args)
    except MySQLdb.IntegrityError:
      # If we have multiple responses and one of them fails to insert, we try
      # them one by one so we don't lose any valid replies.
      if len(responses) > 1:
        for r in responses:
          self._WriteResponses([r], cursor)
      else:
        logging.warn(""Response for unknown request: %s"", responses[0])",0,0
"def WriteToPath(obj, filepath):
  
  with io.open(filepath, mode=""w"", encoding=""utf-8"") as filedesc:
    WriteToFile(obj, filedesc)",0,0
"def WriteManyToPath(objs, filepath):
  
  with io.open(filepath, mode=""w"", encoding=""utf-8"") as filedesc:
    WriteManyToFile(objs, filedesc)",0,0
"def stop(self):
        
        BufferedReader.stop(self)
        self._stop_running_event.set()
        self._writer_thread.join()
        BaseIOHandler.stop(self)",0,0
"def _flush(self):
        
        if self.file.closed:
            return
        cache = b"""".join(self.cache)
        if not cache:
            # Nothing to write
            return
        uncompressed_data = cache[:self.MAX_CACHE_SIZE]
        # Save data that comes after max size to next round
        tail = cache[self.MAX_CACHE_SIZE:]
        self.cache = [tail]
        self.cache_size = len(tail)
        compressed_data = zlib.compress(uncompressed_data,
                                        self.COMPRESSION_LEVEL)
        obj_size = (OBJ_HEADER_V1_STRUCT.size + LOG_CONTAINER_STRUCT.size +
                    len(compressed_data))
        base_header = OBJ_HEADER_BASE_STRUCT.pack(
            b""LOBJ"", OBJ_HEADER_BASE_STRUCT.size, 1, obj_size, LOG_CONTAINER)
        container_header = LOG_CONTAINER_STRUCT.pack(
            ZLIB_DEFLATE, len(uncompressed_data))
        self.file.write(base_header)
        self.file.write(container_header)
        self.file.write(compressed_data)
        # Write padding bytes
        self.file.write(b""\x00"" * (obj_size % 4))
        self.uncompressed_size += OBJ_HEADER_V1_STRUCT.size + LOG_CONTAINER_STRUCT.size
        self.uncompressed_size += len(uncompressed_data)",0,0
"def input_thread(log, stdin, is_alive, quit, close_before_term):
    

    done = False
    closed = False
    alive = True
    poller = Poller()
    poller.register_write(stdin)

    while poller and alive:
        changed = poller.poll(1)
        for fd, events in changed:
            if events & (POLLER_EVENT_WRITE | POLLER_EVENT_HUP):
                log.debug(""%r ready for more input"", stdin)
                done = stdin.write()

                if done:
                    poller.unregister(stdin)
                    if close_before_term:
                        stdin.close()
                        closed = True

        alive, _ = is_alive()

    while alive:
        quit.wait(1)
        alive, _ = is_alive()

    if not closed:
        stdin.close()",0,0
"def DownloadReportToFile(self, report_job_id, export_format, outfile,
                           include_report_properties=False,
                           include_totals_row=None, use_gzip_compression=True):
    
    service = self._GetReportService()

    if include_totals_row is None:  # True unless CSV export if not specified
      include_totals_row = True if export_format != 'CSV_DUMP' else False
    opts = {
        'exportFormat': export_format,
        'includeReportProperties': include_report_properties,
        'includeTotalsRow': include_totals_row,
        'useGzipCompression': use_gzip_compression
    }
    report_url = service.getReportDownloadUrlWithOptions(report_job_id, opts)
    _data_downloader_logger.info('Request Summary: Report job ID: %s, %s',
                                 report_job_id, opts)

    response = self.url_opener.open(report_url)

    _data_downloader_logger.debug(
        'Incoming response: %s %s REDACTED REPORT DATA', response.code,
        response.msg)

    while True:
      chunk = response.read(_CHUNK_SIZE)
      if not chunk: break
      outfile.write(chunk)",0,0
"def write_pretty_dict_str(out, obj, indent=2):
    
    json.dump(obj,
              out,
              indent=indent,
              sort_keys=True,
              separators=(',', ': '),
              ensure_ascii=False,
              encoding=""utf-8"")",0,0
"def access_cache(self, key=None, key_location_on_param=0, expire=None, auto_update=True,
                     cache_loader=None, cache_writer=None, timeout=1):
        

        def decorate(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                k = None
                if len(args) - 1 >= key_location_on_param:
                    k = args[key_location_on_param]
                if key is not None:
                    k = key
                cache_result = self.get(key=k, timeout=timeout)
                # if the cache is miss and cache loader is the existent
                # then query cache from cache loader
                if cache_result is None:
                    if cache_loader is not None:
                        cache_result = cache_loader(k)
                    elif self.cache_loader is not None:
                        cache_result = self.cache_loader(k)
                # if still miss then execute a function that is decorated
                # then update cache on the basis of parameter auto_update
                if cache_result is not None:
                    return cache_result
                else:
                    result = func(*args, **kwargs)

                if auto_update:
                    self.put(key=k, value=result, expire=expire, timeout=timeout)
                    if cache_writer is not None:
                        self.thread_pool.submit(cache_writer, k, result)
                    elif self.cache_writer is not None:
                        self.thread_pool.submit(self.cache_writer, k, result)
                return result

            return wrapper

        return decorate",0,0
"def _write_hmet_card_file(self, hmet_card_file_path, main_output_folder):
        
        with io_open(hmet_card_file_path, 'w') as out_hmet_list_file:
            for hour_time in self.data.lsm.datetime:
                date_str = self._time_to_string(hour_time, ""%Y%m%d%H"")
                out_hmet_list_file.write(u""{0}\n"".format(path.join(main_output_folder, date_str)))",0,0
"def write(self, novel_title='novel', filetype='txt'):
        
        self._compose_chapters()
        self._write_to_file(novel_title, filetype)",0,0
"def save_image_to_disk(self, filename_1 = None, filename_2 = None):
        

        def axes_empty(ax):
            

            is_empty = True

            if ax is not None and len(ax)>0:
                for a in ax:
                    if len(a.lines)+len(a.images)+len(a.patches) != 0:
                        is_empty = False


            return is_empty

        # create and save images
        if (filename_1 is None):
            filename_1 = self.filename('-plt1.png')

        if (filename_2 is None):
            filename_2 = self.filename('-plt2.png')


        # windows can't deal with long filenames so we have to use the prefix '\\\\?\\'
        # if len(filename_1.split('\\\\?\\')) == 1:
        #     filename_1 = '\\\\?\\' + filename_1
        # if len(filename_2.split('\\\\?\\')) == 1:
        #     filename_2 = '\\\\?\\' + filename_2

        if os.path.exists(os.path.dirname(filename_1)) is False:
            os.makedirs(os.path.dirname(filename_1))
        if os.path.exists(os.path.dirname(filename_2)) is False:
            os.makedirs(os.path.dirname(filename_2))


        fig_1 = Figure()
        canvas_1 = FigureCanvas(fig_1)

        fig_2 = Figure()
        canvas_2 = FigureCanvas(fig_2)

        self.force_update()

        self.plot([fig_1, fig_2])

        if filename_1 is not None and not axes_empty(fig_1.axes):
            fig_1.savefig(filename_1)
        if filename_2 is not None and not axes_empty(fig_2.axes):
            fig_2.savefig(filename_2)",0,0
"def _syscal_write_electrode_coords(fid, spacing, N):
    
    fid.write('# X Y Z\n')
    for i in range(0, N):
        fid.write('{0} {1} {2} {3}\n'.format(i + 1, i * spacing, 0, 0))",0,0
"def _syscal_write_quadpoles(fid, quadpoles):
    
    fid.write('# A B M N\n')
    for nr, quadpole in enumerate(quadpoles):
        fid.write(
            '{0} {1} {2} {3} {4}\n'.format(
                nr, quadpole[0], quadpole[1], quadpole[2], quadpole[3]))",0,0
"def iter_alignments(dataset, cognate_sets, column='Segments', method='library'):
    
    if not isinstance(dataset, lingpy.basic.parser.QLCParser):
        wordlist = _cldf2wordlist(dataset)
        cognates = {r['Form_ID']: r for r in cognate_sets}
        wordlist.add_entries(
            'cogid',
            'lid',
            lambda x: cognates[x]['Cognateset_ID'] if x in cognates else 0)
        alm = lingpy.Alignments(
            wordlist,
            ref='cogid',
            row='parameter_id',
            col='language_id',
            segments=column.lower())
        alm.align(method=method)
        for k in alm:
            if alm[k, 'lid'] in cognates:
                cognate = cognates[alm[k, 'lid']]
                cognate['Alignment'] = alm[k, 'alignment']
                cognate['Alignment_Method'] = method
    else:
        alm = lingpy.Alignments(dataset, ref='cogid')
        alm.align(method=method)

        for cognate in cognate_sets:
            idx = cognate['ID'] or cognate['Form_ID']
            cognate['Alignment'] = alm[int(idx), 'alignment']
            cognate['Alignment_Method'] = 'SCA-' + method",0,3
"def _writeBlock(block, blockID):
    
    with open(""blockIDs.txt"", ""a"") as fp:
        fp.write(""blockID: "" + str(blockID) + ""\n"")
        sentences = """"
        for sentence in block:
            sentences += sentence+"",""
        fp.write(""block sentences: ""+sentences[:-1]+""\n"")
        fp.write(""\n"")",0,0
"def _writeSentenceInBlock(sentence, blockID, sentenceID):
    
    with open(""sentenceIDs.txt"", ""a"") as fp:
        fp.write(""sentenceID: ""+str(blockID)+""_""+str(sentenceID)+""\n"")
        fp.write(""sentence string: ""+sentence+""\n"")
        fp.write(""\n"")",0,0
"def _writeWordFromSentenceInBlock(word, blockID, sentenceID, wordID):
    
    with open(""wordIDs.txt"", ""a"") as fp:
        fp.write(""wordID: "" + str(blockID) + ""_"" +
                 str(sentenceID) + ""_"" + str(wordID) + ""\n"")
        fp.write(""wordString: "" + word + ""\n"")
        fp.write(""\n"")",0,0
"def process_bib_files(local_dir):
    
    logger = logging.getLogger(__name__)

    # check the output directory exists
    if not os.path.isdir(local_dir):
        logger.error('Output directory ""{}"" does not exist'.format(local_dir))
        sys.exit(1)

    root_blob_url = ('https://raw.githubusercontent.com/lsst/lsst-texmf/'
                     'master/texmf/bibtex/bib/')
    bib_filenames = ['books.bib', 'lsst-dm.bib', 'lsst.bib', 'refs.bib',
                     'refs_ads.bib']

    error_count = 0
    for bib_filename in bib_filenames:
        url = urllib.parse.urljoin(root_blob_url, bib_filename)
        logger.info('Downloading {}'.format(url))
        try:
            content = _get_content(url)
        except requests.HTTPError as e:
            logger.exception(str(e))
            logger.warning('Could not download {}'.format(url))
            error_count += 1
            continue

        local_filename = os.path.join(local_dir, bib_filename)
        with open(local_filename, 'w') as f:
            f.write(content)

    return error_count",0,1
"def calculate_metrics(self, analysis_set = '', analysis_directory = None, drop_missing = True, case_n_cutoff = 5, verbose = True):
        

        dataframe = self.dataframe
        if drop_missing:
            dataframe = dataframe.dropna(subset=['Predicted'])

        if self.calculate_scalar_adjustments:
            scalar_adjustment = self.scalar_adjustments[analysis_set]
        experimental_field = BenchmarkRun.get_analysis_set_fieldname('Experimental', analysis_set)

        self.metric_latex_objects.append( lr.LatexPageSection('Data tables', None, True) )
        intro_text = lr.LatexText( text = self.ddg_analysis_type_description )
        header_row = ['Statistic name', '{Value}', 'p-value']
        stats_column_format = ['l', 'S[table-format=3.2]', 'l']

        if self.include_derived_mutations:
            running_analysis_str = '\nDerived mutations in analysis are included):'
        else:
            running_analysis_str = '\nDerived mutations in analysis are omitted):'
        intro_text.add_text(running_analysis_str)
        if verbose:
            self.report(running_analysis_str, fn = colortext.message)

        classification_cutoffs_str = 'The stability classification cutoffs are: Experimental=%0.2f kcal/mol, Predicted=%0.2f energy units.' % (self.stability_classication_x_cutoff, self.stability_classication_y_cutoff)
        intro_text.add_text( classification_cutoffs_str )
        if verbose:
            self.report(classification_cutoffs_str, fn = colortext.warning)

        self.metric_latex_objects.append( intro_text )

        amino_acid_details, CAA, PAA, HAA = self.amino_acid_details, self.CAA, self.PAA, self.HAA

        # This dict is used for the print-statement below
        volume_groups = {}
        for aa_code, aa_details in amino_acid_details.iteritems():
            v = int(aa_details['van der Waals volume']) # Note: I only convert to int here to match the old script behavior and because all volumes are integer values so it does not do any harm
            volume_groups[v] = volume_groups.get(v, [])
            volume_groups[v].append(aa_code)

        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Breakdown by volume',
            'A case is considered a small-to-large (resp. large-to-small) mutation if all of the wildtype residues have a smaller (resp. larger) van der Waals volume than the corresponding mutant residue. The order is defined as %s so some cases are considered to have no change in volume e.g. MET -> LEU.' % (' < '.join([''.join(sorted(v)) for k, v in sorted(volume_groups.iteritems())]))
        ) )
        for subcase in ('XX', 'SL', 'LS'):
            subcase_dataframe = dataframe[dataframe['VolumeChange'] == subcase]
            table_header = 'Statistics - %s (%d cases)' % (BenchmarkRun.by_volume_descriptions[subcase], len(subcase_dataframe))
            if len(subcase_dataframe) >= 8:
                list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
                section_latex_objs.append( lr.LatexTable(
                    header_row,
                    list_stats,
                    column_format = stats_column_format,
                    header_text = table_header
                ))
                self.add_stored_metric_to_df(BenchmarkRun.by_volume_descriptions[subcase], len(subcase_dataframe), list_stats)
            else:
                section_latex_objs.append( lr.LatexText(
                    'Not enough data for analysis of mutations ''%s'' (at least 8 cases are required).' % BenchmarkRun.by_volume_descriptions[subcase]
                ))
        if verbose:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )


        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Mutations to alanine',
            'And mutations not to alanine'
        ))
        subcase_dataframe = dataframe[dataframe['MutantAA'] == 'A']
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - all mutations to alanine (including multiple mutations, if they are all to alanine) (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('all mutations to alanine', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[(dataframe['MutantAA'] == 'A') & (dataframe['NumberOfMutations'] == 1)]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - single mutations to alanine (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('single mutations to alanine', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[(dataframe['MutantAA'] == 'A') & (dataframe['NumberOfMutations'] != 1)]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - multiple mutations to alanine (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('multiple mutations to alanine', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[dataframe['MutantAA'] != 'A']
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - mutations to anything other than alanine (including multiple mutations that include a non-alanine mutation) (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('mutations not to alanine', len(subcase_dataframe), list_stats)
        if verbose and len(section_latex_objs) > 0:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )


        section_latex_objs = []
        subcase_dataframe = dataframe[dataframe['HasGPMutation'] == 1]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - cases with G or P (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('cases with G or P', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[dataframe['HasGPMutation'] == 0]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - cases without G or P (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('cases without G or P', len(subcase_dataframe), list_stats)

        if len(section_latex_objs) > 0:
            section_latex_objs.insert( 0, lr.LatexSubSection(
                'Separating out mutations involving glycine or proline.',
                'This cases may involve changes to secondary structure so we separate them out here.'
            ))

        if verbose and len(section_latex_objs) > 0:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )

        #### Single mutations
        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Number of mutations',
        ))
        subcase_dataframe = dataframe[dataframe['NumberOfMutations'] == 1]
        if len(subcase_dataframe) >= case_n_cutoff:
            table_header = 'Statistics - single mutations (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('single mutations', len(subcase_dataframe), list_stats)
        subcase_dataframe = dataframe[dataframe['NumberOfMutations'] > 1]
        if len(subcase_dataframe) >= case_n_cutoff:
            table_header = 'Statistics - multiple mutations (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('multiple mutations', len(subcase_dataframe), list_stats)
        # subcase_dataframe = dataframe[(dataframe.NumberOfMutations >= 2) & (dataframe.NumberOfMutations <= 5)]
        # if len(subcase_dataframe) >= case_n_cutoff:
        #     table_header = 'Statistics - 2-4 mutations (%d cases)' % len(subcase_dataframe)
        #     list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
        #     section_latex_objs.append( LatexTable(
        #         header_row,
        #         list_stats,
        #         column_format = stats_column_format,
        #         header_text = table_header
        #     ))
        #     self.add_stored_metric_to_df('2-4 mutations', len(subcase_dataframe), list_stats)
        # mutation_cutoffs = [5, 10, 20, 50, 100, 200]
        # for i, mutation_cutoff in enumerate(mutation_cutoffs):
        #     if len(mutation_cutoffs) - 1 == i:
        #         break
        #     next_cutoff = mutation_cutoffs[i+1]
        #     subcase_dataframe = dataframe[(dataframe.NumberOfMutations >= mutation_cutoff) & (dataframe.NumberOfMutations <= next_cutoff)]
        #     if len(subcase_dataframe) >= case_n_cutoff:
        #         table_header = 'Statistics - %d $<=$ number of mutations $<=$ %d (%d cases)' % (mutation_cutoff, next_cutoff, len(subcase_dataframe))
        #         list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
        #         section_latex_objs.append( LatexTable(
        #             header_row,
        #             list_stats,
        #             column_format = stats_column_format,
        #             header_text = table_header
        #         ))
        #         self.add_stored_metric_to_df('%d <= mutations<= %d' % (mutation_cutoff, next_cutoff), len(subcase_dataframe), list_stats)
        if verbose:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )
        ####

        #### Complete dataset (scaled)
        if self.calculate_scalar_adjustments:
            section_latex_objs = []
            section_latex_objs.append( lr.LatexSubSection(
                'Entire dataset using a scaling factor of 1/%.03f to improve the fraction correct metric.' % scalar_adjustment,
                'Warning: Results in this section use an averaged scaling factor to improve the value for the fraction correct metric. This scalar will vary over benchmark runs so these results should not be interpreted as performance results; they should be considered as what could be obtained if the predicted values were scaled by a ""magic"" value.'
            ))
            table_header = 'Statistics - complete dataset (scaled) (%d cases)' % len(dataframe)
            # For these statistics, we assume that we have reduced any scaling issues and use the same cutoff for the Y-axis as the user specified for the X-axis
            list_stats = format_stats(get_xy_dataset_statistics_pandas(dataframe, experimental_field, BenchmarkRun.get_analysis_set_fieldname('Predicted_adj', analysis_set), fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('complete dataset (scaled)', len(dataframe), list_stats)
            if verbose:
                self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
            self.metric_latex_objects.extend( section_latex_objs )
        ####

        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Entire dataset',
            'Overall statistics'
        ))
        table_header = 'Statistics - complete dataset (%d cases)' % len(dataframe)
        # For these statistics, we assume that we have reduced any scaling issues and use the same cutoff for the Y-axis as the user specified for the X-axis
        list_stats = format_stats(get_xy_dataset_statistics_pandas(dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
        section_latex_objs.append( lr.LatexTable(
            header_row,
            list_stats,
            column_format = stats_column_format,
            header_text = table_header
        ))
        self.add_stored_metric_to_df('complete dataset', len(dataframe), list_stats)
        if verbose:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )

        # There is probably a better way of writing the pandas code here
        record_with_most_errors = (dataframe[['PDBFileID', 'NumberOfDerivativeErrors', 'Mutations']].sort_values(by = 'NumberOfDerivativeErrors')).tail(1)
        record_index = record_with_most_errors.index.tolist()[0]
        pdb_id, num_errors, mutation_str = dataframe.loc[record_index, 'PDBFileID'], dataframe.loc[record_index, 'NumberOfDerivativeErrors'], dataframe.loc[record_index, 'Mutations']
        if num_errors > 0:
            error_detection_text = '\n\nDerivative errors were found in the run. Record #{0} - {1}, {2} - has the most amount ({3}) of derivative errors.'.format(record_index, pdb_id, mutation_str, num_errors)
            self.metric_latex_objects.append( lr.LatexText(error_detection_text, color = 'red') )
            if verbose:
                self.report(error_detection_text, fn = colortext.warning)

        # Write the analysis to file
        self.create_analysis_directory(analysis_directory)
        self.metrics_filepath = os.path.join(self.analysis_directory, '{0}_metrics.txt'.format(self.benchmark_run_name))
        write_file(self.metrics_filepath, '\n'.join([x.generate_plaintext() for x in self.metric_latex_objects]))",0,3
"def _writeFile(cls, filePath, content, encoding = None):
        

        filePath = os.path.realpath(filePath)
        log.debug(_(""Real file path to write: %s"" % filePath))

        if encoding is None:
            encoding = File.DEFAULT_ENCODING

        try:
            encodedContent = ''.join(content).encode(encoding)
        except LookupError as msg:
            raise SubFileError(_(""Unknown encoding name: '%s'."") % encoding)
        except UnicodeEncodeError:
            raise SubFileError(
                _(""There are some characters in '%(file)s' that cannot be encoded to '%(enc)s'."")
                % {""file"": filePath, ""enc"": encoding})

        tmpFilePath = ""%s.tmp"" % filePath
        bakFilePath = ""%s.bak"" % filePath
        with open(tmpFilePath, 'wb') as f:
            f.write(encodedContent)
            # ensure that all data is on disk.
            # for performance reasons, we skip os.fsync(f.fileno())
            f.flush()

        try:
            os.rename(filePath, bakFilePath)
        except FileNotFoundError:
            # there's nothing to move when filePath doesn't exist
            # note the Python bug: http://bugs.python.org/issue16074
            pass

        os.rename(tmpFilePath, filePath)

        try:
            os.unlink(bakFilePath)
        except FileNotFoundError:
            pass",0,0
"def database_clone(targetcall, databasepath, complete=False):
        
        # Create a file to store the logs; it will be used to determine if the database was downloaded and set-up
        completefile = os.path.join(databasepath, 'complete')
        # Run the system call if the database is not already downloaded
        if not os.path.isfile(completefile):
            out, err = run_subprocess(targetcall)
            if complete:
                # Create the database completeness assessment file and populate it with the out and err streams
                with open(completefile, 'w') as complete:
                    complete.write(out)
                    complete.write(err)",0,0
"def to_dataframe(self):
        
        keys = self.data[0].keys()
        column_list =[]
        for k in keys:
            key_list = []
            for i in xrange(0,len(self.data)):
                key_list.append(self.data[i][k])
            column_list.append(key_list)
        df = DataFrame(np.asarray(column_list).transpose(), columns=keys)
        for i in xrange(0,df.shape[1]):
            if is_number(df.iloc[:,i]):
                df.iloc[:,i] = df.iloc[:,i].astype(float)
        return df",0,1
"def extract(self, sampler, feature_extractor, number_of_examples_per_scale = (100, 100), similarity_thresholds = (0.5, 0.8), parallel = None, mirror = False, use_every_nth_negative_scale = 1):
    

    feature_file = self._feature_file(parallel)
    bob.io.base.create_directories_safe(self.feature_directory)

    if parallel is None or ""SGE_TASK_ID"" not in os.environ or os.environ[""SGE_TASK_ID""] == '1':
      extractor_file = os.path.join(self.feature_directory, ""Extractor.hdf5"")
      hdf5 = bob.io.base.HDF5File(extractor_file, ""w"")
      feature_extractor.save(hdf5)
      del hdf5

    total_positives, total_negatives = 0, 0

    indices = parallel_part(range(len(self)), parallel)
    if not indices:
      logger.warning(""The index range for the current parallel thread is empty."")
    else:
      logger.info(""Extracting features for images in range %d - %d of %d"", indices[0], indices[-1], len(self))

    hdf5 = bob.io.base.HDF5File(feature_file, ""w"")
    for index in indices:
      hdf5.create_group(""Image-%d"" % index)
      hdf5.cd(""Image-%d"" % index)

      logger.debug(""Processing file %d of %d: %s"", index+1, indices[-1]+1, self.image_paths[index])

      # load image
      image = bob.io.base.load(self.image_paths[index])
      if image.ndim == 3:
        image = bob.ip.color.rgb_to_gray(image)
      # get ground_truth bounding boxes
      ground_truth = self.bounding_boxes[index]

      # collect image and GT for originally and mirrored image
      images = [image] if not mirror else [image, bob.ip.base.flop(image)]
      ground_truths = [ground_truth] if not mirror else [ground_truth, [gt.mirror_x(image.shape[1]) for gt in ground_truth]]
      parts = ""om""

      # now, sample
      scale_counter = -1
      for image, ground_truth, part in zip(images, ground_truths, parts):
        for scale, scaled_image_shape in sampler.scales(image):
          scale_counter += 1
          scaled_gt = [gt.scale(scale) for gt in ground_truth]
          positives = []
          negatives = []
          # iterate over all possible positions in the image
          for bb in sampler.sample_scaled(scaled_image_shape):
            # check if the patch is a positive example
            positive = False
            negative = True
            for gt in scaled_gt:
              similarity = bb.similarity(gt)
              if similarity > similarity_thresholds[1]:
                positive = True
                break
              if similarity > similarity_thresholds[0]:
                negative = False
                break

            if positive:
              positives.append(bb)
            elif negative and scale_counter % use_every_nth_negative_scale == 0:
              negatives.append(bb)

          # per scale, limit the number of positive and negative samples
          positives = [positives[i] for i in quasi_random_indices(len(positives), number_of_examples_per_scale[0])]
          negatives = [negatives[i] for i in quasi_random_indices(len(negatives), number_of_examples_per_scale[1])]

          # extract features
          feature_extractor.prepare(image, scale)
          # .. negative features
          if negatives:
            negative_features = numpy.zeros((len(negatives), feature_extractor.number_of_features), numpy.uint16)
            for i, bb in enumerate(negatives):
              feature_extractor.extract_all(bb, negative_features, i)
            hdf5.set(""Negatives-%s-%.5f"" % (part,scale), negative_features)
            total_negatives += len(negatives)

          # positive features
          if positives:
            positive_features = numpy.zeros((len(positives), feature_extractor.number_of_features), numpy.uint16)
            for i, bb in enumerate(positives):
              feature_extractor.extract_all(bb, positive_features, i)
            hdf5.set(""Positives-%s-%.5f"" % (part,scale), positive_features)
            total_positives += len(positives)
      # cd backwards after each image
      hdf5.cd("".."")

    hdf5.set(""TotalPositives"", total_positives)
    hdf5.set(""TotalNegatives"", total_negatives)",0,1
"def writeJsonZipfile(filelike, data, compress=True, mode='w', name='data'):
    
    zipcomp = zipfile.ZIP_DEFLATED if compress else zipfile.ZIP_STORED
    with zipfile.ZipFile(filelike, mode, allowZip64=True) as containerFile:
        containerFile.writestr(name, json.dumps(data, cls=MaspyJsonEncoder),
                               zipcomp
                               )",0,0
"def _dumpArrayToFile(filelike, array):
    
    bytedata = array.tobytes('C')
    start = filelike.tell()
    end = start + len(bytedata)
    metadata = {'start': start, 'end': end, 'size': array.size,
                'dtype': array.dtype.name
                }
    filelike.write(bytedata)
    return metadata",0,0
"def _dumpNdarrayToFile(filelike, ndarray):
    
    bytedata = ndarray.tobytes('C')
    start = filelike.tell()
    end = start + len(bytedata)
    metadata = {'start': start, 'end': end, 'size': ndarray.size,
                'dtype': ndarray.dtype.name, 'shape': ndarray.shape
                }
    filelike.write(bytedata)
    return metadata",0,0
"def writeLogToFile(self):
        
        if not os.path.exists(self.logFolder):
            os.mkdir(self.logFolder)

        with open(self.logFile, mode='a') as f:
            f.write('\n\n' + self.log)",0,0
"def write_bibtex_dict(stream, entries):
    
    from bibtexparser.bwriter import BibTexWriter

    writer = BibTexWriter()
    writer.indent = '  '
    writer.entry_separator = ''
    first = True

    for rec in entries:
        if first:
            first = False
        else:
            stream.write(b'\n')
        stream.write(writer._entry_to_bibtex(rec).encode('utf8'))",0,0
"def write_geoff(discoursegraph, output_file):
    
    if isinstance(output_file, str):
        with open(output_file, 'w') as outfile:
            outfile.write(convert_to_geoff(discoursegraph))
    else:  # output_file is a file object
        output_file.write(convert_to_geoff(discoursegraph))",0,0
"def write_graphml(docgraph, output_file):
    
    dg_copy = deepcopy(docgraph)
    layerset2str(dg_copy)
    attriblist2str(dg_copy)
    remove_root_metadata(dg_copy)
    nx_write_graphml(dg_copy, output_file)",0,0
"def write_exb(docgraph, output_file):
    
    exmaralda_file = ExmaraldaFile(docgraph)
    assert isinstance(output_file, (str, file))
    if isinstance(output_file, str):
        path_to_file = os.path.dirname(output_file)
        if not os.path.isdir(path_to_file):
            create_dir(path_to_file)
        exmaralda_file.write(output_file)
    else:  # output_file is a file object
        output_file.write(exmaralda_file.__str__())",0,0
"def write_conll(docgraph, output_file, coreference_layer=None,
                markable_layer=None):
    
    if markable_layer is None:
        markable_layer = docgraph.ns+':markable'
    conll_file = Conll2009File(docgraph,
                               coreference_layer=coreference_layer,
                               markable_layer=markable_layer)
    assert isinstance(output_file, (str, file))
    if isinstance(output_file, str):
        path_to_file = os.path.dirname(output_file)
        if not os.path.isdir(path_to_file):
            create_dir(path_to_file)
        conll_file.write(output_file)
    else:  # output_file is a file object
        output_file.write(conll_file.__str__())",0,0
"def write_gexf(docgraph, output_file):
    
    dg_copy = deepcopy(docgraph)
    remove_root_metadata(dg_copy)
    layerset2str(dg_copy)
    attriblist2str(dg_copy)
    nx_write_gexf(dg_copy, output_file)",0,0
"def writeFile(filename, content, encoding=None):
    
    if not content:
        raise OSError(""empty content for file %s"" % filename)

    def getfp(filename, encoding):
        
        if encoding:
            return codecs.open(filename, 'w', encoding)
        return open(filename, 'wb')

    try:
        with getfp(filename, encoding) as fp:
            fp.write(content)
            fp.flush()
            os.fsync(fp.fileno())
            size = os.path.getsize(filename)
            if size == 0:
                raise OSError(""empty file %s"" % filename)
    except Exception:
        if os.path.isfile(filename):
            os.remove(filename)
        raise
    else:
        out.info(u""Saved %s (%s)."" % (filename, strsize(size)))",0,0
"def update_db():
    
    logger = get_logger(PROCESS_SCHEDULER)
    managed_process_dao = ManagedProcessDao(logger)
    managed_process_dao.clear()

    for process_name, process_entry in context.process_context.items():
        if not isinstance(process_entry, ManagedProcessEntry):
            continue

        managed_process_dao.update(process_entry)
        logger.info('Updated DB with process entry {0} from the context.'.format(process_entry.key))",0,3
"def main(args=None):
    
    vinfo = sys.version_info
    if not (vinfo >= (2, 7)):
        raise SystemError('Python interpreter version >= 2.7 required, '
                          'found %d.%d instead.' % (vinfo.major, vinfo.minor))

    if args is None:
        parser = get_argument_parser()
        args = parser.parse_args()

    gene2acc_file = args.gene2acc_file
    output_file = args.output_file
    log_file = args.log_file
    quiet = args.quiet
    verbose = args.verbose

    # configure logger
    log_stream = sys.stdout
    if output_file == '-':
        log_stream = sys.stderr

    logger = misc.get_logger(log_stream=log_stream, log_file=log_file,
                             quiet=quiet, verbose=verbose)

    entrez2gene = read_gene2acc(gene2acc_file, logger)
    write_entrez2gene(output_file, entrez2gene, logger)

    return 0",0,0
"def concatenate_fastas(output_fna_clustered,
                       output_fna_failures,
                       output_concat_filepath):
    

    output_fp = open(output_concat_filepath, ""w"")

    for label, seq in parse_fasta(open(output_fna_clustered, ""U"")):
        output_fp.write("">%s\n%s\n"" % (label, seq))
    for label, seq in parse_fasta(open(output_fna_failures, ""U"")):
        output_fp.write("">%s\n%s\n"" % (label, seq))

    return output_concat_filepath",0,0
"def parse_usearch61_failures(seq_path,
                             failures,
                             output_fasta_fp):
    

    parsed_out = open(output_fasta_fp, ""w"")

    for label, seq in parse_fasta(open(seq_path), ""U""):
        curr_label = label.split()[0]
        if curr_label in failures:
            parsed_out.write("">%s\n%s\n"" % (label, seq))
    parsed_out.close()
    return output_fasta_fp",0,0
"def experiment_details_csv(request, pk):
    
    experiment = get_object_or_404(Experiment, pk=pk)
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=experiment.csv'
    writer = csv.writer(response)
    writer.writerow([""Animal"",""Cage"", ""Strain"", ""Genotype"", ""Gender"",""Age"", ""Assay"", ""Values"", ""Feeding"", ""Experiment Date"", ""Treatment""])
    for measurement in experiment.measurement_set.iterator():
        writer.writerow([
			measurement.animal,
            measurement.animal.Cage,
            measurement.animal.Strain,
			measurement.animal.Genotype, 
			measurement.animal.Gender,
			measurement.age(), 
			measurement.assay, 
			measurement.values, 
            measurement.experiment.feeding_state,
            measurement.experiment.date,
			measurement.animal.treatment_set.all()
			])
    return response",0,0
"def aging_csv(request):
    
    animal_list = Animal.objects.all()
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=aging.csv'
    writer = csv.writer(response)
    writer.writerow([""Animal"", ""Strain"", ""Genotype"", ""Gender"", ""Age"", ""Death"", ""Alive""])
    for animal in animal_list.iterator():
        writer.writerow([
            animal.MouseID, 
            animal.Strain, 
            animal.Genotype, 
            animal.Gender,
            animal.age(),
            animal.Cause_of_Death,
            animal.Alive            
            ])
    return response",0,0
"def litters_csv(request):
    
    animal_list = Animal.objects.all()
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=litters.csv'
    writer = csv.writer(response)
    writer.writerow([""Born"", ""Breeding"", ""Strain""])
    for animal in animal_list:
        writer.writerow([
            animal.Born,
            animal.Breeding,
            animal.Strain
            ])
    return response",0,0
"def bencode(data, f=None):
	
	if f is None:
		f = BytesIO()
		_bencode_to_file(data, f)
		return f.getvalue()
	else:
		_bencode_to_file(data, f)",0,0
"def sync_file(self, clusters):
        
        logger.info(""Updating HAProxy config file."")
        if not self.restart_required:
            self.sync_nodes(clusters)

        version = self.control.get_version()

        with open(self.haproxy_config_path, ""w"") as f:
            f.write(self.config_file.generate(clusters, version=version))

        if self.restart_required:
            with self.restart_lock:
                self.restart()",0,0
"def write_tex():
    
    datadir = livvkit.index_dir
    outdir = os.path.join(datadir, ""tex"")
    print(outdir)
    # functions.mkdir_p(outdir)

    data_files = glob.glob(datadir + ""/**/*.json"", recursive=True)

    for each in data_files:
        data = functions.read_json(each)
        tex = translate_page(data)
        outfile = os.path.join(outdir, os.path.basename(each).replace('json', 'tex'))
        with open(outfile, 'w') as f:
            f.write(tex)",0,0
"def run_cmd(cmd, log='log.log', cwd='.', stdout=sys.stdout, bufsize=1, encode='utf-8'):
  
  logfile = '%s/%s' % (cwd, log)
  
  if os.path.exists(logfile):
    os.remove(logfile)
  proc_args = {
    'stdout': subprocess.PIPE,
    'stderr': subprocess.PIPE,
    'cwd': cwd,
    'universal_newlines': True
  }

  proc = subprocess.Popen(cmd, **proc_args)
  
  while True:
    line = proc.stdout.readline()
    if proc.poll() is None:
      stdout.write(line)
    else:
      break
  out, err = proc.communicate()

  with open(logfile, 'w') as f:
    if out:
      f.write(out)
    else:
      f.write(err)",0,0
"def write_to_file(path_file, callback):
        
        try:
            with open(path_file, 'r+', encoding='utf-8') as f:
                content = callback(f.read())
                f.seek(0)
                f.write(content)
                f.truncate()
        except Exception as e:
            raise Exception(
                'unable to minify file %(file)s, exception was %(exception)r' % {
                    'file': path_file,
                    'exception': e,
                }
            )",0,0
"def write_to(self, f):
        
        f = CodeWriter(f)

        # Write all header files
        headers = set()
        for plugin in self.plugins:
            headers = headers.union(plugin.header_files())
        for header in headers:
            f.writeln(""#include <{}>"".format(header))
        f.writeln("""")

        # Write all declarations
        for plugin in self.plugins:
            plugin.write_declarations(f)
        f.writeln("""")

        # Write the setup function
        with f._function(""void"", ""setup""):
            # Setup all plugins
            f.writeln(""// Setup all plugins"")
            for plugin in self.plugins:
                plugin.setup_plugin(f)
            # Setup all modules
            f.writeln(""// Setup all modules"")
            for mod_name in self.modules.keys():
                for plugin in self.plugins:
                    plugin.setup_module(mod_name, f)
        f.writeln("""")

        # Write the loop function
        with f._function(""void"", ""loop""):
            # Update all plugins
            f.writeln(""// Update all plugins"")
            for plugin in self.plugins:
                plugin.update_plugin(f)
            # Update all modules
            f.writeln(""// Update all modules"")
            for mod_name, mod_info in self.modules.items():
                for plugin in self.plugins:
                    plugin.update_module(mod_name, f)

                # Read all module outputs
                for output_name in mod_info[""outputs""]:
                    cond = ""{mod_name}.get_{output_name}({msg_name})"".format(
                        mod_name=mod_name, output_name=output_name,
                        msg_name=self.msg_name(mod_name, output_name)
                    )
                    with f._if(cond):
                        for plugin in self.plugins:
                            plugin.on_output(mod_name, output_name, f)

            # Read statuses of all modules
            f.writeln(""// Read statuses of all modules"")
            with f._if(""should_read_statuses()""):
                for plugin in self.plugins:
                    plugin.start_read_module_status(f)
                for mod_name in self.modules:
                    for plugin in self.plugins:
                        plugin.read_module_status(mod_name, f)
                for plugin in self.plugins:
                    plugin.end_read_module_status(f)",0,0
"def download_channel_image_file(self, channel_name, plate_name,
            well_name, well_pos_y, well_pos_x, cycle_index,
            tpoint, zplane, correct, align, directory):
        
        response = self._download_channel_image(
            channel_name, plate_name, well_name, well_pos_y, well_pos_x,
            cycle_index=cycle_index, tpoint=tpoint, zplane=zplane,
            correct=correct, align = align
        )
        data = response.content
        filename = self._extract_filename_from_headers(response.headers)
        self._write_file(directory, os.path.basename(filename), data)",0,1
"def download_segmentation_image_file(self, mapobject_type_name,
            plate_name, well_name, well_pos_y, well_pos_x, tpoint, zplane, align,
            directory):
        
        response = self._download_segmentation_image(
            mapobject_type_name, plate_name, well_name, well_pos_y, well_pos_x,
            tpoint, zplane, align
        )
        image = np.array(response, np.int32)
        if np.max(image) >= 2**16:
            raise ValueError(
                'Cannot store segmentation image as PNG file because it '
                'contains more than 65536 objects.'
            )
        filename = '{0}_{1}_{2}_y{3:03d}_x{4:03d}_z{5:03d}_t{6:03d}_{7}.png'.format(
            self.experiment_name, plate_name, well_name, well_pos_y,
            well_pos_x, zplane, tpoint, mapobject_type_name
        )
        data = cv2.imencode(filename, image.astype(np.uint16))[1]
        self._write_file(directory, filename, data)",0,1
"def download_workflow_description_file(self, filename):
        
        description = self.download_workflow_description()
        logger.info('write workflow description to file: %s', filename)
        with open(filename, 'w') as f:
            content = yaml.safe_dump(
                description, default_flow_style=False, explicit_start=True
            )
            f.write(content)",0,0
"def write(self, config_dir=None, config_name=None, codec=None):
        
        # get name of config directory
        if not config_dir:
            config_dir = self._meta_config_dir
            if not config_dir:
                raise IOError(""config_dir not set"")

        # get name of config file
        if not config_name:
            config_name = self._defaults.get('config_name', None)
        if not config_name:
            raise KeyError('config_name not set')

        if codec:
            codec = munge.get_codec(codec)()
        else:
            codec = munge.get_codec(self._defaults['codec'])()

        config_dir = os.path.expanduser(config_dir)
        if not os.path.exists(config_dir):
            os.mkdir(config_dir)

        codec.dumpu(self.data, os.path.join(config_dir, 'config.' + codec.extension))",0,0
"def close(self):
        
        for i in range(0, len(active_canvases)):
            if active_canvases[i] == self:
                del active_canvases[i]
                return",0,0
"def write_csv(path, data):
    

    fd = _try_open_file(path, 'w',
                        'The first argument must be a pathname or an object that supports write() method')
    for v in data:
        fd.write("","".join([str(x) for x in v]))
        fd.write(""\n"")
    _try_close_file(fd, path)",0,0
"def stderrHandler(level, object, category, file, line, message):
    

    o = """"
    if object:
        o = '""' + object + '""'

    where = ""(%s:%d)"" % (file, line)

    # level   pid     object   cat      time
    # 5 + 1 + 7 + 1 + 32 + 1 + 17 + 1 + 15 == 80
    safeprintf(sys.stderr, '%s [%5d] %-32s %-17s %-15s ',
               getFormattedLevelName(level), os.getpid(), o, category,
               time.strftime(""%b %d %H:%M:%S""))

    try:
        safeprintf(sys.stderr, '%-4s %s %s\n', """", message, where)
    except UnicodeEncodeError:
        # this can happen if message is a unicode object, convert it back into
        # a string using the UTF-8 encoding
        message = message.encode('UTF-8')
        safeprintf(sys.stderr, '%-4s %s %s\n', """", message, where)

    sys.stderr.flush()",0,2
"def _write(self, what):
        
        try:
            open(self.pipefile, ""a"").write(what)
        except:
            print(""Error writing to %s:"" % self.pipefile)
            traceback.print_exc()",0,0
"def run(self):
    
    empty = False
    while not empty:
      try:
        # Grab fields
        url = self.genre_urls.get()
        namestamp = ""{}.csv"".format(str(int(round(time.time() * 1000000))))
        # GET request
        self.logger.info('Attempting to request %s', url)
        self.crawler.set_url(url)
        series = self.crawler.get_series()
        self.logger.info('Attempting to write %s', url)
        # Grab writer -> writes series
        csv_dir = './{}/{}'.format(self.directory, namestamp)
        writer = csv.writer(open(csv_dir, 'wb'))
        writer.writerow(Series.fields)
        for s in series:
          writer.writerow(s.to_line())
        self.logger.info('Wrote %s', namestamp)
      except Exception, e: # pylint: disable=W0703
        print e
      finally:
        self.genre_urls.task_done()
        empty = self.genre_urls.empty()",0,0
"def pump(fin, fout):
    
    while True:
        tmp = fin.read(4096)
        if not tmp:
            break
        fout.write(tmp)",0,0
"def get_metrics_rollups_queue(self, name, queue_name, metric):
        
        response = self._perform_get(
            self._get_get_metrics_rollup_queue_path(name, queue_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )",3,3
"def get_metrics_rollups_topic(self, name, topic_name, metric):
        
        response = self._perform_get(
            self._get_get_metrics_rollup_topic_path(name, topic_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )",3,3
"def get_metrics_rollups_notification_hub(self, name, hub_name, metric):
        
        response = self._perform_get(
            self._get_get_metrics_rollup_hub_path(name, hub_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )",3,3
"def get_metrics_rollups_relay(self, name, relay_name, metric):
        
        response = self._perform_get(
            self._get_get_metrics_rollup_relay_path(name, relay_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )",3,3
"def delete_cloud_service(self, cloud_service_id):
        
        _validate_not_none('cloud_service_id', cloud_service_id)
        path = self._get_cloud_services_path(cloud_service_id)
        return self._perform_delete(path, as_async=True)",3,3
"def get_job_collection(self, cloud_service_id, job_collection_id):
        
        _validate_not_none('cloud_service_id', cloud_service_id)
        _validate_not_none('job_collection_id', job_collection_id)

        path = self._get_job_collection_path(
            cloud_service_id, job_collection_id)

        return self._perform_get(path, Resource)",3,3
"def get_job(self, cloud_service_id, job_collection_id, job_id):
        
        _validate_not_none('cloud_service_id', cloud_service_id)
        _validate_not_none('job_collection_id', job_collection_id)
        _validate_not_none('job_id', job_id)

        path = self._get_job_collection_path(
            cloud_service_id, job_collection_id, job_id)

        self.content_type = ""application/json""
        payload = self._perform_get(path).body.decode()
        return json.loads(payload)",3,3
"def insert_and_get(self, **fields):
        

        if not self.conflict_target and not self.conflict_action:
            # no special action required, use the standard Django create(..)
            return super().create(**fields)

        compiler = self._build_insert_compiler([fields])
        rows = compiler.execute_sql(return_id=False)

        columns = rows[0]

        # get a list of columns that are officially part of the model and preserve the fact that the attribute name
        # might be different than the database column name
        model_columns = {}
        for field in self.model._meta.local_concrete_fields:
            model_columns[field.column] = field.attname

        # strip out any columns/fields returned by the db that
        # are not present in the model
        model_init_fields = {}
        for column_name, column_value in columns.items():
            try:
                model_init_fields[model_columns[column_name]] = column_value
            except KeyError:
                pass

        return self.model(**model_init_fields)",3,3
"def upsert_and_get(self, conflict_target: List, fields: Dict, index_predicate: str=None):
        

        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)
        return self.insert_and_get(**fields)",3,3
"def upsert_and_get(self, conflict_target: List, fields: Dict, index_predicate: str=None):
        

        return self.get_queryset().upsert_and_get(conflict_target, fields, index_predicate)",3,3
"def config(self):
        
        if self._config is not None:
            return self._config
        try:
            return current_app.upload_set_config[self.name]
        except AttributeError:
            raise RuntimeError(""cannot access configuration outside request"")",3,1
"def url(self, filename):
        
        base = self.config.base_url
        if base is None:
            return url_for('_uploads.uploaded_file', setname=self.name,
                           filename=filename, _external=True)
        else:
            return base + filename",3,1
"def replay_bundle(
            self,
            transaction,
            depth=3,
            min_weight_magnitude=None,
    ):
        # type: (TransactionHash, int, Optional[int]) -> dict
        
        if min_weight_magnitude is None:
            min_weight_magnitude = self.default_min_weight_magnitude

        return extended.ReplayBundleCommand(self.adapter)(
            transaction=transaction,
            depth=depth,
            minWeightMagnitude=min_weight_magnitude,
        )",3,2
"def _connect(obj):
    
    from .columns import MODELS
    if isinstance(obj, MODELS['Model']):
        obj = obj.__class__
    if hasattr(obj, '_conn'):
        return obj._conn
    if hasattr(obj, 'CONN'):
        return obj.CONN
    return get_connection()",3,3
"def __get_config_template_path(self) -> str:
        
        filename = resource_filename(
            Requirement.parse(package_name),
            template_path + config_filename)
        return filename",3,3
"def get_mems_of_org(self):
        
        print 'Getting members\' emails.'
        for member in self.org_retrieved.iter_members():
            login = member.to_json()['login']
            user_email = self.logged_in_gh.user(login).to_json()['email']
            if user_email is not None:
                self.emails[login] = user_email
            else:#user has no public email
                self.emails[login] = 'none'
            #used for sorting regardless of case
            self.logins_lower[login.lower()] = login",3,3
"def get_year_commits(self, username='', password='', organization='llnl', force=True):
        
        date = str(datetime.date.today())
        file_path =  ('year_commits.csv')
        if force or not os.path.isfile(file_path):
            my_github.login(username, password)
            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1
            print 'Rate Limit: ' + str(calls_beginning)
            my_github.get_org(organization)
            my_github.repos(building_stats=True)
            print ""Letting GitHub build statistics.""
            time.sleep(30)
            print ""Trying again.""
            my_github.repos(building_stats=False)
            my_github.calc_total_commits(starting_commits=35163)
            my_github.write_to_file()
            calls_remaining = self.logged_in_gh.ratelimit_remaining
            calls_used = calls_beginning - calls_remaining
            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\nUsed '
                + str(calls_used) + ' API calls.')",3,0
"def data(self):
        
        self.batch_name_value = self.ui.batch_name_value.text()
        self.saa_values = self.ui.saa_values.text()
        self.sza_values = self.ui.sza_values.text()
        self.p_values = self.ui.p_values.text()
        self.x_value = self.ui.x_value.text()
        self.y_value = self.ui.y_value.text()
        self.g_value = self.ui.g_value.text()
        self.s_value = self.ui.s_value.text()
        self.z_value = self.ui.z_value.text()
        self.wavelength_values = self.ui.wavelength_values.text()
        self.verbose_value = self.ui.verbose_value.text()
        self.phytoplankton_path = self.ui.phyto_path.text()
        self.bottom_path = self.ui.bottom_path.text()
        self.executive_path = self.ui.exec_path.text()
        self.nb_cpu = self.ui.nb_cpu.currentText()
        self.report_parameter_value = str(self.ui.report_parameter_value.text())",3,3
"def mouse_move(self, event):
        
        if (self.ui.tabWidget.currentIndex() == TabWidget.NORMAL_MODE):
            self.posX = event.xdata
            self.posY = event.ydata

            self.graphic_target(self.posX, self.posY)",3,2
"def _encode_chunk(self, data, index):
        
        chunk = self._get_chunk(data, index)
        return self._encode_long(self._chunk_to_long(chunk))",3,2
"def fromStringProto(self, inString, proto):
        
        value, = amp.AmpList.fromStringProto(self, inString, proto)
        return value",3,3
"def _to_corrected_pandas_type(dt):
    
    import numpy as np
    if type(dt) == ByteType:
        return np.int8
    elif type(dt) == ShortType:
        return np.int16
    elif type(dt) == IntegerType:
        return np.int32
    elif type(dt) == FloatType:
        return np.float32
    else:
        return None",3,3
"def _get_function_transitions(self,
                                  expression: Union[str, List],
                                  expected_type: PredicateType) -> Tuple[List[str],
                                                                         PredicateType,
                                                                         List[PredicateType]]:
        
        # This first block handles getting the transitions and function type (and some error
        # checking) _just for the function itself_.  If this is a simple function, this is easy; if
        # it's a higher-order function, it involves some recursion.
        if isinstance(expression, list):
            # This is a higher-order function.  TODO(mattg): we'll just ignore type checking on
            # higher-order functions, for now.
            transitions, function_type = self._get_transitions(expression, None)
        elif expression in self._functions:
            name = expression
            function_types = self._function_types[expression]
            if len(function_types) != 1:
                raise ParsingError(f""{expression} had multiple types; this is not yet supported for functions"")
            function_type = function_types[0]
            transitions = [f'{function_type} -> {name}']
        else:
            if isinstance(expression, str):
                raise ParsingError(f""Unrecognized function: {expression[0]}"")
            else:
                raise ParsingError(f""Unsupported expression type: {expression}"")
        if not isinstance(function_type, FunctionType):
            raise ParsingError(f'Zero-arg function or constant called with arguments: {name}')

        # Now that we have the transitions for the function itself, and the function's type, we can
        # get argument types and do the rest of the transitions.
        argument_types = function_type.argument_types
        return_type = function_type.return_type
        right_side = f'[{function_type}, {"", "".join(str(arg) for arg in argument_types)}]'
        first_transition = f'{return_type} -> {right_side}'
        transitions.insert(0, first_transition)
        if expected_type and expected_type != return_type:
            raise ParsingError(f'{expression} did not have expected type {expected_type} '
                               f'(found {return_type})')
        return transitions, return_type, argument_types",3,3
"def attr_dict(self):
        
        size = mx_uint()
        pairs = ctypes.POINTER(ctypes.c_char_p)()
        f_handle = _LIB.MXSymbolListAttr
        check_call(f_handle(self.handle, ctypes.byref(size), ctypes.byref(pairs)))
        ret = {}
        for i in range(size.value):
            name, key = py_str(pairs[i * 2]).split('$')
            val = py_str(pairs[i * 2 + 1])
            if name not in ret:
                ret[name] = {}
            ret[name][key] = val
        return ret",3,1
"def get_xgboost_json(model):
    
    fnames = model.feature_names
    model.feature_names = None
    json_trees = model.get_dump(with_stats=True, dump_format=""json"")
    model.feature_names = fnames

    # this fixes a bug where XGBoost can return invalid JSON
    json_trees = [t.replace("": inf,"", "": 1000000000000.0,"") for t in json_trees]
    json_trees = [t.replace("": -inf,"", "": -1000000000000.0,"") for t in json_trees]

    return json_trees",3,1
"def String(self, off):
        
        N.enforce_number(off, N.UOffsetTFlags)
        off += encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)
        start = off + N.UOffsetTFlags.bytewidth
        length = encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)
        return bytes(self.Bytes[start:start+length])",3,3
"def _get_daily_window_data(self,
                               assets,
                               field,
                               days_in_window,
                               extra_slot=True):
        
        bar_count = len(days_in_window)
        # create an np.array of size bar_count
        dtype = float64 if field != 'sid' else int64
        if extra_slot:
            return_array = np.zeros((bar_count + 1, len(assets)), dtype=dtype)
        else:
            return_array = np.zeros((bar_count, len(assets)), dtype=dtype)

        if field != ""volume"":
            # volumes default to 0, so we don't need to put NaNs in the array
            return_array[:] = np.NAN

        if bar_count != 0:
            data = self._history_loader.history(assets,
                                                days_in_window,
                                                field,
                                                extra_slot)
            if extra_slot:
                return_array[:len(return_array) - 1, :] = data
            else:
                return_array[:len(data)] = data
        return return_array",3,3
"def get_command(self, ctx, cmd_name):
        

        if cmd_name not in self.all_cmds:
            return None
        return EventTypeSubCommand(self.events_lib, cmd_name, self.all_cmds[cmd_name])",3,3
"def get_command(self, ctx, cmd_name):

        

        if cmd_name not in self.subcmd_definition:
            return None
        parameters = []
        for param_name in self.subcmd_definition[cmd_name][self.TAGS].keys():
            default = self.subcmd_definition[cmd_name][self.TAGS][param_name][""default""]
            parameters.append(click.Option(
                [""--{}"".format(param_name)],
                default=default,
                help=""Specify the {} name you'd like, otherwise the default = {}"".format(param_name, default)
            ))

        command_callback = functools.partial(self.cmd_implementation,
                                             self.events_lib,
                                             self.top_level_cmd_name,
                                             cmd_name)
        cmd = click.Command(name=cmd_name,
                            short_help=self.subcmd_definition[cmd_name][""help""],
                            params=parameters,
                            callback=command_callback)

        cmd = debug_option(cmd)
        return cmd",3,2
"def _output_format(out,
                   operation):
    
    return [s.split()[1:] for s in out
            if s.startswith(operation)]",3,0
"def _get_all_groups():
    
    with salt.utils.winapi.Com():
        nt = win32com.client.Dispatch('AdsNameSpaces')
    results = nt.GetObject('', 'WinNT://.')
    results.Filter = ['group']
    return results",3,3
"def _solver(self):
        
        if self._stored_solver is not None:
            return self._stored_solver

        track = o.CONSTRAINT_TRACKING_IN_SOLVER in self.state.options
        approximate_first = o.APPROXIMATE_FIRST in self.state.options

        if o.STRINGS_ANALYSIS in self.state.options:
            if 'smtlib_cvc4' in backend_manager.backends._backends_by_name:
                our_backend = backend_manager.backends.smtlib_cvc4
            elif 'smtlib_z3' in backend_manager.backends._backends_by_name:
                our_backend = backend_manager.backends.smtlib_z3
            elif 'smtlib_abc' in backend_manager.backends._backends_by_name:
                our_backend = backend_manager.backends.smtlib_abc
            else:
                raise ValueError(""Could not find suitable string solver!"")
            if o.COMPOSITE_SOLVER in self.state.options:
                self._stored_solver = claripy.SolverComposite(
                    template_solver_string=claripy.SolverCompositeChild(backend=our_backend, track=track)
                )
        elif o.ABSTRACT_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverVSA()
        elif o.SYMBOLIC in self.state.options and o.REPLACEMENT_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverReplacement(auto_replace=False)
        elif o.SYMBOLIC in self.state.options and o.CACHELESS_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverCacheless(track=track)
        elif o.SYMBOLIC in self.state.options and o.COMPOSITE_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverComposite(track=track)
        elif o.SYMBOLIC in self.state.options and any(opt in self.state.options for opt in o.approximation):
            self._stored_solver = claripy.SolverHybrid(track=track, approximate_first=approximate_first)
        elif o.HYBRID_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverHybrid(track=track, approximate_first=approximate_first)
        elif o.SYMBOLIC in self.state.options:
            self._stored_solver = claripy.Solver(track=track)
        else:
            self._stored_solver = claripy.SolverConcrete()

        return self._stored_solver",3,3
"def _getAttrGroupMember(self, attrGroup, attr):
        
        method_name = '%s_%s' % (attrGroup, attr)
        return getattr(self, method_name)",3,3
"def _recursive_getattr(module, path):
  
  if ""."" not in path:
    return getattr(module, path)
  else:
    first, rest = path.split(""."", 1)
    return _recursive_getattr(getattr(module, first), rest)",3,3
"def run(self, run_config, controller, max_game_steps=0, max_episodes=0,
          game_steps_per_episode=0, save_replay=False):
    
    is_replay = (controller.status == remote_controller.Status.in_replay)
    total_game_steps = 0
    start_time = time.time()
    num_episodes = 0

    try:
      while True:
        self.init(controller.game_info(), controller.data())
        episode_steps = 0
        num_episodes += 1

        controller.step()

        while True:
          total_game_steps += self._step_mul
          episode_steps += self._step_mul
          frame_start_time = time.time()

          obs = controller.observe()
          self.render(obs)

          if obs.player_result:
            break

          cmd = self.get_actions(run_config, controller)
          if cmd == ActionCmd.STEP:
            pass
          elif cmd == ActionCmd.QUIT:
            if not is_replay and save_replay:
              self.save_replay(run_config, controller)
            return
          elif cmd == ActionCmd.RESTART:
            break
          else:
            raise Exception(""Unexpected command: %s"" % cmd)

          controller.step(self._step_mul)

          if max_game_steps and total_game_steps >= max_game_steps:
            return

          if game_steps_per_episode and episode_steps >= game_steps_per_episode:
            break

          with sw(""sleep""):
            elapsed_time = time.time() - frame_start_time
            time.sleep(max(0, 1 / self._fps - elapsed_time))

        if is_replay:
          break

        if save_replay:
          self.save_replay(run_config, controller)

        if max_episodes and num_episodes >= max_episodes:
          break

        print(""Restarting"")
        controller.restart()
    except KeyboardInterrupt:
      pass
    finally:
      self.close()
      elapsed_time = time.time() - start_time
      print(""took %.3f seconds for %s steps: %.3f fps"" %
            (elapsed_time, total_game_steps, total_game_steps / elapsed_time))",3,2
"def rison(schema=None):
    

    def _rison(f):
        def wraps(self, *args, **kwargs):
            value = request.args.get(API_URI_RIS_KEY, None)
            kwargs[""rison""] = dict()
            if value:
                try:
                    kwargs[""rison""] = prison.loads(value)
                except prison.decoder.ParserException:
                    return self.response_400(message=""Not a valid rison argument"")
            if schema:
                try:
                    jsonschema.validate(instance=kwargs[""rison""], schema=schema)
                except jsonschema.ValidationError as e:
                    return self.response_400(
                        message=""Not a valid rison schema {}"".format(e)
                    )
            return f(self, *args, **kwargs)

        return functools.update_wrapper(wraps, f)

    return _rison",3,2
"def _get_container_id(self, labels):
        
        namespace = CadvisorPrometheusScraperMixin._get_container_label(labels, ""namespace"")
        pod_name = CadvisorPrometheusScraperMixin._get_container_label(labels, ""pod_name"")
        container_name = CadvisorPrometheusScraperMixin._get_container_label(labels, ""container_name"")
        return self.pod_list_utils.get_cid_by_name_tuple((namespace, pod_name, container_name))",3,3
"def count(self, X):
        
        # Sum on axis 0 (by columns), each column is a word
        # Convert the matrix to an array
        # Squeeze to remove the 1 dimension objects (like ravel)
        return np.squeeze(np.asarray(X.sum(axis=0)))",3,3
"def getNeighbouringDevices(self):
        
        print '%s call getNeighbouringDevices' % self.port
        neighbourList = []

        # get parent info
        parentAddr = self.getParentAddress()
        if parentAddr != 0:
            neighbourList.append(parentAddr)

        # get ED/SED children info
        childNeighbours = self.getChildrenInfo()
        if childNeighbours != None and len(childNeighbours) > 0:
            for entry in childNeighbours:
                neighbourList.append(entry)

        # get neighboring routers info
        routerNeighbours = self.getNeighbouringRouters()
        if routerNeighbours != None and len(routerNeighbours) > 0:
            for entry in routerNeighbours:
                neighbourList.append(entry)

        print neighbourList
        return neighbourList",3,3
"def get_history(self, symbols, start, end=None, resolution=""1T"", tz=""UTC""):
        
        return self.blotter.history(symbols, start, end, resolution, tz)",3,3
"def FindProxies():
  

  sc = objc.SystemConfiguration()

  # Get the dictionary of network proxy settings
  settings = sc.dll.SCDynamicStoreCopyProxies(None)
  if not settings:
    return []

  try:
    cf_http_enabled = sc.CFDictRetrieve(settings, ""kSCPropNetProxiesHTTPEnable"")
    if cf_http_enabled and bool(sc.CFNumToInt32(cf_http_enabled)):
      # Proxy settings for HTTP are enabled
      cfproxy = sc.CFDictRetrieve(settings, ""kSCPropNetProxiesHTTPProxy"")
      cfport = sc.CFDictRetrieve(settings, ""kSCPropNetProxiesHTTPPort"")
      if cfproxy and cfport:
        proxy = sc.CFStringToPystring(cfproxy)
        port = sc.CFNumToInt32(cfport)
        return [""http://%s:%d/"" % (proxy, port)]

    cf_auto_enabled = sc.CFDictRetrieve(
        settings, ""kSCPropNetProxiesProxyAutoConfigEnable"")

    if cf_auto_enabled and bool(sc.CFNumToInt32(cf_auto_enabled)):
      cfurl = sc.CFDictRetrieve(settings,
                                ""kSCPropNetProxiesProxyAutoConfigURLString"")
      if cfurl:
        unused_url = sc.CFStringToPystring(cfurl)
        # TODO(amoser): Auto config is enabled, what is the plan here?
        # Basically, all we get is the URL of a javascript file. To get the
        # correct proxy for a given URL, browsers call a Javascript function
        # that returns the correct proxy URL. The question is now, do we really
        # want to start running downloaded js on the client?
        return []

  finally:
    sc.dll.CFRelease(settings)
  return []",3,3
"def as_dict(self):
        
        ctrl_dict = {""@module"": self.__class__.__module__,
                     ""@class"": self.__class__.__name__}
        if self.header is not None:
            ctrl_dict[""HEADER""] = self.header
        if self.version is not None:
            ctrl_dict[""VERS""] = self.version
        sga = SpacegroupAnalyzer(self.structure)
        alat = sga.get_conventional_standard_structure().lattice.a
        plat = self.structure.lattice.matrix/alat

        

        eq_atoms = sga.get_symmetry_dataset()['equivalent_atoms']
        ineq_sites_index = list(set(eq_atoms))
        sites = []
        classes = []
        num_atoms = {}
        for s, site in enumerate(self.structure.sites):
            atom = site.specie
            label_index = ineq_sites_index.index(eq_atoms[s])
            if atom.symbol in num_atoms:
                if label_index + 1 > sum(num_atoms.values()):
                    num_atoms[atom.symbol] += 1
                    atom_label = atom.symbol + str(num_atoms[atom.symbol] - 1)
                    classes.append({""ATOM"": atom_label, ""Z"": atom.Z})
            else:
                num_atoms[atom.symbol] = 1
                classes.append({""ATOM"": atom.symbol, ""Z"": atom.Z})
            sites.append({""ATOM"": classes[label_index][""ATOM""],
                          ""POS"": site.coords/alat})

        ctrl_dict.update({""ALAT"": alat/bohr_to_angstrom,
                          ""PLAT"": plat,
                          ""CLASS"": classes,
                          ""SITE"": sites})
        return ctrl_dict",3,3
"def get_content_type(self, msg, content_type=""HTML""):
        
        if ""HTML"" in content_type.upper():
            content_type = self.HTML
        elif ""PLAIN"" in content_type.upper():
            content_type = self.PLAIN

        for part in msg.walk():
            if str(part.get_content_type()) == content_type:
                return str(part.get_payload(decode=True))",3,3
"def query_fetch_all(self, query, values):
        
        self.cursor.execute(query, values)
        retval = self.cursor.fetchall()
        self.__close_db()
        return retval",3,3
"def query_fetch_one(self, query, values):
        
        self.cursor.execute(query, values)
        retval = self.cursor.fetchone()
        self.__close_db()
        return retval",3,3
"def request_token(self, authorization_url, store_token=True,
                      token_path=None, **kwargs):
        

        if self.session is None:
            raise RuntimeError(""Fist call 'get_authorization_url' to ""
                               ""generate a valid oauth object"")

        # TODO: remove token_path in future versions
        if token_path is not None:
            warnings.warn('""token_path"" param will be removed in future versions.'
                          ' Use a TokenBackend instead', DeprecationWarning)
        _, client_secret = self.auth

        # Allow token scope to not match requested scope.
        # (Other auth libraries allow this, but Requests-OAuthlib
        # raises exception on scope mismatch by default.)
        os.environ['OAUTHLIB_RELAX_TOKEN_SCOPE'] = '1'
        os.environ['OAUTHLIB_IGNORE_SCOPE_CHANGE'] = '1'

        try:
            self.token_backend.token = Token(self.session.fetch_token(
                token_url=self._oauth2_token_url,
                authorization_response=authorization_url,
                include_client_id=True,
                client_secret=client_secret))
        except Exception as e:
            log.error('Unable to fetch auth token. Error: {}'.format(str(e)))
            return False

        if store_token:
            self.token_backend.save_token()
        return True",3,2
"def attribute_map_get(self, address, route_dist=None,
                          route_family=RF_VPN_V4):
        

        if route_family not in SUPPORTED_VRF_RF:
            raise ValueError('Unsupported route_family: %s' % route_family)

        func_name = 'neighbor.attribute_map.get'
        param = {
            neighbors.IP_ADDRESS: address,
        }
        if route_dist is not None:
            param[vrfs.ROUTE_DISTINGUISHER] = route_dist
            param[vrfs.VRF_RF] = route_family

        return call(func_name, **param)",3,3
"def ls(self, folder="""", begin_from_file="""", num=-1, get_grants=False, all_grant_data=False):
        
        # S3 object key can't start with /
        folder = re.sub(r""^/"", """", folder)

        bucket_files = self.bucket.list(prefix=folder, marker=begin_from_file)

        # in case listing grants
        if get_grants:
            list_of_files = OrderedDict()
            for (i, v) in enumerate(bucket_files):
                file_info = {v.name: self.__get_grants(v.name, all_grant_data)}
                list_of_files.update(file_info)
                if i == num:
                    break

        else:
            list_of_files = set([])
            for (i, v) in enumerate(bucket_files):
                list_of_files.add(v.name)
                if i == num:
                    break

        return list_of_files",3,1
"def get(self,
            key,
            range_end=None,
            count_only=None,
            keys_only=None,
            limit=None,
            max_create_revision=None,
            min_create_revision=None,
            min_mod_revision=None,
            revision=None,
            serializable=None,
            sort_order=None,
            sort_target=None,
            timeout=None):
        
        assembler = commons.GetRequestAssembler(self._url, key, range_end)

        obj = yield self._post(assembler.url, assembler.data, timeout)

        result = Range._parse(obj)

        returnValue(result)",3,3
"def get(self,
            key,
            range_end=None,
            count_only=None,
            keys_only=None,
            limit=None,
            max_create_revision=None,
            min_create_revision=None,
            min_mod_revision=None,
            revision=None,
            serializable=None,
            sort_order=None,
            sort_target=None,
            timeout=None):
        

        def run(pg_txn):
            pg_txn.execute(""SELECT pgetcd.get(%s,%s)"", (Binary(key), 10))
            rows = pg_txn.fetchall()
            res = ""{0}"".format(rows[0][0])
            return res

        return self._pool.runInteraction(run)",3,3
"def collect_program_info(self, fname):
        
        md = '#AIKIF Technical details\n'
        md += 'Autogenerated list of programs with comments and progress\n'
        md += '\nFilename | Comment | Date | Size\n'
        md += '--- | --- | --- | ---\n'
        for i in self.lstPrograms:
            md += self.get_file_info_line(i, ' | ')
        
        # save the details an Markdown file 
        with open(fname, 'w') as f:
            f.write(md)",3,0
"def agent_color(self, val):
        
        if val == '0': 
            colour = 'blue'
        elif val == '1':
            colour = 'navy'
        elif val == '2':
            colour = 'firebrick'
        elif val == '3':
            colour = 'blue'
        elif val == '4':
            colour = 'blue2'
        elif val == '5':
            colour = 'blue4'
        elif val == '6':
            colour = 'gray22'
        elif val == '7':
            colour = 'gray57'
        elif val == '8':
            colour = 'red4'
        elif val == '9':
            colour = 'red3'

    
        
        return colour",3,3
"def copy_all_files_and_subfolders(src, dest, base_path_ignore, xtn_list):
	
	ensure_dir(dest)
	fl = mod_fl.FileList([src], xtn_list, exclude_folders,  '')
	all_paths = fl.get_list_of_paths()
	fl.save_filelist(os.path.join(dest,'files_backed_up.csv'),  [""name"", ""path"", ""size"", ""date""])
	
	for p in all_paths:
		dest_folder = os.path.join(dest, p[len(base_path_ignore):])
		ensure_dir(dest_folder)
		#print('copying ' + p)
		copy_files_to_folder(p, dest_folder, xtn='*')",3,0
"def getattr(self, key):
		
		if ((key == ""classId"") and (self.__dict__.has_key(key))):
			return self.__dict__[key]

		if UcsUtils.FindClassIdInMoMetaIgnoreCase(self.classId):
			if self.__dict__.has_key(key):
				if key in _ManagedObjectMeta[self.classId]:
					
					return self.__dict__[key]
			else:
				if self.__dict__.has_key('XtraProperty'):
					if self.__dict__['XtraProperty'].has_key(key):
						return self.__dict__['XtraProperty'][UcsUtils.WordU(key)]
					else:
						raise AttributeError(key)
				else:
					# TODO: Add Warning/Error messages in Logger.
					print ""No XtraProperty in mo:"", self.classId, "" key:"", key
		else:
			
			if self.__dict__['XtraProperty'].has_key(key):
				return self.__dict__['XtraProperty'][UcsUtils.WordU(key)]
			elif key == ""Dn"" or key == ""Rn"":
				return None
			else:
				raise AttributeError(key)",3,3
"def getattr(self, key):
		
		if key in _MethodFactoryMeta[self.classId]:
			
			return self.__dict__[key]
		else:
			
			return None",3,3
"def get_config_value(name, path_to_file='config.txt'):
    

    # if the function is called from gui then the file has to be located with respect to the gui folder
    if not os.path.isfile(path_to_file):
        path_to_file = os.path.join('../instruments/', path_to_file)

    path_to_file = os.path.abspath(path_to_file)

    if not os.path.isfile(path_to_file):
        print(('path_to_file', path_to_file))
        #raise IOError('{:s}: config file is not valid'.format(path_to_file))
        return None

    f = open(path_to_file, 'r')
    string_of_file_contents = f.read()

    if name[-1] is not ':':
        name += ':'

    if name not in string_of_file_contents:
        return None
    else:
        config_value = [line.split(name)[1] for line in string_of_file_contents.split('\n')
                        if len(line.split(name)) > 1][0].strip()
        return config_value",3,1
"def trace(self, fnames, _refresh=False):
        r
        # pylint: disable=R0101
        if fnames and (not isinstance(fnames, list)):
            raise RuntimeError(""Argument `fnames` is not valid"")
        if fnames and any([not isinstance(item, str) for item in fnames]):
            raise RuntimeError(""Argument `fnames` is not valid"")
        for fname in fnames:
            if not os.path.exists(fname):
                raise OSError(""File {0} could not be found"".format(fname))
        fnames = [item.replace("".pyc"", "".py"") for item in fnames]
        bobj = collections.namedtuple(""Bundle"", [""lineno"", ""col_offset""])
        for fname in fnames:
            if (fname not in self._fnames) or (
                _refresh
                and (fname in self._fnames)
                and (self._fnames[fname][""date""] < os.path.getmtime(fname))
            ):
                module_name = (
                    _get_module_name_from_fname(fname)
                    if not _refresh
                    else self._fnames[fname][""name""]
                )
                # Remove old module information if it is going to be refreshed
                if _refresh:
                    self._module_names.pop(self._module_names.index(module_name))
                    for cls in self._fnames[fname][""classes""]:
                        self._class_names.pop(self._class_names.index(cls))
                    dlist = []
                    for key, value in self._reverse_callables_db.items():
                        if key[0] == fname:
                            dlist.append(key)
                            try:
                                del self._callables_db[value]
                            except KeyError:
                                pass
                    for item in set(dlist):
                        del self._reverse_callables_db[item]
                lines = _readlines(fname)
                # Eliminate all Unicode characters till the first ASCII
                # character is found in first line of file, to deal with
                # Unicode-encoded source files
                for num, char in enumerate(lines[0]):  # pragma: no cover
                    if not _unicode_char(char):
                        break
                lines[0] = lines[0][num:]
                tree = ast.parse("""".join(lines))
                aobj = _AstTreeScanner(module_name, fname, lines)
                aobj.visit(tree)
                # Create a fake callable at the end of the file to properly
                # 'close', i.e. assign a last line number to the last
                # callable in file
                fake_node = bobj(len(lines) + 1, -1)
                aobj._close_callable(fake_node, force=True)
                self._class_names += aobj._class_names[:]
                self._module_names.append(module_name)
                self._callables_db.update(aobj._callables_db)
                self._reverse_callables_db.update(aobj._reverse_callables_db)
                # Split into modules
                self._modules_dict[module_name] = []
                iobj = [
                    item
                    for item in self._callables_db.values()
                    if item[""name""].startswith(module_name + ""."")
                ]
                for entry in iobj:
                    self._modules_dict[module_name].append(entry)
                self._fnames[fname] = {
                    ""name"": module_name,
                    ""date"": os.path.getmtime(fname),
                    ""classes"": aobj._class_names[:],
                }",3,1
"def get_channel_listing(self):
        
        teams = self.get('/teams')
        for team in teams:
            if team['name'].lower() == self.team:
                channel_listing = self.get('/teams/' + team['id'] + '/channels')
                return channel_listing",3,3
"def get_argv_tail(scriptname, prefer_main=None, argv=None):
    r
    if argv is None:
        argv = sys.argv
    import utool as ut
    modname = ut.get_argval('-m', help_='specify module name to profile', argv=argv)
    if modname is not None:
        # hack to account for -m scripts
        modpath = ut.get_modpath(modname, prefer_main=prefer_main)
        argvx = argv.index(modname) + 1
        argv_tail = [modpath] + argv[argvx:]
    else:
        try:
            argvx = argv.index(scriptname)
        except ValueError:
            for argvx, arg in enumerate(argv):
                # HACK
                if scriptname in arg:
                    break
        argv_tail = argv[(argvx + 1):]
    return argv_tail",3,3
"def start_msstitch(exec_drivers, sysargs):
    
    parser = populate_parser(exec_drivers)
    args = parser.parse_args(sysargs[1:])
    args.func(**vars(args))",3,2
"def str_between(str_, startstr, endstr):
    r
    if startstr is None:
        startpos = 0
    else:
        startpos = str_.find(startstr) + len(startstr)
    if endstr is None:
        endpos = None
    else:
        endpos = str_.find(endstr)
        if endpos == -1:
            endpos = None
    newstr = str_[startpos:endpos]
    return newstr",3,3
"def generate_psms_quanted(quantdb, tsvfn, isob_header, oldheader,
                          isobaric=False, precursor=False):
    
    allquants, sqlfields = quantdb.select_all_psm_quants(isobaric, precursor)
    quant = next(allquants)
    for rownr, psm in enumerate(readers.generate_tsv_psms(tsvfn, oldheader)):
        outpsm = {x: y for x, y in psm.items()}
        if precursor:
            pquant = quant[sqlfields['precursor']]
            if pquant is None:
                pquant = 'NA'
            outpsm.update({mzidtsvdata.HEADER_PRECURSOR_QUANT: str(pquant)})
        if isobaric:
            isoquants = {}
            while quant[0] == rownr:
                isoquants.update({quant[sqlfields['isochan']]:
                                  str(quant[sqlfields['isoquant']])})
                try:
                    quant = next(allquants)
                except StopIteration:
                    # last PSM, break from while loop or it is not yielded at all
                    break
            outpsm.update(get_quant_NAs(isoquants, isob_header))
        else:
            try:
                quant = next(allquants)
            except StopIteration:
                # last PSM, needs explicit yield/break or it will not be yielded
                yield outpsm
                break
        yield outpsm",3,1
"def get_product_metadata_path(product_name):
    

    string_date = product_name.split('_')[-1]
    date = datetime.datetime.strptime(string_date, '%Y%m%dT%H%M%S')
    path = 'products/{0}/{1}/{2}/{3}'.format(date.year, date.month, date.day, product_name)

    return {
        product_name: {
            'metadata': '{0}/{1}'.format(path, 'metadata.xml'),
            'tiles': get_tile_metadata_path('{0}/{1}'.format(path, 'productInfo.json'))
        }
    }",3,3
"def get_class(self, platform) -> Type[Platform]:
        

        if platform in self._classes:
            return self._classes[platform]

        raise PlatformDoesNotExist('Platform ""{}"" is not in configuration'
                                   .format(platform))",3,3
"def get(self, key):
        
        try:
            layers = key.split('.')
            value = self.registrar
            for key in layers:
                value = value[key]
            return value
        except:
            return None",3,3
"def get_all_clients(self, params=None):
        
        return self._iterate_through_pages(
            get_function=self.get_clients_per_page,
            resource=CLIENTS,
            **{'params': params}
        )",3,3
"def get_all_client_properties(self, params=None):
        
        return self._iterate_through_pages(
            get_function=self.get_client_properties_per_page,
            resource=CLIENT_PROPERTIES,
            **{'params': params}
        )",3,3
"def get_all_client_tags(self, params=None):
        
        return self._iterate_through_pages(
            get_function=self.get_client_tags_per_page,
            resource=CLIENT_TAGS,
            **{'params': params}
        )",3,3
"def get_all_contacts_of_client(self, client_id):
        
        return self._iterate_through_pages(
            get_function=self.get_contacts_of_client_per_page,
            resource=CONTACTS,
            **{'client_id': client_id}
        )",3,3
"def get_all_suppliers(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_suppliers_per_page,
            resource=SUPPLIERS,
            **{'params': params}
        )",3,3
"def get_all_supplier_properties(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_supplier_properties_per_page,
            resource=SUPPLIER_PROPERTIES,
            **{'params': params}
        )",3,3
"def get_all_tags_of_supplier(self, supplier_id):
        
        return self._iterate_through_pages(
            get_function=self.get_tags_of_supplier_per_page,
            resource=SUPPLIER_TAGS,
            **{'supplier_id': supplier_id}
        )",3,3
"def get_all_articles(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_articles_per_page, resource=ARTICLES, **{'params': params})",3,3
"def get_all_article_properties(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_article_properties_per_page,
            resource=ARTICLE_PROPERTIES,
            **{'params': params}
        )",3,3
"def get_all_tags_of_article(self, article_id):
        
        return self._iterate_through_pages(
            get_function=self.get_tags_of_article_per_page,
            resource=ARTICLE_TAGS,
            **{'article_id': article_id}
        )",3,3
"def get_all_units(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_units_per_page, resource=UNITS, **{'params': params})",3,3
"def get_all_invoices(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_invoices_per_page, resource=INVOICES, **{'params': params})",3,3
"def get_all_items_of_invoice(self, invoice_id):
        
        return self._iterate_through_pages(
            get_function=self.get_items_of_invoice_per_page,
            resource=INVOICE_ITEMS,
            **{'invoice_id': invoice_id}
        )",3,3
"def get_all_comments_of_invoice(self, invoice_id):
        
        return self._iterate_through_pages(
            get_function=self.get_comments_of_invoice_per_page,
            resource=INVOICE_COMMENTS,
            **{'invoice_id': invoice_id}
        )",3,3
"def get_all_invoice_payments(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_invoice_payments_per_page,
            resource=INVOICE_PAYMENTS,
            **{'params': params}
        )",3,3
"def get_all_tags_of_invoice(self, invoice_id):
        
        return self._iterate_through_pages(
            get_function=self.get_tags_of_invoice_per_page,
            resource=INVOICE_TAGS,
            **{'invoice_id': invoice_id}
        )",3,3
"def get_all_recurrings(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_recurrings_per_page, resource=RECURRINGS, **{'params': params})",3,3
"def get_all_items_of_recurring(self, recurring_id):
        
        return self._iterate_through_pages(
            get_function=self.get_items_of_recurring_per_page,
            resource=RECURRING_ITEMS,
            **{'recurring_id': recurring_id}
        )",3,3
"def get_all_tags_of_recurring(self, recurring_id):
        
        return self._iterate_through_pages(
            get_function=self.get_tags_of_recurring_per_page,
            resource=RECURRING_TAGS,
            **{'recurring_id': recurring_id}
        )",3,3
"def get_all_email_receivers_of_recurring(self, recurring_id):
        
        return self._iterate_through_pages(
            get_function=self.get_email_receivers_of_recurring_per_page,
            resource=RECURRING_EMAIL_RECEIVERS,
            **{'recurring_id': recurring_id}
        )",3,3
"def get_all_incomings(self, params=None):
        
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_incomings_per_page, resource=INCOMINGS, **{'params': params})",3,3
"def get_all_comments_of_incoming(self, incoming_id):
        
        return self._iterate_through_pages(
            get_function=self.get_comments_of_incoming_per_page,
            resource=INCOMING_COMMENTS,
            **{'incoming_id': incoming_id}
        )",3,3
"def _get_filename(self, path):
        
        match = re.search(""[a-z]{2,3}_[A-Z]{2}"", path)

        if match:
            start = match.start(0)
            filename = path[start:]
        else:
            filename = os.path.basename(path)

        return filename",3,3
"def dqueries2queriessam(cfg,dqueries):    
    
    datatmpd=cfg['datatmpd']
    dqueries=set_index(dqueries,'query id')
    queryls=dqueries.loc[:,'query sequence'].apply(len).unique()
    for queryl in queryls:
        logging.debug(f""now aligning queries of length {queryl}"")
        queriesfap = f'{datatmpd}/01_queries_queryl{queryl:02}.fa'
        logging.info(basename(queriesfap))
        if not exists(queriesfap) or cfg['force']:
            with open(queriesfap,'w') as f:
                for gi in dqueries.index:
                    f.write('>{}\n{}\n'.format(gi.replace(' ','_'),dqueries.loc[gi,'query sequence']))
        ## BWA alignment command is adapted from cripror 
        ## https://github.com/rraadd88/crisporWebsite/blob/master/crispor.py
        # BWA allow up to X mismatches
        # maximum number of occurences in the genome to get flagged as repeats. 
        # This is used in bwa samse, when converting the sam file
        # and for warnings in the table output.
        MAXOCC = 60000

        # the BWA queue size is 2M by default. We derive the queue size from MAXOCC
        MFAC = 2000000/MAXOCC

        genomep=cfg['genomep']
        genomed = dirname(genomep) # make var local, see below
        genomegffp=cfg['genomegffp']

        # increase MAXOCC if there is only a single query, but only in CGI mode
        bwaM = MFAC*MAXOCC # -m is queue size in bwa
        queriessap = f'{datatmpd}/01_queries_queryl{queryl:02}.sa'
        logging.info(basename(queriessap))
        if not exists(queriessap) or cfg['force']:
            cmd=f""{cfg['bwa']} aln -t 1 -o 0 -m {bwaM} -n {cfg['mismatches_max']} -k {cfg['mismatches_max']} -N -l {queryl} {genomep} {queriesfap} > {queriessap} 2> {queriessap}.log""
            runbashcmd(cmd)

        queriessamp = f'{datatmpd}/01_queries_queryl{queryl:02}.sam'
        logging.info(basename(queriessamp))        
        if not exists(queriessamp) or cfg['force']:
            cmd=f""{cfg['bwa']} samse -n {MAXOCC} {genomep} {queriessap} {queriesfap} > {queriessamp} 2> {queriessamp}.log""
            runbashcmd(cmd)
    return cfg",3,0
"def _get_row_tag(row, tag):
        

        is_empty = True
        data = []
        for column_label in row.find_all(tag):  # cycle through all labels
            data.append(
                String(column_label.text).strip_bad_html()
            )
            if data[-1]:
                is_empty = False

        if not is_empty:
            return data

        return None",3,3
"def get_answer(self, question):
        
        self.last_question = str(question).strip()
        user_answer = input(self.last_question)
        return user_answer.strip()",3,3
"def get_number(self, question, min_i=float(""-inf""), max_i=float(""inf""),
                   just_these=None):
        
        try:
            user_answer = self.get_answer(question)
            user_answer = float(user_answer)

            if min_i < user_answer < max_i:
                if just_these:
                    if user_answer in just_these:
                        return user_answer

                    exc = ""Number cannot be accepted. Just these: ""
                    exc += str(just_these)
                    raise Exception(exc)

                return user_answer

            exc = ""Number is not within limits. ""
            exc += ""Min is "" + str(min_i) + "". Max is "" + str(max_i) + """"
            raise Exception(exc)
        except Exception as exc:
            print(str(exc))
            return self.get_number(
                self.last_question,
                min_i=min_i,
                max_i=max_i,
                just_these=just_these
            )",3,3
"def get_list(self, question,
                 splitter="","", at_least=0, at_most=float(""inf"")):
        
        try:
            user_answer = self.get_answer(question)  # ask question
            user_answer = user_answer.split(splitter)  # split items
            user_answer = [str(item).strip() for item in user_answer]  # strip

            if at_least < len(user_answer) < at_most:
                return user_answer

            exc = ""List is not correct. ""
            exc += ""There must be at least "" + str(at_least) + "" items, ""
            exc += ""and at most "" + str(at_most) + "". ""
            exc += ""Use '"" + str(splitter) + ""' to separate items""
            raise Exception(exc)
        except Exception as exc:
            print(str(exc))
            return self.get_list(
                self.last_question,
                at_least=at_least,
                at_most=at_most
            )",3,3
"def get_forecast_api(self, longitude: str, latitude: str) -> {}:
        
        api_url = APIURL_TEMPLATE.format(longitude, latitude)

        response = urlopen(api_url)
        data = response.read().decode('utf-8')
        json_data = json.loads(data)

        return json_data",3,3
"async def async_get_forecast_api(self, longitude: str,
                                     latitude: str) -> {}:
        
        api_url = APIURL_TEMPLATE.format(longitude, latitude)

        if self.session is None:
            self.session = aiohttp.ClientSession()

        async with self.session.get(api_url) as response:
            if response.status != 200:
                raise SmhiForecastException(
                    ""Failed to access weather API with status code {}"".format(
                        response.status)
                )
            data = await response.text()
            return json.loads(data)",3,3
"def count_collisions(Collisions):
    
    CollisionCount = 0
    CollisionIndicies = []
    lastval = True
    for i, val in enumerate(Collisions):
        if val == True and lastval == False:
            CollisionIndicies.append(i)
            CollisionCount += 1
        lastval = val
    return CollisionCount, CollisionIndicies",3,3
"def process_result_value(self, value, dialect):
        
        if value is not None:
            cmd = ""value = {}"".format(value)
            exec(cmd)
        return value",3,3
"def logError(self):
        
        # got from http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Error_handling_with_Python
        import traceback

        self.logMsg('ERROR!!!')
        errMsg = traceback.format_exc()
        self.logMsg(errMsg)
        return errMsg",3,3
"def prep_doc(self, doc_obj):
        
        doc = doc_obj._data.copy()
        for key, prop in list(doc_obj._base_properties.items()):
            prop.validate(doc.get(key), key)
            raw_value = prop.get_python_value(doc.get(key))
            if prop.unique:
                self.check_unique(doc_obj, key, raw_value)
            value = prop.get_db_value(raw_value)
            doc[key] = value

        doc['_doc_type'] = get_doc_type(doc_obj.__class__)
        return doc",3,3
"def _get_http_request(netloc, path=""/"", headers=None, ssl=False):
    
    if ssl:
        port = 443
    else:
        port = 80

    host = netloc

    if len(netloc.split("":"")) == 2:
        host, port = netloc.split("":"")

    request = {""host"": host,
               ""port"": port,
               ""path"": path,
               ""ssl"": ssl,
               ""method"": ""GET""}
    if headers:
        request[""headers""] = headers

    response = {}

    try:
        conn = ICHTTPConnection(host=host, port=port, timeout=10)

        conn.request(path, headers, ssl, timeout=10)
        response[""status""] = conn.status
        response[""reason""] = conn.reason
        response[""headers""] = conn.headers
        body = conn.body

        try:
            response[""body""] = body.encode('utf-8')
        except UnicodeDecodeError:
            # if utf-8 fails to encode, just use base64
            response[""body.b64""] = body.encode('base64')

    except Exception as err:
        response[""failure""] = str(err)

    result = {""response"": response,
              ""request"": request}

    return result",3,2
"def send(self, node, message):
        

        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:
            return False
        self._connections[node].send(message)
        if self._connections[node].state != CONNECTION_STATE.CONNECTED:
            return False
        return True",2,2
"async def asgi_send(self, message: dict) -> None:
        
        if message[""type""] == ""http.response.start"" and self.state == ASGIHTTPState.REQUEST:
            self.response = message
        elif message[""type""] == ""http.response.body"" and self.state in {
            ASGIHTTPState.REQUEST,
            ASGIHTTPState.RESPONSE,
        }:
            if self.state == ASGIHTTPState.REQUEST:
                headers = build_and_validate_headers(self.response[""headers""])
                headers.extend(self.response_headers())
                await self.asend(
                    h11.Response(status_code=int(self.response[""status""]), headers=headers)
                )
                self.state = ASGIHTTPState.RESPONSE

            if (
                not suppress_body(self.scope[""method""], int(self.response[""status""]))
                and message.get(""body"", b"""") != b""""
            ):
                await self.asend(h11.Data(data=bytes(message[""body""])))

            if not message.get(""more_body"", False):
                if self.state != ASGIHTTPState.CLOSED:
                    await self.asend(h11.EndOfMessage())
                    await self.asgi_put({""type"": ""http.disconnect""})
                    self.state = ASGIHTTPState.CLOSED
        else:
            raise UnexpectedMessage(self.state, message[""type""])",2,2
"async def asgi_send(self, message: dict) -> None:
        
        if message[""type""] == ""websocket.accept"" and self.state == ASGIWebsocketState.HANDSHAKE:
            headers = build_and_validate_headers(message.get(""headers"", []))
            raise_if_subprotocol_present(headers)
            headers.extend(self.response_headers())
            await self.asend(
                AcceptConnection(
                    extensions=[PerMessageDeflate()],
                    extra_headers=headers,
                    subprotocol=message.get(""subprotocol""),
                )
            )
            self.state = ASGIWebsocketState.CONNECTED
            self.config.access_logger.access(
                self.scope, {""status"": 101, ""headers"": []}, time() - self.start_time
            )
        elif (
            message[""type""] == ""websocket.http.response.start""
            and self.state == ASGIWebsocketState.HANDSHAKE
        ):
            self.response = message
            self.config.access_logger.access(self.scope, self.response, time() - self.start_time)
        elif message[""type""] == ""websocket.http.response.body"" and self.state in {
            ASGIWebsocketState.HANDSHAKE,
            ASGIWebsocketState.RESPONSE,
        }:
            await self._asgi_send_rejection(message)
        elif message[""type""] == ""websocket.send"" and self.state == ASGIWebsocketState.CONNECTED:
            data: Union[bytes, str]
            if message.get(""bytes"") is not None:
                await self.asend(BytesMessage(data=bytes(message[""bytes""])))
            elif not isinstance(message[""text""], str):
                raise TypeError(f""{message['text']} should be a str"")
            else:
                await self.asend(TextMessage(data=message[""text""]))
        elif message[""type""] == ""websocket.close"" and self.state == ASGIWebsocketState.HANDSHAKE:
            await self.send_http_error(403)
            self.state = ASGIWebsocketState.HTTPCLOSED
        elif message[""type""] == ""websocket.close"":
            await self.asend(CloseConnection(code=int(message[""code""])))
            self.state = ASGIWebsocketState.CLOSED
        else:
            raise UnexpectedMessage(self.state, message[""type""])",2,2
"def send(msg_type, send_async=False, *args, **kwargs):
    
    message = message_factory(msg_type, *args, **kwargs)

    try:
        if send_async:
            message.send_async()
        else:
            message.send()
    except MessageSendError as e:
        err_exit(""Unable to send message: "", e)",2,2
"def _send_coroutine():
    
    with PoolExecutor() as executor:
        while True:
            msg = yield
            future = executor.submit(msg.send)
            future.add_done_callback(_exception_handler)",2,2
"def send_message(msg_type, kwds):
    
    if kwds[""file""]:
        get_body_from_file(kwds)
    kwargs = trim_args(kwds)
    send(msg_type, send_async=False, **kwargs)",2,2
"def send(client, sender, targets, msg_name, dest_name=None, block=None):
    
    dest_name = msg_name if dest_name is None else dest_name
    def _send(targets, m_name):
        msg = globals()[m_name]
        return com.send(targets, msg)
        
    client[sender].apply_async(_send, targets, msg_name)
    
    return client[targets].execute('%s=com.recv()'%dest_name, block=None)",2,2
"def send_notification(self, to=None, msg=None, label=None,
                          title=None, uri=None):
        
        url = self.root_url + ""send_notification""
        values = {}
        if to is not None:
            values[""to""] = to
        if msg is not None:
            values[""msg""] = msg
        if label is not None:
            values[""label""] = label
        if title is not None:
            values[""title""] = title
        if uri is not None:
            values[""uri""] = uri
        return self._query(url, values)",2,2
"def send_message(self, to=None, msg=None):
        
        url = self.root_url + ""send_message""
        values = {}
        if to is not None:
            values[""to""] = to
        if msg is not None:
            values[""msg""] = msg
        return self._query(url, values)",2,2
"def send_apply_request(self, socket, f, args=None, kwargs=None, subheader=None, track=False,
                            ident=None):
        

        if self._closed:
            raise RuntimeError(""Client cannot be used after its sockets have been closed"")
        
        # defaults:
        args = args if args is not None else []
        kwargs = kwargs if kwargs is not None else {}
        subheader = subheader if subheader is not None else {}

        # validate arguments
        if not callable(f) and not isinstance(f, Reference):
            raise TypeError(""f must be callable, not %s""%type(f))
        if not isinstance(args, (tuple, list)):
            raise TypeError(""args must be tuple or list, not %s""%type(args))
        if not isinstance(kwargs, dict):
            raise TypeError(""kwargs must be dict, not %s""%type(kwargs))
        if not isinstance(subheader, dict):
            raise TypeError(""subheader must be dict, not %s""%type(subheader))

        bufs = util.pack_apply_message(f,args,kwargs)

        msg = self.session.send(socket, ""apply_request"", buffers=bufs, ident=ident,
                            subheader=subheader, track=track)

        msg_id = msg['header']['msg_id']
        self.outstanding.add(msg_id)
        if ident:
            # possibly routed to a specific engine
            if isinstance(ident, list):
                ident = ident[-1]
            if ident in self._engines.values():
                # save for later, in case of engine death
                self._outstanding_dict[ident].add(msg_id)
        self.history.append(msg_id)
        self.metadata[msg_id]['submitted'] = datetime.now()

        return msg",2,2
"def send(self, stream, msg_or_type, content=None, parent=None, ident=None,
             buffers=None, subheader=None, track=False, header=None):
        

        if not isinstance(stream, (zmq.Socket, ZMQStream)):
            raise TypeError(""stream must be Socket or ZMQStream, not %r""%type(stream))
        elif track and isinstance(stream, ZMQStream):
            raise TypeError(""ZMQStream cannot track messages"")

        if isinstance(msg_or_type, (Message, dict)):
            # We got a Message or message dict, not a msg_type so don't
            # build a new Message.
            msg = msg_or_type
        else:
            msg = self.msg(msg_or_type, content=content, parent=parent,
                           subheader=subheader, header=header)

        buffers = [] if buffers is None else buffers
        to_send = self.serialize(msg, ident)
        flag = 0
        if buffers:
            flag = zmq.SNDMORE
            _track = False
        else:
            _track=track
        if track:
            tracker = stream.send_multipart(to_send, flag, copy=False, track=_track)
        else:
            tracker = stream.send_multipart(to_send, flag, copy=False)
        for b in buffers[:-1]:
            stream.send(b, flag, copy=False)
        if buffers:
            if track:
                tracker = stream.send(buffers[-1], copy=False, track=track)
            else:
                tracker = stream.send(buffers[-1], copy=False)

        # omsg = Message(msg)
        if self.debug:
            pprint.pprint(msg)
            pprint.pprint(to_send)
            pprint.pprint(buffers)

        msg['tracker'] = tracker

        return msg",2,2
"def _post_message(user,
                  device,
                  message,
                  title,
                  priority,
                  expire,
                  retry,
                  sound,
                  api_version=1,
                  token=None):
    

    user_validate = salt.utils.pushover.validate_user(user, device, token)
    if not user_validate['result']:
        return user_validate

    parameters = dict()
    parameters['user'] = user
    parameters['device'] = device
    parameters['token'] = token
    parameters['title'] = title
    parameters['priority'] = priority
    parameters['expire'] = expire
    parameters['retry'] = retry
    parameters['message'] = message

    if sound:
        sound_validate = salt.utils.pushover.validate_sound(sound, token)
        if sound_validate['res']:
            parameters['sound'] = sound

    result = salt.utils.pushover.query(function='message',
                                       method='POST',
                                       header_dict={'Content-Type': 'application/x-www-form-urlencoded'},
                                       data=_urlencode(parameters),
                                       opts=__opts__)

    return result",2,2
"def post_card(message,
              hook_url=None,
              title=None,
              theme_color=None):
    

    if not hook_url:
        hook_url = _get_hook_url()

    if not message:
        log.error('message is a required option.')

    payload = {
        ""text"": message,
        ""title"": title,
        ""themeColor"": theme_color
    }

    result = salt.utils.http.query(hook_url,
                                   method='POST',
                                   data=salt.utils.json.dumps(payload),
                                   status=True)

    if result['status'] <= 201:
        return True
    else:
        return {
            'res': False,
            'message': result.get('body', result['status'])
        }",2,2
"def _post_message(channel,
                  message,
                  username,
                  as_user,
                  api_key=None):
    

    parameters = dict()
    parameters['channel'] = channel
    parameters['username'] = username
    parameters['as_user'] = as_user
    parameters['text'] = '```' + message + '```'  # pre-formatted, fixed-width text

    # Slack wants the body on POST to be urlencoded.
    result = salt.utils.slack.query(function='message',
                                    api_key=api_key,
                                    method='POST',
                                    header_dict={'Content-Type': 'application/x-www-form-urlencoded'},
                                    data=_urlencode(parameters))

    log.debug('Slack message post result: %s', result)
    if result:
        return True
    else:
        return False",2,2
"def post_message(message,
                 channel=None,
                 username=None,
                 api_url=None,
                 hook=None):
    
    if not api_url:
        api_url = _get_api_url()

    if not hook:
        hook = _get_hook()

    if not username:
        username = _get_username()

    if not channel:
        channel = _get_channel()

    if not message:
        log.error('message is a required option.')

    parameters = dict()
    if channel:
        parameters['channel'] = channel
    if username:
        parameters['username'] = username
    parameters['text'] = '```' + message + '```'  # pre-formatted, fixed-width text
    log.debug('Parameters: %s', parameters)
    data = salt.utils.json.dumps(parameters)
    result = salt.utils.mattermost.query(
        api_url=api_url,
        hook=hook,
        data=str('payload={0}').format(data))  # future lint: blacklisted-function

    if result:
        return True
    else:
        return result",2,3
"def post_event(event,
               channel=None,
               username=None,
               api_url=None,
               hook=None):
    
    if not api_url:
        api_url = _get_api_url()

    if not hook:
        hook = _get_hook()

    if not username:
        username = _get_username()

    if not channel:
        channel = _get_channel()

    if not event:
        log.error('message is a required option.')

    log.debug('Event: %s', event)
    log.debug('Event data: %s', event['data'])
    message = 'tag: {0}\r\n'.format(event['tag'])
    for key, value in six.iteritems(event['data']):
        message += '{0}: {1}\r\n'.format(key, value)
    result = post_message(channel,
                          username,
                          message,
                          api_url,
                          hook)
    return bool(result)",2,2
"def send(self, load, tries=3, timeout=60, raw=False):
        
        if self.crypt == 'clear':
            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)
        else:
            ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout, raw=raw)
        raise tornado.gen.Return(ret)",2,2
"def send_msg_multi(message,
                   recipients=None,
                   rooms=None,
                   jid=None,
                   password=None,
                   nick=""SaltStack Bot"",
                   profile=None):
    

    # Remove: [WARNING ] Use of send mask waiters is deprecated.
    for handler in logging.root.handlers:
        handler.addFilter(SleekXMPPMUC())

    if profile:
        creds = __salt__['config.option'](profile)
        jid = creds.get('xmpp.jid')
        password = creds.get('xmpp.password')

    xmpp = SendMsgBot.create_multi(
        jid, password, message, recipients=recipients, rooms=rooms, nick=nick)

    if rooms:
        xmpp.register_plugin('xep_0045')  # MUC plugin
    if xmpp.connect():
        try:
            xmpp.process(block=True)
            return True
        except XMPPError as err:
            log.error(""Could not send message, error: %s"", err)
    else:
        log.error(""Could not connect to XMPP server"")
    return False",2,2
"def NDP_Attack_Fake_Router(ra, iface=None, mac_src_filter=None,
                           ip_src_filter=None):
    

    def is_request(req, mac_src_filter, ip_src_filter):
        

        if not (Ether in req and IPv6 in req and ICMPv6ND_RS in req):
            return 0

        mac_src = req[Ether].src
        if mac_src_filter and mac_src != mac_src_filter:
            return 0

        ip_src = req[IPv6].src
        if ip_src_filter and ip_src != ip_src_filter:
            return 0

        return 1

    def ra_reply_callback(req, iface):
        

        src = req[IPv6].src
        sendp(ra, iface=iface, verbose=0)
        print(""Fake RA sent in response to RS from %s"" % src)

    if not iface:
        iface = conf.iface
    sniff_filter = ""icmp6""

    sniff(store=0,
          filter=sniff_filter,
          lfilter=lambda x: is_request(x, mac_src_filter, ip_src_filter),
          prn=lambda x: ra_reply_callback(x, iface),
          iface=iface)",2,2
"def begin_send(self, p):
        
        if hasattr(p, ""sent_time""):
            p.sent_time = time.time()

        return self.outs.begin_send(bytes(p))",2,2
"def should_add_ClientCertificate(self):
        
        hs_msg = [type(m) for m in self.cur_session.handshake_messages_parsed]
        if TLSCertificateRequest not in hs_msg:
            return
        certs = []
        if self.mycert:
            certs = [self.mycert]
        self.add_msg(TLSCertificate(certs=certs))
        raise self.ADDED_CLIENTCERTIFICATE()",2,2
"def send_audio(self, chat_id, audio, caption=None, duration=None, performer=None, title=None,
                   reply_to_message_id=None, reply_markup=None, parse_mode=None, disable_notification=None,
                   timeout=None):
        
        return types.Message.de_json(
            apihelper.send_audio(self.token, chat_id, audio, caption, duration, performer, title, reply_to_message_id,
                                 reply_markup, parse_mode, disable_notification, timeout))",2,2
"def send_video(self, chat_id, data, duration=None, caption=None, reply_to_message_id=None, reply_markup=None,
                   parse_mode=None, supports_streaming=None, disable_notification=None, timeout=None):
        
        return types.Message.de_json(
            apihelper.send_video(self.token, chat_id, data, duration, caption, reply_to_message_id, reply_markup,
                                 parse_mode, supports_streaming, disable_notification, timeout))",2,2
"def send_video_note(self, chat_id, data, duration=None, length=None, reply_to_message_id=None, reply_markup=None,
                        disable_notification=None, timeout=None):
        
        return types.Message.de_json(
            apihelper.send_video_note(self.token, chat_id, data, duration, length, reply_to_message_id, reply_markup,
                                      disable_notification, timeout))",2,2
"def answer_inline_query(self, inline_query_id, results, cache_time=None, is_personal=None, next_offset=None,
                            switch_pm_text=None, switch_pm_parameter=None):
        
        return apihelper.answer_inline_query(self.token, inline_query_id, results, cache_time, is_personal, next_offset,
                                             switch_pm_text, switch_pm_parameter)",2,2
"def MGMT_PANID_QUERY(self, sAddr, xCommissionerSessionId, listChannelMask, xPanId):
        
        print '%s call MGMT_PANID_QUERY' % self.port
        panid = ''
        channelMask = ''
        channelMask = '0x' + self.__convertLongToString(self.__convertChannelMask(listChannelMask))

        if not isinstance(xPanId, str):
            panid = str(hex(xPanId))

        try:
            cmd = 'commissioner panid %s %s %s' % (panid, channelMask, sAddr)
            print cmd
            return self.__sendCommand(cmd) == 'Done'
        except Exception, e:
            ModuleHelper.writeintodebuglogger(""MGMT_PANID_QUERY() error: "" + str(e))",2,2
"def MGMT_PANID_QUERY(self, sAddr, xCommissionerSessionId, listChannelMask, xPanId):
        
        print '%s call MGMT_PANID_QUERY' % self.port
        panid = ''
        channelMask = ''
        channelMask = self.__ChannelMaskListToStr(listChannelMask)

        if not isinstance(xPanId, str):
            panid = str(hex(xPanId))

        try:
            cmd = WPANCTL_CMD + 'commissioner pan-id-query %s %s %s' % (panid, channelMask, sAddr)
            print cmd
            return self.__sendCommand(cmd) != 'Fail'
        except Exception, e:
            ModuleHelper.WriteIntoDebugLogger('MGMT_PANID_QUERY() error: ' + str(e))",2,2
"def MGMT_ANNOUNCE_BEGIN(self, sAddr, xCommissionerSessionId, listChannelMask, xCount, xPeriod):
        
        print '%s call MGMT_ANNOUNCE_BEGIN' % self.port
        channelMask = ''
        channelMask = self.__ChannelMaskListToStr(listChannelMask)
        try:
            cmd = WPANCTL_CMD + 'commissioner announce-begin %s %s %s %s' % (channelMask, xCount, xPeriod, sAddr)
            print cmd
            return self.__sendCommand(cmd) != 'Fail'
        except Exception, e:
            ModuleHelper.WriteIntoDebugLogger('MGMT_ANNOUNCE_BEGIN() error: ' + str(e))",2,2
"def SendReply(self, response, tag=None):
    
    if not isinstance(response, rdfvalue.RDFValue):
      raise ValueError(""SendReply can only send RDFValues"")

    if self.rdf_flow.parent_flow_id:
      response = rdf_flow_objects.FlowResponse(
          client_id=self.rdf_flow.client_id,
          request_id=self.rdf_flow.parent_request_id,
          response_id=self.GetNextResponseId(),
          payload=response,
          flow_id=self.rdf_flow.parent_flow_id,
          tag=tag)

      self.flow_responses.append(response)
    else:
      reply = rdf_flow_objects.FlowResult(
          client_id=self.rdf_flow.client_id,
          flow_id=self.rdf_flow.flow_id,
          hunt_id=self.rdf_flow.parent_hunt_id,
          payload=response,
          tag=tag)
      self.replies_to_write.append(reply)
      self.replies_to_process.append(reply)

    self.rdf_flow.num_replies_sent += 1",2,2
"def SendReply(self, response, tag=None):
    
    del tag

    if not isinstance(response, rdfvalue.RDFValue):
      raise ValueError(""SendReply can only send a Semantic Value"")

    # Only send the reply if we have a parent, indicated by knowing our parent's
    # request state.
    if self.runner_args.request_state.session_id:

      request_state = self.runner_args.request_state

      request_state.response_count += 1

      # Make a response message
      msg = rdf_flows.GrrMessage(
          session_id=request_state.session_id,
          request_id=request_state.id,
          response_id=request_state.response_count,
          auth_state=rdf_flows.GrrMessage.AuthorizationState.AUTHENTICATED,
          type=rdf_flows.GrrMessage.Type.MESSAGE,
          payload=response,
          args_rdf_name=response.__class__.__name__,
          args_age=int(response.age))

      # Queue the response now
      self.queue_manager.QueueResponse(msg)

      if self.runner_args.write_intermediate_results:
        self.QueueReplyForResultCollection(response)

    else:
      # Only write the reply to the collection if we are the parent flow.
      self.QueueReplyForResultCollection(response)",2,2
"def request(self, message, timeout=False, *args, **kwargs):
        
        if not self.connection_pool.full():
            self.connection_pool.put(self._register_socket())

        _socket = self.connection_pool.get()

        # setting timeout to None enables the socket to block.
        if timeout or timeout is None:
            _socket.settimeout(timeout)

        data = self.send_and_receive(_socket, message, *args, **kwargs)

        if self.connection.proto in Socket.streams:
            _socket.shutdown(socket.SHUT_RDWR)

        return Response(data, None, None)",2,2
"def onboarding_message(**payload):
    
    # Get WebClient so you can communicate back to Slack.
    web_client = payload[""web_client""]

    # Get the id of the Slack user associated with the incoming event
    user_id = payload[""data""][""user""][""id""]

    # Open a DM with the new user.
    response = web_client.im_open(user_id)
    channel = response[""channel""][""id""]

    # Post the onboarding message.
    start_onboarding(web_client, user_id, channel)",2,2
"def _handle_msg(self, msg):
        
        LOG.debug('Received msg from %s << %s', self._remotename, msg)

        # If we receive open message we try to bind to protocol
        if msg.type == BGP_MSG_OPEN:
            if self.state == BGP_FSM_OPEN_SENT:
                # Validate open message.
                self._validate_open_msg(msg)
                self.recv_open_msg = msg
                self.state = BGP_FSM_OPEN_CONFIRM
                self._peer.state.bgp_state = self.state

                # Try to bind this protocol to peer.
                self._is_bound = self._peer.bind_protocol(self)

                # If this protocol failed to bind to peer.
                if not self._is_bound:
                    # Failure to bind to peer indicates connection collision
                    # resolution choose different instance of protocol and this
                    # instance has to close. Before closing it sends
                    # appropriate notification msg. to peer.
                    raise bgp.CollisionResolution()

                # If peer sends Hold Time as zero, then according to RFC we do
                # not set Hold Time and Keep Alive timer.
                if msg.hold_time == 0:
                    LOG.info('The Hold Time sent by the peer is zero, hence '
                             'not setting any Hold Time and Keep Alive'
                             ' timers.')
                else:
                    # Start Keep Alive timer considering Hold Time preference
                    # of the peer.
                    self._start_timers(msg.hold_time)
                    self._send_keepalive()

                # Peer does not see open message.
                return
            else:
                # If we receive a Open message out of order
                LOG.error('Open message received when current state is not '
                          'OpenSent')
                # Received out-of-order open message
                # We raise Finite state machine error
                raise bgp.FiniteStateMachineError()
        elif msg.type == BGP_MSG_NOTIFICATION:
            if self._peer:
                self._signal_bus.bgp_notification_received(self._peer, msg)
            # If we receive notification message
            LOG.error('Received notification message, hence closing '
                      'connection %s', msg)
            self._socket.close()
            return

        # If we receive keepalive or update message, we reset expire timer.
        if (msg.type == BGP_MSG_KEEPALIVE or
                msg.type == BGP_MSG_UPDATE):
            if self._expiry:
                self._expiry.reset()

        # Call peer message handler for appropriate messages.
        if (msg.type in
                (BGP_MSG_UPDATE, BGP_MSG_KEEPALIVE, BGP_MSG_ROUTE_REFRESH)):
            self._peer.handle_msg(msg)
        # We give chance to other threads to run.
        self.pause(0)",2,2
"def _send_query(self):
        
        timeout = 60
        ofproto = self._datapath.ofproto
        parser = self._datapath.ofproto_parser
        if ofproto_v1_0.OFP_VERSION == ofproto.OFP_VERSION:
            send_port = ofproto.OFPP_NONE
        else:
            send_port = ofproto.OFPP_ANY

        # create a general query.
        res_igmp = igmp.igmp(
            msgtype=igmp.IGMP_TYPE_QUERY,
            maxresp=igmp.QUERY_RESPONSE_INTERVAL * 10,
            csum=0,
            address='0.0.0.0')
        res_ipv4 = ipv4.ipv4(
            total_length=len(ipv4.ipv4()) + len(res_igmp),
            proto=inet.IPPROTO_IGMP, ttl=1,
            src='0.0.0.0',
            dst=igmp.MULTICAST_IP_ALL_HOST)
        res_ether = ethernet.ethernet(
            dst=igmp.MULTICAST_MAC_ALL_HOST,
            src=self._datapath.ports[ofproto.OFPP_LOCAL].hw_addr,
            ethertype=ether.ETH_TYPE_IP)
        res_pkt = packet.Packet()
        res_pkt.add_protocol(res_ether)
        res_pkt.add_protocol(res_ipv4)
        res_pkt.add_protocol(res_igmp)
        res_pkt.serialize()

        flood = [parser.OFPActionOutput(ofproto.OFPP_FLOOD)]

        while True:
            # reset reply status.
            for status in self._mcast.values():
                for port in status.keys():
                    status[port] = False

            # send a general query to the host that sent this message.
            self._do_packet_out(
                self._datapath, res_pkt.data, send_port, flood)
            hub.sleep(igmp.QUERY_RESPONSE_INTERVAL)

            # QUERY timeout expired.
            del_groups = []
            for group, status in self._mcast.items():
                del_ports = []
                actions = []
                for port in status.keys():
                    if not status[port]:
                        del_ports.append(port)
                    else:
                        actions.append(parser.OFPActionOutput(port))
                if len(actions) and len(del_ports):
                    self._set_flow_entry(
                        self._datapath, actions, self.server_port, group)
                if not len(actions):
                    self._del_flow_entry(
                        self._datapath, self.server_port, group)
                    del_groups.append(group)
                if len(del_ports):
                    for port in del_ports:
                        self._del_flow_entry(self._datapath, port, group)
                for port in del_ports:
                    del status[port]
            for group in del_groups:
                del self._mcast[group]

            rest_time = timeout - igmp.QUERY_RESPONSE_INTERVAL
            hub.sleep(rest_time)",2,2
"def update_screen_id(self):
        
        self.status_update_event.clear()
        # This gets the screenId but always throws. Couldn't find a better way.
        try:
            self.send_message({MESSAGE_TYPE: TYPE_GET_SCREEN_ID})
        except UnsupportedNamespace:
            pass
        self.status_update_event.wait()
        self.status_update_event.clear()",2,2
"def send(self, message_type, data, connection_id, callback=None,
             one_way=False):
        
        if connection_id not in self._connections:
            raise ValueError(""Unknown connection id: {}"".format(connection_id))
        connection_info = self._connections.get(connection_id)
        if connection_info.connection_type == \
                ConnectionType.ZMQ_IDENTITY:
            message = validator_pb2.Message(
                correlation_id=_generate_id(),
                content=data,
                message_type=message_type)

            timer_tag = get_enum_name(message.message_type)
            timer_ctx = self._get_send_response_timer(timer_tag).time()
            fut = future.Future(
                message.correlation_id,
                message.content,
                callback,
                timeout=self._connection_timeout,
                timer_ctx=timer_ctx)
            if not one_way:
                self._futures.put(fut)

            self._send_receive_thread.send_message(msg=message,
                                                   connection_id=connection_id)
            return fut

        return connection_info.connection.send(
            message_type,
            data,
            callback=callback,
            one_way=one_way)",2,2
"def send_last_message(self, message_type, data,
                          connection_id, callback=None, one_way=False):
        
        if connection_id not in self._connections:
            raise ValueError(""Unknown connection id: {}"".format(connection_id))
        connection_info = self._connections.get(connection_id)
        if connection_info.connection_type == \
                ConnectionType.ZMQ_IDENTITY:
            message = validator_pb2.Message(
                correlation_id=_generate_id(),
                content=data,
                message_type=message_type)

            fut = future.Future(message.correlation_id, message.content,
                                callback, timeout=self._connection_timeout)

            if not one_way:
                self._futures.put(fut)

            self._send_receive_thread.send_last_message(
                msg=message,
                connection_id=connection_id)
            return fut

        del self._connections[connection_id]
        return connection_info.connection.send_last_message(
            message_type,
            data,
            callback=callback)",2,2
"def send(self, data, opcode=ABNF.OPCODE_TEXT):
        

        if not self.sock or self.sock.send(data, opcode) == 0:
            raise WebSocketConnectionClosedException(
                ""Connection is already closed."")",2,2
"def send_message(self, text: str, reply: int=None, link_preview: bool=None,
                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):
        
        self.twx.send_message(self, text=text, reply=reply, link_preview=link_preview, on_success=on_success,
                              reply_markup=reply_markup)",2,2
"def send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None, on_success: callable=None):
        
        pass",2,2
"def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        
        pass",2,2
"def send_audio(self, peer: Peer, audio: str, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        
        pass",2,2
"def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,
                      reply_markup: botapi.ReplyMarkup=None):
        
        pass",2,2
"def send_sticker(self, peer: Peer, sticker: str, reply: int=None, on_success: callable=None,
                     reply_markup: botapi.ReplyMarkup=None):
        
        pass",2,2
"def send_video(self, peer: Peer, video: str, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        
        pass",2,2
"def send_location(self, peer: Peer, latitude: float, longitude: float, reply: int=None, on_success: callable=None,
                      reply_markup: botapi.ReplyMarkup=None):
        
        pass",2,2
"def send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None,
                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):
        
        if isinstance(reply, Message):
            reply = reply.id

        botapi.send_message(chat_id=peer.id, text=text, disable_web_page_preview=not link_preview,
                            reply_to_message_id=reply, on_success=on_success, reply_markup=reply_markup,
                            **self.request_args).run()",2,2
"def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        
        if isinstance(reply, Message):
            reply = reply.id

        photo = botapi.InputFile('photo', botapi.InputFileInfo(photo, open(photo, 'rb'), get_mimetype(photo)))

        botapi.send_photo(chat_id=peer.id, photo=photo, caption=caption, reply_to_message_id=reply, on_success=on_success,
                          reply_markup=reply_markup, **self.request_args).run()",2,2
"def send_audio(self, peer: Peer, audio: str, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        
        if isinstance(reply, Message):
            reply = reply.id

        audio = botapi.InputFile('audio', botapi.InputFileInfo(audio, open(audio, 'rb'), get_mimetype(audio)))

        botapi.send_audio(chat_id=peer.id, audio=audio, reply_to_message_id=reply, on_success=on_success,
                          reply_markup=reply_markup, **self.request_args).run()",2,2
"def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,
                      reply_markup: botapi.ReplyMarkup=None):
        
        if isinstance(reply, Message):
            reply = reply.id

        document = botapi.InputFile('document', botapi.InputFileInfo(document, open(document, 'rb'),
                                                                     get_mimetype(document)))

        botapi.send_document(chat_id=peer.id, document=document, reply_to_message_id=reply, on_success=on_success,
                             reply_markup=reply_markup, **self.request_args).run()",2,2
"async def try_sending(self,msg,timeout_secs, max_attempts):
        
        if timeout_secs is None:
            timeout_secs = self.timeout
        if max_attempts is None:
            max_attempts = self.retry_count

        attempts = 0
        while attempts < max_attempts:
            if msg.seq_num not in self.message: return
            event = aio.Event()
            self.message[msg.seq_num][1]= event
            attempts += 1
            if self.transport:
                self.transport.sendto(msg.packed_message)
            try:
                myresult = await aio.wait_for(event.wait(),timeout_secs)
                break
            except Exception as inst:
                if attempts >= max_attempts:
                    if msg.seq_num in self.message:
                        callb = self.message[msg.seq_num][2]
                        if callb:
                            callb(self, None)
                        del(self.message[msg.seq_num])
                    #It's dead Jim
                    self.unregister()",2,2
"def req_with_ack(self, msg_type, payload, callb = None, timeout_secs=None, max_attempts=None):
        
        msg = msg_type(self.mac_addr, self.source_id, seq_num=self.seq_next(), payload=payload, ack_requested=True, response_requested=False)
        self.message[msg.seq_num]=[Acknowledgement,None,callb]
        xx=self.loop.create_task(self.try_sending(msg,timeout_secs, max_attempts))
        return True",2,2
"def discover(self):
        
        if self.transport:
            if self.discovery_countdown <= 0:
                self.discovery_countdown = self.discovery_interval
                msg = GetService(BROADCAST_MAC, self.source_id, seq_num=0, payload={}, ack_requested=False, response_requested=True)
                self.transport.sendto(msg.generate_packed_message(), (self.broadcast_ip, UDP_BROADCAST_PORT))
            else:
                self.discovery_countdown -= self.discovery_step
            self.loop.call_later(self.discovery_step, self.discover)",2,2
"def raw_message_to(self, token, message):
        
        if token not in self._clients:
            log.critical(""Client with token '{0}' not found!"".format(token))
            return
        client = self._clients[token]
        try:
            client.write_message(message)
        except (AttributeError, tornado.websocket.WebSocketClosedError):
            log.error(""Lost connection to client '{0}'"".format(client))
        else:
            print(""Sent {0} bytes."".format(len(message)))",2,2
"def message(self, data, kind=""info""):
        
        message = pd.io.json.dumps({'kind': kind, 'data': data})
        print(""Sent {0} bytes."".format(len(message)))
        self.write_message(message)",2,0
"def send_headers(self):
        
        hkeys = [key.lower() for key, value in self.outheaders]
        status = int(self.status[:3])
        
        if status == 413:
            # Request Entity Too Large. Close conn to avoid garbage.
            self.close_connection = True
        elif ""content-length"" not in hkeys:
            # ""All 1xx (informational), 204 (no content),
            # and 304 (not modified) responses MUST NOT
            # include a message-body."" So no point chunking.
            if status < 200 or status in (204, 205, 304):
                pass
            else:
                if (self.response_protocol == 'HTTP/1.1'
                    and self.method != 'HEAD'):
                    # Use the chunked transfer-coding
                    self.chunked_write = True
                    self.outheaders.append((""Transfer-Encoding"", ""chunked""))
                else:
                    # Closing the conn is the only way to determine len.
                    self.close_connection = True
        
        if ""connection"" not in hkeys:
            if self.response_protocol == 'HTTP/1.1':
                # Both server and client are HTTP/1.1 or better
                if self.close_connection:
                    self.outheaders.append((""Connection"", ""close""))
            else:
                # Server and/or client are HTTP/1.0
                if not self.close_connection:
                    self.outheaders.append((""Connection"", ""Keep-Alive""))
        
        if (not self.close_connection) and (not self.chunked_read):
            # Read any remaining request body data on the socket.
            # ""If an origin server receives a request that does not include an
            # Expect request-header field with the ""100-continue"" expectation,
            # the request includes a request body, and the server responds
            # with a final status code before reading the entire request body
            # from the transport connection, then the server SHOULD NOT close
            # the transport connection until it has read the entire request,
            # or until the client closes the connection. Otherwise, the client
            # might not reliably receive the response message. However, this
            # requirement is not be construed as preventing a server from
            # defending itself against denial-of-service attacks, or from
            # badly broken client implementations.""
            remaining = getattr(self.rfile, 'remaining', 0)
            if remaining > 0:
                self.rfile.read(remaining)
        
        if ""date"" not in hkeys:
            self.outheaders.append((""Date"", rfc822.formatdate()))
        
        if ""server"" not in hkeys:
            self.outheaders.append((""Server"", self.server.server_name))
        
        buf = [self.server.protocol + "" "" + self.status + CRLF]
        for k, v in self.outheaders:
            buf.append(k + "": "" + v + CRLF)
        buf.append(CRLF)
        self.conn.wfile.sendall("""".join(buf))",2,2
"def send_email(to=None, message=None, template='base',
               context={}, subject=None):
    
    from_email = settings.DEFAULT_FROM_EMAIL
    if to is None:
        if len(settings.ADMINS) > 0:
            to = settings.ADMINS[0][1]
        else:
            raise AttributeError(""Not Admins defined"")

    if isinstance(to, (tuple, str)) or isinstance(to, (list, str)):
        pass
    elif unicode:
        if not isinstance(to, unicode):
            raise TypeError(
                ""email_to parameter has to be a List, Tuple or a String"")
    else:
            raise TypeError(
                ""email_to parameter has to be a List, Tuple or a String"")

    email_to = to if isinstance(to, tuple) else (to,)

    context.update(get_default_context())

    if message is not None:
        context.update({'message': message})

    try:
        email_template = get_email_template(template)
    except EmailTemplateNotFound:
        email_template = get_email_template('email/base')

    email_subject = subject or ""System Notification""

    if email_template.get('txt'):
        template_txt = email_template.get('txt')
        msg = EmailMultiAlternatives(
            email_subject,
            template_txt.render(context), from_email, email_to)
        if email_template.get('html'):
            template_html = email_template.get('html')
            html_content = template_html.render(context)
            msg.attach_alternative(html_content, 'text/html')
        return msg.send()
    else:
        raise AttributeError("".txt template does not exist"")

    raise Exception(""Could Not Send Email"")",2,2
"def send_voice(self, chat_id, voice, duration=None, reply_to_message_id=None, reply_markup=None):
        
        payload = dict(chat_id=chat_id,
                       duration=duration,
                       reply_to_message_id=reply_to_message_id,
                       reply_markup=reply_markup)
        files = dict(voice=open(voice, 'rb'))
        return Message.from_api(self, **self._post('sendVoice', payload, files))",2,2
"def send(self, msg):
        
        app = self.app or current_app
        mailer = app.extensions['marrowmailer']
        mailer.start()
        if not hasattr(msg, '__iter__'):
            result = mailer.send(msg)
        else:
            result = map(lambda message: mailer.send(message), msg)
        mailer.stop()
        return result",2,2
"def send_message(slack):
    
    channel = input('Which channel would you like to message? ')
    message = input('What should the message be? ')
    channel_id = channel_to_id(slack, channel)

    print(f""Sending message to #{channel} (id: {channel_id})!"")
    slack.rtm_send_message(channel_id, message)",2,2
"def register(self, user_data, base_confirm_url='', send_welcome=True):
        
        user = self.__model__(**user_data)
        schema = RegisterSchema()
        valid = schema.process(user)
        if not valid:
            return valid

        db.session.add(user)
        db.session.commit()
        if not user.id:
            return False

        # send welcome message
        if send_welcome:
            self.send_welcome_message(user, base_confirm_url)

        events.register_event.send(user)
        return user",2,2
"def request_password_reset(self, user, base_url):
        
        user.generate_password_link()
        db.session.add(user)
        db.session.commit()
        events.password_change_requested_event.send(user)
        self.send_password_change_message(user, base_url)",2,2
"def run(self, message_id, **kwargs):
        
        log = self.get_logger(**kwargs)

        error_retry_count = kwargs.get(""error_retry_count"", 0)
        if error_retry_count >= self.max_error_retries:
            raise MaxRetriesExceededError(
                ""Can't retry {0}[{1}] args:{2} kwargs:{3}"".format(
                    self.name, self.request.id, self.request.args, kwargs
                )
            )

        log.info(""Loading Outbound Message <%s>"" % message_id)
        try:
            message = Outbound.objects.select_related(""channel"").get(id=message_id)
        except ObjectDoesNotExist:
            logger.error(""Missing Outbound message"", exc_info=True)
            return

        if message.attempts < settings.MESSAGE_SENDER_MAX_RETRIES:
            if error_retry_count > 0:
                retry_delay = calculate_retry_delay(error_retry_count)
            else:
                retry_delay = self.default_retry_delay
            log.info(""Attempts: %s"" % message.attempts)
            # send or resend
            try:
                if not message.channel:
                    channel = Channel.objects.get(default=True)
                else:
                    channel = message.channel

                sender = self.get_client(channel)
                ConcurrencyLimiter.manage_limit(self, channel)

                if not message.to_addr and message.to_identity:
                    message.to_addr = get_identity_address(
                        message.to_identity, use_communicate_through=True
                    )
                    if not message.to_addr:
                        self.fire_failed_msisdn_lookup(message.to_identity)
                        return

                if message.to_addr and not message.to_identity:
                    result = get_identity_by_address(message.to_addr)

                    if result:
                        message.to_identity = result[0][""id""]
                    else:
                        identity = {
                            ""details"": {
                                ""default_addr_type"": ""msisdn"",
                                ""addresses"": {
                                    ""msisdn"": {message.to_addr: {""default"": True}}
                                },
                            }
                        }
                        identity = create_identity(identity)
                        message.to_identity = identity[""id""]

                if ""voice_speech_url"" in message.metadata:
                    # OBD number of tries metric
                    fire_metric.apply_async(
                        kwargs={
                            ""metric_name"": ""vumimessage.obd.tries.sum"",
                            ""metric_value"": 1.0,
                        }
                    )

                    # Voice message
                    speech_url = message.metadata[""voice_speech_url""]
                    vumiresponse = sender.send_voice(
                        voice_to_addr_formatter(message.to_addr),
                        message.content,
                        speech_url=speech_url,
                        session_event=""new"",
                    )
                    log.info(""Sent voice message to <%s>"" % message.to_addr)

                elif ""image_url"" in message.metadata:
                    # Image message
                    image_url = message.metadata[""image_url""]
                    vumiresponse = sender.send_image(
                        text_to_addr_formatter(message.to_addr),
                        message.content,
                        image_url=image_url,
                    )
                    log.info(""Sent image message to <%s>"" % (message.to_addr,))

                else:
                    # Plain content
                    vumiresponse = sender.send_text(
                        text_to_addr_formatter(message.to_addr),
                        message.content,
                        metadata=message.metadata,
                        session_event=""new"",
                    )
                    log.info(""Sent text message to <%s>"" % (message.to_addr,))

                message.last_sent_time = timezone.now()
                message.attempts += 1
                message.vumi_message_id = vumiresponse[""message_id""]
                message.save()
                fire_metric.apply_async(
                    kwargs={""metric_name"": ""vumimessage.tries.sum"", ""metric_value"": 1.0}
                )
            except requests_exceptions.ConnectionError as exc:
                log.info(""Connection Error sending message"")
                fire_metric.delay(""sender.send_message.connection_error.sum"", 1)
                kwargs[""error_retry_count""] = error_retry_count + 1
                self.retry(
                    exc=exc, countdown=retry_delay, args=(message_id,), kwargs=kwargs
                )
            except requests_exceptions.Timeout as exc:
                log.info(""Sending message failed due to timeout"")
                fire_metric.delay(""sender.send_message.timeout.sum"", 1)
                kwargs[""error_retry_count""] = error_retry_count + 1
                self.retry(
                    exc=exc, countdown=retry_delay, args=(message_id,), kwargs=kwargs
                )
            except (requests_exceptions.HTTPError, HTTPServiceError) as exc:
                # retry message sending if in 500 range (3 default
                # retries)
                log.info(
                    ""Sending message failed due to status: %s""
                    % exc.response.status_code
                )
                metric_name = (
                    ""sender.send_message.http_error.%s.sum"" % exc.response.status_code
                )
                fire_metric.delay(metric_name, 1)
                kwargs[""error_retry_count""] = error_retry_count + 1
                self.retry(
                    exc=exc, countdown=retry_delay, args=(message_id,), kwargs=kwargs
                )

            # If we've gotten this far the message send was successful.
            fire_metric.apply_async(
                kwargs={""metric_name"": ""message.sent.sum"", ""metric_value"": 1.0}
            )
            return vumiresponse

        else:
            # This is for retries based on async nacks from the transport.
            log.info(""Message <%s> at max retries."" % str(message_id))
            message.to_addr = """"
            message.save(update_fields=[""to_addr""])
            fire_metric.apply_async(
                kwargs={
                    ""metric_name"": ""vumimessage.maxretries.sum"",
                    ""metric_value"": 1.0,
                }
            )
            # Count failures on exhausted tries.
            fire_metric.apply_async(
                kwargs={""metric_name"": ""message.failures.sum"", ""metric_value"": 1.0}
            )",2,3
"def send(self,message,message_type,topic=''):
        
        if message_type == RAW:
            self._sock.send(message)
        elif message_type == PYOBJ:
            self._sock.send_pyobj(message)
        elif message_type == JSON:
            self._sock.send_json(message)
        elif message_type == MULTIPART:
            self._sock.send_multipart([topic, message])
        elif message_type == STRING:
            self._sock.send_string(message)
        elif message_type == UNICODE:
            self._sock.send_unicode(message)
        else:
            raise Exception(""Unknown message type %s""%(message_type,))",2,2
"def publish(self,message,message_type,topic=''):
        
        if message_type == MULTIPART:
            raise Exception(""Unsupported request type"")

        super(Publisher,self).send(message,message_type,topic)",2,2
"async def send_audio(self, url, user, options=None):
        
        tasks = [interface.send_audio(user, url, options) for _, interface in self.interfaces.items()]
        return [body for body in await asyncio.gather(*tasks)]",2,2
"async def send_audio(self, url, user, options=None):
        
        return await self.chat.send_audio(url, user, options)",2,2
"def send(self, email=None):
        
        if email is None and self.send_as_one:
            self.smtp.send_message(
                self.multipart, self.config['EMAIL'], self.addresses)
        elif email is not None and self.send_as_one is False:
            self.smtp.send_message(
                self.multipart, self.config['EMAIL'], email)
        self.multipart = MIMEMultipart('alternative')",2,2
"def send_durable_exchange_message(self, exchange_name, body):
        
        self.connect()
        channel = self.connection.channel()
        # Fanout will send message to multiple subscribers
        channel.exchange_declare(exchange=exchange_name, type='fanout')
        result = channel.basic_publish(exchange=exchange_name, routing_key='', body=body,
                                       properties=pika.BasicProperties(
                                           delivery_mode=2,  # make message persistent
                                       ))
        self.close()
        return result",2,2
"def send_message(self, threadid, msgFormat, message):
        
        # using deprecated v3 message create because of bug in codegen of v4 ( multipart/form-data )
        response, status_code = self.__agent__.Messages.post_v3_stream_sid_message_create(
            sessionToken=self.__session__,
            keyManagerToken=self.__keymngr__,
            sid=threadid,
            message={""format"": msgFormat,
                     ""message"": message}
        ).result()
        self.logger.debug('%s: %s' % (status_code, response))
        return status_code, response",2,2
"def begin(self):
        
        log.debug(""handshake (version %s)"" % PROTOCOL_VERSION)
        version = Version()
        version.services = 0    # can't send blocks
        log.debug(""send Version"")
        self.send_message(version)",2,2
"def handle_version(self, message_header, message):
        
        log.debug(""handle version"")
        verack = VerAck()
        log.debug(""send VerAck"")
        self.send_message(verack)
        self.verack = True

        start_block_height = sorted(self.blocks.keys())[0]
        if start_block_height < 1:
            start_block_height = 1

        # ask for all blocks
        block_hashes = []
        for height in sorted(self.blocks.keys()):
            block_hashes.append( int(self.blocks[height], 16) )

        start_block_height = sorted(self.blocks.keys())[0]
        end_block_height = sorted(self.blocks.keys())[-1]
    
        log.debug(""send getdata for %s-%s (%064x-%064x)"" % (start_block_height, end_block_height, block_hashes[0], block_hashes[-1]))

        # send off the getdata
        getdata = GetData()
        block_inv_vec = []
        for block_hash in block_hashes:
            block_inv = Inventory()
            block_inv.inv_type = INVENTORY_TYPE[""MSG_BLOCK""]
            block_inv.inv_hash = block_hash

            block_inv_vec.append(block_inv)

        getdata.inventory = block_inv_vec
        self.send_message(getdata)",2,2
"def send(self, data, registration_ids=None, **kwargs):
        

        if not isinstance(data, dict):
            data = {'msg': data}

        registration_ids = registration_ids or []

        if len(registration_ids) > conf.GCM_MAX_RECIPIENTS:
            ret = []
            for chunk in self._chunks(
                    registration_ids, conf.GCM_MAX_RECIPIENTS):
                ret.append(self.send(data, registration_ids=chunk, **kwargs))
            return ret

        values = {
            'data': data,
            'collapse_key': 'message'}
        if registration_ids:
            values.update({'registration_ids': registration_ids})
        values.update(kwargs)

        values = json.dumps(values)

        headers = {
            'UserAgent': ""GCM-Server"",
            'Content-Type': 'application/json',
            'Authorization': 'key=' + self.api_key}

        response = requests.post(
            url=""https://gcm-http.googleapis.com/gcm/send"",
            data=values, headers=headers)

        response.raise_for_status()
        return registration_ids, json.loads(force_text(response.content))",2,2
"def send(self, payload, token, expiration=None, priority=None, identifier=None):
        

        # we only use one conn at a time currently but we may as well do this...
        created_conn = False
        while not created_conn:
            if len(self.conns) == 0:
                self.conns.append(PushConnection(self, self.address, self.certfile, self.keyfile))
                created_conn = True
            conn = random.choice(self.conns)
            try:
                conn.send(payload, token, expiration=expiration, priority=priority, identifier=identifier)
                return
            except:
                logger.info(""Connection died: removing"")
                self.conns.remove(conn)
        raise SendFailedException()",2,2
"def send_email(self, **kwargs):
        
        endpoint = self.messages_endpoint

        data = {
            'async': kwargs.pop('async', False),
            'ip_pool': kwargs.pop('ip_pool', ''),
            'key': kwargs.pop('key', self.api_key),
        }

        if not data.get('key', None):
            raise ValueError('No Mandrill API key has been configured')

        # Sending a template through Mandrill requires a couple extra args
        # and a different endpoint.
        if kwargs.get('template_name', None):
            data['template_name'] = kwargs.pop('template_name')
            data['template_content'] = kwargs.pop('template_content', [])
            endpoint = self.templates_endpoint

        data['message'] = kwargs

        if self.app:
            data['message'].setdefault(
                'from_email',
                self.app.config.get('MANDRILL_DEFAULT_FROM', None)
            )

        if endpoint != self.templates_endpoint and not data['message'].get('from_email', None):
            raise ValueError(
                'No from email was specified and no default was configured')


        response = requests.post(endpoint,
                                 data=json.dumps(data),
                                 headers={'Content-Type': 'application/json'})
        response.raise_for_status()
        return response",2,2
"def compose(self, to, subject, text):
        
        if isinstance(to, Account):
            to = to.name
        data = dict(to=to, subject=subject, text=text)
        j = self.post('api', 'compose', data=data)
        return assert_truthy(j)",2,2
"def on_message(self, msg):
        
        self.log('Received message: %r', msg)

        # Check if it isn't expired message
        time_left = time.left(msg.expiration_time)
        if time_left < 0:
            self.log('Throwing away expired message. Time left: %s, '
                     'msg_class: %r', time_left, msg.get_msg_class())
            return False

        # Check for duplicated message
        if msg.message_id in self._message_ids:
            self.log(""Throwing away duplicated message %r"",
                     msg.get_msg_class())
            return False
        else:
            self._message_ids.set(msg.message_id, True, msg.expiration_time)

        # Check for known traversal ids:
        if IFirstMessage.providedBy(msg):
            t_id = msg.traversal_id
            if t_id is None:
                self.warning(
                    ""Received corrupted message. The traversal_id is None ! ""
                    ""Message: %r"", msg)
                return False
            if t_id in self._traversal_ids:
                self.log('Throwing away already known traversal id %r, '
                         'msg_class: %r', t_id, msg.get_msg_class())
                recp = msg.duplication_recipient()
                if recp:
                    resp = msg.duplication_message()
                    self.post(recp, resp)
                return False
            else:
                self._traversal_ids.set(t_id, True, msg.expiration_time)

        # Handle registered dialog
        if IDialogMessage.providedBy(msg):
            recv_id = msg.receiver_id
            if recv_id is not None and \
               recv_id in self._agency_agent._protocols:
                protocol = self._agency_agent._protocols[recv_id]
                protocol.on_message(msg)
                return True

        # Handle new conversation coming in (interest)
        # if msg.protocol_id == 'alert':
        #     print self._agency_agent._interests
        p_type = msg.protocol_type
        if p_type in self._agency_agent._interests:
            p_id = msg.protocol_id
            interest = self._agency_agent._interests[p_type].get(p_id)
            if interest and interest.schedule_message(msg):
                return True

        self.debug(""Couldn't find appropriate protocol for message: ""
                   ""%s"", msg.get_msg_class())
        return False",2,3
"def sendMessage(self, exchange, routing_key, message, properties=None,
                    UUID=None):
        
        if properties is None:
            properties = pika.BasicProperties(
                content_type=self.content_type,
                delivery_mode=1,
                headers={}
            )

        if UUID is not None:
            if properties.headers is None:
                properties.headers = {}
            properties.headers[""UUID""] = UUID

        self.channel.basic_publish(
            exchange=exchange,
            routing_key=routing_key,
            properties=properties,
            body=message
        )",2,2
"def send(self, picture, *args):
        
        return lib.zsock_send(self._as_parameter_, picture, *args)",2,2
"def onMessageReceived(self, method_frame, properties, body):
        
        # if UUID is not in headers, just ack the message and ignore it
        if ""UUID"" not in properties.headers:
            self.process_exception(
                e=ValueError(""No UUID provided, message ignored.""),
                uuid="""",
                routing_key=self.parseKey(method_frame),
                body=body
            )
            return True  # ack message

        key = self.parseKey(method_frame)
        uuid = properties.headers[""UUID""]
        try:
            result = self.react_fn(
                serializers.deserialize(body, self.globals),
                self.get_sendback(uuid, key)
            )

            print ""sending response"", key

            self.sendResponse(
                serializers.serialize(result),
                uuid,
                key
            )
        except Exception, e:
            self.process_exception(
                e=e,
                uuid=uuid,
                routing_key=key,
                body=str(e),
                tb=traceback.format_exc().strip()
            )

        return True",2,2
"def send_message(self, subject=None, text=None, markdown=None, message_dict=None):
        
        message = FiestaMessage(self.api, self, subject, text, markdown, message_dict)
        return message.send()",2,2
"def send_error_json(self, code, message, headers=None):
        ""send an error to the client. text message is formatted in a json stream""
        if headers is None:
            headers = {}

        self.end_response(HttpResponseJson(code,
                                           {'code':    code,
                                            'message': message},
                                           headers))",2,2
"def SMS(self, to, body):
        
        logging.debug('Texting someone')
        return self.twilio_SMS(self._credentials['TWILIO_PHONE_NUMBER'], to, body)",2,2
"def SMS_me(self, body):
        
        logging.debug('Texting myself')
        return self.text(self._credentials['PERSONAL_PHONE_NUMBER'], body)",2,2
"def gmail_email(self, from_, to, msg):
        
        logging.debug('Emailing from Gmail')
        smtpConn = smtplib.SMTP('smtp.gmail.com', 587)
        smtpConn.ehlo()
        smtpConn.starttls()
        login_response = smtpConn.login(self._credentials['GMAIL_EMAIL'], self._credentials['GMAIL_EMAIL_PASSWORD'])
        
        # if email is of type email.message.Message, flatten and send
        # if anything else, convert to string and try and send
        if isinstance(msg, email.message.Message):
            logging.debug('Flattening MIME to string')
            # If From is already set, overwrite
            msg['From'] = from_
            # If To is string, convert to list and add each to header
            if isinstance(to, str):
                to = [to]
            for x in to:
                msg['To'] = x
            msg = msg.as_string()
        else:
            msg = str(msg)
        
        logging.debug(msg.replace('\n', ' '))
        response = smtpConn.sendmail(from_, to, msg)
        logging.info('Response from Gmail: {0}'.format(response))
        smtpConn.quit()
        return response",2,2
"def Send(self, url, opname, obj, nsdict={}, soapaction=None, wsaction=None, 
             endPointReference=None, soapheaders=(), **kw):
        
        url = url or self.url
        endPointReference = endPointReference or self.endPointReference

        # Serialize the object.
        d = {}
        d.update(self.nsdict)
        d.update(nsdict)

        sw = SoapWriter(nsdict=d, header=True, outputclass=self.writerclass, 
                 encodingStyle=kw.get('encodingStyle'),)
        
        requesttypecode = kw.get('requesttypecode')
        if kw.has_key('_args'): #NamedParamBinding
            tc = requesttypecode or TC.Any(pname=opname, aslist=False)
            sw.serialize(kw['_args'], tc)
        elif not requesttypecode:
            tc = getattr(obj, 'typecode', None) or TC.Any(pname=opname, aslist=False)
            try:
                if type(obj) in _seqtypes:
                    obj = dict(map(lambda i: (i.typecode.pname,i), obj))
            except AttributeError:
                # can't do anything but serialize this in a SOAP:Array
                tc = TC.Any(pname=opname, aslist=True)
            else:
                tc = TC.Any(pname=opname, aslist=False)

            sw.serialize(obj, tc)
        else:
            sw.serialize(obj, requesttypecode)
            
        for i in soapheaders:
           sw.serialize_header(i) 
            
        # 
        # Determine the SOAP auth element.  SOAP:Header element
        if self.auth_style & AUTH.zsibasic:
            sw.serialize_header(_AuthHeader(self.auth_user, self.auth_pass),
                _AuthHeader.typecode)

        # 
        # Serialize WS-Address
        if self.wsAddressURI is not None:
            if self.soapaction and wsaction.strip('\'""') != self.soapaction:
                raise WSActionException, 'soapAction(%s) and WS-Action(%s) must match'\
                    %(self.soapaction,wsaction)

            self.address = Address(url, self.wsAddressURI)
            self.address.setRequest(endPointReference, wsaction)
            self.address.serialize(sw)

        # 
        # WS-Security Signature Handler
        if self.sig_handler is not None:
            self.sig_handler.sign(sw)

        scheme,netloc,path,nil,nil,nil = urlparse.urlparse(url)
        transport = self.transport
        if transport is None and url is not None:
            if scheme == 'https':
                transport = self.defaultHttpsTransport
            elif scheme == 'http':
                transport = self.defaultHttpTransport
            else:
                raise RuntimeError, 'must specify transport or url startswith https/http'

        # Send the request.
        if issubclass(transport, httplib.HTTPConnection) is False:
            raise TypeError, 'transport must be a HTTPConnection'

        soapdata = str(sw)
        self.h = transport(netloc, None, **self.transdict)
        self.h.connect()
        self.SendSOAPData(soapdata, url, soapaction, **kw)",2,2
"def send_message(self, phone_number, message_text):

        

        response = self.client.messages.save(
            to=phone_number,
            from_=self.twilio_phone,
            body=message_text
        )

        keys = ['body', 'status', 'error_code', 'direction', 'date_updated', 'to', 'from_']
        response_details = {}
        import re
        builtin_pattern = re.compile('^_')
        for method in dir(response):
            if not builtin_pattern.findall(method):
                if method == 'date_updated':
                    from labpack.records.time import labDT
                    python_dt = getattr(response, method)
                    from tzlocal import get_localzone
                    python_dt = python_dt.replace(tzinfo=get_localzone())
                    response_details[method] = labDT.fromPython(python_dt).epoch()
                elif method in keys:
                    response_details[method] = getattr(response, method)

        return response_details",2,2
"def send_message(self, user_id, message_text, message_style='', button_list=None, small_buttons=True, persist_buttons=False, link_preview=True):

        

        title = '%s.send_message' % self.__class__.__name__

    # validate inputs
        input_fields = {
            'user_id': user_id,
            'message_text': message_text,
            'message_style': message_style,
            'button_list': button_list
        }
        for key, value in input_fields.items():
            if value:
                object_title = '%s(%s=%s)' % (title, key, str(value))
                self.fields.validate(value, '.%s' % key, object_title)

    # construct key word arguments
        request_kwargs = {
            'url': '%s/sendMessage' % self.api_endpoint,
            'data': {
                'chat_id': user_id,
                'text': message_text
            }
        }
        if message_style:
            if message_style == 'markdown':
                request_kwargs['data']['parse_mode'] = 'Markdown'
            elif message_style == 'html':
                request_kwargs['data']['parse_mode'] = 'HTML'
        if button_list:
            request_kwargs['data']['reply_markup'] = self._compile_buttons(button_list, small_buttons, persist_buttons)
        # elif keypad_type:
        #     request_kwargs['data']['reply_markup'] = self._compile_keypad(keypad_type, persist_buttons)
        if not link_preview:
            request_kwargs['data']['disable_web_page_preview'] = True

    # send request
        response_details = self._post_request(**request_kwargs)

        return response_details",2,2
"def send_photo(self, user_id, photo_id='', photo_path='', photo_url='', caption_text='', button_list=None, small_buttons=True, persist_buttons=False):

        

        title = '%s.send_photo' % self.__class__.__name__

    # validate inputs
        input_fields = {
            'user_id': user_id,
            'caption_text': caption_text,
            'photo_id': photo_id,
            'photo_path': photo_path,
            'photo_url': photo_url,
            'button_list': button_list
        }
        for key, value in input_fields.items():
            if value:
                object_title = '%s(%s=%s)' % (title, key, str(value))
                self.fields.validate(value, '.%s' % key, object_title)

    # construct extension map
        extension_map = self.fields.schema['photo_extensions']

    # construct key word arguments
        request_kwargs = {
            'url': '%s/sendPhoto' % self.api_endpoint,
            'data': {
                'chat_id': user_id
            }
        }
        if caption_text:
            request_kwargs['data']['caption'] = caption_text
        if button_list:
            request_kwargs['data']['reply_markup'] = self._compile_buttons(button_list, small_buttons, persist_buttons)

    # add photo to request keywords
        if photo_path:
            import os
            self._validate_type(photo_path, extension_map, title, 'photo_path')
            if not os.path.exists(photo_path):
                raise ValueError('%s is not a valid file path.' % photo_path)
            request_kwargs['files'] = { 'photo': open(photo_path, 'rb') }
        elif photo_id:
            request_kwargs['data']['photo'] = photo_id
        elif photo_url:
            file_extension = self._validate_type(photo_url, extension_map, title, 'photo_url')
            file_buffer = self._get_data(photo_url, 'photo%s' % file_extension, title, 'photo_url')
            request_kwargs['files'] = { 'photo': file_buffer }
        else:
            raise IndexError('%s(...) requires either a photo_path, photo_id or photo_url argument' % title)

    # send request
        response_details = self._post_request(**request_kwargs)

        return response_details",2,2
"def send(self, uid, event, payload=None):
        
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        if uid in self.controllers.keys():
            addr = self.controllers[uid][0]
            port = self.controllers[uid][1]
            if event == E_MESSAGE:
                #print('/message/{} => {}:{}'.format(payload, addr, port))
                return sock.sendto('/message/{}'.format(payload).encode('utf-8'), (addr, port))
            elif event == E_RUMBLE:
                #print('/rumble/{} => {}:{}'.format(payload, addr, port))
                return sock.sendto('/rumble/{}'.format(payload).encode('utf-8'), (addr, port))
            else:
                pass
        else:
            pass
        return False",2,2
"def privmsg(self, target, message):
        
        with self.lock:
            self.send('PRIVMSG ' + target + ' :' + message)
            if self.readable():
                msg = self._recv(expected_replies=('301',))
                if msg[0] == '301':
                    return 'AWAY', msg[2].split(None, 1)[1].replace(':', '', 1)",2,2
"def msg(self, message, *args, **kwargs):
        

        target = kwargs.pop('target', None)
        raw = kwargs.pop('raw', False)

        if not target:
            target = self.line.sender.nick if self.line.pm else \
                self.line.target

        if not raw:
            kw = {
                'm': self,
                'b': chr(2),
                'c': chr(3),
                'u': chr(31),
            }
            kw.update(kwargs)

            try:
                message = message.format(*args, **kw)
            except IndexError:
                if len(args) == 1 and isinstance(args[0], list):
                    # Message might be: msg, [arg1, arg2], kwargs
                    message = message.format(*args[0], **kw)
                else:
                    raise

        self.connection.msg(target, message)",2,3
"def disconnect(self, user):
        
        self.remove_user(user)
        self.send_message(create_message('RoomServer', 'Please all say goodbye to {name}!'.format(name=user.id.name)))
        self.send_message(create_disconnect(user.id.name))",2,2
"def send_message(self, message):
        
        for handler in self.users:
            logging.info('Handler: ' + str(handler))
            handler.write_message(message)",2,2
"def _user(self, user, real_name):
        
        with self.lock:
            self.send('USER %s 0 * :%s' % (user, real_name))
            if self.readable():
                self._recv()
                self.stepback()",2,2
"def _send(self):
        
        while len(self.queue) > 0:
            metric = self.queue.popleft()
            topic, value, timestamp = str(metric).split()
            msg = json.dumps({""event"": {topic: value}})
            req = urllib2.Request(""https://js.logentries.com/v1/logs/"" +
                                  self.log_token, msg)
            try:
                urllib2.urlopen(req)
            except urllib2.URLError as e:
                logging.error(""Can't send log message to Logentries %s"", e)",2,2
"async def reply_voice(self, voice: typing.Union[base.InputFile, base.String],
                          caption: typing.Union[base.String, None] = None,
                          duration: typing.Union[base.Integer, None] = None,
                          disable_notification: typing.Union[base.Boolean, None] = None,
                          reply_markup=None,
                          reply=True) -> Message:
        
        return await self.bot.send_voice(chat_id=self.chat.id,
                                         voice=voice,
                                         caption=caption,
                                         duration=duration,
                                         disable_notification=disable_notification,
                                         reply_to_message_id=self.message_id if reply else None,
                                         reply_markup=reply_markup)",2,2
"async def send_voice(self, chat_id: typing.Union[base.Integer, base.String],
                         voice: typing.Union[base.InputFile, base.String],
                         caption: typing.Union[base.String, None] = None,
                         parse_mode: typing.Union[base.String, None] = None,
                         duration: typing.Union[base.Integer, None] = None,
                         disable_notification: typing.Union[base.Boolean, None] = None,
                         reply_to_message_id: typing.Union[base.Integer, None] = None,
                         reply_markup: typing.Union[types.InlineKeyboardMarkup,
                                                    types.ReplyKeyboardMarkup,
                                                    types.ReplyKeyboardRemove,
                                                    types.ForceReply, None] = None) -> types.Message:
        
        reply_markup = prepare_arg(reply_markup)
        payload = generate_payload(**locals(), exclude=['voice'])
        if self.parse_mode:
            payload.setdefault('parse_mode', self.parse_mode)

        files = {}
        prepare_file(payload, files, 'voice', voice)

        result = await self.request(api.Methods.SEND_VOICE, payload, files)
        return types.Message(**result)",2,2
"def send(self, *args, **kwargs):
        
        return self.send_with_options(args=args, kwargs=kwargs)",2,2
"def send_with_options(self, *, args=None, kwargs=None, delay=None, **options):
        
        message = self.message_with_options(args=args, kwargs=kwargs, **options)
        return self.broker.enqueue(message, delay=delay)",2,2
