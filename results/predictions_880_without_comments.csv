code,label,prediction
"def process_data(self, fname, tokenizer, max_size):
        
        logging.info(f'Processing data from {fname}')
        data = []
        with open(fname) as dfile:
            for idx, line in enumerate(dfile):
                if max_size and idx == max_size:
                    break
                entry = tokenizer.segment(line)
                entry = torch.tensor(entry)
                data.append(entry)
        return data",1,1
"def get(self, name, default=None):
        

        session_object = super(NotificationManager, self).get(name, default)
        if session_object is not None:
            self.delete(name)
        return session_object",3,3
"def log_all(self, file):
        
        global rflink_log
        if file == None:
            rflink_log = None
        else:
            log.debug('logging to: %s', file)
            rflink_log = open(file, 'a')",1,0
"def on_consumer_cancelled(self, method_frame):
        
        logger.debug('Consumer was cancelled remotely, shutting down: %r', method_frame)
        if self._channel:
            self._channel.close()",2,2
"def open_video(specifier):
    
    if specifier.startswith('device:'):
        specifier_dev = int(specifier[7:])
        log.debug('Opening video device {0}...'.format(specifier_dev))
        vc = cv2.VideoCapture(specifier_dev)
    elif specifier.startswith('raw:'):
        try:
            w, h = tuple(int(x) for x in specifier[4:].split('x'))
        except ValueError:
            log.error('Could not parse raw specifier size from ""{0}""'.format(specifier))
            return 1
        log.debug('Using raw video with shape {0}x{1}'.format(w,h))
        vc = _RawVideoCapture(w, h)
    else:
        log.debug('Using OpenCV video capture on {0}'.format(specifier))
        vc = cv2.VideoCapture(specifier)

    return vc",1,1
"def writeXMLFile(filename, content):
    
    xmlfile = open(filename, 'w')
    
    content = etree.tostring(content, pretty_print=True)
    xmlfile.write(content)
    xmlfile.close()",0,0
"def general_setting(key, default=None, expected_type=None, qsettings=None):
    
    if qsettings is None:
        qsettings = QSettings()
    try:
        if isinstance(expected_type, type):
            return qsettings.value(key, default, type=expected_type)
        else:
            return qsettings.value(key, default)

    except TypeError as e:
        LOGGER.debug('exception %s' % e)
        LOGGER.debug('%s %s %s' % (key, default, expected_type))
        return qsettings.value(key, default)",3,3
"def convert_to_base64(self, path):
        

        mime = magic.Magic(mime=True)
        content_type = mime.from_file(path)
        archive = ''
        with open(path, ""rb"") as image_file:
            archive = b64encode(image_file.read())
            archive = archive.decode('utf-8')
        return 'data:' + content_type + ';base64,' + archive",2,1
"def process_rules(self, path: Path, system: System):
        
        self.context.update({
            'system': system,
        })
        document = FileSystem.load_yaml(path, required=True)
        for module, rules in document.items():
            click.secho('process: {0}'.format(module), fg='green')
            self._process_rules(rules, system)",1,1
"def _reload_version(self):
        
        md_version = _version_from_file(self._get_metadata(self.PKG_INFO))
        if md_version:
            self._version = md_version
        return self",3,1
"def hasInfo(self):
        
        count = len([None
                     for (fromUUID, size)
                     in Diff.theKnownSizes[self.uuid].iteritems()
                     if size is not None and fromUUID is not None
                     ])
        return count > 0",0,1
"def parse_eprocess(self, eprocess_data):
        
        Name = eprocess_data['_EPROCESS']['Cybox']['Name']
        PID = eprocess_data['_EPROCESS']['Cybox']['PID']
        PPID = eprocess_data['_EPROCESS']['Cybox']['Parent_PID']
        return {'Name': Name, 'PID': PID, 'PPID': PPID}",3,3
"def get_db_references(cls, entry):
        
        db_refs = []

        for db_ref in entry.iterfind(""./dbReference""):

            db_ref_dict = {'identifier': db_ref.attrib['id'], 'type_': db_ref.attrib['type']}
            db_refs.append(models.DbReference(**db_ref_dict))

        return db_refs",3,3
"def __writeData(self, targetPath, fields, rows):
        
        if path.exists(targetPath):
            LOG.error(""Target file exists: %s"" % path.abspath(targetPath) )
            raise UfException(Errors.FILE_EXIST, ""can't write to a existing file"") 

        with ExcelLib(fileName = targetPath, mode = ExcelLib.WRITE_MODE) as excel:
            excel.writeRow(0, fields)
            for index, row in enumerate(rows):
                excel.writeRow(index+1, row)",0,0
"async def write_message_data(self, data: bytes, timeout: NumType = None) -> None:
        
        data = LINE_ENDINGS_REGEX.sub(b""\r\n"", data)
        data = PERIOD_REGEX.sub(b"".."", data)
        if not data.endswith(b""\r\n""):
            data += b""\r\n""
        data += b"".\r\n""

        await self.write_and_drain(data, timeout=timeout)",0,0
"def _get_json(location):
    
    location = os.path.expanduser(location)
    try:
        if os.path.isfile(location):
            with io.open(location, encoding=""utf-8"") as json_data:
                return json.load(json_data, object_pairs_hook=OrderedDict).get(""tests"")
        elif ""http"" in location:
            json_data = requests.get(location)
            if not json_data:
                raise Dump2PolarionException(""Failed to download"")
            return json.loads(json_data.text, object_pairs_hook=OrderedDict).get(""tests"")
        else:
            raise Dump2PolarionException(""Invalid location"")
    except Exception as err:
        raise Dump2PolarionException(""Failed to parse JSON from {}: {}"".format(location, err))",1,1
"def slack_ver():
    

    if _meta_.slackware_version in [""off"", ""OFF""]:
        sv = Utils().read_file(""/etc/slackware-version"")
        version = re.findall(r""\d+"", sv)
        if len(sv) > 2:
            return (""."".join(version[:2]))
        else:
            return (""."".join(version))
    else:
        return _meta_.slackware_version",1,3
"def before_after_send_handling(self):
        
        self._init_delivery_statuses_dict()
        self.before_send()

        try:
            yield

        finally:
            self.after_send()
            self._update_dispatches()",2,2
"def get_bed_from_phytozome(genelist):
    
    genelist = "","".join(genelist)
    dataset = get_phytozome_dataset()
    filters = dict(gene_name_filter=genelist)
    attributes = ""chr_name1,gene_chrom_start,gene_chrom_end,gene_name1"".split("","")

    data = dataset.query(filters=filters, attributes=attributes)
    return data",1,3
"def eval_command(self, cmd):
        
        if cmd == 'restart':
            self.restart(soft=True)
        elif cmd == 'stop':
            self.stop()",3,2
"def read(self):
        try:
            bytes = self.sock.recv(self.max_size)
        except:
            self.torrent.kill_peer(self)
            return
        
        if len(bytes) == 0:
            print 'Got 0 bytes from fileno {}.'.format(self.fileno())
            self.torrent.kill_peer(self)
        self.process_input(bytes)",0,1
"def send_action(self, recipient_id, action, notification_type=NotificationType.regular):
        
        return self.send_recipient(recipient_id, {
            'sender_action': action
        }, notification_type)",2,2
"def send_email(recipient,
               subject,
               message,
               sender=""ploneintranet@netsight.co.uk""):
    
    try:
        api.portal.send_email(
            recipient=recipient,
            sender=sender,
            subject=subject,
            body=message)
    except ValueError, e:
        log.error(""MailHost error: {0}"".format(e))",2,2
"def _new_segment_data(self):
        

        if self.data_type.nptype is not None:
            return np.zeros(self.number_values, dtype=self.data_type.nptype)
        else:
            return [None] * self.number_values",1,1
"def _normalize_keys(self, data):
        
        encode = self.encoder.encode
        decode = self.encoder.decode
        return {decode(encode(k)): v for k, v in iteritems(data)}",3,3
"def send_voice(self, chat_id, voice, caption=None, duration=None, reply_to_message_id=None, reply_markup=None,
                   parse_mode=None, disable_notification=None, timeout=None):
        
        return types.Message.de_json(
            apihelper.send_voice(self.token, chat_id, voice, caption, duration, reply_to_message_id, reply_markup,
                                 parse_mode, disable_notification, timeout))",2,2
"def get_user_info(self, recipient_id, fields=None):
        
        params = {}
        if fields is not None and isinstance(fields, (list, tuple)):
            params['fields'] = "","".join(fields)

        params.update(self.auth_args)

        request_endpoint = '{0}/{1}'.format(self.graph_url, recipient_id)
        response = requests.get(request_endpoint, params=params)
        if response.status_code == 200:
            return response.json()

        return None",2,3
"def _configure_csv_file(self, file_handle, schema):
        
        csv_writer = csv.writer(file_handle, encoding='utf-8',
                                delimiter=self.field_delimiter)
        csv_writer.writerow(schema)
        return csv_writer",0,0
"def is_safe_to_append(l, sym, input_v):
    
    input_v = l._versions.find_one({'symbol': sym, 'version': input_v}) if isinstance(input_v, int) else input_v
    return not _fast_check_corruption(l._collection, sym, input_v,
                                      check_count=True, check_last_segment=True, check_append_safe=True)",1,0
"def _get_auth(self, force_console=False):
        
        if not self.target:
            raise ValueError(""Unspecified target ({!r})"".format(self.target))
        elif not force_console and self.URL_RE.match(self.target):
            auth_url = urlparse(self.target)
            source = 'url'
            if auth_url.username:
                self.user = auth_url.username
            if auth_url.password:
                self.password = auth_url.password
            if not self.auth_valid():
                source = self._get_auth_from_keyring()
            if not self.auth_valid():
                source = self._get_auth_from_netrc(auth_url.hostname)
            if not self.auth_valid():
                source = self._get_auth_from_console(self.target)
        else:
            source = self._get_auth_from_console(self.target)

        if self.auth_valid():
            self.source = source",3,3
"def _error_msg_iface(iface, option, expected):
    
    msg = 'Invalid option -- Interface: {0}, Option: {1}, Expected: [{2}]'
    return msg.format(iface, option, '|'.join(str(e) for e in expected))",3,3
"def get(self, name, default=None):
        
        option = self._options.get(name, None)
        if option is None:

            return default

        return option.__get__(self)",3,3
"async def set_outside_temp(self, temp, timeout=OTGW_DEFAULT_TIMEOUT):
        
        cmd = OTGW_CMD_OUTSIDE_TEMP
        status = {}
        if temp < -40:
            return None
        value = '{:2.1f}'.format(temp)
        ret = await self._wait_for_cmd(cmd, value, timeout)
        if ret is None:
            return
        if ret not in ['-', None]:
            ret = float(ret)
        if ret == '-':
            status[DATA_OUTSIDE_TEMP] = 0.0
        else:
            status[DATA_OUTSIDE_TEMP] = ret
        self._update_status(status)
        return ret",2,2
"def render_to_mail(subject, template, context, recipient, fail_silently = False, headers = None):
    

    if 'djcelery' in settings.INSTALLED_APPS:
        render_to_mail_task.delay(
            subject,
            template,
            context,
            recipient,
            fail_silently,
            headers
        )
    else:
        render_to_mail_task(
            subject,
            template,
            context,
            recipient,
            fail_silently,
            headers
        )",2,2
"def __patch_pipe_read_method(tango_device_klass, pipe):
    
    read_method = getattr(pipe, ""fget"", None)
    if read_method:
        method_name = ""__read_{0}__"".format(pipe.pipe_name)
        pipe.read_method_name = method_name
    else:
        method_name = pipe.read_method_name
        read_method = getattr(tango_device_klass, method_name)

    read_pipe = _get_wrapped_pipe_read_method(pipe, read_method)
    method_name = ""__read_{0}_wrapper__"".format(pipe.pipe_name)
    pipe.read_method_name = method_name

    setattr(tango_device_klass, method_name, read_pipe)",0,1
"def commit(self, softCommit=False, waitFlush=None, waitSearcher=None, expungeDeletes=None, handler='update'):
        
        if expungeDeletes is not None:
            msg = '<commit expungeDeletes=""%s"" />' % str(bool(expungeDeletes)).lower()
        else:
            msg = '<commit />'

        return self._update(msg, commit=not softCommit, softCommit=softCommit, waitFlush=waitFlush, waitSearcher=waitSearcher,
                            handler=handler)",0,2
"def __send_ready_cb(self, last_send_failure_time):
        
        logger.debug('Readiness notification (last failed=%s)', last_send_failure_time)
        
        
        if last_send_failure_time is not None:
            self.__send_retry_requests_timer.cancel()
            
            self.__send_retry_requests_timer = Timer(10, self.__send_retry_requests, args=(last_send_failure_time,))
            self.__send_retry_requests_timer.start()",2,2
"def _prt_read_msg(self, prt, fin_txt, exclude_ungrouped):
        
        if self.sections_seen or exclude_ungrouped:
            
            dat = SummarySec2dHdrGos().summarize_sec2hdrgos(self.section2goids.items())
            sys.stdout.write(Grouper.fmtsum.format(
                GO_DESC='hdr', SECs=len(dat['S']), GOs=len(dat['G']),
                UNGRP=""N/A"", undesc=""unused"", ACTION=""READ: "", FILE=fin_txt))
        elif self.goids_fin:
            prt.write(""  {G} GO IDs READ: {FIN}\n"".format(G=len(self.goids_fin), FIN=fin_txt))",1,0
"def _get_field(self, field_name, default=None):
        
        full_field_name = 'extra__grpc__{}'.format(field_name)
        if full_field_name in self.extras:
            return self.extras[full_field_name]
        else:
            return default",3,3
"def seek(self, offset, whence):
        
        if not isinstance(offset, baseinteger):
            raise TypeError(""offset can only be an instance of type baseinteger"")
        if not isinstance(whence, FileSeekOrigin):
            raise TypeError(""whence can only be an instance of type FileSeekOrigin"")
        new_offset = self._call(""seek"",
                     in_p=[offset, whence])
        return new_offset",1,0
"def _ReadCompressedData(self, read_size):
    
    compressed_data = self._file_object.read(read_size)

    read_count = len(compressed_data)

    self._compressed_data = b''.join([self._compressed_data, compressed_data])

    self._uncompressed_data, self._compressed_data = (
        self._decompressor.Decompress(self._compressed_data))

    self._uncompressed_data_size = len(self._uncompressed_data)

    return read_count",1,1
"def diffuse_advanced(self, heatColumnName=None, time=None, verbose=False):
        
        PARAMS=set_param([""heatColumnName"",""time""],[heatColumnName,time])
        response=api(url=self.__url+""/diffuse_advanced"", PARAMS=PARAMS, method=""POST"", verbose=verbose)
        return response",2,2
"def writetitle(self, titlestring, colour = None):
        
        
        self.checkforpilimage()
        colour = self.defaultcolour(colour)
        self.changecolourmode(colour)
        self.makedraw()
        
        self.loadtitlefont()
        
        imgwidth = self.pilimage.size[0]
        imgheight = self.pilimage.size[1]
        textwidth = self.draw.textsize(titlestring, font = self.titlefont)[0]
        textxpos = imgwidth/2.0 - textwidth/2.0
        textypos = imgheight - 30
        
        self.draw.text((textxpos, textypos), titlestring, fill = colour, font = self.titlefont)
        
        if self.verbose :
            print ""I've written a title on the image.""",0,0
"def _parse_flowcontrol_send(self, config):
        
        value = 'off'
        match = re.search(r'flowcontrol send (\w+)$', config, re.M)
        if match:
            value = match.group(1)
        return dict(flowcontrol_send=value)",2,2
"def print_conf(self):
        

        config_file_type = self._get_option('admin.print_conf').value

        @contextlib.contextmanager
        def stdout_opener():
            yield sys.stdout

        skip_keys = [
            k for (k, v)
            in six.iteritems(self.option_definitions)
            if isinstance(v, Option) and v.exclude_from_print_conf
        ]
        self.write_conf(config_file_type, stdout_opener, skip_keys=skip_keys)",0,0
"def send_profile(self, profile_user_id, user_ids, text='', thread_id=None):
    
    profile_id = self.convert_to_user_id(profile_user_id)
    user_ids = _get_user_ids(self, user_ids)
    if not isinstance(text, str) and not isinstance(user_ids, (list, str)):
        self.logger.error('Text must be an string, user_ids must be an list or string')
        return False

    if self.reached_limit('messages'):
        self.logger.info(""Out of messages for today."")
        return False

    self.delay('message')
    if self.api.send_direct_item(
        'profile',
        user_ids,
        text=text,
        thread=thread_id,
        profile_user_id=profile_id
    ):
        self.total['messages'] += 1
        return True
    self.logger.info(""Message to {user_ids} wasn't sent"".format(user_ids=user_ids))
    return False",2,2
"def serve_file(load, fnd):
    
    ret = {'data': '',
           'dest': ''}
    required_load_keys = ('path', 'loc', 'saltenv')
    if not all(x in load for x in required_load_keys):
        log.debug(
            'Not all of the required keys present in payload. Missing: %s',
            ', '.join(required_load_keys.difference(load))
        )
        return ret
    if not fnd['path']:
        return ret
    ret['dest'] = fnd['rel']
    gzip = load.get('gzip', None)
    fpath = os.path.normpath(fnd['path'])
    with salt.utils.files.fopen(fpath, 'rb') as fp_:
        fp_.seek(load['loc'])
        data = fp_.read(__opts__['file_buffer_size'])
        if data and six.PY3 and not salt.utils.files.is_binary(fpath):
            data = data.decode(__salt_system_encoding__)
        if gzip and data:
            data = salt.utils.gzip_util.compress(data, gzip)
            ret['gzip'] = gzip
        ret['data'] = data
    return ret",1,1
"def Serialize(self, writer):
        
        writer.WriteVarString(self.name)
        writer.WriteVarString(self.symbol)
        writer.WriteUInt8(self.decimals)",0,0
"def get_environ(self, key, default=None, cast=None):
        
        key = key.upper()
        data = self.environ.get(key, default)
        if data:
            if cast in converters:
                data = converters.get(cast)(data)
            if cast is True:
                data = parse_conf_data(data, tomlfy=True)
        return data",3,3
"def send_media_file(self, filename):
        
        cache_timeout = self.get_send_file_max_age(filename)
        return send_from_directory(self.config['MEDIA_FOLDER'], filename,
                                   cache_timeout=cache_timeout)",2,2
"def drop(self):
        
        Statement = self.get_model('statement')
        Tag = self.get_model('tag')

        Statement.objects.all().delete()
        Tag.objects.all().delete()",1,3
"def _form_loader(self, _):
        
        data = {}
        for key in self.request.arguments:
            val = self.get_arguments(key)
            if len(val) == 1:
                data[key] = val[0]
            else:
                data[key] = val
        return data",3,3
"def summarizeReads(file_handle, file_type):
    
    base_counts = defaultdict(int)
    read_number = 0
    total_length = 0
    length_list = []

    records = SeqIO.parse(file_handle, file_type)

    for record in records:
        total_length += len(record)
        read_number += 1
        length_list.append(len(record))
        for base in record:
            base_counts[base] += 1

    result = {
        ""read_number"": read_number,
        ""total_length"": total_length,
        ""average_length"": total_length / read_number if read_number > 0 else 0,
        ""max_length"": max(length_list) if length_list else 0,
        ""min_length"": min(length_list) if length_list else 0,
        ""median_length"": median(length_list) if length_list else 0,
        ""base_counts"": base_counts
    }

    return result",1,1
"def remove_class(text):
    

    
    lines = text.split(""\n"")
    ret = []
    for line in lines:
        tokens = tokenize_line(line)
        tokens_upper = [t.upper() for t in tokens]

        if ""IN"" in tokens_upper:
            tokens.remove(""IN"")
        elif ""CS"" in tokens_upper:
            tokens.remove(""CS"")
        elif ""CH"" in tokens_upper:
            tokens.remove(""CH"")
        elif ""HS"" in tokens_upper:
            tokens.remove(""HS"")

        ret.append(serialize(tokens))

    return ""\n"".join(ret)",3,3
"def write_and_get_hash(path, write_to_file, hash=None):
    
    hash = hash or hashlib.md5()
    try:
        
        r = config.DOWNLOAD_SESSION.get(path, stream=True)
        r.raise_for_status()
        for chunk in r:
            write_to_file.write(chunk)
            hash.update(chunk)

    except (MissingSchema, InvalidSchema):
        
        with open(path, 'rb') as fobj:
            for chunk in iter(lambda: fobj.read(2097152), b""""):
                write_to_file.write(chunk)
                hash.update(chunk)

    assert write_to_file.tell() > 0, ""File failed to write (corrupted).""

    return hash",0,0
"def stderr(msg, silent=False):
    
    if not silent:
        print(msg, file=sys.stderr)",0,0
"def from_json(cls, json_dump):
        
        context = cls()
        if json_dump is None:
            return None
        ctxt = json.loads(json_dump)
        for k in ctxt:
            context[k] = ctxt[k]
        return context",3,3
"def parse_srec(srec):
    
    record_type = srec[0:2]
    data_len = srec[2:4]
    addr_len = __ADDR_LEN.get(record_type) * 2
    addr = srec[4:4 + addr_len]
    data = srec[4 + addr_len:len(srec)-2]
    checksum = srec[len(srec) - 2:]
    return record_type, data_len, addr, data, checksum",0,3
"def _dbus_exception_to_reason(exc, args):
    
    error = exc.get_dbus_name()
    if error == 'error.unknown_config':
        return ""Unknown configuration '{0}'"".format(args['config'])
    elif error == 'error.illegal_snapshot':
        return 'Invalid snapshot'
    else:
        return exc.get_dbus_name()",3,3
"def from_file (filename):
    
    entries = []
    with open(filename) as fd:
        lines = []
        for line in fd.readlines():
            line = line.rstrip()
            if not line:
                if lines:
                    entries.append(from_headers(""\r\n"".join(lines)))
                lines = []
            else:
                lines.append(line)
        if lines:
            entries.append(from_headers(""\r\n"".join(lines)))
        return entries",1,1
"def read_postmaster_opts(self):
        
        result = {}
        try:
            with open(os.path.join(self._data_dir, 'postmaster.opts')) as f:
                data = f.read()
                for opt in data.split('"" ""'):
                    if '=' in opt and opt.startswith('--'):
                        name, val = opt.split('=', 1)
                        result[name.strip('-')] = val.rstrip('""\n')
        except IOError:
            logger.exception('Error when reading postmaster.opts')
        return result",1,1
"def unzip_data():
    
    with ZipFile(os.path.join(CACHE_FOLDER, CACHE_ZIP), 'r') as zf:
        zf.extractall(path=CACHE_FOLDER)",1,1
"def write_response_html_to_file(response,filename):
        
        fout = open(filename,'w')
        if not str(response.status_code).startswith(""2""):
            Model.debug_logger.debug(response.text)
        fout.write(response.text)
        fout.close()",0,0
"def create_celerybeat_schedule(apps):
    
    beat_schedule = {}
    for app in apps:
        try:
            config = import_object_by_string(app)
            module = importlib.import_module('{}.cron'.format(config.name))
        except Exception:
            try:
                module = importlib.import_module('{}.cron'.format(app))
            except Exception:
                continue

        if not (hasattr(module, 'schedule') and isinstance(module.schedule, dict)):
            logger.warning('{} has no schedule or schedule is not a dict'.format(module.__name__))
            continue

        
        for name, schedule in module.schedule.items():
            options = schedule.get('options', {})
            if 'queue' not in options:
                options['queue'] = 'cron'
                schedule['options'] = options

                beat_schedule[name] = schedule

    return beat_schedule",3,3
"def get_psms(self):
        
        self.header = actions.create_header(self.oldheader, self.spectracol)
        self.psms = actions.generate_psms_spectradata(self.lookup, self.fn,
                                                      self.oldheader)",0,1
"def recvx(source, string_p, *args):
        
        return lib.zstr_recvx(source, byref(c_char_p.from_param(string_p)), *args)",1,3
"def send_cmd(self, command, connId='default'):
        
        thisConn = self.__getConnection(connId)
        outputMsg = """"
        try:
            outputMsg += str(thisConn.sendcmd(command))
        except ftplib.all_errors as e:
            raise FtpLibraryError(str(e))
        if self.printOutput:
            logger.info(outputMsg)
        return outputMsg",2,2
"def store_sent_mail(self, mail):
        
        if self.sent_box is not None:
            return self.store_mail(self.sent_box, mail)",2,2
"def send_mail(template, to, **kwargs):
    

    def cb():
        return mail.send(to=to, template=template, **kwargs)

    return signals.send_mail(cb, data={""to"": to, ""template"": template, ""kwargs"": kwargs})",2,2
"def decrypt_from(self, f, mac_bytes=10):
        
        ctx = DecryptionContext(self.curve, f, self, mac_bytes)
        yield ctx
        ctx.read()",3,1
"def read_file(filename, binary_mode=False):
  
  mode = 'rb' if binary_mode else 'r'
  with open(filename, mode) as f:
    return f.read()",1,1
"def _multiplexed_buffer_helper(self, response):
        
        buf = self._result(response, binary=True)
        buf_length = len(buf)
        walker = 0
        while True:
            if buf_length - walker < STREAM_HEADER_SIZE_BYTES:
                break
            header = buf[walker:walker + STREAM_HEADER_SIZE_BYTES]
            _, length = struct.unpack_from('>BxxxL', header)
            start = walker + STREAM_HEADER_SIZE_BYTES
            end = start + length
            walker = end
            yield buf[start:end]",1,1
"def command(self, cmd, expected_retcode=0):  
        
        
        return self.bench.execute_command(self.endpoint_id, cmd, expected_retcode=expected_retcode)",2,2
"def _cmd(self, cmd, *args, **kw):
        
        ok = kw.setdefault('ok', False)

        self._wakeup()
        if args:
            cmd = ""%s %s"" % (cmd, ' '.join(str(a) for a in args))
        for i in xrange(3):
            log.info(""send: "" + cmd)
            self.port.write(cmd + '\n')
            if ok:
                ack = self.port.read(len(self.OK))  
                log_raw('read', ack)
                if ack == self.OK:
                    return
            else:
                ack = self.port.read(len(self.ACK))  
                log_raw('read', ack)
                if ack == self.ACK:
                    return
        raise NoDeviceException('Can not access weather station')",0,2
"def get_file(self,pattern):
        
        flag=[fnmatch.fnmatch(l,pattern) for l in self.filelist]
        id=self.fid_list.compress(flag)
        flag = self.fileid == id
        return flag",1,1
"def cli(ctx, file, quote):
    
    ctx.obj = {}
    ctx.obj['FILE'] = file
    ctx.obj['QUOTE'] = quote",3,0
"def _drain(writer, ion_event):
    
    result_event = _WRITE_EVENT_HAS_PENDING_EMPTY
    while result_event.type is WriteEventType.HAS_PENDING:
        result_event = writer.send(ion_event)
        ion_event = None
        yield result_event",0,0
"def do_req(self, method, url, body=None, headers=None, status=None):
        
        if body is None:
            body = ''
        else:
            body = json.dumps(body)
        res = self.backend.dispatch_request(method=method,
                                            url=url,
                                            body=body,
                                            headers=self.get_headers(headers),
                                            auth=self.auth)
        if not isinstance(res, MapiResponse):
            res = MapiResponse(*res)
        if status is None:
            if res.status // 100 != 2:
                raise MapiError(*res)
        elif res.status != status:
            raise MapiError(*res)

        return res",2,2
"def __write_noaas(dat, path):
    
    for filename, text in dat.items():
        try:
            with open(os.path.join(path, filename), ""w+"") as f:
                f.write(text)
        except Exception as e:
            print(""write_noaas: There was a problem writing the NOAA text file: {}: {}"".format(filename, e))
    return",0,0
"def error_response(self, code, content=''):
        
        self.send_response(code)
        self.send_header('Content-Type', 'text/xml')
        self.add_compliance_header()
        self.end_headers()
        self.wfile.write(content)",2,2
"def keepalive(self, address, port):
        

        address = self._process_value(address, 'ipaddr')
        port = self._process_value(port, 'int')

        payload = {""address"": address, ""port"": port}

        resp = self.call('keepalive', payload)

        return resp == {}",2,2
"def load_with_scipy(file, data_name):
    import scipy.io

    

    logger.debug('Loading data {} of netcdf file {} with scipy.io.'.format(data_name, file))

    f = scipy.io.netcdf.netcdf_file(file, 'r')
    data_netcdf = f.variables[data_name]
    data = np.array(data_netcdf.data, copy = True)
    data[data == data_netcdf.missing_value] = np.nan
    f.close()

    return data",1,1
"def email(self, to, msg):
        
        logging.debug('Emailing someone')
        return self.gmail_email(self._credentials['GMAIL_EMAIL'], to, msg)",2,2
"def finish(self):
        
        stylesheet = ''.join(self._buffer)
        parser = CSSParser()
        css = parser.parseString(stylesheet)
        replaceUrls(css, self._replace)
        self.request.write(css.cssText)
        return self.request.finish()",0,0
"def prepare_index_file(self):
        
        if os.getenv('AETROS_GIT_INDEX_FILE'):
            self.index_path = os.getenv('AETROS_GIT_INDEX_FILE')
            return

        import tempfile
        h, path = tempfile.mkstemp('aetros-git', '', self.temp_path)

        self.index_path = path

        
        
        os.close(h)
        os.unlink(self.index_path)

        self.logger.debug('GIT_INDEX_FILE created at ' + self.index_path)",1,0
"def on_delivery(self, name, channel, method, properties, body):
        
        message = data.Message(name, channel, method, properties, body)
        if self.is_processing:
            return self.pending.append(message)
        self.invoke_consumer(message)",3,2
"def _make_passphrase(length=None, save=False, file=None):
    
    if not length:
        length = 40

    passphrase = _make_random_string(length)

    if save:
        ruid, euid, suid = os.getresuid()
        gid = os.getgid()
        now = mktime(localtime())

        if not file:
            filename = str('passphrase-%s-%s' % uid, now)
            file = os.path.join(_repo, filename)

        with open(file, 'a') as fh:
            fh.write(passphrase)
            fh.flush()
            fh.close()
            os.chmod(file, stat.S_IRUSR | stat.S_IWUSR)
            os.chown(file, ruid, gid)

        log.warn(""Generated passphrase saved to %s"" % file)
    return passphrase",0,0
"def pp_tpl_to_dataframe(tpl_filename):
    
    inlines = open(tpl_filename, 'r').readlines()
    header = inlines.pop(0)
    marker = header.strip().split()[1]
    assert len(marker) == 1
    usecols = [0,1,2,3]
    df = pd.read_csv(tpl_filename, delim_whitespace=True,skiprows=1,
                     header=None, names=PP_NAMES[:-1],usecols=usecols)
    df.loc[:,""name""] = df.name.apply(str).apply(str.lower)
    df[""parnme""] = [i.split(marker)[1].strip() for i in inlines]


    return df",1,1
"def _configure_device(commands, **kwargs):
    
    if salt.utils.platform.is_proxy():
        return __proxy__['nxos.proxy_config'](commands, **kwargs)
    else:
        return _nxapi_config(commands, **kwargs)",2,2
"def write_main_socket(c):
    
    the_stdin_thread.socket_write.send(c)
    while True:
        try:
            the_stdin_thread.socket_write.recv(1)
        except socket.error as e:
            if e.errno != errno.EINTR:
                raise
        else:
            break",0,0
"def get(self):
        
        url = self.getUrl()

        
        for retry in range(3):
            try:
                self.logger.debug(""Making request to slick at url %s"", url)
                r = requests.get(url)
                self.logger.debug(""Request returned status code %d"", r.status_code)
                if r.status_code is 200:
                    return self.model.from_dict(r.json())
                else:
                    self.logger.debug(""Body of what slick returned: %s"", r.text)
            except BaseException as error:
                self.logger.warn(""Received exception while connecting to slick at %s"", url, exc_info=sys.exc_info())
        raise SlickCommunicationError(
            ""Tried 3 times to request data from slick at url %s without a successful status code."", url)",3,3
"def fetch_data(dest_dir='.', clobber=False, url=DATA_URL):
    

    dest_dir = os.path.abspath(dest_dir)
    try:
        os.mkdir(dest_dir)
    except OSError:
        pass

    fout = os.path.join(dest_dir, os.path.split(url)[-1])

    if os.access(fout, os.F_OK) and not clobber:
        downloaded = False
        logging.info(fout + ' exists; not downloading')
    else:
        downloaded = True
        logging.info('downloading {} to {}'.format(url, fout))
        request.urlretrieve(url, fout)

    return (fout, downloaded)",1,1
"def clone_parler_translations(self, dst_obj):
        
        
        translation_attrs = []
        if hasattr(self, '_parler_meta'):
            for parler_meta in self._parler_meta:
                translation_attrs.append(parler_meta.rel_name)
        
        for translation_attr in translation_attrs:
            
            
            
            setattr(dst_obj, translation_attr, [])
            
            for translation in getattr(self, translation_attr).all():
                translation.pk = None
                translation.master = dst_obj
                translation.save()",3,0
"def _retryable_write(self, retryable, func, session):
        
        with self._tmp_session(session) as s:
            return self._retry_with_session(retryable, func, s, None)",0,0
"def sendeof(self):
        

        n, byte = self.ptyproc.sendeof()
        self._log_control(byte)",1,2
"def put(self, val, reg):
        
        offset = self.lookup_register(self.irsb_c.irsb.arch, reg)
        self.irsb_c.put(val.rdt, offset)",3,0
"def input_waiting(self):
        
        
        buf = array.array('I', [0])
        try:
            fcntl.ioctl(self._fd, termios.TIOCINQ, buf, True)
        except OSError as e:
            raise SerialError(e.errno, ""Querying input waiting: "" + e.strerror)

        return buf[0]",1,1
"async def write(self, data, container, eof = False, ignoreexception = False, buffering = True, split = True):
        
        if not ignoreexception:
            raise IOError('Stream is closed')",2,0
"def run_simulation(topo, **kwargs):
    
    log_debug(""Threads at startup:"")
    for t in threading.enumerate():
        log_debug(""\tthread at startup {}"".format(t.name))

    with yellow():
        log_info(""Starting up switchyard simulation substrate."")
    glue = SyssGlue(topo, **kwargs)
    cli = Cli(glue, topo)
    try:
        cli.cmdloop()
    except KeyboardInterrupt:
        print(""Received SIGINT --- shutting down."")
        cli.stop()",3,2
"def set_write_buffer_limits(self, high=None, low=None):
        

        
        self._transport.set_write_buffer_limits(high=high, low=low)",0,0
"async def message(request):
    

    msg = request.get('msg')
    if msg is None:
        return request.text(""To send a message, go to 0.0.0.0:8000/""
                            ""message/?msg='insert message here'."")
    await app.webhook.send(msg)
    return response.text(""Send."")",2,2
"def _set_file_encoding_utf8(filename):
    
    with open(filename, 'r+') as f:
        content = f.read()
        f.seek(0, 0)
        f.write(""",3,1
"def _statsd_address(self):
        
        return (self.application.settings.get('statsd',
                                              {}).get('host',
                                                      self.STATSD_HOST),
                self.application.settings.get('statsd',
                                              {}).get('port',
                                                      self.STATSD_PORT))",2,2
"def process_data(self):
        ""read and process input from self.socket""

        try:
            reader = getattr(self.socket, 'read', self.socket.recv)
            new_data = reader(2 ** 14)
        except socket.error:
            
            self.disconnect(""Connection reset by peer"")
            return
        if not new_data:
            
            self.disconnect(""Connection reset by peer"")
            return

        self.buffer.feed(new_data)

        
        for line in self.buffer:
            log.debug(""FROM SERVER: %s"", line)
            if not line:
                continue
            self._process_line(line)",1,1
"def get_blocked_usernames():
    
    if config.DISABLE_USERNAME_LOCKOUT:
        
        return []
    key = get_username_blocked_cache_key(""*"")
    key_list = [redis_key.decode('utf-8')
                for redis_key in REDIS_SERVER.keys(key)]
    return strip_keys(key_list)",3,3
"def parse_json(pairs):
    
    new_pairs = []
    for key, value in pairs:
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        if isinstance(key, unicode):
            key = key.encode('utf-8')
        new_pairs.append((key, value))
    return dict(new_pairs)",3,3
"def fill(self):
        
        if self.exclude:
            exclude = self.exclude
            ap = functools.partial(os.path.join, self.path)
            self._tarfile.add(
                self.path, """",
                filter=lambda info: None if exclude(ap(info.name)) else info
                )
        else:
            self._tarfile.add(self.path, """")
        self._tarfile.close()  
        self._finished += 1
        if not self._result.is_set():
            self._result.set()",0,0
"def countries():
    
    global _countries
    if not _countries:
        v = {}
        _countries = v
        try:
            from pytz import country_names
            for k, n in country_names.items():
                v[k.upper()] = n
        except Exception:
            pass
    return _countries",3,3
"def receive(cfg, code, **kwargs):
    
    for name, value in kwargs.items():
        setattr(cfg, name, value)
    with cfg.timing.add(""import"", which=""cmd_receive""):
        from . import cmd_receive
    if len(code) == 1:
        cfg.code = code[0]
    elif len(code) > 1:
        print(""Pass either no code or just one code; you passed""
              "" {}: {}"".format(len(code), ', '.join(code)))
        raise SystemExit(1)
    else:
        cfg.code = None

    return go(cmd_receive.receive, cfg)",3,2
"def pread(self, position, length):
        r
        _complain_ifclosed(self.closed)
        if position > self.size:
            raise IOError(""position cannot be past EOF"")
        if length < 0:
            length = self.size - position
        data = self.f.raw.pread(position, length)
        if self.__encoding:
            return data.decode(self.__encoding, self.__errors)
        else:
            return data",1,1
"def write_frames(self, input, nframes = -1):
        
        if nframes == -1:
            if input.ndim == 1:
                nframes = input.size
            elif input.ndim == 2:
                nframes = input.shape[0]
            else:
                raise ValueError(""Input has to be rank 1 (mono) or rank 2 ""\
                                 ""(multi-channels)"")
        return self._sndfile.write_frames(input[:nframes,...])",0,0
"def _speak(self):
        
        if hasattr(self, 'account'):
            bot.info('connected to %s' %self.account.name.display_name)",0,3
"def get_structure_from_mp(formula):
    
    m = MPRester()
    entries = m.get_entries(formula, inc_structure=""final"")
    if len(entries) == 0:
        raise ValueError(""No structure with formula %s in Materials Project!"" %
                         formula)
    elif len(entries) > 1:
        warnings.warn(""%d structures with formula %s found in Materials ""
                      ""Project. The lowest energy structure will be returned."" %
                      (len(entries), formula))
    return min(entries, key=lambda e: e.energy_per_atom).structure",3,3
"def _get_ensemble_bed_files(items):
    
    bed_files = []
    for data in items:
        for sv in data.get(""sv"", []):
            if sv[""variantcaller""] == ""sv-ensemble"":
                if (""vrn_file"" in sv and not vcfutils.get_paired_phenotype(data) == ""normal""
                      and file_exists(sv[""vrn_file""])):
                    bed_files.append(sv[""vrn_file""])
    return bed_files",3,1
"def bulkWrite(self, endpoint, data, timeout=0):
        
        
        endpoint = (endpoint & ~ENDPOINT_DIR_MASK) | ENDPOINT_OUT
        
        data, _ = create_initialised_buffer(data)
        return self._bulkTransfer(endpoint, data, sizeof(data), timeout)",0,0
"def save(self, io, little_endian=False):
        
        if little_endian:
            write = lambda fmt, *args: io.write(pack('<' + fmt, *args))
        else:
            write = lambda fmt, *args: io.write(pack('>' + fmt, *args))
        write.io = io

        self.write(write)",0,0
"def have_consistent_saxis(magmoms):
        
        magmoms = [Magmom(magmom) for magmom in magmoms]
        ref_saxis = magmoms[0].saxis
        match_ref = [magmom.saxis == ref_saxis for magmom in magmoms]
        if np.all(match_ref):
            return True
        else:
            return False",0,3
"def _write_jsonl(filepath, data, kwargs):
    
    with io_stl.open(filepath, 'w', encoding='utf8') as outfile:
        kwargs['indent'] = None  
        if 'sort_keys' not in kwargs:
            kwargs['sort_keys'] = True
        if 'separators' not in kwargs:
            kwargs['separators'] = (',', ': ')
        if 'ensure_ascii' not in kwargs:
            kwargs['ensure_ascii'] = False
        for line in data:
            str_ = json.dumps(line, **kwargs)
            outfile.write(to_unicode(str_))
            outfile.write(u'\n')
    return data",0,0
"def send_message(registration_ids, data, cloud_type, application_id=None, **kwargs):
	
	if cloud_type in (""FCM"", ""GCM""):
		max_recipients = get_manager().get_max_recipients(cloud_type, application_id)
	else:
		raise ImproperlyConfigured(""cloud_type must be FCM or GCM not %s"" % str(cloud_type))

	
	if registration_ids is None and ""/topics/"" not in kwargs.get(""to"", """"):
		return

	
	if not isinstance(registration_ids, list):
		registration_ids = [registration_ids] if registration_ids else None

	
	
	if registration_ids:
		ret = []
		for chunk in _chunks(registration_ids, max_recipients):
			ret.append(_cm_send_request(
				chunk, data, cloud_type=cloud_type, application_id=application_id, **kwargs
			))
		return ret[0] if len(ret) == 1 else ret
	else:
		return _cm_send_request(None, data, cloud_type=cloud_type, **kwargs)",2,2
"def write_json_file(
        path: str,
        contents: dict,
        mode: str = 'w',
        retry_count: int = 3
) -> typing.Tuple[bool, typing.Union[None, Exception]]:
    
    error = None
    for i in range(retry_count):
        error = attempt_json_write(path, contents, mode)
        if error is None:
            return True, None
        time.sleep(0.2)

    return False, error",0,0
"def _write_method(schema):
    
    def method(
        self,
        filename=None,
        schema=schema,
        id_col='uid',
        sequence_col='sequence',
        extra_data=None,
        alphabet=None,
        **kwargs):
        
        return _write(
            self._data,
            filename=filename,
            schema=schema,
            id_col=id_col,
            sequence_col=sequence_col,
            extra_data=extra_data,
            alphabet=alphabet,
            **kwargs
        )
    
    method.__doc__ = _write_doc_template(schema)
    return method",0,0
"def run(self, resources):
        

        hwman = resources['connection']

        updater = hwman.hwman.app(name='device_updater')
        updater.run_script(self._script, no_reboot=self._no_reboot)",2,2
"def socket_reader(connection: socket, buffer_size: int = 1024):
    
    while connection is not None:
        try:
            buffer = connection.recv(buffer_size)
            
            if not len(buffer):
                raise ConnectionAbortedError
        except ConnectionAbortedError:
            
            print('connection aborted')
            connection.close()
            yield None
        except OSError:
            
            print('socket closed')
            connection.close()
            yield None
        else:
            yield buffer",1,1
"def add_preset(self, name=None, desc=None, note=None, opts=SoSOptions()):
        
        presets_path = self.presets_path

        if not name:
            raise ValueError(""Preset name cannot be empty"")

        if name in self.presets.keys():
            raise ValueError(""A preset with name '%s' already exists"" % name)

        preset = PresetDefaults(name=name, desc=desc, note=note, opts=opts)
        preset.builtin = False
        self.presets[preset.name] = preset
        preset.write(presets_path)",0,0
"def write_input_files(pst):
    
    par = pst.parameter_data
    par.loc[:,""parval1_trans""] = (par.parval1 * par.scale) + par.offset
    for tpl_file,in_file in zip(pst.template_files,pst.input_files):
        write_to_template(pst.parameter_data.parval1_trans,tpl_file,in_file)",0,0
"def get_slugignores(root, fname='.slugignore'):
    
    try:
        with open(os.path.join(root, fname)) as f:
            return [l.rstrip('\n') for l in f]
    except IOError:
        return []",1,1
"def parse(self, limit=None):
        
        if limit is not None:
            LOG.info(""Only parsing first %s rows fo each file"", str(limit))

        LOG.info(""Parsing files..."")

        self._process_straininfo(limit)
        
        

        
        self._process_ontology_mappings_file(limit)
        
        self._process_measurements_file(limit)
        
        self._process_strainmeans_file(limit)

        
        
        self._fill_provenance_graph(limit)

        LOG.info(""Finished parsing."")
        return",0,1
"def get_quantity(self) -> Decimal:
        
        from pydatum import Datum
        
        today = Datum()
        today.today()
        today.end_of_day()
        return self.get_num_shares_on(today.value)",3,3
"def log_exceptions(self, c, broker):
        

        if c in broker.exceptions:
            ex = broker.exceptions.get(c)
            ex = ""Exception in {0} - {1}"".format(dr.get_name(c), str(ex))
            self.logit(ex, self.pid, self.user, ""insights-run"", logging.ERROR)",2,3
"def update_version(self, version, step=1):
        ""Compute an new version and write it as a tag""

        
        if self.config.patch:
            version.patch += step
        if self.config.minor:
            version.minor += step
        if self.config.major:
            version.major += step
        if self.config.build:
            version.build_number += step
        if self.config.build_number:
            version.build_number = self.config.build_number

        
        if self.config.dry_run:
            log.info('Not updating repo to version {0}, because of --dry-run'.format(version))
        else:
            version = self.call_plugin_function('set_version', version)

        return version",0,0
"def info(*messages):
    
    sys.stderr.write(""%s.%s: "" % get_caller_info())
    sys.stderr.write(' '.join(map(str, messages)))
    sys.stderr.write('\n')",3,0
"def _read_bytes(self, b):
        
        if hasattr(self, ""footer""):
            _LOGGER.debug(""Source stream processing complete"")
            return

        buffer_length = len(self.output_buffer)
        if 0 <= b <= buffer_length:
            _LOGGER.debug(""%d bytes requested less than or equal to current output buffer size %d"", b, buffer_length)
            return

        if self._header.content_type == ContentType.FRAMED_DATA:
            self.output_buffer += self._read_bytes_from_framed_body(b)
        elif self._header.content_type == ContentType.NO_FRAMING:
            self.output_buffer += self._read_bytes_from_non_framed_body(b)
        else:
            raise NotSupportedError(""Unsupported content type"")",3,1
"def ready(self, node_id, metadata_priority=True):
        
        self.maybe_connect(node_id)
        return self.is_ready(node_id, metadata_priority=metadata_priority)",2,3
"def _read(name):
    
    logging.debug(""Reading config-file: %s"" % name)
    try:
        with open(name, ""r"") as config_file:
            prm_dict = yaml.load(config_file)
    except yaml.YAMLError:
        raise yaml.YAMLErrorr
    else:
        return prm_dict",1,1
"def fault_barrier(fn):
    
    @functools.wraps(fn)
    def process(self, tup):
        try:
            return fn(self, tup)
        except Exception as e:
            if isinstance(e, KeyboardInterrupt):
                return
            print(str(e), file=sys.stderr)
            self.fail(tup)
    return process",2,2
"def twilio_SMS(self, from_, to, body):
        
        logging.debug('Texting from Twilio')
        client = TwilioRestClient(self._credentials['TWILIO_ACCOUNT_SID'], self._credentials['TWILIO_AUTH_TOKEN']) 
        response = client.messages.create(
            to=to, 
            from_=from_, 
            body=body,  
        )
        logging.debug('Response from Twilio: {0}'.format(response))
        return response",3,2
"def read_alignment(out_sam, loci, seqs, out_file):
    
    hits = defaultdict(list)
    with open(out_file, ""w"") as out_handle:
        samfile = pysam.Samfile(out_sam, ""r"")
        for a in samfile.fetch():
            if not a.is_unmapped:
                nm = int([t[1] for t in a.tags if t[0] == ""NM""][0])
                a = makeBED(a)
                if not a:
                    continue
                ref, locus = get_loci(samfile.getrname(int(a.chr)), loci)
                hits[a.name].append((nm, ""%s %s %s %s %s %s"" % (a.name, a.name.split(""-"")[0], locus, ref, a.start, a.end)))
        for hit in hits.values():
            nm = hit[0][0]
            for l in hit:
                if nm == l[0]:
                    print(l[1], file=out_handle)
    return out_file",1,1
"def send(self, packet, retry=True):
        
        addr = ('255.255.255.255', self.port)  
        try:
            self.send_socket.sendto(packet, addr)
        except Exception as e:
            self.log(""Link failed to send packet over socket %s"" % e)
            sleep(0.2)
            if retry:
                self.send(packet, retry=False)",2,2
"def decode_response(client_message, to_object=None):
    
    parameters = dict(response=None)
    response_size = client_message.read_int()
    response = []
    for _ in range(0, response_size):
        response_item = client_message.read_data()
        response.append(response_item)
    parameters['response'] = ImmutableLazyDataList(response, to_object)
    return parameters",3,3
"def __flush(self):
        
        if self.__args is None or self.__disconnecting:
            return False
        
        self.__sigDelayed.emit(self.__args)
        self.__args = None
        self.__timer.stop()
        self.__lastFlushTime = time.time()
        return True",2,2
"def import_from_csv(
    filename_or_fobj,
    encoding=""utf-8"",
    dialect=None,
    sample_size=262144,
    *args,
    **kwargs
):
    
    source = Source.from_file(
        filename_or_fobj, plugin_name=""csv"", mode=""rb"", encoding=encoding
    )

    if dialect is None:
        dialect = discover_dialect(
            sample=read_sample(source.fobj, sample_size), encoding=source.encoding
        )

    reader = unicodecsv.reader(source.fobj, encoding=encoding, dialect=dialect)

    meta = {""imported_from"": ""csv"", ""source"": source}
    return create_table(reader, meta=meta, *args, **kwargs)",1,1
"def buffer_write(library, session, data):
    

    return_count = ViUInt32()
    
    ret = library.viBufWrite(session, data, len(data), byref(return_count))
    return return_count.value, ret",0,0
"def get_python_files(files):
        
        python_files = []
        for file_name in files:
            if file_name.endswith("".py""):
                python_files.append(file_name)

        return python_files",3,1
"def cssify(css_dict):
    
    css = ''
    for key, value in dict_items(css_dict):
        css += '{key} {{ '.format(key=key)
        for field, field_value in dict_items(value):
            css += ' {field}: {field_value};'.format(field=field,
                                                     field_value=field_value)
        css += '} '
    return css.strip()",3,2
"def universal_read(fname):
    
    with open(fname,'rU') as f:
        data = f.read()
    enc_guess = chardet.detect(data)
    return data.decode(enc_guess['encoding'])",1,1
"def was_packet_accepted(self, packet):
        
        self._validatepacket(packet)
        cmd = ord(packet[2])
        if cmd == Vendapin.ACK: 
            return True
        elif cmd == Vendapin.NAK: 
            print('NAK - Rejected/Negative Status')
            return False
        elif cmd == Vendapin.INC: 
            raise Exception('INC - Incomplete Command Packet')
        elif cmd == Vendapin.UNR: 
            raise Exception('UNR - Unrecognized Command Packet')
        elif cmd == Vendapin.CER: 
            raise Exception('CER - Data Packet Checksum Error')
        else:
            raise Exception('Received bad CMD in response from card dispenser')",3,3
"def _append_value(self, value, _file, _name):
        
        _tabs = '\t' * self._tctr
        _keys = '{tabs}<key>{name}</key>\n'.format(tabs=_tabs, name=_name)
        _file.seek(self._sptr, os.SEEK_SET)
        _file.write(_keys)

        self._append_dict(value, _file)",0,0
"def _nxapi_request(commands,
                   method='cli_conf',
                   **kwargs):
    
    if salt.utils.platform.is_proxy():
        return __proxy__['nxos._nxapi_request'](commands, method=method, **kwargs)
    else:
        api_kwargs = __salt__['config.get']('nxos', {})
        api_kwargs.update(**kwargs)
        return __utils__['nxos.nxapi_request'](commands, method=method, **api_kwargs)",2,2
"def download_data_dictionary(request, dataset_id):
    
    dataset = Dataset.objects.get(pk=dataset_id)
    dataDict = dataset.data_dictionary
    fields = DataDictionaryField.objects.filter(
        parent_dict=dataDict
    ).order_by('columnIndex')

    response = HttpResponse(content_type='text/csv')
    csvName = slugify(dataset.title + ' data dict') + '.csv'
    response['Content-Disposition'] = 'attachment; filename=%s' % (csvName)

    csvWriter = writer(response)
    metaHeader = [
        'Data Dictionary for {0} prepared by {1}'.format(
            dataset.title,
            dataset.uploaded_by
        )
    ]
    csvWriter.writerow(metaHeader)
    trueHeader = ['Column Index', 'Heading', 'Description', 'Data Type']
    csvWriter.writerow(trueHeader)

    for field in fields:
        mappedIndex = field.COLUMN_INDEX_CHOICES[field.columnIndex-1][1]
        csvWriter.writerow(
            [mappedIndex, field.heading, field.description, field.dataType]
        )

    return response",1,0
"def read(self, n=-1):
        
        pos = self.tell()
        num_items_to_read = n if n != -1 else self.length - pos
        
        new_pos = min(pos + num_items_to_read, self.length)

        if new_pos > self._current_lob_length:
            missing_num_items_to_read = new_pos - self._current_lob_length
            self._read_missing_lob_data_from_db(self._current_lob_length, missing_num_items_to_read)
        
        self.seek(pos, SEEK_SET)
        return self.data.read(n)",1,1
"def from_csv(filename_or_buffer, copy_index=True, **kwargs):
    
    import pandas as pd
    return from_pandas(pd.read_csv(filename_or_buffer, **kwargs), copy_index=copy_index)",1,1
"def write(self, output):
        
        view_str = output.encode('ascii', 'ignore')
        if (len(view_str) > 0):
            self.m_ser.write(view_str)
            self.m_ser.flush()
            self.m_ser.reset_input_buffer()
            time.sleep(self.m_force_wait)
        pass",0,0
"def decode_response(client_message, to_object=None):
    
    parameters = dict(base=None, increment=None, batch_size=None)
    parameters['base'] = client_message.read_long()
    parameters['increment'] = client_message.read_long()
    parameters['batch_size'] = client_message.read_int()
    return parameters",3,3
"def send_audio_url(self, recipient_id, audio_url, notification_type=NotificationType.regular):
        
        return self.send_attachment_url(recipient_id, ""audio"", audio_url, notification_type)",2,2
"def send_frame(self, frm):
        
        if self._debug:
            log.info(""outgoing message: %s, %s"", self.id, frm[:200])

        if self.state != STATE_OPEN:
            return

        self._feed(FRAME_MESSAGE_BLOB, frm)",2,2
"def _read_hdf_columns(path_or_buf, columns, num_splits, kwargs):  
    

    df = pandas.read_hdf(path_or_buf, columns=columns, **kwargs)
    
    return _split_result_for_readers(0, num_splits, df) + [len(df.index)]",1,1
"def record_id(self, record, type_=None, selector=None):
        
        pk = record_id(record, type_, selector, self.normalize_object_slot)
        return pk",3,3
"def hlp_source_under(folder):
    
    
    packages = []
    package_dir = dict()
    for root, dirs, files in os.walk(folder):
        for file in files:
            if file != '__init__.py':
                continue
            full = os.path.dirname(os.path.join(root, file))
            relative = os.path.relpath(full, folder)
            packages.append(relative)
            package_dir[relative] = full
    
    return 'packages={0},\npackage_dir={1}'.format(sorted(packages), pprint.pformat(package_dir))",0,1
"def run(self):
        
        if not self._should_auto_commit and not self._should_auto_send:
            return
        last_send, last_commit = 0, 0
        while not self._stopped:
            if self._should_auto_commit:
                if last_commit > self._commit_interval:
                    self._docman.commit()
                    
                    last_send, last_commit = 0, 0
                    
                    if self._stopped:
                        break

            if self._should_auto_send:
                if last_send > self._send_interval:
                    self._docman.send_buffered_operations()
                    last_send = 0
            time.sleep(self._sleep_interval)
            last_send += self._sleep_interval
            last_commit += self._sleep_interval",2,2
"def attachments_ping(request):
    
    
    if asbool(request.registry.settings.get('readonly', False)):
        return True

    
    extensions = request.attachment.extensions or {'json'}
    allowed_extension = ""."" + list(extensions)[-1]

    status = False
    try:
        content = cgi.FieldStorage()
        content.filename = HEARTBEAT_FILENAME + allowed_extension
        content.file = BytesIO(HEARTBEAT_CONTENT.encode('utf-8'))
        content.type = 'application/octet-stream'

        stored = utils.save_file(request, content, keep_link=False, replace=True)

        relative_location = stored['location'].replace(request.attachment.base_url, '')
        request.attachment.delete(relative_location)

        status = True
    except Exception as e:
        logger.exception(e)
    return status",0,1
"def drop_relation(self, relation):
        
        with self.connections.fresh_transaction():
            parent = super(RedshiftAdapter, self)
            return parent.drop_relation(relation)",3,3
"def _send_one(self, stream, handle, target_id, name):
        
        if not stream:
            
            return

        data = str(target_id)
        if name:
            data = '%s:%s' % (target_id, name)
        stream.send(
            mitogen.core.Message(
                handle=handle,
                data=data.encode('utf-8'),
                dst_id=stream.remote_id,
            )
        )",2,2
"def supports_gzip(self, context):
        
        if 'request' in context and client.supports_gzip():
            enc = context['request'].META.get('HTTP_ACCEPT_ENCODING', '')
            return 'gzip' in enc and msettings['SERVE_REMOTE']
        return False",2,0
"def save_data(fpath, data, **kwargs):
    
    ext = splitext(fpath)[1]
    if ext in ['.pickle', '.cPkl', '.pkl']:
        return save_cPkl(fpath, data, **kwargs)
    elif ext in ['.json']:
        return save_json(fpath, data, **kwargs)
    elif ext in ['.hdf5']:
        return save_hdf5(fpath, data, **kwargs)
    elif ext in ['.txt']:
        return save_text(fpath, **kwargs)
    elif HAS_NUMPY and ext in ['.npz', '.npy']:
        return save_numpy(fpath, data, **kwargs)
    else:
        assert False, 'unknown ext=%r for fpath=%r' % (ext, fpath)",0,0
"def obj_to_md(self, file_path=None, title_columns=False,
                  quote_numbers=True):
        
        return self.obj_to_mark_down(file_path=file_path,
                                     title_columns=title_columns,
                                     quote_numbers=quote_numbers)",0,0
"def hdr(data, filename):
    
    hdrobj = data if isinstance(data, HDRobject) else HDRobject(data)
    hdrobj.write(filename)",0,0
"def kill(name, signal):
    
    if not exists(name):
        raise ContainerNotExists(""The container (%s) does not exist!"" % name)
    cmd = ['lxc-kill', '--name=%s' % name, signal]
    subprocess.check_call(cmd)",2,2
"def asarray(self, copy=True, cache=False, **kwargs):
        
        data = self._data
        if data is None:
            data = self._read_data(self._fh, **kwargs)
            if cache:
                self._data = data
            else:
                return data
        return deepcopy(data) if copy else data",1,1
"def vasp_version_from_outcar( filename='OUTCAR' ):
    
    with open( filename ) as f:
        line = f.readline().strip()
    return line",3,1
"def get_data_times_for_job_legacy(self, num_job):
        
        
        shift_dur = self.curr_seg[0] + int(self.job_time_shift * num_job)
        job_data_seg = self.data_chunk.shift(shift_dur)

        
        if num_job == (self.num_jobs - 1):
            dataPushBack = job_data_seg[1] - self.curr_seg[1]
            assert dataPushBack >= 0
            job_data_seg = segments.segment(job_data_seg[0] - dataPushBack,
                                                 self.curr_seg[1])
            assert (abs(job_data_seg) == self.data_length)
        return job_data_seg",1,3
"def _on_call_service_msg(self, msg):
        
        if not msg.is_dead:
            th = threading.Thread(target=self._service_stub_main, args=(msg,))
            th.start()",3,3
"def add_configuration_file(self, file_name):
        

        logger.info('adding %s to configuration files', file_name)

        if file_name not in self.configuration_files and self._inotify:
            self._watch_manager.add_watch(file_name, pyinotify.IN_MODIFY)

        if os.access(file_name, os.R_OK):
            self.configuration_files[file_name] = SafeConfigParser()
            self.configuration_files[file_name].read(file_name)
        else:
            logger.warn('could not read %s', file_name)
            warnings.warn('could not read {}'.format(file_name), ResourceWarning)",1,1
"async def _send_sack(self):
        
        gaps = []
        gap_next = None
        for tsn in sorted(self._sack_misordered):
            pos = (tsn - self._last_received_tsn) % SCTP_TSN_MODULO
            if tsn == gap_next:
                gaps[-1][1] = pos
            else:
                gaps.append([pos, pos])
            gap_next = tsn_plus_one(tsn)

        sack = SackChunk()
        sack.cumulative_tsn = self._last_received_tsn
        sack.advertised_rwnd = max(0, self._advertised_rwnd)
        sack.duplicates = self._sack_duplicates[:]
        sack.gaps = [tuple(x) for x in gaps]

        await self._send_chunk(sack)

        self._sack_duplicates.clear()
        self._sack_needed = False",2,2
"def min_pulse_sp(self):
        
        self._min_pulse_sp, value = self.get_attr_int(self._min_pulse_sp, 'min_pulse_sp')
        return value",0,0
"def decode(data, encoding='utf-8', errors='strict'):
    
    data_type = type(data)

    if data_type == bytes:
        return bytes2unicode(data, encoding, errors)
    if data_type in (dict, list, tuple):
        if data_type == dict:
            data = data.items()
        return data_type(map(decode, data))
    return data",2,3
"def create_context(self, message_queue, task_id):
        
        params = (self.settings.dest_directory, self.file_url.json_data, self.seek_amt, self.bytes_to_read)
        return DownloadContext(self.settings, params, message_queue, task_id)",2,1
"def send_file(service, file, fileName=None, password=None, ignoreVersion=False):

    if checkServerVersion(service, ignoreVersion=ignoreVersion) == False:
        print('\033[1;41m!!! Potentially incompatible server version !!!\033[0m')

    
    fileName = fileName if fileName != None else os.path.basename(file.name)

    print('Encrypting data from ""' + fileName + '""')
    keys = secretKeys()
    encData = encrypt_file(file, keys)
    encMeta = encrypt_metadata(keys, fileName)

    print('Uploading ""' + fileName + '""')
    secretUrl, fileId, fileNonce, owner_token = api_upload(service, encData, encMeta, keys)

    if password != None:
        print('Setting password')
        sendclient.password.set_password(secretUrl, owner_token, password)

    return secretUrl, fileId, owner_token",2,2
"def send_and_read(self, packet, endpoint, timeout=15):
        
        queue = self.get_endpoint_queue(endpoint)
        self.send_packet(packet)
        try:
            return queue.get(timeout=timeout)
        finally:
            queue.close()",3,2
"def _broadcast_transport_message(self, origin, message):
        
        self.event_handler.broadcast_event((_EventType.Transport, type(origin), type(message)), message)",3,3
"def read_json(filepath, intkeys=True, intvalues=True):
    
    d = json.load(ensure_open(find_filepath(filepath), mode='rt'))
    d = update_dict_types(d, update_keys=intkeys, update_values=intvalues)
    return d",1,1
"def get_psms(self):
        
        self.header = actions.create_header(self.oldheader)
        self.psms = actions.add_genes_to_psm_table(self.fn, self.oldheader,
                                                   self.lookup)",0,1
"def get_inasafe_default_value_qsetting(
        qsetting, category, inasafe_field_key):
    
    key = 'inasafe/default_value/%s/%s' % (category, inasafe_field_key)
    default_value = qsetting.value(key)
    if default_value is None:
        if category == GLOBAL:
            
            inasafe_field = definition(inasafe_field_key)
            default_value = inasafe_field.get('default_value', {})
            return default_value.get('default_value', zero_default_value)

        return zero_default_value
    try:
        return float(default_value)
    except ValueError:
        return zero_default_value",3,3
"def _is_partial_index(gbi_file):
    
    with open(gbi_file) as in_handle:
        for i, _ in enumerate(in_handle):
            if i > 2:
                return False
    return True",0,1
"def mission_request_partial_list_send(self, target_system, target_component, start_index, end_index, force_mavlink1=False):
                
                return self.send(self.mission_request_partial_list_encode(target_system, target_component, start_index, end_index), force_mavlink1=force_mavlink1)",2,2
"def _get_contigs_to_use(self, contigs_to_use):
        
        if type(contigs_to_use) == set:
            return contigs_to_use
        elif contigs_to_use is None:
            return set()
        else:
            f = pyfastaq.utils.open_file_read(contigs_to_use)
            contigs_to_use = set([line.rstrip() for line in f])
            pyfastaq.utils.close(f)
            return contigs_to_use",3,1
"def connect(self, service_name):
        
        self.close()
        self.socket = nfc.llcp.Socket(self.llc, nfc.llcp.DATA_LINK_CONNECTION)
        self.socket.connect(service_name)
        self.send_miu = self.socket.getsockopt(nfc.llcp.SO_SNDMIU)",2,2
"def send_results(self, client, addr, results):
        
        json_results = json.dumps(results)
        json_results = json_results.encode()

        sent = 0
        total_len = len(json_results)
        got_error = False
        while (sent < total_len):
            this_sent = client.send(json_results[sent:])
            if this_sent == 0:
                got_error = True
                break
            sent += this_sent
        if got_error or sent != total_len:
            self.log_error(""(%s:%s) Failed to send back results to client, ""
                           ""results: %s"" % (addr[0], addr[1], json_results))
        else:
            self.log_debug(""(%s:%s) Results sent back to client successfully.""
                           % (addr[0], addr[1]))",2,2
"def get_label(self):
        
        label = callBigDlFunc(self.bigdl_type, ""imageFeatureToLabelTensor"", self.value)
        return label.to_ndarray()",3,3
"def get_build_status(req_id, nodename):
    
    counter = 0
    req_id = six.text_type(req_id)
    while counter < 10:
        queue = clc.v1.Blueprint.GetStatus(request_id=(req_id))
        if queue[""PercentComplete""] == 100:
            server_name = queue[""Servers""][0]
            creds = get_creds()
            clc.v2.SetCredentials(creds[""user""], creds[""password""])
            ip_addresses = clc.v2.Server(server_name).ip_addresses
            internal_ip_address = ip_addresses[0][""internal""]
            return internal_ip_address
        else:
            counter = counter + 1
            log.info('Creating Cloud VM %s Time out in %s minutes',
                     nodename, six.text_type(10 - counter))
            time.sleep(60)",3,3
"def getOverlayAutoCurveDistanceRangeInMeters(self, ulOverlayHandle):
        

        fn = self.function_table.getOverlayAutoCurveDistanceRangeInMeters
        pfMinDistanceInMeters = c_float()
        pfMaxDistanceInMeters = c_float()
        result = fn(ulOverlayHandle, byref(pfMinDistanceInMeters), byref(pfMaxDistanceInMeters))
        return result, pfMinDistanceInMeters.value, pfMaxDistanceInMeters.value",3,3
"def load(self, draw_bbox = False, **kwargs):
        
        im = Image.new('RGBA', self.img_size)
        draw = None
        if draw_bbox:
            draw = ImageDraw.Draw(im)

        for sprite in self.images:
            data = sprite.load()
            sprite_im = Image.open(BytesIO(data))

            size = sprite.imgrect
            im.paste(sprite_im, (size[0], size[2]))
            if draw_bbox:
                draw.rectangle((size[0], size[2], size[1], size[3]), outline='red')

        del draw
        b = BytesIO()
        im.save(b, format = 'PNG')
        return b.getvalue()",0,1
"def messages(fp, key='@message'):
    
    for line in lines(fp):
        txt = line.rstrip('\n')
        yield {key: txt}",1,3
"def prep_exp_dir(directory=EXP_DIR):
    
    if not os.path.isdir(directory):
        os.makedirs(directory)
    exp_dir = _prepare_directory(directory)
    try:
        
        dirname = os.path.dirname(os.path.realpath(__file__))
        repo = Repo(dirname, search_parent_directories=True)
        with open(os.path.join(exp_dir, ""git_hash.txt""), ""w"") as f:
            print(""SHA1 hash: {hexsha}"".format(hexsha=repo.head.commit.hexsha), file=f)
    except git.exc.InvalidGitRepositoryError: 
        
        
        with open(os.path.join(exp_dir, ""version.txt""), ""w"") as f:
            print(""Persephone version {}"".format(persephone.__version__), file=f)

    return exp_dir",0,0
"def send_sms(body, from_phone, to, flash=False, fail_silently=False,
             auth_user=None, auth_password=None, connection=None):
    
    from sendsms.message import SmsMessage
    connection = connection or get_connection(
        username = auth_user,
        password = auth_password,
        fail_silently = fail_silently
    )
    return SmsMessage(body=body, from_phone=from_phone, to=to, \
        flash=flash, connection=connection).send()",2,2
"def write(self, obj: BioCDocument or BioCPassage or BioCSentence):
        
        if self.level == DOCUMENT and not isinstance(obj, BioCDocument):
            raise ValueError
        if self.level == PASSAGE and not isinstance(obj, BioCPassage):
            raise ValueError
        if self.level == SENTENCE and not isinstance(obj, BioCSentence):
            raise ValueError
        self.writer.write(BioCJSONEncoder().default(obj))",0,0
"def get_sequence(cls, entry):
        
        seq_tag = entry.find(""./sequence"")
        seq = seq_tag.text
        seq_tag.clear()
        return models.Sequence(sequence=seq)",3,3
"def write(self, symbol, data):
        

        
        cursor = self._collection.find()
        for res in cursor:
            library = self._arctic_lib.arctic[res['library_name']]
            dslice = self._slice(data, to_dt(res['start'], mktz('UTC')), to_dt(res['end'], mktz('UTC')))
            if len(dslice) != 0:
                library.write(symbol, dslice)",0,0
"def set_sort(self, request):
        
        
        sort_request = request.GET.get(self.sort_parameter, self.default_sort)
        if sort_request.startswith('-'):
            sort_order = '-'
            sort_field = sort_request.split('-')[1]
        else:
            sort_order = ''
            sort_field = sort_request
        
        if not sort_field in self._allowed_sort_fields:
            sort_order = self.default_sort_order
            sort_field = self.default_sort_field
        return (sort_order, sort_field)",3,3
"def sendall_stderr(self, s):
        
        while s:
            sent = self.send_stderr(s)
            s = s[sent:]
        return None",2,2
"def _get_metric(self, metric, tablename, index_name=None):
        
        end = time.time()
        begin = end - 3 * 60  
        dimensions = [{""Name"": ""TableName"", ""Value"": tablename}]
        if index_name is not None:
            dimensions.append({""Name"": ""GlobalSecondaryIndexName"", ""Value"": index_name})
        period = 60
        data = self.cloudwatch_connection.get_metric_statistics(
            Period=period,
            StartTime=begin,
            EndTime=end,
            MetricName=metric,
            Namespace=""AWS/DynamoDB"",
            Statistics=[""Sum""],
            Dimensions=dimensions,
        )
        points = data[""Datapoints""]
        if not points:
            return 0
        else:
            points.sort(key=lambda r: r[""Timestamp""])
            return float(points[-1][""Sum""]) / period",0,3
"def fetch(self, failures=True, wait=0):
        
        if self.started:
            return fetch_group(self.group, failures=failures, wait=wait, count=self.length(), cached=self.cached)",3,3
"def sendeof(self):
        

        n, byte = self.ptyproc.sendeof()
        self._log_control(byte)",2,2
"def from_url(cls, path):
        
        if os.path.isfile(path):
            with open(path) as fd:
                data = fd.read()
        else:
            try:
                response = urllib.urlopen(path)
                if response.code >= 300:
                    raise RuntimeError('Unable to load repo from %s' % path)

                data = response.read()
                response.close()
            except IOError:
                raise RuntimeError(
                    'Unable to load repo from %s (IO error)' % path
                )

        return cls(json.loads(data), path)",1,1
"def _append_object(self, value, _file):
        
        _labs = ' {'
        _file.write(_labs)
        self._tctr += 1

        for (_item, _text) in value.items():
            _tabs = '\t' * self._tctr
            _cmma = ',' if self._vctr[self._tctr] else ''
            _keys = '{cmma}\n{tabs}""{item}"" :'.format(cmma=_cmma, tabs=_tabs, item=_item)
            _file.write(_keys)

            self._vctr[self._tctr] += 1

            _text = self.object_hook(_text)
            _type = type(_text).__name__
            _MAGIC_TYPES[_type](self, _text, _file)

        self._vctr[self._tctr] = 0
        self._tctr -= 1
        _tabs = '\t' * self._tctr
        _labs = '\n{tabs}{}'.format('}', tabs=_tabs)
        _file.write(_labs)",0,0
"def _preprocess_data(self, data):
        
        if hasattr(data, 'read') and not self.chunksize:
            data = data.read()
        if not hasattr(data, 'read') and self.chunksize:
            data = StringIO(data)

        return data",1,1
"def maybe_download(self, filename, work_directory, source_url):
        
        if not os.path.exists(work_directory):
            os.makedirs(work_directory)
        filepath = os.path.join(work_directory, filename)
        if not os.path.exists(filepath):
            temp_file_name, _ = urllib.request.urlretrieve(source_url)
            copyfile(temp_file_name, filepath)
            print('Successfully downloaded', filename)
        return filepath",1,1
"def Handle_Delete_Label(self, msg):
        

        try:
            label_name = msg['object']['label']
        except KeyError:
            r_msg = {'status': 'ERROR',
                     'type': 'Delete_Label',
                     'object': {'msg': 'improper request'}}
            return json.dumps(r_msg)
        else:
            r_val = {'status': 'OK',
                     'type': 'Delete',
                     'object': self.db.deleteLabel(label_name)}
            return json.dumps(r_val)",3,3
"def read_PIA0_B_control(self, cpu_cycles, op_address, address):
        
        value = self.pia_0_B_control.value
        log.error(
            ""%04x| read $%04x (PIA 0 B side Control reg.) send $%02x (%s) back.\t|%s"",
            op_address, address, value, byte2bit_string(value),
            self.cfg.mem_info.get_shortest(op_address)
        )
        return value",1,1
"def get_addresses(message: list) -> list:
    
    
    addresses = []
    for address in message:
        
        if address == b'':
            break
        addresses.append(address)

    return addresses",3,3
"def __all_curve_names(self, uwis=None, unique=True, count=False, nodepth=True):
        
        uwis = uwis or self.uwis
        c = utils.flatten_list([list(w.data.keys()) for w in self if w.uwi in uwis])
        if nodepth:
            c = filter(lambda x: x not in ['DEPT', 'DEPTH'], c)
        if unique:
            if count:
                return Counter(c).most_common()
            else:
                return [i[0] for i in Counter(c).most_common()]
        return list(c)",3,3
"def _write_items(self, row_lookup, col_lookup, item):
        
        self.qc.write_items(row_lookup, col_lookup, item)",0,0
"def get_reqs(lookup=None, key='INSTALL_REQUIRES'):
    

    if lookup == None:
        lookup = get_lookup()

    install_requires = []
    for module in lookup[key]:
        module_name = module[0]
        module_meta = module[1]
        if ""exact_version"" in module_meta:
            dependency = ""%s==%s"" %(module_name,module_meta['exact_version'])
        elif ""min_version"" in module_meta:
            if module_meta['min_version'] == None:
                dependency = module_name
            else:
                dependency = ""%s>=%s"" %(module_name,module_meta['min_version'])
        install_requires.append(dependency)
    return install_requires",3,3
"def _wait_response(self, message):
        
        if message is None or message.code != defines.Codes.CONTINUE.number:
            self.queue.put(message)",3,3
"def make(self):
        
        eval = self.command.eval()
        with open(self.filename, 'w') as f:
            f.write(eval)",0,0
"def put_file(buffer, modified_file):
    
    import mimetypes
    import boto3
    file_type, _ = mimetypes.guess_type(modified_file)

    s3 = boto3.resource('s3')
    bucket_name, object_key = _parse_s3_file(modified_file)
    extra_args = {
        'ACL': 'public-read',
        'ContentType': file_type
    }
    bucket = s3.Bucket(bucket_name)
    logger.info(""Uploading {0} to {1}"".format(object_key, bucket_name))
    bucket.upload_fileobj(buffer, object_key, ExtraArgs=extra_args)",0,1
"def parse_unifrac_v1_9(unifrac, file_data):
    
    unifrac[""eigvals""] = [float(entry) for entry in file_data[0].split(""\t"")]
    unifrac[""varexp""] = [float(entry)*100 for entry in file_data[3].split(""\t"")]

    for line in file_data[8:]:
        if line == """":
            break
        line = line.split(""\t"")
        unifrac[""pcd""][line[0]] = [float(e) for e in line[1:]]
    return unifrac",1,1
"def get_option(namespace, option_name):
    
    envvalue = _get_env_value(namespace, option_name)

    idx = 0
    final_value = []

    if envvalue:
        final_value.append(envvalue)

    while _get_env_value(namespace, option_name, idx):
        final_value.append(_get_env_value(namespace, option_name, idx))
        idx += 1

    return final_value",1,3
"async def update_emoji(**payload):
    
    data = payload[""data""]
    web_client = payload[""web_client""]
    channel_id = data[""item""][""channel""]
    user_id = data[""user""]

    
    onboarding_tutorial = onboarding_tutorials_sent[channel_id][user_id]

    
    onboarding_tutorial.reaction_task_completed = True

    
    message = onboarding_tutorial.get_message_payload()

    
    updated_message = await web_client.chat_update(**message)

    
    onboarding_tutorial.timestamp = updated_message[""ts""]",3,3
"def cache_git_tag():
    
    try:
        version = __get_git_tag()
        with __open_cache_file('w') as vf:
            vf.write(version)
    except Exception:
        version = __default_version__
    return version",1,0
"def _write_to_filepath(content, output):
    
    outdir = os.path.dirname(output)
    if outdir and not os.path.exists(outdir):
        os.makedirs(outdir)

    with anytemplate.compat.copen(output, 'w') as out:
        out.write(content)",0,0
"def send_msg(self, msg, wait_nak=True, wait_timeout=WAIT_TIMEOUT):
        
        msg_info = MessageInfo(msg=msg, wait_nak=wait_nak,
                               wait_timeout=wait_timeout)
        _LOGGER.debug(""Queueing msg: %s"", msg)
        self._send_queue.put_nowait(msg_info)",2,2
"def read(fname):
    "" read the passed file ""
    if exists(fname):
        return open(join(dirname(__file__), fname)).read()",1,1
"def record_source(self, src, prg=''):
        
        self._log(self.logFileSource , force_to_string(src), prg)",1,0
"def flatten(input, file=None, encoding=None, errors='strict'):
	
	
	input = stream(input, encoding, errors)
	
	if file is None:  
		return b''.join(input) if encoding else ''.join(input)
	
	counter = 0
	
	for chunk in input:
		file.write(chunk)
		counter += len(chunk)
	
	return counter",0,0
"def form_valid(self, form):
        
        LOGGER.debug('termsandconditions.views.EmailTermsView.form_valid')

        template = get_template(""termsandconditions/tc_email_terms.html"")
        template_rendered = template.render({""terms"": form.cleaned_data.get('terms')})

        LOGGER.debug(""Email Terms Body:"")
        LOGGER.debug(template_rendered)

        try:
            send_mail(form.cleaned_data.get('email_subject', _('Terms')),
                      template_rendered,
                      settings.DEFAULT_FROM_EMAIL,
                      [form.cleaned_data.get('email_address')],
                      fail_silently=False)
            messages.add_message(self.request, messages.INFO, _(""Terms and Conditions Sent.""))
        except SMTPException:  
            messages.add_message(self.request, messages.ERROR, _(""An Error Occurred Sending Your Message.""))

        self.success_url = form.cleaned_data.get('returnTo', '/') or '/'

        return super(EmailTermsView, self).form_valid(form)",2,2
"def subjects_create(self, filename):
        
        
        if get_filename_suffix(filename, ARCHIVE_SUFFIXES) is None:
            raise ValueError('invalid file suffix: ' + os.path.basename(os.path.normpath(filename)))
        
        
        return self.subjects.upload_file(filename)",1,1
"def _read_returned_msg(self, method_frame):
        
        header_frame, body = self._reap_msg_frames(method_frame)

        return_info = {
            'channel': self.channel,
            'reply_code': method_frame.args.read_short(),
            'reply_text': method_frame.args.read_shortstr(),
            'exchange': method_frame.args.read_shortstr(),
            'routing_key': method_frame.args.read_shortstr()
        }

        return Message(body=body, return_info=return_info,
                       **header_frame.properties)",1,3
"def write(self, basename = 'graph', layout = None, format='png'):
        
        if layout == None:
            layout = self.get_layout()
        f = open(basename, ""w+b"")
        if format == 'dot':
            f.write(self.to_string())
        else:
            f.write(self.create(layout, format))
        f.close()",0,0
"def mifare_classic_read_block(self, block_number):
        
        
        response = self.call_function(PN532_COMMAND_INDATAEXCHANGE,
                                      params=[0x01, MIFARE_CMD_READ, block_number & 0xFF],
                                      response_length=17)
        
        if response[0] != 0x00:
            return None
        
        return response[1:]",1,1
"def update_retention(self):
        
        
        
        if self.pushed_conf.retention_update_interval == 0:
            logger.debug(""Should have saved retention but it is not enabled"")
            return

        _t0 = time.time()
        self.hook_point('save_retention')
        statsmgr.timer('hook.retention-save', time.time() - _t0)

        self.add(make_monitoring_log('INFO', 'RETENTION SAVE: %s' % self.my_daemon.name))
        logger.info('Retention data saved: %.2f seconds', time.time() - _t0)",0,2
"def send_external(self, http_verb, host, url, http_headers, chunk):
        
        if http_verb == 'PUT':
            return self.http.put(host + url, data=chunk, headers=http_headers)
        elif http_verb == 'POST':
            return self.http.post(host + url, data=chunk, headers=http_headers)
        else:
            raise ValueError(""Unsupported http_verb:"" + http_verb)",2,2
"def set_data(self, source_data):
        
        self.street = source_data['street']
        self.zipcode = source_data['zipcode']
        self.city = source_data['city']
        self.state = source_data['state']
        self.latitude = source_data['latitude']
        self.longitude = source_data['longitude']",3,3
"def heartbeat(self):
        

        self.thread_debug(""heartbeat"")

        
        if self.last_stats:
            if self.stats.http_run <= self.last_stats.http_run:
                self.NOTIFY(""No monitors run since last heartbeat!"", service=""heartbeat"")
                return
            elif self.stats.http_handled <= self.last_stats.http_handled:
                self.NOTIFY(""No monitor results handled since last heartbeat!"", service=""heartbeat"")
                return

        
        if self.config.get('heartbeat-hook'):
            result = requests.get(self.config.get('heartbeat-hook'))
            if result.status_code != 200:
                self.NOTIFY(""Heartbeat ping to statuscake failed!"", level=""ERROR"")

        
        self.last_stats = self.stats.copy()",2,2
"def split_six(series=None):
    
    if pd is None:
        raise ImportError('The Pandas package is required'
                          ' for this functionality')
    if np is None:
        raise ImportError('The NumPy package is required'
                          ' for this functionality')

    def base(x):
        if x > 0:
            base = pow(10, math.floor(math.log10(x)))
            return round(x/base)*base
        else:
            return 0

    quants = [0, 50, 75, 85, 90]
    
    arr = series.values
    return [base(np.percentile(arr, x)) for x in quants]",3,3
"def _get_file_iterator(self, file_obj):
        
        file_obj.seek(0)

        return iter(lambda: file_obj.read(self.read_bs), '')",1,1
"def default_classification_value_maps(classification):
    
    value_maps = {}
    for hazard_class in classification['classes']:
        value_maps[hazard_class['key']] = hazard_class.get(
            'string_defaults', [])

    return value_maps",3,3
"def get_items(self):
        
        
        reader = csv.reader(self.source)
        
        headers = reader.next()
        
        
        for row in reader:
            
            if not row:
                continue
            yield dict(zip(headers, row))",1,1
"def get_coordinates(filename, fmt):
    
    if fmt == ""xyz"":
        get_func = get_coordinates_xyz
    elif fmt == ""pdb"":
        get_func = get_coordinates_pdb
    else:
        exit(""Could not recognize file format: {:s}"".format(fmt))

    return get_func(filename)",1,1
"def concretize(self, **kwargs):
        
        return (self._read_file.concretize(**kwargs), self._write_file.concretize(**kwargs))",0,0
"def write(self, chars, output, format='png'):
        
        im = self.generate_image(chars)
        return im.save(output, format=format)",0,0
"def error(self, interface_id, errorcode, msg):
        
        LOG.debug(""RPCFunctions.error: interface_id = %s, errorcode = %i, message = %s"" % (
            interface_id, int(errorcode), str(msg)))
        if self.systemcallback:
            self.systemcallback('error', interface_id, errorcode, msg)
        return True",2,2
"def send(self, symbol, start_date, end_date):
        
        query = self.prepare_query(symbol, start_date, end_date)

        self.parameters['q'] = query
        response = requests.get(self.api, params=self.parameters).json()

        results = response['query']['results']['quote']

        return results",2,2
"def read_jp2_image(filename):
    
    
    
    image = read_image(filename)

    with open(filename, 'rb') as file:
        bit_depth = get_jp2_bit_depth(file)

    return fix_jp2_image(image, bit_depth)",1,1
"def readString(self, bytes=False):
        
        l = self.stream.read_ushort()
        b = self.stream.read(l)

        if bytes:
            return b

        return self.context.getStringForBytes(b)",1,1
"def query(self, transport, protocol, *data):
        
        if not self._query:
            raise AttributeError('Command is not queryable')
        if self.protocol:
            protocol = self.protocol
        if self._query.data_type:
            data = _dump(self._query.data_type, data)
        else:
            
            data = ()
        if isinstance(transport, SimulatedTransport):
            response = self.simulate_query(data)
        else:
            response = protocol.query(transport, self._query.header, *data)
        response = _load(self._query.response_type, response)

        
        return response[0] if len(response) == 1 else response",2,2
"def flush(self):
        

        self.clear()
        self.delete(self.session_key)
        self.create()",1,0
"def putResourceValue(self,ep,res,data,cbfn=""""):
		
		result = asyncResult(callback=cbfn)
		result.endpoint = ep
		result.resource = res
		data = self._putURL(""/endpoints/""+ep+res,payload=data)
		if data.status_code == 200: 
			result.error = False
			result.is_done = True
		elif data.status_code == 202:
			self.database['async-responses'][json.loads(data.content)[""async-response-id""]]= result
		else:
			result.error = response_codes(""resource"",data.status_code)
			result.is_done = True
		result.raw_data = data.content
		result.status_code = data.status_code
		return result",2,2
"def get_matrix_dimensions(filename):
    
    stream = None
    try:
        stream = open(filename, 'r')
        length = len(stream.readlines())
        stream.seek(0)
        width = len(stream.readline().split(','))
        return (length, width)
    except IOError as err:  
        LOG.info(""Something wrong happend while reading the file %s ""
                 % filename)
        LOG.debug(""ERROR: %s"" % err)
    finally:
        if stream:
            stream.close()",1,1
"def setup_allelespecific_database(fasta_file, database_folder, allele_list):
    
    index = SeqIO.index(os.path.join(database_folder, 'rMLST_combined.fasta'), 'fasta')
    seqs = list()
    for s in allele_list:
        try:
            seqs.append(index[s])
        except KeyError:
            logging.warning('Tried to add {} to allele-specific database, but could not find it.'.format(s))
    SeqIO.write(seqs, fasta_file, 'fasta')",0,1
"def job_step_complete(self, job_request_payload):
        
        if job_request_payload.success_command == JobCommands.STORE_JOB_OUTPUT_COMPLETE:
            raise ValueError(""Programmer error use use job_step_store_output_complete instead."")
        payload = JobStepCompletePayload(job_request_payload)
        self.send(job_request_payload.success_command, payload)",3,2
"def vncRequestPassword(self):
        
        if self.factory.password is None:
            log.msg(""need a password"")
            self.transport.loseConnection()
            return
        self.sendPassword(self.factory.password)",2,2
"def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):
        
        super(RevokeResponsePayload, self).read(
            istream,
            kmip_version=kmip_version
        )
        tstream = BytearrayStream(istream.read(self.length))

        self.unique_identifier = attributes.UniqueIdentifier()
        self.unique_identifier.read(tstream, kmip_version=kmip_version)

        self.is_oversized(tstream)
        self.validate()",1,1
"def from_pem(cls, data, password=None):
        
        obj = cls()
        obj.import_from_pem(data, password)
        return obj",1,1
"def widget_df(self):
    

    if hasattr(self, 'widget_instance') == True:

      if self.widget_instance.mat_string != '':

        tmp_net = deepcopy(Network())

        df_string = self.widget_instance.mat_string

        tmp_net.load_file_as_string(df_string)

        df = tmp_net.export_df()

        return df

      else:
        return self.export_df()

    else:
      if hasattr(self, 'widget_class') == True:
        print('Please make the widget before exporting the widget DataFrame.')
        print('Do this using the widget method: net.widget()')

      else:
        print('Can not make widget because Network has no attribute widget_class')
        print('Please instantiate Network with clustergrammer_widget using: Network(clustergrammer_widget)')",3,1
"def add_message(self, request, level, message_template,
                    message_context=None, extra_tags=''):
        
        if 'django.contrib.messages' in settings.INSTALLED_APPS:
            try:
                if message_context is None:
                    message_context = {}
                message = render_to_string(message_template,
                                           message_context).strip()
                if message:
                    messages.add_message(request, level, message,
                                         extra_tags=extra_tags)
            except TemplateDoesNotExist:
                pass",3,3
"def scan(ctx, sources=None, endpoint=False, raw=False, extra=False):
    
    verbose = ctx.obj['VERBOSE']
    sTime = ctx.obj['STIME']
    print_opts = {
        'labels': verbose,
        'extra': extra,
    }
    if sources or (sources and endpoint):
        action_analyze(sources, endpoint, print_opts, verbose, extra, raw)
        eTime = time.time()
        tTime = eTime - sTime
        printDebug(""\n-----------\n"" + ""Time:	   %0.2fs"" % tTime, ""comment"")

    else:
        click.echo(ctx.get_help())",3,3
"def build_payload(self, payload):
        
        remaining_size = self.MAX_SEGMENT_PAYLOAD_SIZE

        for part in self.parts:
            part_payload = part.pack(remaining_size)
            payload.write(part_payload)
            remaining_size -= len(part_payload)",0,0
"def dictToFile(dictionary,replicateKey,outFileName):
    
    replicateToFile=h5py.File(outFileName,""w"")
    for i in range(len(dictionary[replicateKey])):
        replicateToFile.create_dataset(""{}"".format(dictionary[replicateKey].keys()[i])\
                                    ,data=dictionary[replicateKey].values()[i]\
                                    ,compression=""gzip"")
    replicateToFile.close()",1,0
"def send_custom_svc_notification(self, service, options, author, comment):
        
        logger.warning(""The external command 'SEND_CUSTOM_SVC_NOTIFICATION' ""
                       ""is not currently implemented in Alignak. If you really need it, ""
                       ""request for its implementation in the project repository: ""
                       ""https://github.com/Alignak-monitoring/alignak"")
        self.send_an_element(make_monitoring_log(
            'warning', 'SEND_CUSTOM_SVC_NOTIFICATION: this command is not implemented!'))",2,2
"def rewrite_schema(r, df, doc=None):
    

    from metapack.cli.core import write_doc

    if doc is None:
        doc = open_source_package()

    rebuild_schema(doc, r, df)

    write_doc(doc, doc.ref)",0,0
"def validate_checksum( filename, md5sum ):
    
    filename = match_filename( filename )
    md5_hash = file_md5( filename=filename )
    if md5_hash != md5sum:
        raise ValueError('md5 checksums are inconsistent: {}'.format( filename ))",1,1
"def load(path, use_nep8=True):
        

        Compiler.__instance = None

        compiler = Compiler.instance()
        compiler.nep8 = use_nep8
        compiler.entry_module = Module(path)

        return compiler",0,1
"def promote(self):
        
        with self.mutex:
            self.rwlock -= 1
            while self.rwlock != 0:
                self._writer_wait()
            self.rwlock = -1",0,0
"def qindex2index(index):
        
        r = index.row()
        c = index.column()
        if c > 0x10:
            return (0x10 * r) + c - 0x11
        else:
            return (0x10 * r) + c",3,3
"def delete_file(self, commit, path):
        
        req = proto.DeleteFileRequest(file=proto.File(commit=commit_from(commit), path=path))
        self.stub.DeleteFile(req, metadata=self.metadata)",3,3
"def on_privmsg(self, connection, event):
        
        sender = self.get_nick(event.source)
        message = event.arguments[0]
        
        if sender == 'NickServ':
            logging.info(""Got message from NickServ: %s"", message)
            if ""password"" in message.lower():
                connection.privmsg(""NickServ"", ""pass"")
            else:
                connection.join('
            
            return
        
        self._pool.enqueue(self.__on_message, connection, sender, message)",3,3
"def delete(self, msg, claim_id=None):
        
        msg_id = utils.get_id(msg)
        if claim_id:
            uri = ""/%s/%s?claim_id=%s"" % (self.uri_base, msg_id, claim_id)
        else:
            uri = ""/%s/%s"" % (self.uri_base, msg_id)
        return self._delete(uri)",3,3
"def write(self, content):
        
        self.buffer.write(content)
        if settings.DEBUG:
            self.buffer.write('\n')",0,0
"def pickle(self, f, protocol=-1):
        
        try: 
            import cPickle as pickle
            if isinstance(f, basestring):
                with open(f, 'wb') as f:
                    pickle.dump(self, f, protocol)
            else:
                pickle.dump(self, f, protocol)
        except ImportError: 
            import pickle
            if isinstance(f, str):
                with open(f, 'wb') as f:
                    pickle.dump(self, f, protocol)
            else:
                pickle.dump(self, f, protocol)",0,0
"def load_pid(pidfile):
    
    if pidfile and os.path.isfile(pidfile):
        with open(pidfile, ""r"", encoding=""utf-8"") as fobj:
            return int(fobj.readline().strip())
    return 0",1,1
"def write(self, msg):
        
        if self.output.closed:
            raise IOError(""writing %s on a closed file"" % msg)
        self.output.write(msg)
        if self.flush_after_write: self.flush()
        return",0,0
"def InternalExchange(self, cmd, payload_in):
    
    
    self.logger.debug('payload: ' + str(list(payload_in)))
    payload = bytearray()
    payload[:] = payload_in
    for _ in range(2):
      self.InternalSend(cmd, payload)
      ret_cmd, ret_payload = self.InternalRecv()

      if ret_cmd == UsbHidTransport.U2FHID_ERROR:
        if ret_payload == UsbHidTransport.ERR_CHANNEL_BUSY:
          time.sleep(0.5)
          continue
        raise errors.HidError('Device error: %d' % int(ret_payload[0]))
      elif ret_cmd != cmd:
        raise errors.HidError('Command mismatch!')

      return ret_payload
    raise errors.HidError('Device Busy.  Please retry')",3,2
"def send_text_message(self, recipient_id, message, notification_type=NotificationType.regular):
        
        return self.send_message(recipient_id, {
            'text': message
        }, notification_type)",2,2
"def fromfile(cls, f, n=-1):
        
        headerlen = calcsize(cls.FILE_FMT)

        if 0 < n < headerlen:
            raise ValueError('n too small!')

        filter = cls(1)  
        filter._setup(*unpack(cls.FILE_FMT, f.read(headerlen)))
        filter.bitarray = bitarray.bitarray(endian='little')
        if n > 0:
            (filter.bitarray.frombytes(f.read(n-headerlen)) if is_string_io(f)
             else filter.bitarray.fromfile(f, n - headerlen))
        else:
            (filter.bitarray.frombytes(f.read()) if is_string_io(f)
             else filter.bitarray.fromfile(f))
        if filter.num_bits != filter.bitarray.length() and \
               (filter.num_bits + (8 - filter.num_bits % 8)
                != filter.bitarray.length()):
            raise ValueError('Bit length mismatch!')

        return filter",1,1
"def handle_message(self, msg):
        
        
        if self._logger.isEnabledFor(logging.DEBUG):
            self._logger.debug(
                ""received from {}: {}""
                .format(self.bind_address_string, repr(str(msg))))

        if msg.mtype == Message.INFORM:
            return self.handle_inform(msg)
        elif msg.mtype == Message.REPLY:
            return self.handle_reply(msg)
        elif msg.mtype == Message.REQUEST:
            return self.handle_request(msg)
        else:
            self._logger.error(""Unexpected message type from server ['%s'].""
                               % (msg,))",3,3
"def gen_send_version_url(ip, port):
    
    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, VERSION_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)",2,2
"def _read_frame(self, blocksize):
        
        try:
            read_func = _fr_type_map[self.channel_type][0]
            dtype = _fr_type_map[self.channel_type][1]
            data = read_func(self.stream, self.channel_name,
                             self.read_pos, int(blocksize), 0)
            return TimeSeries(data.data.data, delta_t=data.deltaT,
                              epoch=self.read_pos,
                              dtype=dtype)
        except Exception:
            raise RuntimeError('Cannot read {0} frame data'.format(self.channel_name))",1,1
"def getMessage(self, message):
        
        if message.filename != self._filename:
            raise RuntimeError(
                'Message filename ({:}) does not match batch filename ({:}).'
                .format(message.filename, self._filename))
        self._file.seek(message.offset)
        self._logger.debug(
            'Retrieved message offset={:}, size={:}'
            .format(message.offset, message.size))
        return self._file.read(message.size)",3,3
"def send_text(hwnd, txt):
    
    try:
        for c in txt:
            if c == '\n':
                win32api.SendMessage(hwnd, win32con.WM_KEYDOWN, win32con.VK_RETURN, 0)
                win32api.SendMessage(hwnd, win32con.WM_KEYUP, win32con.VK_RETURN, 0)
            else:
                win32api.SendMessage(hwnd, win32con.WM_CHAR, ord(c), 0)            
    except Exception as ex:
        print('error calling SendMessage ' + str(ex))",2,2
"def set_data(self, source_data):
        
        self.region_name = source_data['region']['@name']
        self.region_id = source_data['region']['@id']
        self.region_type =  source_data['region']['@type']
        self.zillow_home_value_index = source_data.get('zindexValue', None)
        self.overview_link =  source_data['region']['links']['overview']
        self.fsbo_link =  source_data['region']['links']['forSaleByOwner']
        self.sale_link =  source_data['region']['links']['forSale']",3,3
"def _handle_command_response(self, res, event):
        

        response_handler = None

        if isinstance(res, basestring):
            response_handler = functools.partial(self._send_rtm_message, event['channel'])
        elif isinstance(res, dict):
            response_handler = self._send_api_message

        if response_handler is not None:
            response_handler(res)",2,2
"def verify(self, signed):
        
        
        buf = create_string_buffer(libcrypto.RSA_size(self._rsa))
        signed = salt.utils.stringutils.to_bytes(signed)
        size = libcrypto.RSA_public_decrypt(len(signed), signed, buf, self._rsa, RSA_X931_PADDING)
        if size < 0:
            raise ValueError('Unable to decrypt message')
        return buf[0:size]",3,3
"def _wrap_callback_parse_cmdhist_data(subscription, on_data, message):
    
    if (message.type == message.DATA and
            message.data.type == yamcs_pb2.CMD_HISTORY):
        entry = getattr(message.data, 'command')
        
        rec = subscription._process(entry)
        if on_data:
            on_data(rec)",3,3
"def _launch_editor(starting_text=''):
    ""Launch editor, let user write text, then return that text.""
    
    
    editor = os.environ.get('EDITOR', 'vim')

    with tempfile.TemporaryDirectory() as dirname:
        filename = pathlib.Path(dirname) / 'metadata.yml'
        with filename.open(mode='wt') as handle:
            handle.write(starting_text)
        subprocess.call([editor, filename])

        with filename.open(mode='rt') as handle:
            text = handle.read()
    return text",0,0
"def read_fits_bintable (self, hdu=1, drop_nonscalar_ok=True, **kwargs):
        
        from astropy.io import fits
        from .numutil import fits_recarray_to_data_frame as frtdf

        with fits.open (text_type (self), mode='readonly', **kwargs) as hdulist:
            return frtdf (hdulist[hdu].data, drop_nonscalar_ok=drop_nonscalar_ok)",1,1
"def _downloadStream(self, fileName, encrypt=True):
        
        blob = self.bucket.get_blob(bytes(fileName), encryption_key=self.sseKey if encrypt else None)
        if blob is None:
            raise NoSuchFileException(fileName)

        class DownloadPipe(ReadablePipe):
            def writeTo(self, writable):
                try:
                    blob.download_to_file(writable)
                finally:
                    writable.close()

        with DownloadPipe() as readable:
            yield readable",1,0
"def write_file(fname, *lines):
    'write lines to a file'
    yield 'touch {}'.format(fname)
    for line in lines:
        yield ""echo {} >> {}"".format(line, fname)",0,0
"def check_sim_out(self):
        
        now = time.time()
        if now - self.last_sim_send_time < 0.02 or self.rc_channels_scaled is None:
            return
        self.last_sim_send_time = now

        servos = []
        for ch in range(1,9):
            servos.append(self.scale_channel(ch, getattr(self.rc_channels_scaled, 'chan%u_scaled' % ch)))
        servos.extend([0,0,0, 0,0,0])
        buf = struct.pack('<14H', *servos)
        try:
            self.sim_out.send(buf)
        except socket.error as e:
            if not e.errno in [ errno.ECONNREFUSED ]:
                raise
            return",2,2
"def send_video(self, recipient_id, video_path, notification_type=NotificationType.regular):
        
        return self.send_attachment(recipient_id, ""video"", video_path, notification_type)",2,2
"def read_preferences_file(self):
        
        user_data_dir = find_pmag_dir.find_user_data_dir(""thellier_gui"")
        if not user_data_dir:
            return {}
        if os.path.exists(user_data_dir):
            pref_file = os.path.join(user_data_dir, ""thellier_gui_preferences.json"")
            if os.path.exists(pref_file):
                with open(pref_file, ""r"") as pfile:
                    return json.load(pfile)
        return {}",1,1
"def send(self):
        
        sent_chunks = 0
        chunk_num = self.index
        with open(self.filename, 'rb') as infile:
            infile.seek(self.index * self.chunk_size)
            while sent_chunks != self.num_chunks_to_send:
                chunk = infile.read(self.chunk_size)
                self._send_chunk(chunk, chunk_num)
                self.progress_queue.processed(1)
                chunk_num += 1
                sent_chunks += 1",2,2
"def read_py_file(filename, skip_encoding_cookie=True):
    
    with open(filename) as f:   
        if skip_encoding_cookie:
            return """".join(strip_encoding_cookie(f))
        else:
            return f.read()",1,1
"def import_from_netcdf(network, path, skip_time=False):
    

    assert has_xarray, ""xarray must be installed for netCDF support.""

    basename = os.path.basename(path) if isinstance(path, string_types) else None
    with ImporterNetCDF(path=path) as importer:
        _import_from_importer(network, importer, basename=basename,
                              skip_time=skip_time)",1,1
"def sharepoint(self, *, resource=''):
        

        if not isinstance(self.protocol, MSGraphProtocol):
            
            raise RuntimeError(
                'Sharepoint api only works on Microsoft Graph API')

        return Sharepoint(parent=self, main_resource=resource)",1,2
"def tempfile(self, mode='wb', **args):
        ""write the contents of the file to a tempfile and return the tempfile filename""
        tf = tempfile.NamedTemporaryFile(mode=mode)
        self.write(tf.name, mode=mode, **args)
        return tfn",0,0
"def write_df_tpl(filename,df,sep=',',tpl_marker='~',**kwargs):
    
    with open(filename,'w') as f:
        f.write(""ptf {0}\n"".format(tpl_marker))
        f.flush()
        df.to_csv(f,sep=sep,mode='a',**kwargs)",0,0
"def _write_script(path, lines, chmod=True):
        
        if len(lines) > 0:
            lastline = lines.pop()
        for line in lines:
            self.install.append('echo ""%s"" >> %s' %path)
        self.install.append(lastline)     

        if chmod is True:
            self.install.append('chmod u+x %s' %path)",0,0
"def read_multi_vars(self, items):
        
        result = self.library.Cli_ReadMultiVars(self.pointer, byref(items),
                                                c_int32(len(items)))
        check_error(result, context=""client"")
        return result, items",1,1
"def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):
        
        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)",0,2
"def print_dict():
    
    global g_ok_java_messages
    global g_java_messages_to_ignore_text_filename

    allKeys = sorted(g_ok_java_messages.keys())

    with open(g_java_messages_to_ignore_text_filename,'w') as ofile:
        for key in allKeys:

            for mess in g_ok_java_messages[key]:
                ofile.write('KeyName: '+key+'\n')
                ofile.write('IgnoredMessage: '+mess+'\n')

            print('KeyName: ',key)
            print('IgnoredMessage: ',g_ok_java_messages[key])
            print('\n')",1,0
"def add_raw_data(self, data, attrs):
        
        self.assert_writeable()
        if ""Raw"" not in self.handle:
            self.handle.create_group(""Raw"")
        if ""Signal"" in self.handle['Raw']:
            msg = ""Fast5 file already has raw data for read '{}' in: {}""
            raise KeyError(msg.format(self.read_id, self.filename))
        self.handle['Raw'].create_dataset('Signal', data=data, compression='gzip', shuffle=True, dtype='i2')
        self._add_attributes(""Raw"", attrs, clear=True)",1,0
"def _function_with_partly_reduce(chunk_list, map_function, kwargs):
    
    kwargs = kwargs or {}
    results = (map_function(chunk, **kwargs) for chunk in chunk_list)
    results = list(itertools.chain.from_iterable(results))
    return results",2,3
"def send(self, stack: Layers):
        

        if not isinstance(stack, Stack):
            stack = Stack(stack)

        if not self.platform.accept(stack):
            raise UnacceptableStack('The platform does not allow ""{}""'
                                    .format(stack.describe()))

        self._stacks.append(stack)",2,2
"def delete_message(self, message, callback=None):
        
        return self.connection.delete_message(self, message, callback=callback)",3,3
"def _append_value(self, value, _file, _name):
        
        _tabs = '\t' * self._tctr
        _cmma = ',\n' if self._vctr[self._tctr] else ''
        _keys = '{cmma}{tabs}""{name}"" :'.format(cmma=_cmma, tabs=_tabs, name=_name)

        _file.seek(self._sptr, os.SEEK_SET)
        _file.write(_keys)

        self._vctr[self._tctr] += 1
        self._append_object(value, _file)",0,0
"def send_collected_data(self):
        
        data = self.__collected_data
        self.__collected_data = []
        for listener in self.listeners:
            
            listener.monitoring_data(copy.deepcopy(data))",2,2
"def write_tf_examples(filename, tf_examples, serialize=True):
    
    with tf.python_io.TFRecordWriter(
            filename, options=TF_RECORD_CONFIG) as writer:
        for ex in tf_examples:
            if serialize:
                writer.write(ex.SerializeToString())
            else:
                writer.write(ex)",0,0
"def collect_analysis(self):
        
        analysis = {}
        for field in self.fields.values():
            for analyzer_name in ('analyzer', 'index_analyzer', 'search_analyzer'):
                if not hasattr(field, analyzer_name):
                    continue

                analyzer = getattr(field, analyzer_name)

                if not isinstance(analyzer, Analyzer):
                    continue

                definition = analyzer.get_analysis_definition()
                if definition is None:
                    continue

                for key in definition:
                    analysis.setdefault(key, {}).update(definition[key])

        return analysis",3,3
"def sndwrite_chunked(sr: int, outfile: str, encoding: str) -> _SndWriter:
    
    backends = [backend for backend in _getBackends() if backend.can_write_chunked]
    if not backends:
        raise SndfileError(""No backend found to support the given format"")
    print(backends)
    backend = min(backends, key=lambda backend:backend.priority)
    logger.debug(f""sndwrite_chunked: using backend {backend.name}"")
    return backend.writer(sr, outfile, encoding)",0,0
"def run(self, schedule_id, **kwargs):
        
        subscriptions = Subscription.objects.filter(
            schedule_id=schedule_id, active=True, completed=False, process_status=0
        ).values(""id"")
        for subscription in subscriptions.iterator():
            send_next_message.delay(str(subscription[""id""]))",2,2
"def get_autostart() -> AutostartSettings:
    
    autostart_file = Path(common.AUTOSTART_DIR) / ""autokey.desktop""
    if not autostart_file.exists():
        return AutostartSettings(None, False)
    else:
        return _extract_data_from_desktop_file(autostart_file)",1,1
"def add_lbaas_port(self, port_id, lb_id):
        
        port_info = self.neutronclient.show_port(port_id)
        port = port_info.get('port')
        if not port:
            LOG.error(""Can not retrieve port info for port %s"" % port_id)
            return
        LOG.debug(""lbaas add port, %s"", port)
        if not port['binding:host_id']:
            LOG.info(""No host bind for lbaas port, octavia case"")
            return
        port[""device_id""] = lb_id

        vm_info = self._make_vm_info(port, 'up', constants.LBAAS_PREFIX)
        self.port[port_id] = vm_info
        if self.send_vm_info(vm_info):
            self.add_vms_db(vm_info, constants.RESULT_SUCCESS)
        else:
            self.add_vms_db(vm_info, constants.CREATE_FAIL)",2,2
"def prev(self, n=1):
        
        
        i = abs(self.tell - n)

        
        return self.get(i, n)",1,1
"def get_wildcard_mimetype(self):
		
		mimetype = self.wildcard_mimetype
		if mimetype is None:	
			mimetype = WILDCARD_MIMETYPE
		if mimetype is None:	
			mimetype = 'application/rdf+xml'
		return mimetype",2,3
"def _on_message(channel, method, header, body):
    
    print ""Message:""
    print ""\t%r"" % method
    print ""\t%r"" % header
    print ""\t%r"" % body

    
    channel.basic_ack(method.delivery_tag)

    
    channel.stop_consuming()",3,3
"def get(self, number, arch, abi_list=()):
        
        name, arch, abi = self._canonicalize(number, arch, abi_list)
        proc = super(SimSyscallLibrary, self).get(name, arch)
        proc.is_syscall = True
        self._apply_numerical_metadata(proc, number, arch, abi)
        return proc",3,3
"def update(self, searched_resource, uri_parameters=None, request_body_dict=None, query_parameters_dict=None,
               additional_headers=None):
        
        return self._request(searched_resource, 'put', uri_parameters, request_body_dict, query_parameters_dict,
                             additional_headers)",2,2
"def parse_message(cls, message):
        
        length = struct.unpack_from('!H', message, 0)[0] + 4
        if len(message) < length:
            raise IncompleteMessage()
        command, = struct.unpack_from('!H', message, 2)
        if command in _PacketRegistry:
            return _PacketRegistry[command].parse(message[4:length])[0], length
        else:
            return None, length",3,3
"def getReturnPage(context, prior=False):
    
    siteHistory = getattr(context.get('request',None),'session',{}).get('SITE_HISTORY',{})
    return returnPageHelper(siteHistory,prior=prior)",3,3
"def sendall(self, data, flags=0):
        
        sent = self.send(data, flags)
        while sent < len(data):
            sent += self.send(data[sent:], flags)",2,2
"def _onOutgoingConnected(self, conn):
        

        if self._syncObj.encryptor:
            conn.setOnMessageReceivedCallback(functools.partial(self._onOutgoingMessageReceived, conn)) 
            conn.recvRandKey = os.urandom(32)
            conn.send(conn.recvRandKey)
        else:
            
            if not self._selfIsReadonlyNode:
                conn.send(self._selfNode.address)
            else:
                conn.send('readonly')
            self._onNodeConnected(self._connToNode(conn))",3,2
"def _read_bam(bam_fn, precursors):
    
    mode = ""r"" if bam_fn.endswith(""sam"") else ""rb""
    handle = pysam.Samfile(bam_fn, mode)
    reads = defaultdict(realign)
    for line in handle:
        chrom = handle.getrname(line.reference_id)
        
        query_name = line.query_name
        if query_name not in reads:
            reads[query_name].sequence = line.query_sequence
        iso = isomir()
        iso.align = line
        iso.start = line.reference_start
        iso.subs, iso.add = _realign(reads[query_name].sequence, precursors[chrom], line.reference_start)
        reads[query_name].set_precursor(chrom, iso)

    reads = _clean_hits(reads)
    return reads",1,1
"def on_consumer_cancelled(self, method_frame):
        
        _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)
        if self._channel:
            self._channel.close()",2,2
"def PLAY(self):
        
        message = ""PLAY "" + self.session.url + "" RTSP/1.0\r\n""
        message += self.sequence
        message += self.authentication
        message += self.user_agent
        message += self.session_id
        message += '\r\n'
        return message",2,2
"def write_matrix_to_tsv(net, filename=None, df=None):
  
  import pandas as pd

  if df is None:
    df = net.dat_to_df()

  return df['mat'].to_csv(filename, sep='\t')",1,0
"def is_valid_py_file(path):
    
    import os

    is_valid = False
    if os.path.isfile(path) and not os.path.splitext(path)[1] == '.pyx':
        try:
            with open(path, 'rb') as f:
                compile(f.read(), path, 'exec')
                is_valid = True
        except:
            pass
    return is_valid",1,1
"def ekffld(handle, segno, rcptrs):
    
    handle = ctypes.c_int(handle)
    segno = ctypes.c_int(segno)
    rcptrs = stypes.toIntVector(rcptrs)
    libspice.ekffld_c(handle, segno,
                      ctypes.cast(rcptrs, ctypes.POINTER(ctypes.c_int)))",0,0
"def config_reader(self, config_level=None):
        
        files = None
        if config_level is None:
            files = [self._get_config_path(f) for f in self.config_level]
        else:
            files = [self._get_config_path(config_level)]
        return GitConfigParser(files, read_only=True)",0,1
"def write_gtfsr(feed, path, *, to_json=False):
    
    path = Path(path)

    if to_json:
        with path.open('w') as tgt:
            tgt.write(json_format.MessageToJson(feed))
    else:
        with path.open('wb') as tgt:
            tgt.write(feed.SerializeToString())",0,0
"def __get_translation(self, surah, ayah, lang):
        
        
        url = '{base}/translations/{lang}/{lang}_translation_{surah}.json'.format(
            base=self.BASE_API, lang=lang, surah=int(surah)
        )
        try:
            response = urlopen(url)                             
            data = json.loads(response.read().decode('utf-8'))  
            translation = data['verse'][ayah]
        except ODOAException:
            return None
        else:
            return translation",3,3
"def format_results(self, rows):
        
        needs_utc = _DJANGO_VERSION >= 14 and settings.USE_TZ
        if not (needs_utc or not self.driver_supports_utf8):
            return tuple(rows)
        
        
        fr = []
        for row in rows:
            if not self.driver_supports_utf8 and isinstance(row, binary_type):
                row = row.decode(self.encoding)

            elif needs_utc and isinstance(row, datetime.datetime):
                row = row.replace(tzinfo=timezone.utc)
            fr.append(row)
        return tuple(fr)",1,3
"def delete_row(dbconn, table_name, field, value):
    
    cur = dbconn.cursor()
    cur.execute(""DELETE FROM '{name}' WHERE {field}='{value}'"".format(name=table_name, field=field, value=value))
    dbconn.commit()",1,3
"def _write_header(po_path, lang, header):
    
    po_file = open(po_path, 'w')
    po_file.write(header + '\n')
    po_file.write(
        'msgid """"' +
        '\nmsgstr """"' +
        '\n""MIME-Version: ' + settings.METADATA['MIME-Version'] + r'\n""'
        '\n""Content-Type: ' + settings.METADATA['Content-Type'] + r'\n""'
        '\n""Content-Transfer-Encoding: ' +
        settings.METADATA['Content-Transfer-Encoding'] + r'\n""'
        '\n""Language: ' + lang + r'\n""' + '\n')
    po_file.close()",1,0
"async def add_unknown_id(self, unknown_id, timeout=OTGW_DEFAULT_TIMEOUT):
        
        cmd = OTGW_CMD_UNKNOWN_ID
        unknown_id = int(unknown_id)
        if unknown_id < 1 or unknown_id > 255:
            return None
        ret = await self._wait_for_cmd(cmd, unknown_id, timeout)
        if ret is not None:
            return int(ret)",2,3
"def addItem(self, itemType, itemContents, itemID=None):
        

        if itemType not in self.noteDB.collection_names():
            fields = [(ii, pymongo.TEXT) for ii in itemContents]
            self.noteDB[itemType].ensure_index(fields)

        collection = self.noteDB[itemType]

        if itemID is None:
            itemContents['timestamps'] = [time.time()]
            itemID = self.getNewID()
            itemContents[""ID""] = itemID
            collection.insert(itemContents)
        else:
            _id = collection.find_one({""ID"": itemID})[""_id""]
            timestamps = collection.find_one({""ID"": itemID})[""timestamps""]
            timestamps.append(time.time())
            itemContents[""timestamps""] = timestamps
            itemContents[""ID""] = itemID
            collection.update({""_id"": _id}, itemContents)

        return itemID",2,3
"def extern_type_to_str(self, context_handle, type_id):
    
    c = self._ffi.from_handle(context_handle)
    return c.utf8_buf(text_type(c.from_id(type_id.tup_0).__name__))",0,3
"def to_dlpack_for_read(data):
    
    data.wait_to_read()
    dlpack = DLPackHandle()
    check_call(_LIB.MXNDArrayToDLPack(data.handle, ctypes.byref(dlpack)))
    return ctypes.pythonapi.PyCapsule_New(dlpack, _c_str_dltensor, _c_dlpack_deleter)",0,1
"def from_dict(self, document):
        
        identifier = str(document['_id'])
        active = document['active']
        
        
        directory = os.path.join(self.directory, identifier)
        timestamp = datetime.datetime.strptime(document['timestamp'], '%Y-%m-%dT%H:%M:%S.%f')
        properties = document['properties']
        return FunctionalDataHandle(identifier, properties, directory, timestamp=timestamp, is_active=active)",1,3
"def get_organism_hosts(cls, entry):
        

        query = ""./organismHost/dbReference[@type='NCBI Taxonomy']""
        return [models.OrganismHost(taxid=x.get('id')) for x in entry.iterfind(query)]",3,3
"def read_sync(self, timeout=None):
        
        if self.saved_data:
            return self.saved_data.pop(0)

        self._sync_ioloop_running = True
        self._read_sync_future = self._read_sync(timeout)
        self.io_loop.start()
        self._sync_ioloop_running = False

        ret_future = self._read_sync_future
        self._read_sync_future = None
        return ret_future.result()",3,1
"def parse_message(message, nodata=False):
    
    header = read_machine_header(message)
    h_len = __get_machine_header_length(header)
    meta_raw = message[h_len:h_len + header['meta_len']]
    meta = __parse_meta(meta_raw, header)
    data_start = h_len + header['meta_len']
    data = b''
    if not nodata:
        data = __decompress(
            meta,
            message[data_start:data_start + header['data_len']]
        )
    return header, meta, data",3,3
"def maybe_send_signal(self, signum, include_pgrp=True):
    
    remote_pid = self._maybe_last_pid()
    if remote_pid is not None:
      safe_kill(remote_pid, signum)
    if include_pgrp:
      remote_pgrp = self._maybe_last_pgrp()
      if remote_pgrp:
        safe_kill(remote_pgrp, signum)",2,2
"def rget(d, key):
    
    if not isinstance(d, dict):
        return None
    assert isinstance(key, str) or isinstance(key, list)

    keys = key.split('.') if isinstance(key, str) else key
    cdrs = cdr(keys)
    cars = car(keys)
    return rget(d.get(cars), cdrs) if cdrs else d.get(cars)",3,3
"def safe_send(self, connection, target, message, *args, **kwargs):
        
        
        prefix = ""PRIVMSG {0} :"".format(target)
        max_len = 510 - len(prefix)

        for chunk in chunks(message.format(*args, **kwargs), max_len):
            connection.send_raw(""{0}{1}"".format(prefix, chunk))",2,2
"def send_kwargs(self) -> dict:
        

        display_page = self.display_page
        page_num = f'\nPage {display_page + 1}/{self.page_count}'
        content = self.pages[display_page] + page_num
        return {'content': content}",2,2
"def write_file(self, filepath, filename=None, directory=None):
        
        arcname = None
        if filename or directory:
            directory = directory.rstrip(""/"") + ""/"" if directory else """"
            filename = filename or os.path.basename(filepath)
            arcname = ""{}{}"".format(directory, filename)
        self._copy_to_zipfile(filepath, arcname=arcname)
        return arcname or filepath",0,0
"def write_connection_file(self):
        
        if os.path.basename(self.connection_file) == self.connection_file:
            cf = os.path.join(self.profile_dir.security_dir, self.connection_file)
        else:
            cf = self.connection_file
        write_connection_file(cf, ip=self.ip, key=self.session.key,
        shell_port=self.shell_port, stdin_port=self.stdin_port, hb_port=self.hb_port,
        iopub_port=self.iopub_port)
        
        self._full_connection_file = cf",0,0
"def flush_on_close(self, stream):
        
        assert get_thread_ident() == self.ioloop_thread_id
        
        stream.KATCPServer_closing = True
        
        return stream.write('\n')",0,0
"def readQuotes(self, start, end):
        
        if self.symbol is None:
            LOG.debug('Symbol is None')
            return []

        return self.__yf.getQuotes(self.symbol, start, end)",1,1
"def get_release_date(pdb_id):
    

    pdb_id = pdb_id.upper()
    if pdb_id not in _property_table().index:
        raise ValueError('PDB ID not in property table')
    else:
        release_date = _property_table().ix[pdb_id, 'releaseDate']
        if pd.isnull(release_date):
            log.debug('{}: no release date available')
            release_date = None

    return release_date",3,3
"def flag(self, key, env=None):
        
        env = env or self.ENVVAR_PREFIX_FOR_DYNACONF or ""DYNACONF""
        with self.using_env(env):
            value = self.get_fresh(key)
            return value is True or value in true_values",0,3
"def unmarshall_value(self, value):
        
        value = str(value)
        if self.escapeValues:
            value = value.decode('hex')
        if self.compressValues:
            value = zlib.decompress(value)
        value = pickle.loads(value)
        return value",1,3
"def load_host_keys(self, filename):
        
        self._host_keys_filename = filename
        self._host_keys.load(filename)",1,1
"def write_u2d(self, u2d):
        
        filename = os.path.split(u2d.filename)[-1]
        np.savetxt(os.path.join(self.m.model_ws,self.arr_org,filename),
                   u2d.array,fmt=""%15.6E"")
        return filename",0,0
"def _normalize_assets(self, assets):
        
        new_assets = []

        for asset in assets:
            
            if len(asset) == 24:
                
                asset = asset[:-2]
            new_assets.append(asset)

        return new_assets",3,3
"def mavlink_packet(self, m):
        
        if m.get_type() == 'GLOBAL_POSITION_INT':
            if abs(m.lat) < 1000 and abs(m.lon) < 1000:
                return
            self.vehicle_pos = VehiclePos(m)",3,3
"def read_data(self, **kwargs):
        
        trigger_id = kwargs.get('trigger_id')
        data = list()
        cache.set('th_joplin_' + str(trigger_id), data)",3,1
"def _document_lines(self, text):
        
        inlines = text.split('\n')
        doc_lines = [(re.sub(r'^ +| +$', '', line), num)
                     for line, num
                     in zip(inlines, xrange(1, len(inlines) + 1))]
        return doc_lines",3,3
"def addReader(self, selectable):
        
        try:
            self._reads[selectable].resume()
        except KeyError:
            self._reads[selectable] = g = Stream.spawn(self, selectable, 'doRead')
            self.addToGreenletPool(g)",1,1
"def get_view(self, mo_ref, properties=None):
        
        
        kls = classmapper(mo_ref._type)
        view = kls(mo_ref, self)
        
        

        return view",3,3
"def clean_by_fields(self, obj, fields, get_field_fn, exclude_list):
        
        cleaned_list = []
        obj_model = get_model_from_instance(obj)

        for field in fields:
            field_accessor = get_field_fn(field)
            
            
            
            
            is_excluded = obj_model in exclude_list and field_accessor in exclude_list[obj_model]

            if not is_excluded:
                cleaned_list.append(field)

        return cleaned_list",3,3
"def process_message(message, notification):
    
    
    
    if not set(VITAL_MESSAGE_FIELDS) <= set(message):
        
        
        
        logger.info('JSON Message Missing Vital Fields')
        return HttpResponse('Missing Vital Fields')

    if message['notificationType'] == 'Complaint':
        return process_complaint(message, notification)
    if message['notificationType'] == 'Bounce':
        return process_bounce(message, notification)
    if message['notificationType'] == 'Delivery':
        return process_delivery(message, notification)
    else:
        return HttpResponse('Unknown Notification Type')",3,3
"def yield_json_corpus(fnm):
    
    with codecs.open(fnm, 'rb', 'ascii') as f:
        line = f.readline()
        while line != '':
            yield Text(json.loads(line))
            line = f.readline()",1,1
"def _write_method(schema):
    
    def method(
        self,
        filename=None,
        schema=schema,
        taxon_col='uid',
        taxon_annotations=[],
        node_col='uid',
        node_annotations=[],
        branch_lengths=True,
        **kwargs):
        
        return _write(
            self._data,
            filename=filename,
            schema=schema,
            taxon_col=taxon_col,
            taxon_annotations=taxon_annotations,
            node_col=node_col,
            node_annotations=node_annotations,
            branch_lengths=branch_lengths,
            **kwargs
        )
    
    method.__doc__ = _write_doc_template(schema)
    return method",0,0
"def readable(path):
    
    try:
        st = os.stat(path)
        return 0 != st.st_mode & READABLE_MASK
    except os.error:
        return None
    return True",1,1
"def parse(self, fp, headersonly=True):
        
        feedparser = FeedParser(self._class)
        feedparser._set_headersonly()

        try:
            mp = mmap.mmap(fp.fileno(), 0, access=mmap.ACCESS_READ)
        except:
            mp = fp

        data = """"

        
        while True:
            line = mp.readline()
            data = data + line.decode(""us-ascii"")
            if line == b""\n"":
                break
        feedparser.feed(data) 
        return feedparser.close()",3,1
"def _get_value(self, key, func=None, split_val=None, as_boolean=False,
		exception_default=None):
		
		try:
			if as_boolean:
				return self.config.getboolean(key[0], key[1])
			value = self.config.get(key[0], key[1])
			if split_val is not None:
				value = value.split(split_val)
			if func is not None:
				return func(value)
			return value
		except (KeyError, configparser.NoSectionError, configparser.NoOptionError) as e:
			if exception_default is not None:
				return exception_default
			raise KeyError(e)",3,3
"def write_membership(filename,config,srcfile,section=None):
    
    source = Source()
    source.load(srcfile,section=section)
    loglike = createLoglike(config,source)
    loglike.write_membership(filename)",0,0
"def read(self, addr, size):
        
        for segment in self._elf.iter_segments():
            seg_addr = segment[""p_paddr""]
            seg_size = min(segment[""p_memsz""], segment[""p_filesz""])
            if addr >= seg_addr + seg_size:
                continue
            if addr + size <= seg_addr:
                continue
            

            if addr >= seg_addr and addr + size <= seg_addr + seg_size:
                
                data = segment.data()
                start = addr - seg_addr
                return data[start:start + size]",1,1
"def assertDutTraceDoesNotContain(dut, message, bench):
    
    if not hasattr(bench, ""verify_trace""):
        raise AttributeError(""Bench object does not contain verify_trace method!"")
    if bench.verify_trace(dut, message, False):
        raise TestStepFail('Assert: Message(s) ""%s"" in response' % message)",3,3
"def forecast_names(self):
        
        if ""forecasts"" in self.pestpp_options.keys():
            return self.pestpp_options[""forecasts""].lower().split(',')
        elif ""predictions"" in self.pestpp_options.keys():
            return self.pestpp_options[""predictions""].lower().split(',')
        else:
            return None",3,3
"def send_email(sender, pw, to, subject, content, files=None, service='163'):
    
    se = EmailSender(from_=sender, pw=pw, service=service)
    se.send_email(to=to, subject=subject, content=content, files=files)
    se.quit()",2,2
"def _input_to_raw_value(self, value: int) -> float:
        
        return (float(value) - self.min_raw_value) / self.max_raw_value",1,3
"def __send_receive_buffer(self):
        
        self.__clear_in_buffer()
        self.__send_buffer()
        read_string = self.serial.read(len(self.__in_buffer))
        if self.DEBUG_MODE:
            print(""Read: '{}'"".format(binascii.hexlify(read_string)))
        if len(read_string) != len(self.__in_buffer):
            raise IOError(""{} bytes received for input buffer of size {}"".format(len(read_string),
                                                                                 len(self.__in_buffer)))
        if not self.__is_valid_checksum(read_string):
            raise IOError(""Checksum validation failed on received data"")
        self.__in_buffer.value = read_string",2,1
"def load_or_create_vocab(data: str, vocab_path: Optional[str], num_words: int, word_min_count: int,
                         pad_to_multiple_of: Optional[int] = None) -> Vocab:
    
    if vocab_path is None:
        return build_from_paths(paths=[data], num_words=num_words, min_count=word_min_count,
                                pad_to_multiple_of=pad_to_multiple_of)
    else:
        return vocab_from_json(vocab_path)",1,1
"def addfile(self, tarinfo, fileobj=None):
        
        self._check(""aw"")

        tarinfo = copy.copy(tarinfo)

        buf = tarinfo.tobuf(self.format, self.encoding, self.errors)
        self.fileobj.write(buf)
        self.offset += len(buf)

        
        if fileobj is not None:
            copyfileobj(fileobj, self.fileobj, tarinfo.size)
            blocks, remainder = divmod(tarinfo.size, BLOCKSIZE)
            if remainder > 0:
                self.fileobj.write(NUL * (BLOCKSIZE - remainder))
                blocks += 1
            self.offset += blocks * BLOCKSIZE

        self.members.append(tarinfo)",1,0
"def check_write_permission(self, user_id, do_raise=True):
        

        return self.network.check_write_permission(user_id, do_raise=do_raise)",0,0
"def send_event(self, destination, event, event_mask = 0, propagate = 0,
                   onerror = None):
        
        request.SendEvent(display = self.display,
                          onerror = onerror,
                          propagate = propagate,
                          destination = destination,
                          event_mask = event_mask,
                          event = event)",2,2
"def load_file(filename, out=sys.stdout):
    
    fp = open(filename, 'rb')
    try:
      source = fp.read()
      try:
          if PYTHON_VERSION < 2.6:
              co = compile(source, filename, 'exec')
          else:
              co = compile(source, filename, 'exec', dont_inherit=True)
      except SyntaxError:
          out.write('>>Syntax error in %s\n' % filename)
          raise
    finally:
      fp.close()
    return co",0,1
"def _on_client_ready(self):
        
        _legacy_twisted_log.msg(""Successfully connected to the AMQP broker."")
        yield self.client.resumeProducing()

        yield self.client.declare_exchanges(self.exchanges)
        yield self.client.declare_queues(self.queues)
        yield self.client.bind_queues(self.bindings)
        for queue, callback in self.consumers.items():
            yield self.client.consume(callback, queue)

        _legacy_twisted_log.msg(""Successfully declared all AMQP objects."")
        self._client_ready.callback(None)",2,2
"def process (self, c):
        
        if isinstance(c, bytes):
            c = self._decode(c)
        self.state.process(c)",0,3
"def get_command(self, ctx, name):

        

        
        self.connect(ctx)

        if not hasattr(ctx, ""widget"") or name in [""shell""]:
            return super(Engineer, self).get_command(ctx, name)

        if name == ""--help"":
            return None

        
        info = ctx.widget.engineer_info(name)

        
        return self.make_command(ctx, name, info)",3,3
"def header_body_from_content(content):
    
    m = _CLASSIFIED_BY_PATTERN.search(content)
    idx = m and m.end() or 0
    m = _SUMMARY_PATTERN.search(content)
    summary_idx = m and m.start() or None
    m = _FIRST_PARAGRAPH_PATTERN.search(content)
    para_idx = m and m.start() or None
    if summary_idx and para_idx:
        idx = max(idx, min(summary_idx, para_idx))
    elif summary_idx:
        idx = max(summary_idx, idx)
    elif para_idx:
        idx = max(para_idx, idx)
    if idx > 0:
        return content[:idx], content[idx:]
    return None, None",3,3
"def ifast_comp(seq1, seqs, transpositions=False):
	
	for seq2 in seqs:
		dist = fast_comp(seq1, seq2, transpositions)
		if dist != -1:
			yield dist, seq2",3,3
"def send_socket(self, message):
        
        if self.log_sends:
            start_time = time.time()
        try:
            self.send_message(message)
        except Exception as error:
            logger.error('error sending message {!r}: {}'.format(message, error))
        else:
            if self.log_sends:
                elapsed_time = time.time() - start_time
                logger.info('sent message {!r} to {}:{} in {:.03f} seconds'.format(
                        message, self.host, self.port, elapsed_time))",2,2
"def _afterpass(self):
        
        if hasattr(self, '_fi'):
            return
        fi = 0
        for codeobj in self.walk_preorder():
            codeobj._fi = fi
            fi += 1
            if isinstance(codeobj, CodeOperator) and codeobj.is_assignment:
                if codeobj.arguments and isinstance(codeobj.arguments[0],
                                                    CodeReference):
                    var = codeobj.arguments[0].reference
                    if isinstance(var, CodeVariable):
                        var.writes.append(codeobj)",0,0
"def parseFile( self, file_or_filename ):
        
        try:
            file_contents = file_or_filename.read()
        except AttributeError:
            f = open(file_or_filename, ""rb"")
            file_contents = f.read()
            f.close()
        return self.parseString(file_contents)",1,1
"def read(fname):
    
    content = None
    with open(os.path.join(here, fname)) as f:
        content = f.read()
    return content",1,1
"def write_bed_with_trackline(bed, out, trackline, add_chr=False):
    
    df = pd.read_table(bed, index_col=None, header=None)
    bt = pbt.BedTool('\n'.join(df.apply(lambda x: '\t'.join(x.astype(str)), 
                                        axis=1)) + '\n',
                     from_string=True)
    if add_chr:
        bt = add_chr_to_contig(bt)
    bt = bt.saveas(out, trackline=trackline)",0,0
"def get_current_thread_id(thread):
    
    try:
        
        tid = thread.__pydevd_id__
        if tid is None:
            
            
            
            raise AttributeError()
    except AttributeError:
        tid = _get_or_compute_thread_id_with_lock(thread, is_current_thread=True)

    return tid",3,3
"def write_aims(filename, atoms):
    

    lines = """"
    lines += ""
    lines += ""

    lattice_vector_line = ""lattice_vector "" + ""%16.16f ""*3 + ""\n""
    for vec in atoms.get_cell():
        lines += lattice_vector_line % tuple(vec)

    N = atoms.get_number_of_atoms()

    atom_line = ""atom "" + ""%16.16f ""*3 + ""%s \n""
    positions = atoms.get_positions()
    symbols = atoms.get_chemical_symbols()

    initial_moment_line = ""initial_moment %16.6f\n""
    magmoms = atoms.get_magnetic_moments()

    for n in range(N):
        lines += atom_line % (tuple(positions[n]) + (symbols[n],))
        if magmoms is not None:
            lines += initial_moment_line % magmoms[n]

    with open(filename, 'w') as f:
        f.write(lines)",0,0
"def send_file_offer(self, file_id, user_id, auto_open=False, scope='content/send'):
        

        params = {
            'UserId': user_id,
            'autoOpen': 'true' if auto_open else 'false'
        }

        return _post(
            token=self.oauth.get_app_token(scope),
            uri='/user/media/file/send/' + urllib.quote(file_id) + '?' + urllib.urlencode(params)
        )",0,2
"def et_node_to_string(et_node, default=''):
    

    return str(et_node.text).strip() if et_node is not None and et_node.text else default",3,3
"def send_theme_file(self, filename):
        
        cache_timeout = self.get_send_file_max_age(filename)
        return send_from_directory(self.config['THEME_STATIC_FOLDER'], filename,
                                   cache_timeout=cache_timeout)",2,2
"def do_eb(self, arg):
        
        
        
        pid        = self.get_process_id_from_prefix()
        token_list = self.split_tokens(arg, 2)
        address    = self.input_address(token_list[0], pid)
        data       = HexInput.hexadecimal(' '.join(token_list[1:]))
        self.write_memory(address, data, pid)",0,0
"def execute_scalar(self, query_string, params=None):
        
        self.execute(query_string, params)
        row = self.fetchone()
        if not row:
            return None
        return row[0]",2,3
"def read(self):
        
        if 'r' == self._mode:
            return self._read_response.text
        elif 'rb' == self._mode:
            return self._read_response.content
        else:
            raise IOError(""File not opened in read mode."")",1,1
"def send(self, line):
        
        line = line.strip()
        if line == ""."":
            self.stop()
            return
        mav = self.master.mav
        if line != '+++':
            line += ""\r\n""
        buf = [ord(x) for x in line]
        buf.extend([0]*(70-len(buf)))

        flags = mavutil.mavlink.SERIAL_CONTROL_FLAG_RESPOND
        flags |= mavutil.mavlink.SERIAL_CONTROL_FLAG_MULTI
        flags |= mavutil.mavlink.SERIAL_CONTROL_FLAG_EXCLUSIVE
        mav.serial_control_send(self.serial_settings.port,
                                flags,
                                0, self.serial_settings.baudrate,
                                len(line), buf)",2,2
"def fill_arg(self, *args):
        
        missing = self.missing_vars()
        if len(args) == len(missing) == 1:
            self.setval(missing[0], args[0])",3,3
"def answer_callback_query(self, callback_query_id, text=None, show_alert=None, url=None, cache_time=None):
        
        return apihelper.answer_callback_query(self.token, callback_query_id, text, show_alert, url, cache_time)",2,2
"def dataset_nan_locs(ds):
    
    ans = []
    for sampnum, sample in enumerate(ds):
        if pd.isnull(sample).any():
            ans += [{
                'sample': sampnum,
                'input':  pd.isnull(sample[0]).nonzero()[0],
                'output': pd.isnull(sample[1]).nonzero()[0],
                }]
    return ans",3,3
"def cli_progress_bar(start, end, bar_length=50):
    
    percent = float(start) / end
    hashes = '
    spaces = '-' * (bar_length - len(hashes))
    stdout.write(
        ""\r[{0}] {1}/{2} ({3}%)"".format(
            hashes + spaces,
            start,
            end,
            int(round(percent * 100))
        )
    )
    stdout.flush()",0,0
"def play_Track(self, track):
        
        if hasattr(track, 'name'):
            self.set_track_name(track.name)
        self.delay = 0
        instr = track.instrument
        if hasattr(instr, 'instrument_nr'):
            self.change_instrument = True
            self.instrument = instr.instrument_nr
        for bar in track:
            self.play_Bar(bar)",0,2
"def parseFile( self, file_or_filename, parseAll=False ):
        
        try:
            file_contents = file_or_filename.read()
        except AttributeError:
            with open(file_or_filename, ""r"") as f:
                file_contents = f.read()
        try:
            return self.parseString(file_contents, parseAll)
        except ParseBaseException as exc:
            if ParserElement.verbose_stacktrace:
                raise
            else:
                
                raise exc",1,1
"def mark_error(self, dispatch, error_log, message_cls):
        
        if message_cls.send_retry_limit is not None and (dispatch.retry_count + 1) >= message_cls.send_retry_limit:
            self.mark_failed(dispatch, error_log)
        else:
            dispatch.error_log = error_log
            self._st['error'].append(dispatch)",2,2
"def techport(Id):
    

    base_url = 'http://techport.nasa.gov/xml-api/'

    if not isinstance(Id, str):
        raise ValueError(""The Id arg you provided is not the type of str"")
    else:
        base_url += Id

    return dispatch_http_get(base_url)",1,3
"def read_json(fn):
    
    with open(fn) as f:
        return json.load(f, object_hook=_operator_object_hook)",1,1
"def get(self, ii, default=None, msg_if_none=None):
        
        try:
            out = self[ii]

        except (IndexError, ValueError):
            if default is not None:
                out = default

            else:
                if msg_if_none is not None:
                    raise ValueError(msg_if_none)

                else:
                    raise

        return out",3,3
"def verify_from_file(self, data_path,
                         sig_path=None, keyrings=None, homedir=None):
        
        cmd_line = ['gpg', '--homedir', homedir or self.homedir]
        cmd_line.extend(self._get_keyrings_cl(keyrings))

        cmd_line.append('--verify')
        if sig_path:
            cmd_line.extend([sig_path, data_path])
        else:
            cmd_line.append(data_path)

        p = subprocess.Popen(cmd_line, stderr=subprocess.PIPE)
        stdout, stderr = p.communicate()
        if p.returncode:
            raise GpgBinaryError(stderr)
        return True",1,1
"def readf(prompt, default=None, minval=None, maxval=None,
          allowed_single_chars=None, question_mark=True):
    

    return read_value(ftype=float,
                      prompt=prompt,
                      default=default,
                      minval=minval,
                      maxval=maxval,
                      allowed_single_chars=allowed_single_chars,
                      question_mark=question_mark)",1,1
"def on_sigchld(self, _signum, _unused_frame):
        
        LOGGER.info('SIGCHLD received from child')
        if not self.active_processes(False):
            LOGGER.info('Stopping with no active processes and child error')
            signal.setitimer(signal.ITIMER_REAL, 0, 0)
            self.set_state(self.STATE_STOPPED)",2,2
"def serve_file(load, fnd):
    
    if 'env' in load:
        
        load.pop('env')

    ret = {'data': '',
           'dest': ''}
    if 'path' not in load or 'loc' not in load or 'saltenv' not in load:
        return ret
    if not fnd['path']:
        return ret
    ret['dest'] = fnd['rel']
    gzip = load.get('gzip', None)
    fpath = os.path.normpath(fnd['path'])
    with salt.utils.files.fopen(fpath, 'rb') as fp_:
        fp_.seek(load['loc'])
        data = fp_.read(__opts__['file_buffer_size'])
        if gzip and data:
            data = salt.utils.gzip_util.compress(data, gzip)
            ret['gzip'] = gzip
        ret['data'] = data
    return ret",1,1
"def runInBackground(self, pollInterval=.1, encoding=False):
    
        
    from .BackgroundTask import BackgroundTaskThread

    taskInfo = BackgroundTaskInfo(encoding)
    thread = BackgroundTaskThread(self, taskInfo, pollInterval, encoding)

    thread.start()
    
    return taskInfo",1,2
"def write (self, bytes, header=None):
        
        if type(bytes) is str:
            bytes = bytearray(bytes)

        if not isinstance(header, PCapPacketHeader):
            header = PCapPacketHeader(orig_len=len(bytes))

        packet = bytes[0:header.incl_len]

        self._stream.write( str(header) )
        self._stream.write( packet      )
        self._stream.flush()

        return header.incl_len",0,0
"def read(self, to_read, timeout_ms):
        
        if not isinstance(to_read, baseinteger):
            raise TypeError(""to_read can only be an instance of type baseinteger"")
        if not isinstance(timeout_ms, baseinteger):
            raise TypeError(""timeout_ms can only be an instance of type baseinteger"")
        data = self._call(""read"",
                     in_p=[to_read, timeout_ms])
        return data",1,1
"def sendto(self, data, addr, flags=0):
        
        return self.llc.sendto(self._tco, data, addr, flags)",2,2
"def lines(self, encoding=None, errors='strict', retain=True):
        r
        return self.text(encoding, errors).splitlines(retain)",1,3
"def run(self):
        
        kwargs = {'query': self.get_data()}
        if self.data_type == ""ip"":
            kwargs.update({'query_type': 'ip'})
        elif self.data_type == ""network"":
            kwargs.update({'query_type': 'network'})
        elif self.data_type == 'autonomous-system':
            kwargs.update({'query_type': 'asn'})
        elif self.data_type == 'port':
            kwargs.update({'query_type': 'port'})
        else:
            self.notSupported()
            return False

        if self.service == 'observations':
            response = self.bs.get_observations(**kwargs)
            self.report(response)
        elif self.service == 'enrichment':
            response = self.bs.enrich(**kwargs)
            self.report(response)
        else:
            self.report({'error': 'Invalid service defined.'})",3,3
"def main(ctx, host, port, transport_type, timeout, ca_certs):
    
    if transport_type == 'udp':
        if timeout is not None:
            ctx.fail('--timeout cannot be used with the UDP transport')
        transport = riemann_client.transport.UDPTransport(host, port)
    elif transport_type == 'tcp':
        transport = riemann_client.transport.TCPTransport(host, port, timeout)
    elif transport_type == 'tls':
        if ca_certs is None:
            ctx.fail('--ca-certs must be set when using the TLS transport')
        transport = riemann_client.transport.TLSTransport(
            host, port, timeout, ca_certs)
    elif transport_type == 'none':
        transport = riemann_client.transport.BlankTransport()

    ctx.obj = transport",2,2
"def CopyFileInZip(from_zip, from_name, to_zip, to_name=None):
  
  data = from_zip.read(from_name)
  if to_name is None:
    to_name = from_name
  to_zip.writestr(to_name, data)",0,0
"def read_corpus(file_name):
    
    with io.open(file_name, encoding='utf-8') as data_file:
        return yaml.load(data_file)",1,1
"def send_chat_action(self, chat_id, action):
        
        return apihelper.send_chat_action(self.token, chat_id, action)",3,2
"def dictToFile(dictionary,replicateKey,outFileName):
    
    replicateToFile=h5py.File(outFileName,""w"")
    for i in range(len(dictionary[replicateKey])):
        replicateToFile.create_dataset(""{}"".format(dictionary[replicateKey].keys()[i])\
                                    ,data=dictionary[replicateKey].values()[i]\
                                    ,compression=""gzip"")
    replicateToFile.close()",0,0
"def load_vertex_buffer(self, fd, material, length):
        
        material.vertices = struct.unpack('{}f'.format(length // 4), fd.read(length))",1,1
"def type_consumer():
    
    

    
    
    while True:
        item = _task_queue.get()
        if isinstance(item, KeyAndTypes):
            if item.key in collected_args:
                
                
                
                _flush_signature(item.key, UnknownType)
            collected_args[item.key] = ArgTypes(item.types)
        else:
            assert isinstance(item, KeyAndReturn)
            if item.key in collected_args:
                _flush_signature(item.key, item.return_type)
        _task_queue.task_done()",3,3
"def render_to_file(self, filename, **kwargs):
        
        with io.open(filename, 'w', encoding='utf-8') as f:
            f.write(self.render(is_unicode=True, **kwargs))",0,0
"def _merge_ensemble(ensemble, col_nums, col_vals):
    
    try:
        
        for num in col_nums:
            
            
            ensemble[num-1] = col_vals[num - 2]

    except IndexError:
        logger_csvs.error(""merge_ensemble: IndexError: index out of range"")

    return ensemble",0,0
"def name2idfobject(idf, groupnamess=None, objkeys=None, **kwargs):
    
    
    if not objkeys:
        objkeys = idfobjectkeys(idf)
    for objkey in objkeys:
        idfobjs = idf.idfobjects[objkey.upper()]
        for idfobj in idfobjs:
            for key, val in kwargs.items():
                try:
                    if idfobj[key] == val:
                        return idfobj
                except BadEPFieldError as e:
                    continue",2,3
"def handleOneNodeMsg(self, wrappedMsg):
        
        try:
            vmsg = self.validateNodeMsg(wrappedMsg)
            if vmsg:
                logger.trace(""{} msg validated {}"".format(self, wrappedMsg),
                             extra={""tags"": [""node-msg-validation""]})
                self.unpackNodeMsg(*vmsg)
            else:
                logger.debug(""{} invalidated msg {}"".format(self, wrappedMsg),
                             extra={""tags"": [""node-msg-validation""]})
        except SuspiciousNode as ex:
            self.reportSuspiciousNodeEx(ex)
        except Exception as ex:
            msg, frm = wrappedMsg
            self.discard(msg, ex, logger.info)",3,3
"def get_queue_url(queue_name):
    
    client = boto3.client(""sqs"", CURRENT_REGION)
    queue = client.get_queue_url(QueueName=queue_name)

    return queue[""QueueUrl""]",2,3
"def set_data(self, source_data):
        
        self.home_details = source_data['homedetails']
        try:
            self.graphs_and_data = source_data['graphsanddata']
        except:
            self.graphs_and_data = None
        self.map_this_home = source_data['mapthishome']
        self.comparables = source_data['comparables']",3,1
"def _mainType(self, resp):
        
        if self.PY2:
            return resp.headers.maintype
        elif self.PY3:
            return resp.headers.get_content_maintype()
        else:
            return None",3,3
"def write_then_read_again(las, do_compress=False):
    
    out = io.BytesIO()
    las.write(out, do_compress=do_compress)
    out.seek(0)
    return read_las(out)",1,0
"def _read_check(self, filepath):
        
        if self.is_ssh(filepath):
            self._check_ftp()
            
            
            source = self._get_remote(filepath)
            target = self._get_hashed_path(filepath)
            self.ftp.get(source, target)
            
        else:
            target = filepath
        
        return target",1,1
"def _listen_commands(self):
        

        self._last_update = None
        update_body = {'timeout': 2}

        while True:
            latest = self._last_update
            
            update_body.update({'offset': latest + 1} if latest else {})
            update_resp = self.client.get_updates(update_body)
            update_resp.add_done_callback(self._respond_commands)
            yield gen.sleep(5)",2,2
"def read(self, pos, size, **kwargs):
        
        data, realsize = self.read_data(size, **kwargs)
        if not self.state.solver.is_true(realsize == 0):
            self.state.memory.store(pos, data, size=realsize)
        return realsize",1,1
"def set_web_hook(self, url=None, certificate=None):
        
        payload = dict(url=url, certificate=certificate)
        return self._get('setWebHook', payload)",2,2
"def lockedtransfersigned_from_message(message: 'LockedTransfer') -> 'LockedTransferSignedState':
    
    balance_proof = balanceproof_from_envelope(message)

    lock = HashTimeLockState(
        message.lock.amount,
        message.lock.expiration,
        message.lock.secrethash,
    )

    transfer_state = LockedTransferSignedState(
        message.message_identifier,
        message.payment_identifier,
        message.token,
        balance_proof,
        lock,
        message.initiator,
        message.target,
    )

    return transfer_state",3,3
"def _StripThenGetNicknameAndText(self, text):
    
    stripped = self.STRIPPER.transformString(text)
    structure = self.MSG_ENTRY.parseString(stripped)
    text = structure.text.replace('\t', ' ')
    return structure.nickname, text",3,3
"def resume_writing(self):
        
        if not self._can_send.is_set():
            self._can_send.set()
            self.transport.resume_reading()",2,0
"def cast_to_subclass(self):
        
        self.import_lib()
        self.load_requirements()

        try:
            self.commit()  
            bsf = self.build_source_files.file(File.BSFILE.BUILD)
        except Exception as e:
            self.log('Error trying to create a bundle source file ... {} '.format(e))
            raise
            self.rollback()
            return self

        try:
            clz = bsf.import_bundle()

        except Exception as e:

            raise BundleError('Failed to load bundle code file, skipping : {}'.format(e))

        b = clz(self._dataset, self._library, self._source_url, self._build_url)
        b.limited_run = self.limited_run
        b.capture_exceptions = self.capture_exceptions
        b.multi = self.multi


        return b",3,1
"def read_noise(noisefile):
    

    noises = pickle.load(open(noisefile, 'r'))

    scan = []; seg = []; noiseperbl = []; flagfrac = []; imnoise = []
    if len(noises[0]) == 4:
        for noise in noises:
            seg.append(noise[0]); noiseperbl.append(noise[1])
            flagfrac.append(noise[2]); imnoise.append(noise[3])
        return (np.array(seg), np.array(noiseperbl), np.array(flagfrac), np.array(imnoise))
    elif len(noises[0]) == 5:
        for noise in noises:
            scan.append(noise[0])
            seg.append(noise[1]); noiseperbl.append(noise[2])
            flagfrac.append(noise[3]); imnoise.append(noise[4])
        return (np.array(scan), np.array(seg), np.array(noiseperbl), np.array(flagfrac), np.array(imnoise))
    else:
        logger.warn('structure of noise file not understood. first entry should be length 4 of 5.')",1,1
"def sismember(self, name, value):
        
        return self.storage.sismember(name, self.dump(value))",2,3
"def protect(self, password=None, read_protect=False, protect_from=0):
        
        return super(Topaz512, self).protect(
            password, read_protect, protect_from)",0,1
"def _can_send_eth(irs):
        
        for ir in irs:
            if isinstance(ir, (HighLevelCall, LowLevelCall, Transfer, Send)):
                if ir.call_value:
                    return True
        return False",2,2
"def upload_gallery_file(self, folder_name, file_name, data=None, input_file_path=None,
                            prevent_share=False, content_type=""image/png"", scope='content/write'):
        
        if input_file_path:
            with open(input_file_path, 'rb') as f:
                data = f.read()

        if not data:
            raise ValueError('Either the data of a file or the path to a file must be provided')

        params = {
            'fileName': file_name,
            'preventShare': 'true' if prevent_share else 'false',
        }

        return _post(
            token=self.oauth.get_user_token(scope),
            uri='/user/media/file/' + urllib.quote(folder_name) + '?' + urllib.urlencode(params),
            data=data,
            content_type=content_type,
        )",0,0
"def batch_write(self, tablename, return_capacity=None,
                    return_item_collection_metrics=NONE):
        
        return_capacity = self._default_capacity(return_capacity)
        return BatchWriter(self, tablename, return_capacity=return_capacity,
                           return_item_collection_metrics=return_item_collection_metrics)",0,0
"def _read_file_data(self, path):
        
        _, ext = os.path.splitext(path)
        
        if ext in ('.yml', '.yaml'):
            try:
                with open(path) as f:
                    return yaml.safe_load(f)
            except YAMLError as e:
                raise ConfigParseError(path, e)
        
        elif ext == '.json':
            try:
                return json.loads(open(path).read())
            except ValueError as e:
                raise ConfigParseError(path, e)
        else:
            raise ConfigParseError(path, TypeError(""Unsupported file type {}"".format(ext)))",1,1
"def sendOACK(self):
        
        log.debug(""In sendOACK with options %s"", self.context.options)
        pkt = TftpPacketOACK()
        pkt.options = self.context.options
        self.context.sock.sendto(pkt.encode().buffer,
                                 (self.context.host,
                                  self.context.tidport))
        self.context.last_pkt = pkt",2,2
"def _extract(self, raw: str, station: str = None) -> str:
        
        resp = parsexml(raw)
        try:
            report = resp['response']['body']['items']['item'][self.rtype.lower() + 'Msg']
        except KeyError:
            raise self.make_err(raw)
        
        report = report.replace('\n', '')
        
        for item in (self.rtype.upper(), 'SPECI'):
            if report.startswith(item + ' '):
                report = report[len(item) + 1:]
        report = report.rstrip('=')
        
        return ' '.join(report.split())",3,3
"def dist(self,*args,**kwargs):
        
        _check_roSet(self,kwargs,'dist')
        lbd= self._lbd(*args,**kwargs)
        return lbd[:,2].astype('float64')",3,3
"def srbt(bt_address, pkts, inter=0.1, *args, **kargs):
    
    if ""port"" in kargs:
        s = conf.BTsocket(bt_address=bt_address, port=kargs.pop(""port""))
    else:
        s = conf.BTsocket(bt_address=bt_address)
    a, b = sndrcv(s, pkts, inter=inter, *args, **kargs)
    s.close()
    return a, b",2,2
"def augment_send(self, send_func):
        
        def augmented(*aa, **kw):
            sent = send_func(*aa, **kw)

            if self._enable_chat and self._contains_callback_data(kw):
                self.capture_origin(message_identifier(sent))

            return sent
        return augmented",2,2
"def get_most_unrolled_urls(tweet):
    
    unrolled_urls = []
    for url in get_tweet_links(tweet):
        if url.get(""unwound"", {""url"": None}).get(""url"", None) is not None:
            unrolled_urls.append(url[""unwound""][""url""])
        elif url.get(""expanded_url"", None) is not None:
            unrolled_urls.append(url[""expanded_url""])
        else:
            unrolled_urls.append(url[""url""])
    return unrolled_urls",3,3
"def write(self,f):
        
        f.write(""* regularization\n"")
        for vline in REG_VARIABLE_LINES:
            vraw = vline.strip().split()
            for v in vraw:
                v = v.replace(""["",'').replace(""]"",'')
                if v not in self.optional_dict.keys():
                    raise Exception(""RegData missing attribute {0}"".format(v))
                f.write(""{0} "".format(self.__getattribute__(v)))
            f.write(""\n"")",0,0
"def _send(self):
        

        
        while True:
            
            
            self._send_lock.release()
            self._sendbuf_event.wait()
            self._send_lock.acquire()

            
            while self._sendbuf:
                sent = self._sock.send(self._sendbuf)

                
                
                self._sendbuf = self._sendbuf[sent:]

            
            self._sendbuf_event.clear()",2,2
"def run_command(self, input_file, output_dir=None):
        
        base_name = os.path.basename(input_file)
        name, suffix = base_name.split('.', 1)
        output_file = '{}{}.tif'.format(name, self.split_pattern)
        if output_dir:
            output_file = os.path.join(output_dir, output_file)
        return ['bfconvert', input_file, output_file]",0,0
"def get_next_logs(self, n):
        
        
        
        if n > self.available():
            
            raise IllegalState('not enough elements available in this list')
        else:
            next_list = []
            i = 0
            while i < n:
                try:
                    next_list.append(next(self))
                except StopIteration:
                    break
                i += 1
            return next_list",3,3
"def get_connection(self, fail_silently=False):
        
        from protean.services.email import get_connection

        if not self.connection:
            self.connection = get_connection(fail_silently=fail_silently)

        return self.connection",2,3
"def _process_exception(e, body, tb):
    
    
    msg = e.message if hasattr(e, ""message"") else str(e)
    exception_type = str(e.__class__)
    exception_name = str(e.__class__.__name__)

    properties = pika.BasicProperties(
        content_type=""application/text"",
        delivery_mode=2,
        headers={
            ""exception"": msg,
            ""exception_type"": exception_type,
            ""exception_name"": exception_name,
            ""traceback"": tb,
            ""UUID"": str(uuid.uuid4())
        }
    )

    send_message(""harvester"", body, properties=properties)",2,2
"def position(self):
        
        legendPos = self._element.legendPos
        if legendPos is None:
            return XL_LEGEND_POSITION.RIGHT
        return legendPos.val",0,0
"def write(self, vals):
        
        if 'isroom' in vals and vals['isroom'] is False:
            vals.update({'color': 2, 'status': 'occupied'})
        if 'isroom'in vals and vals['isroom'] is True:
            vals.update({'color': 5, 'status': 'available'})
        ret_val = super(HotelRoom, self).write(vals)
        return ret_val",0,0
"def build_vrt(source_file, destination_file, **kwargs):
    
    with rasterio.open(source_file) as src:
        vrt_doc = boundless_vrt_doc(src, **kwargs).tostring()

        with open(destination_file, 'wb') as dst:
            dst.write(vrt_doc)

    return destination_file",0,0
"def _sendData(self, data):
        
        d = self._callRemote(Transmit, connection=self.connection, data=data)
        d.addErrback(log.err)",2,2
"def addFailure(self, test: unittest.case.TestCase, exc_info: tuple) -> None:
        
        
        self.add_result(TestState.failure, test, exc_info)",2,2
"def change_mpl_backend(self, command):
        
        if command.startswith('%matplotlib') and \
          len(command.splitlines()) == 1:
            if not 'inline' in command:
                self.silent_execute(command)",2,0
"def buildMessage(headers, parts):
    
    message = multipart.MIMEMultipart('alternative')

    for name, value in headers.iteritems():
        name = name.title()
        if name == ""From"":
            multipart[name] = _encodeAddress(value)
        elif name in [""To"", ""Cc"", ""Bcc""]:
            multipart[name] = _encodeAddresses(value)
        else:
            multipart[name] = _encodeHeader(value)

    for partType, part in parts.iteritems():
        mimeText = text.MIMEText(part.encode(""utf-8""), partType, ""UTF-8"")
        message.attach(mimeText.encode())

    return message",3,2
"def eventFilter(self, widget, event):
        
        if event.type() == QEvent.MouseButtonPress:
            if event.button() == Qt.LeftButton:
                self.sig_canvas_clicked.emit(self)
        return super(FigureThumbnail, self).eventFilter(widget, event)",2,2
"def returnData(self, dsptr):
        
        dataPtr = dsptr & 0x00FFFFFFL
        dataLen = (dsptr >> 24) & 0xFF

        self.__f.seek(dataPtr)

        data = self.__f.read(dataLen)
        result = data[4:].split('|')
        location = Location(self.getLong(data, 0), result[0], result[1], result[2], result[3], result[4])

        return location",3,1
"def output_for_debugging(self, stream, data):
        
        with open('%s.spec.out' % stream.name, 'w') as f: f.write(str(data))",0,0
"def lookup(self, pathogenName, sampleName):
        
        pathogenIndex = self._pathogens[pathogenName]
        sampleIndex = self._samples[sampleName]
        return self._readsFilenames[(pathogenIndex, sampleIndex)]",1,1
"def copy_no_perm(src, dst):
    
    shutil.copy(src, dst)
    perm = os.stat(dst).st_mode
    shutil.copystat(src, dst)
    os.chmod(dst, perm)",1,0
"def write_pp_file(filename,pp_df):
    
    with open(filename,'w') as f:
       f.write(pp_df.to_string(col_space=0,
                                columns=PP_NAMES,
                                formatters=PP_FMT,
                                justify=""right"",
                                header=False,
                                index=False) + '\n')",0,0
"def _parse_args(args: List[str]) -> ProjectRunConfig:
    
    parser = argparse.ArgumentParser(prog=""gitlab-get-variables"", description=""Tool for getting a GitLab project's ""
                                                                              ""build variables"")
    add_common_arguments(parser, project=True)
    arguments = parser.parse_args(args)
    return ProjectRunConfig(project=arguments.project, url=arguments.url, token=arguments.token, debug=arguments.debug)",3,3
"def txt_file_to_digits(filename, the_type=str):
    
    with open(filename, 'r') as f:
        for line in f.readlines():
            for c in line:
                if c != '\n' and c!= ' ':
                    yield the_type(c)",1,1
"def get_parameter(self, twig=None, **kwargs):
        
        kwargs['twig'] = twig
        kwargs['check_default'] = False
        kwargs['check_visible'] = False
        ps = self.vars.filter(**kwargs)
        if len(ps)==1:
            return ps.get(check_visible=False, check_default=False)
        elif len(ps) > 1:
            
            
            
            return ps.to_list()[0]
        else:
            raise KeyError(""no result found"")",3,3
"def mission_request_partial_list_encode(self, target_system, target_component, start_index, end_index):
                
                return MAVLink_mission_request_partial_list_message(target_system, target_component, start_index, end_index)",2,2
"def create_module_file(app, env, package, module, dest, suffix, dryrun, force):
    
    logger.debug('Create module file: package %s, module %s', package, module)
    template_file = MODULE_TEMPLATE_NAME
    template = env.get_template(template_file)
    fn = makename(package, module)
    var = get_context(app, package, module, fn)
    var['ispkg'] = False
    rendered = template.render(var)
    write_file(app, makename(package, module), rendered, dest, suffix, dryrun, force)",0,0
"def dump(obj, fp, **user_kwargs):
    
    return json.dump(obj, fp, **_encoder_kwargs(user_kwargs))",0,0
"def _read_config_file(self):
        

        try:
            with open(self.config, 'r') as f:
                config_data = json.load(f)
        except FileNotFoundError:
            config_data = {}

        return config_data",1,1
"def write_json_file(
        path: str,
        contents: dict,
        mode: str = 'w',
        retry_count: int = 3
) -> typing.Tuple[bool, typing.Union[None, Exception]]:
    
    error = None
    for i in range(retry_count):
        error = attempt_json_write(path, contents, mode)
        if error is None:
            return True, None
        time.sleep(0.2)

    return False, error",1,0
"def init_fileoutput (self, args):
        
        self.filename = None
        self.close_fd = False
        self.fd = None
        if args.get('fileoutput'):
            self.filename = os.path.expanduser(args['filename'])
        elif 'fd' in args:
            self.fd = args['fd']
        else:
            self.fd = self.create_fd()",0,0
"def call_openssl(cmd, message, silent=False):
    
    if silent:
        with open(os.devnull, 'w') as devnull:
            return subprocess.check_call(cmd, shell=True, stdout=devnull,
                                         stderr=subprocess.STDOUT)
    else:
        print message
        return subprocess.check_call(cmd, shell=True)",2,0
"async def run_async_task(self, container, asynctask, newthread = True):
        ""Run asynctask(sender) in task pool, call sender(events) to send customized events, return result""
        e = TaskEvent(self, async_task = asynctask, newthread = newthread)
        await container.wait_for_send(e)
        ev = await TaskDoneEvent.createMatcher(e)
        if hasattr(ev, 'exception'):
            raise ev.exception
        else:
            return ev.result",2,2
"def read(self):
        
        while len(self.rcv_data) == 0:
            sleep(0)

        if self.rcv_data[0] is None:
            raise DAPAccessIntf.DeviceError(""Device %s read thread exited"" %
                                            self.serial_number)
        return self.rcv_data.pop(0)",1,1
"def recv(self):
        
        try:
            message = self.ws.recv()
            return json.loads(message)
        except websocket._exceptions.WebSocketConnectionClosedException as ex:
            raise SelenolWebSocketClosedException() from ex",3,3
"def generate_annotations_json_string(source_path, only_simple=False):
    
    
    items = parse_json(source_path)
    results = []
    for item in items:
        signature = unify_type_comments(item.type_comments)
        if is_signature_simple(signature) or not only_simple:
            data = {
                'path': item.path,
                'line': item.line,
                'func_name': item.func_name,
                'signature': signature,
                'samples': item.samples
            }  
            results.append(data)
    return results",1,1
"def create_send_message(self, string_message, controller, zone=None, parameter=None):
        

        cc = hex(int(controller) - 1).replace('0x', '')  
        if zone is not None:
            zz = hex(int(zone) - 1).replace('0x', '')  
        else:
            zz = ''
        if parameter is not None:
            pr = hex(int(parameter)).replace('0x', '')
        else:
            pr = ''

        string_message = string_message.replace('@cc', cc)  
        string_message = string_message.replace('@zz', zz)  
        string_message = string_message.replace('@kk', KEYPAD_CODE)  
        string_message = string_message.replace('@pr', pr)  

        
        send_msg = string_message.split()
        send_msg = self.calc_checksum(send_msg)
        return send_msg",3,2
"def _update_proxy(self, change):
        
        if change['type'] == 'event':
            self.proxy.set_refreshed(True)
        else:
            super(SwipeRefreshLayout, self)._update_proxy(change)",2,2
"def to_json(self, buffer_or_path=None, indent=2):
        
        
        return json_data_to_json(
            self.to_json_data(),
            buffer_or_path=buffer_or_path,
            indent=indent
        )",0,0
"async def close(self):
        
        if not self._sendHeaders:
            self._startResponse()
        if self.inputstream is not None:
            self.inputstream.close(self.connection.scheduler)
        if self.outputstream is not None:
            await self.flush(True)
        if hasattr(self, 'session') and self.session:
            self.session.unlock()",2,2
"def delete(self, identifier):
        

        try:
            node = self.global_plate_definitions[identifier]
        except NodeIDAbsentError:
            logging.info(""Meta data {} not present during deletion"".format(identifier))
            return

        
        
        
        

        self.global_plate_definitions.remove_node(identifier)

        with switch_db(MetaDataModel, 'hyperstream'):
            meta_data = MetaDataModel.objects(tag=node.tag, data=node.data, parent=node.bpointer).first()
            if meta_data is not None:
                meta_data.delete()

        logging.info(""Meta data {} deleted"".format(identifier))",1,3
"def _main(self, fileobj, data, offset):
        
        fileobj.seek(offset)
        fileobj.write(data)",0,0
"def from_app(cls, app, environ, buffered=False):
        
        return cls(*_run_wsgi_app(app, environ, buffered))",3,3
"def seek(self, offset, whence=0):
        
        self._check_open_file()
        if not self._append:
            self._io.seek(offset, whence)
        else:
            self._read_seek = offset
            self._read_whence = whence
        if not self.is_stream:
            self.flush()",1,1
"def zscore_df(df, axis='row', keep_orig=False):
  
  df_z = {}

  for mat_type in df:
    if keep_orig and mat_type == 'mat':
      mat_orig = deepcopy(df[mat_type])

    inst_df = df[mat_type]

    if axis == 'row':
      inst_df = inst_df.transpose()

    df_z[mat_type] = (inst_df - inst_df.mean())/inst_df.std()

    if axis == 'row':
      df_z[mat_type] = df_z[mat_type].transpose()

  if keep_orig:
    df_z['mat_orig'] = mat_orig

  return df_z",0,3
"def _publish_status(self, status, parent=None):
        
        self.session.send(self.iopub_socket,
                          u'status',
                          {u'execution_state': status},
                          parent=parent,
                          ident=self._topic('status'),
                          )",2,2
"def write(self, chunk: bytes) -> ""Future[None]"":
        
        future = None
        if self.stream.closed():
            future = self._write_future = Future()
            self._write_future.set_exception(iostream.StreamClosedError())
            self._write_future.exception()
        else:
            future = self._write_future = Future()
            self._pending_write = self.stream.write(self._format_chunk(chunk))
            future_add_done_callback(self._pending_write, self._on_write_complete)
        return future",0,0
"def ReadMessageHandlerRequests(self, cursor=None):
    

    query = (""SELECT UNIX_TIMESTAMP(timestamp), request,""
             ""       UNIX_TIMESTAMP(leased_until), leased_by ""
             ""FROM message_handler_requests ""
             ""ORDER BY timestamp DESC"")

    cursor.execute(query)

    res = []
    for timestamp, request, leased_until, leased_by in cursor.fetchall():
      req = rdf_objects.MessageHandlerRequest.FromSerializedString(request)
      req.timestamp = mysql_utils.TimestampToRDFDatetime(timestamp)
      req.leased_by = leased_by
      req.leased_until = mysql_utils.TimestampToRDFDatetime(leased_until)
      res.append(req)
    return res",3,3
"def qn_df(df, axis='row', keep_orig=False):
  
  df_qn = {}

  for mat_type in df:
    inst_df = df[mat_type]

    
    if axis == 'row':
      inst_df = inst_df.transpose()

    missing_values = inst_df.isnull().values.any()

    
    if missing_values:

      
      missing_mask = pd.isnull(inst_df)

      
      inst_df = inst_df.fillna(value=0)

    
    common_dist = calc_common_dist(inst_df)

    
    inst_df = swap_in_common_dist(inst_df, common_dist)

    
    if missing_values:
      inst_df = inst_df.mask(missing_mask, other=np.nan)

    
    if axis == 'row':
      inst_df = inst_df.transpose()

    df_qn[mat_type] = inst_df

  return df_qn",0,3
"def get_keywords(self, entry):
        
        keyword_objects = []

        for keyword in entry.iterfind(""./keyword""):
            identifier = keyword.get('id')
            name = keyword.text
            keyword_hash = hash(identifier)

            if keyword_hash not in self.keywords:
                self.keywords[keyword_hash] = models.Keyword(**{'identifier': identifier, 'name': name})

            keyword_objects.append(self.keywords[keyword_hash])

        return keyword_objects",3,3
"def complete_from_dir(directory):
    
    global currDirModule
    if currDirModule is not None:
        if len(sys.path) > 0 and sys.path[0] == currDirModule:
            del sys.path[0]

    currDirModule = directory
    sys.path.insert(0, directory)",3,1
"def delete_avatar(self, scope='avatar/write'):
        
        return _delete(
            token=self.oauth.get_user_token(scope),
            uri='/user/avatar'
        )",0,0
"def updateConfig(self, eleobj, config, type='simu'):
        
        eleobj.setConf(config, type=type)",0,2
"def config_wdl(args):
    
    r = fapi.get_workspace_config(args.project, args.workspace,
                                  args.namespace, args.config)
    fapi._check_response_code(r, 200)
    
    method = r.json()[""methodRepoMethod""]
    args.namespace   = method[""methodNamespace""]
    args.method      = method[""methodName""]
    args.snapshot_id = method[""methodVersion""]
    
    return meth_wdl(args)",2,3
"def get_text(self, deserialized_json):
        
        if isinstance(deserialized_json, dict):
            result = ''
            for key in sorted(deserialized_json):
                result += self.get_text(deserialized_json[key]) + ' '
            return result

        if isinstance(deserialized_json, list):
            result = ''
            for item in deserialized_json:
                result += self.get_text(item) + ' '
            return result

        if isinstance(deserialized_json, six.string_types):
            return deserialized_json
        else:
            return ''",1,3
"def send_message(self, sender, um_to_user_list, body):
        
        msg = self.model(sender=sender,
                         body=body)
        msg.save()

        
        msg.save_recipients(um_to_user_list)
        msg.update_contacts(um_to_user_list)
        signals.email_sent.send(sender=None,msg=msg)

        return msg",3,2
"def infer_ast(src):
    
    if isinstance(src, _ast.AST):
        return src
    elif isinstance(src, str):
        return _ast.parse(src)
    else:
        
        
        
        
        
        src = cypy.fn_get_source(src)
        return _ast.parse(src)",3,3
"def _open_for_read(self):
        
        ownerid, datasetid = parse_dataset_key(self._dataset_key)
        response = requests.get(
            '{}/file_download/{}/{}/{}'.format(
                self._query_host, ownerid, datasetid, self._file_name),
            headers={
                'User-Agent': self._user_agent,
                'Authorization': 'Bearer {}'.format(
                    self._config.auth_token)
            }, stream=True)
        try:
            response.raise_for_status()
        except Exception as e:
            raise RestApiError(cause=e)
        self._read_response = response",1,1
"def interact(self):
        

        sockets = [sys.stdin, self.channel]
        while True:
            ready = select.select(sockets, [], [])[0]

            if sys.stdin in ready:
                line = sys.stdin.readline().encode('latin1')
                if not line:
                    break
                self.write(line)

            if self.channel in ready:
                self.read(1, echo=True)",2,0
"def read_xml(self):
        

        with reading_ancillary_files(self):
            root = super(ImpactLayerMetadata, self).read_xml()
            if root is not None:
                self._read_provenance_from_xml(root)
        return root",1,1
"def detect_fastq_annotations(fastq_file):
    
    annotations = set()
    queryread = tz.first(read_fastq(fastq_file))
    for k, v in BARCODEINFO.items():
        if v.readprefix in queryread:
            annotations.add(k)
    return annotations",1,1
"def atomic_write(filename, filesync=False):
  

  tmpf = tempfile.NamedTemporaryFile(delete=False)
  try:
    with open(tmpf.name, 'w') as curfile:
      yield curfile
      if filesync:
        curfile.flush()
        os.fsync(curfile.fileno())
    os.rename(tmpf.name, filename)
  finally:
    try:
      os.remove(tmpf.name)
    except (IOError, OSError):
      pass",0,0
"def _par_write(self, dirname):
        
        filename = dirname + '/' + 'template_parameters.csv'
        with open(filename, 'w') as parfile:
            for template in self.templates:
                for key in template.__dict__.keys():
                    if key not in ['st', 'event']:
                        parfile.write(key + ': ' +
                                      str(template.__dict__[key]) + ', ')
                parfile.write('\n')
        return self",0,0
"def set_cover_image(self, data=None, input_file_path=None, scope='avatar/write',
                        content_type='application/octet-stream'):
        
        if input_file_path:
            with open(input_file_path, 'rb') as f:
                data = f.read()

        if not data:
            raise ValueError('Either the data of an image file or the path to an image file must be provided')

        return _post(
            token=self.oauth.get_user_token(scope),
            uri='/user/cover',
            data=data,
            content_type=content_type,
            )",0,0
"def send(self, data):
        
        sock = socket.socket(*self.sock_args)
        try:
            sock.connect(self.sock_addr)
        except socket.error as exc:
            raise socket.error(""Can't connect to %r (%s)"" % (self.url.geturl(), exc))

        try:
            
            sock.send(data)

            
            while True:
                chunk = sock.recv(self.CHUNK_SIZE)
                if chunk:
                    yield chunk
                else:
                    break
        finally:
            
            sock.close()",2,2
"def _index_item(self, uri, num, batch_num):
        

        data = RdfDataset(get_all_item_data(uri, self.namespace),
                          uri).base_class.es_json()
        self.batch_data[batch_num].append(data)
        self.count += 1",2,3
"def acknowledge_message(self, delivery_tag):
        
        logger.debug('Acknowledging message %s', delivery_tag)
        self._channel.basic_ack(delivery_tag)",3,3
"def _get_all_filtered_channels(self, topics_without_signature):
        
        mpe_address     = self.get_mpe_address()
        event_signature = self.ident.w3.sha3(text=""ChannelOpen(uint256,uint256,address,address,address,bytes32,uint256,uint256)"").hex()
        topics = [event_signature] + topics_without_signature
        logs = self.ident.w3.eth.getLogs({""fromBlock"" : self.args.from_block, ""address""   : mpe_address, ""topics""    : topics})
        abi           = get_contract_def(""MultiPartyEscrow"")
        event_abi     = abi_get_element_by_name(abi, ""ChannelOpen"")
        channels_ids  = [get_event_data(event_abi, l)[""args""][""channelId""] for l in logs]
        return channels_ids",3,3
"def _thread_error(self, thread):
        

        
        if thread == self._send_thread:
            self._send_thread = None
        if thread == self._recv_thread:
            self._recv_thread = None

        
        if thread.successful():
            exception = socket.error('thread exited prematurely')
        elif isinstance(thread.exception, gevent.GreenletExit):
            
            self.close()
            return
        else:
            exception = thread.exception

        
        self.close()

        
        self.closed(exception)",2,2
"def shrink_file(in_filepath, api_key=None, out_filepath=None):
    
    info = get_shrink_file_info(in_filepath, api_key, out_filepath)
    write_shrunk_file(info)
    return info",0,0
"def estimate_maximum_read_length(fastq_file, quality_format=""fastq-sanger"",
                                 nreads=1000):
    
    in_handle = SeqIO.parse(open_fastq(fastq_file), quality_format)
    lengths = []
    for _ in range(nreads):
        try:
            lengths.append(len(next(in_handle).seq))
        except StopIteration:
            break
    in_handle.close()
    return max(lengths)",1,1
"def read(*paths):
    
    with open(os.path.join(os.path.dirname(__file__), *paths)) as fp:
        return fp.read()",1,1
"def use_ws_subhandlers(handler):
    
    async def new_handler(request, ws):
        handlers = await handler(request, ws)
        tasks = [request.app.loop.create_task(h(request, ws))
                 for h in handlers]

        try:
            await asyncio.gather(*tasks)
        finally:
            for task in tasks:
                task.cancel()

            await ws.close()
    return new_handler",0,2
"def get_selective_mirrors(self, number=None, pop_sorted=None):
        
        if pop_sorted is None:
            if hasattr(self, 'pop_sorted'):
                pop_sorted = self.pop_sorted
            else:
                return None
        if number is None:
            number = self.sp.lam_mirr
        res = []
        for i in xrange(1, number + 1):
            res.append(self.mean_old - pop_sorted[-i])
        return res",3,3
"def write_mmtf(file_path, input_data, input_function):
    
    mmtf_encoder = MMTFEncoder()
    pass_data_on(input_data, input_function, mmtf_encoder)
    mmtf_encoder.write_file(file_path)",0,0
"def _locateConvergencePoint(stats, minOverlap, maxOverlap):
    
    for i, v in enumerate(stats[::-1]):
      if not (v >= minOverlap and v <= maxOverlap):
        return len(stats) - i + 1

    
    return 1",3,3
"def reload(self):
        
        try:
            data = self._api.get(self.href, append_base=False).json()
            resource = File(api=self._api, **data)
        except Exception:
            try:
                data = self._api.get(
                    self._URL['get'].format(id=self.id)).json()
                resource = File(api=self._api, **data)
            except Exception:
                raise SbgError('Resource can not be refreshed!')

        self._data = resource._data
        self._dirty = resource._dirty
        self._old = copy.deepcopy(self._data.data)

        
        
        
        
        
        try:
            delattr(self, '_method')
        except AttributeError:
            pass",1,3
"def write(self, session, directory, name, replaceParamFile=None, **kwargs):
        
        if self.raster is not None or self.rasterText is not None:
            super(RasterMapFile, self).write(session, directory, name, replaceParamFile, **kwargs)",0,0
"def read(self, file_des, num_bytes):
        
        file_handle = self.filesystem.get_open_file(file_des)
        file_handle.raw_io = True
        return file_handle.read(num_bytes)",1,1
"def client_tokens(self):
        
        client_id, client_secret = self.get_auth_basic()
        if not client_id and not client_secret:
            client_id = self.query_kwargs.get('client_id', '')
            client_secret = self.query_kwargs.get('client_secret', '')
            if not client_id and not client_secret:
                client_id = self.body_kwargs.get('client_id', '')
                client_secret = self.body_kwargs.get('client_secret', '')

        return client_id, client_secret",3,3
"def typelogged(memb):
    
    if not pytypes.typelogging_enabled:
        return memb
    if _check_as_func(memb):
        return typelogged_func(memb)
    if isclass(memb):
        return typelogged_class(memb)
    if ismodule(memb):
        return typelogged_module(memb)
    if memb in sys.modules or memb in pytypes.typechecker._pending_modules:
        return typelogged_module(memb)
    return memb",0,3
"def send_root_file(self, filename):
        
        cache_timeout = self.get_send_file_max_age(filename)
        return send_from_directory(self.config['ROOT_FOLDER'], filename,
                                   cache_timeout=cache_timeout)",2,2
"def send_message(self, message):
        
        if isinstance(message, Request):
            request = self._requestLayer.send_request(message)
            request = self._observeLayer.send_request(request)
            request = self._blockLayer.send_request(request)
            transaction = self._messageLayer.send_request(request)
            self.send_datagram(transaction.request)
            if transaction.request.type == defines.Types[""CON""]:
                self._start_retransmission(transaction, transaction.request)
        elif isinstance(message, Message):
            message = self._observeLayer.send_empty(message)
            message = self._messageLayer.send_empty(None, None, message)
            self.send_datagram(message)",2,2
"def read(self, size):
        
        
        
        if self._update_buffer and not self._updated_buffer and self.duration:
            self.update_buffer((self.duration * 1000) + 5000)
            self._updated_buffer = True

        if not self._buf or len(self._buf) != size:
            self._buf = ffi.new(""char[]"", size)
            self._view = ffi.buffer(self._buf, size)

        res = librtmp.RTMP_Read(self.client.rtmp, self._buf, size)

        if res < 0:
            raise IOError(""Failed to read data"")

        return self._view[:res]",1,1
"def subtract_water(w_sig, w_supp_sig):
    
    mean_nw = np.mean(w_supp_sig,0)
    water_only = np.mean(w_sig - mean_nw, 0)
    mean_water = np.mean(w_sig, 0)

    scale_factor = water_only/mean_nw

    corrected = w_supp_sig - water_only/scale_factor[...,0,np.newaxis]
    return corrected",3,3
"def untag(self, querystring, tags, afterwards=None):
        
        if self.ro:
            raise DatabaseROError()
        self.writequeue.append(('untag', afterwards, querystring, tags))",0,0
"def do_write(self):
        
        while True:
            try:
                written = 0

                if hasattr(self.fd, 'send'):
                    written = self.fd.send(self.buffer)
                else:
                    written = os.write(self.fd.fileno(), self.buffer)

                self.buffer = self.buffer[written:]

                
                if self.close_requested and len(self.buffer) == 0:
                    self.close()

                return written
            except EnvironmentError as e:
                if e.errno not in Stream.ERRNO_RECOVERABLE:
                    raise e",0,0
"def _load_reports(self, report_files):
        
        contents = []
        for file_handle in report_files:
            
            contents.append(
                file_handle.read().decode(
                    'utf-8',
                    'replace'
                )
            )
        return contents",1,1
"def command(self):
        
        msg = self.gui.status_message
        n = len(msg)
        n_cur = len(self.cursor)
        return msg[:n - n_cur]",0,3
"def interruptWrite(self, endpoint, data, timeout=0):
        
        
        endpoint = (endpoint & ~ENDPOINT_DIR_MASK) | ENDPOINT_OUT
        
        data, _ = create_initialised_buffer(data)
        return self._interruptTransfer(endpoint, data, sizeof(data), timeout)",0,0
"def _client_run(self):
        
        
        self._waiting_messages = 0
        self._pending_messages = self._filter_pending()
        if self._backoff and not self._waiting_messages:
            _logger.info(""Client told to backoff - sleeping for %r seconds"", self._backoff)
            self._connection.sleep(self._backoff)
            self._backoff = 0
        self._connection.work()
        return True",2,2
"def get_token_from_env():
    
    token = os.getenv('VAULT_TOKEN')
    if not token:
        token_file_path = os.path.expanduser('~/.vault-token')
        if os.path.exists(token_file_path):
            with open(token_file_path, 'r') as f_in:
                token = f_in.read().strip()

    if not token:
        return None

    return token",3,1
"def data_from_file(self, file, apple_fix=False):
        
        with open(file, mode='rb') as f:
            content = f.read()

        if not content:
            raise IOError(""File %f is not readable or is empty!"" % file)

        return self.decode(content, apple_fix=apple_fix)",1,1
"def _spot_col(self, colname):
        
        try:
            col = self._spot_cols[colname]
        except KeyError:
            col = self._spot_cols[colname] = self._table[colname]
        return col",1,0
"def read_file(self, f, source=None):
        
        if source is None:
            try:
                source = f.name
            except AttributeError:
                source = '<???>'
        self._read(f, source)",1,1
"def send_signal(signal, request, user, **kwargs):
    
    params = {
        'user_ip': request.remote_addr
    }
    params.update(kwargs)
    if user.is_authenticated:
        params['uid'] = user.id
    signal.send(request.url, **params)",2,2
"def _get_gl_object_from_persistent_id(type_tag, gl_archive_abs_path):
    
    if type_tag == ""SFrame"":
        obj = _SFrame(gl_archive_abs_path)
    elif type_tag == ""SGraph"":
        obj = _load_graph(gl_archive_abs_path)
    elif type_tag == ""SArray"":
        obj = _SArray(gl_archive_abs_path)
    elif type_tag == ""Model"":
        from . import load_model as _load_model
        obj = _load_model(gl_archive_abs_path)
    else:
        raise _pickle.UnpicklingError(""Turi pickling Error: Unsupported object.""
              "" Only SFrames, SGraphs, SArrays, and Models are supported."")
    return obj",3,1
"def prepareToRemove(self):
        
        
        for node in (self._outputNode, self._inputNode):
            self.disconnectSignals(node)

        
        self._inputNode = None
        self._outputNode = None

        return True",3,2
"def rdpcap(filename, count=-1):
    
    with PcapReader(filename) as pcap:
        return pcap.read_all(count=count)",1,1
"def _vagrant_ssh_config(vm_):
    
    machine = vm_['machine']
    log.info('requesting vagrant ssh-config for VM %s', machine or '(default)')
    cmd = 'vagrant ssh-config {}'.format(machine)
    reply = __salt__['cmd.shell'](cmd,
                                  runas=vm_.get('runas'),
                                  cwd=vm_.get('cwd'),
                                  ignore_retcode=True)
    ssh_config = {}
    for line in reply.split('\n'):  
        tokens = line.strip().split()
        if len(tokens) == 2:  
            ssh_config[tokens[0]] = tokens[1]
    log.debug('ssh_config=%s', repr(ssh_config))
    return ssh_config",3,2
"def parsemsg(s): 
    
    prefix = ''
    trailing = []
    if not s:
        raise Exception(""Empty line."")
    if s[0] == ':':
        prefix, s = s[1:].split(' ', 1)
    if s.find(' :') != -1:
        s, trailing = s.split(' :', 1)
        args = s.split()
        args.append(trailing)
    else:
        args = s.split()
    command = args.pop(0)
    return prefix, command, args",3,3
"def data_size(self):
        
        
        if is_container(self._data):
            byte_length, bit_length = self._data.container_size()
            return byte_length + math.ceil(bit_length / 8)
        
        elif is_field(self._data):
            return math.ceil(self._data.bit_size / 8)
        else:
            return 0",1,1
"def cmd_led(self, args):
        
        if len(args) < 3:
            print(""Usage: led RED GREEN BLUE <RATE>"")
            return
        pattern = [0] * 24
        pattern[0] = int(args[0])
        pattern[1] = int(args[1])
        pattern[2] = int(args[2])
        
        if len(args) == 4:
            plen = 4
            pattern[3] = int(args[3])
        else:
            plen = 3
            
        self.master.mav.led_control_send(self.settings.target_system,
                                         self.settings.target_component,
                                         0, 0, plen, pattern)",2,2
"def kill(self):
        
        log.debug(""kill"")
        self._smoothie_hard_halt()
        self._reset_from_error()
        self._setup()",2,2
"def _get_attr_from_file(self, name, **kwargs):
        
        current_value = getattr(self, '_' + name)
        if current_value is not None:
            value = current_value
        else:
            parser_kwargs = getattr(self, 'read%s_kwargs' % name, {})
            value = getattr(self, 'read_%s' % name)(**parser_kwargs)
        return value",1,1
"def send_button_message(self, recipient_id, text, buttons, notification_type=NotificationType.regular):
        
        return self.send_message(recipient_id, {
            ""attachment"": {
                ""type"": ""template"",
                ""payload"": {
                    ""template_type"": ""button"",
                    ""text"": text,
                    ""buttons"": buttons
                }
            }
        }, notification_type)",2,2
"def get_patient_vcf(job, patient_dict):
    
    temp = job.fileStore.readGlobalFile(patient_dict['mutation_vcf'],
                                        os.path.join(os.getcwd(), 'temp.gz'))
    if is_gzipfile(temp):
        outfile = job.fileStore.writeGlobalFile(gunzip(temp))
        job.fileStore.deleteGlobalFile(patient_dict['mutation_vcf'])
    else:
        outfile = patient_dict['mutation_vcf']
    return outfile",3,1
"def read_file(fname):
    
    print(""NO - dont use this function read_file(fname):"")
    exit(1)
    lst = []
    with open(fname, 'r') as f:
        
        for line in f:
            lst.append(line.strip('\n'))
    return lst",1,1
"def send_video_url(self, recipient_id, video_url, notification_type=NotificationType.regular):
        
        return self.send_attachment_url(recipient_id, ""video"", video_url, notification_type)",2,2
"def run_command(cmd, sudo=False):
    
    if sudo is True:
        cmd = ['sudo'] + cmd

    try:
        output = Popen(cmd, stderr=STDOUT, stdout=PIPE)

    except FileNotFoundError:
        cmd.pop(0)
        output = Popen(cmd, stderr=STDOUT, stdout=PIPE)

    t = output.communicate()[0],output.returncode
    output = {'message':t[0],
              'return_code':t[1]}

    if isinstance(output['message'], bytes):
        output['message'] = output['message'].decode('utf-8')

    return output",2,2
"def drop(self):
        
        self._request_queue.put(
            requests.DropRequest(ack_id=self._ack_id, byte_size=self.size)
        )",3,2
"def getrawpart(self, msgid, stream=sys.stdout):
        
        for hdr, part in self._get(msgid):
            pl = part.get_payload(decode=True)
            if pl != None:
                print(pl, file=stream)
                break",3,3
"def _area_is_empty(self, screen, write_position):
        
        wp = write_position
        Transparent = Token.Transparent

        for y in range(wp.ypos, wp.ypos + wp.height):
            if y in screen.data_buffer:
                row = screen.data_buffer[y]

                for x in range(wp.xpos, wp.xpos + wp.width):
                    c = row[x]
                    if c.char != ' ' or c.token != Transparent:
                        return False

        return True",0,0
"def param (self, param, kwargs, default_value=False):
        
        if param in kwargs:
            value= kwargs[param]
            del kwargs[param]
        else:
            value= default_value
        setattr (self, param, value)",3,3
"def import_from_ding0(self, file, **kwargs):
        
        import_from_ding0(file=file, network=self.network)",1,1
"def write_quota(outbytes: int, outfn: Path) -> Union[None, int]:
    
    if not outfn:
        return None

    anch = Path(outfn).resolve().anchor
    freeout = shutil.disk_usage(anch).free

    if freeout < 10 * outbytes:
        raise IOError(f'out of disk space on {anch}.'
                      '{freeout/1e9} GB free, wanting to write {outbytes/1e9} GB.')

    return freeout",0,0
"async def execute_command(self, command):
        
        self.apply_future = asyncio.Future(loop=self.loop)

        entry = self.log.write(self.storage.term, command)
        asyncio.ensure_future(self.append_entries(), loop=self.loop)

        await self.apply_future",2,2
"def run(self):
        
        count = {}

        
        for f in self.input():  
            for line in f.open('r'):  
                for word in line.strip().split():
                    count[word] = count.get(word, 0) + 1

        
        f = self.output().open('w')
        for word, count in six.iteritems(count):
            f.write(""%s\t%d\n"" % (word, count))
        f.close()",0,0
"def read_entry(self):
    
    if len(self.bedarray) <= self.curr_ind: return None
    val = self.bedarray[self.curr_ind]
    self.curr_ind += 1
    return val",3,1
"def fetch_command(self, global_options, subcommand):
        
        commands = self.get_commands(global_options)
        try:
            klass = commands[subcommand]
        except KeyError:
            sys.stderr.write(""Unknown command: %r\nType '%s help' for usage.\nMany commands will only run at project directory, maybe the directory is not right.\n"" % \
                (subcommand, self.prog_name))
            sys.exit(1)
        return klass",3,3
"def resend(self):
        
        response = self.api_client._post(self.resource_path, 'resend')
        return self.api_client._make_api_object(response, APIObject)",2,2
"def notify_items(**kwargs):
    
    instance = kwargs.get('instance')
    created = kwargs.get('created', False)
    if hasattr(instance, 'send_knock') and active_knocks(instance):
        try:
            
            if hasattr(instance, 'get_available_languages'):
                langs = instance.get_available_languages()
            else:
                langs = [get_language()]
            for lang in langs:
                with override(lang):
                    instance.send_knock(created)
            return True
        except AttributeError:  
            pass
    return False",2,2
"def underline(self):
        
        u = self._rPr.u
        if u is MSO_UNDERLINE.NONE:
            return False
        if u is MSO_UNDERLINE.SINGLE_LINE:
            return True
        return u",0,0
"def _feed_stream(feeder, in_stream, out_stream, block_size = BLOCK_SIZE):
    'Uses feeder to read and convert from in_stream and write to out_stream.'

    while True:
        chunk = in_stream.read(block_size)
        if not chunk:
            break
        converted = feeder.feed(chunk)
        out_stream.write(converted)
    converted = feeder.feed()
    out_stream.write(converted)",1,1
"def isValidFeatureWriter(klass):
    
    if not isclass(klass):
        logger.error(""%r is not a class"", klass)
        return False
    if not hasattr(klass, ""tableTag""):
        logger.error(""%r does not have required 'tableTag' attribute"", klass)
        return False
    if not hasattr(klass, ""write""):
        logger.error(""%r does not have a required 'write' method"", klass)
        return False
    if (
        getargspec(klass.write).args
        != getargspec(BaseFeatureWriter.write).args
    ):
        logger.error(""%r 'write' method has incorrect signature"", klass)
        return False
    return True",0,0
"def get_db1():
    
    all_data = client.db_get(1)

    for i in range(400):                 
        row_size = 130                   
        index = i * row_size
        offset = index + row_size        
        util.print_row(all_data[index:offset])",1,1
"def save(cls, dct, filename):
        r
        fname = cls._parse_filename(filename=filename, ext='dct')
        dct = sanitize_dict(dct)
        with open(fname, 'wb') as f:
            pickle.dump(dct, f)",1,0
"def submit(self, form, url=None, **kwargs):
        
        if isinstance(form, Form):
            form = form.form
        response = self._request(form, url, **kwargs)
        Browser.add_soup(response, self.soup_config)
        return response",2,2
"def get_vtkjs_url(*args):
    
    if len(args) == 1:
        host = 'dropbox'
        inURL = args[0]
    elif len(args) == 2:
        host = args[0]
        inURL = args[1]
    else:
        raise RuntimeError('Arguments not understood.')
    if host.lower() == ""dropbox"":
        convertURL = convert_dropbox_url(inURL)
    else:
        print(""--> Warning: Web host not specified or supported. URL is simply appended to standalone scene loader link."")
        convertURL = inURL
    
    return generate_viewer_url(convertURL)",3,3
"def add(self, steamid_or_accountname_or_email):
        
        m = MsgProto(EMsg.ClientAddFriend)

        if isinstance(steamid_or_accountname_or_email, (intBase, int)):
            m.body.steamid_to_add = steamid_or_accountname_or_email
        elif isinstance(steamid_or_accountname_or_email, SteamUser):
            m.body.steamid_to_add = steamid_or_accountname_or_email.steam_id
        else:
            m.body.accountname_or_email_to_add = steamid_or_accountname_or_email

        self._steam.send_job(m)",2,2
"def _load(db_data, db):
    
    if db.name != db_data[""name""]:
        raise ValueError(""dbname doesn't matches! Maybe wrong database data."")

    db.__init__(client=db._client, name=db.name)
    for col_name, col_data in iteritems(db_data[""_collections""]):
        collection = db.get_collection(col_name)
        collection._documents = col_data[""_documents""]
        collection._uniques = col_data[""_uniques""]
        db._collections[col_name] = collection

    return db",1,3
"def post_events(self, events):

        

        url = ""{0}/{1}/projects/{2}/events"".format(self.base_url, self.api_version,
                                                   self.project_id)
        headers = utilities.headers(self.write_key)
        payload = json.dumps(events)
        response = self.fulfill(HTTPMethods.POST, url, data=payload, headers=headers, timeout=self.post_timeout)
        self._error_handling(response)
        return self._get_response_json(response)",0,2
"def _checkResponseWriteData(payload, writedata):
    
    _checkString(payload, minlength=4, description='payload')
    _checkString(writedata, minlength=2, maxlength=2, description='writedata')

    BYTERANGE_FOR_WRITEDATA = slice(2, 4)

    receivedWritedata = payload[BYTERANGE_FOR_WRITEDATA]

    if receivedWritedata != writedata:
        raise ValueError('Wrong write data in the response: {0!r}, but commanded is {1!r}. The data payload is: {2!r}'.format( \
            receivedWritedata, writedata, payload))",0,0
"def fromSuccessResponse(cls, success_response):
        
        self = cls()

        
        args = success_response.getSignedNS(self.ns_uri)

        
        
        if args is not None:
            self.parseExtensionArgs(args)
            return self
        else:
            return None",3,3
"def _send_loop(self):
        
        while True:
            try:
                line = self.oqueue.get().splitlines()[0][:500]
                self._obuffer += line + '\r\n'
                while self._obuffer:
                    sent = self._socket.send(self._obuffer)
                    self._obuffer = self._obuffer[sent:]
            except Exception:
                break",2,2
"def send_many(kwargs_list):
    
    emails = []
    for kwargs in kwargs_list:
        emails.append(send(commit=False, **kwargs))
    Email.objects.bulk_create(emails)",2,2
"def _complete_batch_send(self, resp):
        
        self._batch_send_d = None
        self._req_attempts = 0
        self._retry_interval = self._init_retry_interval
        if isinstance(resp, Failure) and not resp.check(tid_CancelledError,
                                                        CancelledError):
            log.error(""Failure detected in _complete_batch_send: %r\n%r"",
                      resp, resp.getTraceback())
        return",2,2
"def _dumpOnlineConf(self, format):
        
        oinfod = {k: v for k, v in self.simuinfo.items()}
        for k in (set(oinfod.keys()) & set(self.ctrlkeys)):
            oinfod[k] = self.ctrlinfo[k]
        return {self.name.upper(): {self.typename: oinfod}}",1,0
"def participate(self):
        
        try:
            while True:
                left = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable((By.ID, ""left_button""))
                )
                right = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable((By.ID, ""right_button""))
                )

                random.choice((left, right)).click()
                time.sleep(1.0)
        except TimeoutException:
            return False",2,2
"def open(filename, frame='unspecified'):
        
        data = BagOfPoints.load_data(filename)
        return RgbCloud(data, frame)",1,1
"def open_sciobj_file_by_path(abs_path, write=False):
    
    if write:
        d1_common.utils.filesystem.create_missing_directories_for_file(abs_path)
    return open(abs_path, 'wb' if write else 'rb')",1,0
"def write_image_to_disk(self, msg, result, fh):
        
        fh.write(base64.b64decode(result.group(1).encode('utf-8')))",0,0
"def parse(self, fp, headersonly=False):
        
        feedparser = FeedParser(self._class, policy=self.policy)
        if headersonly:
            feedparser._set_headersonly()
        while True:
            data = fp.read(8192)
            if not data:
                break
            feedparser.feed(data)
        return feedparser.close()",3,1
"def from_raw_message(cls, rawmessage):
        
        return ManageAllLinkRecord(rawmessage[2:3],
                                   rawmessage[3:4],
                                   rawmessage[4:7],
                                   rawmessage[7:8],
                                   rawmessage[8:9],
                                   rawmessage[9:10],
                                   rawmessage[10:11])",3,3
"def _read_bias_rating(self, short_filename):
        
        res = {}
        full_name = os.path.join(root_fldr, 'aikif', 'data', 'ref', short_filename)
        lg.record_process('bias.py','reading ' + full_name)
         
        with open(full_name, 'r') as f:
            for line in f:
                if line.strip('') == '':
                    break
                bias_line = []
                cols = line.split(',')
                bias_line.extend([short_filename])
                for col in cols:
                    bias_line.extend([col.strip('""').strip('\n')])
                self.bias_details.append(bias_line)",1,1
"def ping(self):
        
        d = self.replyQueue.get_ping()
        packet = RequestPing(self.charset, self.errors)
        self.transport.write(bytes(packet))
        return d.addCallback(self.handle_reply, self.charset, self.errors, None)",2,2
"def _redshift(distance, **kwargs):
    r
    cosmology = get_cosmology(**kwargs)
    return z_at_value(cosmology.luminosity_distance, distance, units.Mpc)",3,3
"def _ReadEncryptedData(self, read_size):
    
    encrypted_data = self._file_object.read(read_size)

    read_count = len(encrypted_data)

    self._encrypted_data = b''.join([self._encrypted_data, encrypted_data])

    self._decrypted_data, self._encrypted_data = (
        self._decrypter.Decrypt(self._encrypted_data))

    self._decrypted_data_size = len(self._decrypted_data)

    return read_count",1,1
"def read_key(pysam_alignment_record):
    
    return (
        pysam_alignment_record.query_name,
        pysam_alignment_record.is_duplicate,
        pysam_alignment_record.is_read1,
        pysam_alignment_record.is_read2,
    )",1,1
"def raw_broadcast(self, destination, message, **kwargs):
        
        self._broadcast(destination, message, **kwargs)",2,2
"def run_get_clusters_from_file(self, clusters_infile, all_ref_seqs, rename_dict=None):
        
        if rename_dict is None:
            rename_dict = {}

        
        
        seq_reader = pyfastaq.sequences.file_reader(self.infile)
        names_list_from_fasta_file = [seq.id for seq in seq_reader]
        names_set_from_fasta_file = set(names_list_from_fasta_file)

        clusters = self._load_user_clusters_file(clusters_infile, all_ref_seqs, rename_dict=rename_dict)

        if len(names_set_from_fasta_file) != len(names_list_from_fasta_file):
            raise Error('At least one duplicate name in fasta file ' + self.infile + '. Cannot continue')

        names_from_clusters_file = set()
        for new_names in clusters.values():
            names_from_clusters_file.update(new_names)

        if not names_set_from_fasta_file.issubset(names_from_clusters_file):
            raise Error('Some names in fasta file ""' + self.infile + '"" not given in cluster file. Cannot continue')

        return clusters",3,1
"def get_input(prompt, default=None, choices=None, option_value=None):
    
    if option_value is not None:
        return option_value
    
    choices = choices or []
    while 1:
        r = input(prompt+' ').strip()
        if not r and default is not None:
            return default
        if choices:
            if r not in choices:
                r = None
            else:
                break
        else:
            break
    return r",3,3
"def send_file(self, recipient_id, file_path, notification_type=NotificationType.regular):
        
        return self.send_attachment(recipient_id, ""file"", file_path, notification_type)",2,2
"def send_dict(self, data, timestamp=None, formatter=None):
        
        if formatter is None:
            formatter = self.formatter

        metric_list = []

        for metric, value in data.items():
            tmp_message = formatter(metric, value, timestamp)
            metric_list.append(tmp_message)

        message = """".join(metric_list)
        return self._dispatch_send(message)",2,2
"def dummy_client(reason):
    

    sender = applicationinsights.channel.NullSender()
    queue = applicationinsights.channel.SynchronousQueue(sender)
    channel = applicationinsights.channel.TelemetryChannel(None, queue)
    return applicationinsights.TelemetryClient(""00000000-0000-0000-0000-000000000000"", channel)",2,2
"def read_xml(self):
        
        if self.xml_uri is None:
            root = self._read_xml_db()
        else:
            root = self._read_xml_file()
        if root is not None:
            for name, path in list(self._standard_properties.items()):
                value = read_property_from_xml(root, path)
                if value is not None:
                    
                    setattr(self, name, value)

        return root",1,1
"async def continue_conversation(self, reference: ConversationReference, logic):
        
        request = TurnContext.apply_conversation_reference(Activity(), reference, is_incoming=True)
        context = self.create_context(request)
        return await self.run_middleware(context, logic)",2,3
"def concretize(self, **kwargs):
        
        return (self._read_file.concretize(**kwargs), self._write_file.concretize(**kwargs))",1,0
"def save(self, commit=True):
        
        contact = super(ContactFormBase, self).save()
        context = {'contact': contact}
        context.update(get_site_metas())

        subject = ''.join(render_to_string(self.mail_subject_template, context).splitlines())
        content = render_to_string(self.mail_content_template, context)

        send_mail(subject, content,
                  settings.DEFAULT_FROM_EMAIL,
                  settings.CONTACT_FORM_TO,
                  fail_silently=not settings.DEBUG)

        return contact",2,2
"def answer (self, headers, **options):
        
        self._steps.append(Answer (headers, **options).obj)",2,2
"def get_next_batch(self):
        
        try:
            batch = self.get_from_kafka()
            for message in batch:
                item = BaseRecord(message)
                self.increase_read()
                yield item
        except:
            self.finished = True
        self.logger.debug('Done reading batch')",1,1
"def erase(self):
        
        if self.use_file:
            if self.filename:
                file_be_gone(self.filename)
        self.lines = {}
        self.arcs = {}",1,0
"def to_binary(self,filename):
        

        retrans = False
        if self.istransformed:
            self._back_transform(inplace=True)
            retrans = True
        if self.isnull().values.any():
            warnings.warn(""NaN in par ensemble"",PyemuWarning)
        self.as_pyemu_matrix().to_coo(filename)
        if retrans:
            self._transform(inplace=True)",0,0
"async def send_container_update_to_client(self, client_addrs):
        
        self._logger.debug(""Sending containers updates..."")
        available_containers = tuple(self._containers.keys())
        msg = BackendUpdateContainers(available_containers)
        for client in client_addrs:
            await ZMQUtils.send_with_addr(self._client_socket, client, msg)",2,2
"def rename_gallery_file(self, file_id, new_file_name, scope='content/write'):
        
        return _put(
            token=self.oauth.get_user_token(scope),
            uri='/user/media/file/' + urllib.quote(file_id),
            data=new_file_name
        )",0,0
"def _setSmsMemory(self, readDelete=None, write=None):
        
        
        if write != None and write != self._smsMemWrite:
            self.write()
            readDel = readDelete or self._smsMemReadDelete
            self.write('AT+CPMS=""{0}"",""{1}""'.format(readDel, write))
            self._smsMemReadDelete = readDel
            self._smsMemWrite = write
        elif readDelete != None and readDelete != self._smsMemReadDelete:
            self.write('AT+CPMS=""{0}""'.format(readDelete))
            self._smsMemReadDelete = readDelete",0,0
"def create_monitoring_request(
            self,
            balance_proof: BalanceProofSignedState,
            reward_amount: TokenAmount,
    ) -> Optional[RequestMonitoring]:
        
        
        monitor_request = RequestMonitoring.from_balance_proof_signed_state(
            balance_proof=balance_proof,
            reward_amount=reward_amount,
        )
        
        monitor_request.sign(self.raiden.signer)
        return monitor_request",2,3
"def kill(self, container, signal=None):
        
        url = self._url(""/containers/{0}/kill"", container)
        params = {}
        if signal is not None:
            if not isinstance(signal, six.string_types):
                signal = int(signal)
            params['signal'] = signal
        res = self._post(url, params=params)

        self._raise_for_status(res)",2,2
"def read_column_data_from_txt(fname):
    
    datafile = open(fname)
    datarows = []
    for line in datafile:
        datarows.append([float(li) for li in line.split()])
    datacols = list(zip(*datarows))
    x_values = datacols[1:]
    y_values = datacols[0]

    return x_values, y_values",1,1
"def get(self, name, default=None, domain=None, path=None):
        
        try:
            return self._find_no_duplicates(name, domain, path)
        except KeyError:
            return default",3,3
"def receive_notification(self):
        
        if not self._endpoint.receive_messages():
            raise EOFError(""EOF"")
        self._process_input_notification()
        self._process_input_request()",2,3
"def walk_dir(path, args, state):
    
    if args.debug:
        sys.stderr.write(""Walking %s\n"" % path)

    for root, _dirs, files in os.walk(path):
        if not safe_process_files(root, files, args, state):
            return False
        if state.should_quit():
            return False
    return True",2,1
"def pdf(self, text=TEXT):
        
        self.logger.debug(""Generating the PDF report..."")
        html = HTML(string=self.html())
        css_file = self.css or join(dirname(abspath(__file__)),
                                    ""{}.css"".format(self.theme))
        css = [css_file, CSS(string=PAGE_CSS % self.__dict__)]
        html.write_pdf(""{}.pdf"".format(self.filename), stylesheets=css)",1,0
"def next_frame_header(socket):
    
    try:
        data = read_exactly(socket, 8)
    except SocketError:
        return (-1, -1)

    stream, actual = struct.unpack('>BxxxL', data)
    return (stream, actual)",1,1
"def main():
    

    for entry in b6_evalue_filter(args.b6, args.e_value):
        args.output.write(entry.write())",0,0
"def read_sources_from_numpy_file(npfile):
    
    srcs = np.load(npfile).flat[0]['sources']

    roi = ROIModel()
    roi.load_sources(srcs.values())
    return roi.create_table()",1,1
"def add_file(self,fName,content) :
        
        if not self.isdir() : raise Exception(""FSQuery tried to add a file in a node which is not a directory : %s"" % self.abs)
        self.write_file(""%s/%s""%(self.abs,fName),content)",0,0
"def notify(provider_name: str, **kwargs) -> Response:
    
    return get_notifier(provider_name=provider_name, strict=True).notify(**kwargs)",2,3
"def stop(self):
        
        if not self.running:
            return

        self.logger.debug(""stopping main task of %r"", self, stack_info=True)
        self._main_task.cancel()",2,2
"def setting(key, default=None, expected_type=None, qsettings=None):
    
    if default is None:
        default = inasafe_default_settings.get(key, None)
    full_key = '%s/%s' % (APPLICATION_NAME, key)
    return general_setting(full_key, default, expected_type, qsettings)",3,3
"def copyfrom_file_object(self, query, file_object, compress=True,
                             compression_level=DEFAULT_COMPRESSION_LEVEL):
        
        chunk_generator = self._read_in_chunks(file_object)
        return self.copyfrom(query, chunk_generator, compress,
                             compression_level)",1,1
"def _close_writable(self):
        
        
        for part in self._write_futures:
            part['ETag'] = part.pop('response').result()['ETag']

        
        with _handle_client_error():
            try:
                self._client.complete_multipart_upload(
                    MultipartUpload={'Parts': self._write_futures},
                    UploadId=self._upload_args['UploadId'],
                    **self._client_kwargs)
            except _ClientError:
                
                self._client.abort_multipart_upload(
                    UploadId=self._upload_args['UploadId'],
                    **self._client_kwargs)
                raise",0,0
"def send_multipart(self, *args, **kwargs):
        
        self.__in_send_multipart = True
        try:
            msg = super(GreenSocket, self).send_multipart(*args, **kwargs)
        finally:
            self.__in_send_multipart = False
            self.__state_changed()
        return msg",2,2
"def all_elements_by_type(name):
    
    if name:
        entry = element_entry_point(name)
        if entry:  
            result = element_by_href_as_json(entry)
            return result",3,3
"def bulkRead(self, endpoint, length, timeout=0):
        
        
        endpoint = (endpoint & ~ENDPOINT_DIR_MASK) | ENDPOINT_IN
        
        data, data_buffer = create_binary_buffer(length)
        try:
            transferred = self._bulkTransfer(endpoint, data, length, timeout)
        except USBErrorTimeout as exception:
            exception.received = data_buffer[:exception.transferred]
            raise
        return data_buffer[:transferred]",1,1
"def _read_info_as_dict(fid, values):
    
    output = {}
    for key, fmt in values:
        val = unpack(fmt, fid.read(calcsize(fmt)))
        if len(val) == 1:
            output[key] = val[0]
        else:
            output[key] = val
    return output",1,1
"def show_progress(self, message=None):
        
        if self.in_progress_hanging:
            if message is None:
                sys.stdout.write('.')
                sys.stdout.flush()
            else:
                if self.last_message:
                    padding = ' ' * max(0, len(self.last_message)-len(message))
                else:
                    padding = ''
                sys.stdout.write('\r%s%s%s%s' % (' '*self.indent, self.in_progress, message, padding))
                sys.stdout.flush()
                self.last_message = message",0,0
"def recvSecurityList(self, data):
        
        securityList = []
        while data.dataLen() > 0:
            securityElement = UInt8()
            data.readType(securityElement)
            securityList.append(securityElement)
        
        for s in securityList:
            if s.value in [SecurityType.NONE, SecurityType.VNC] and s > self._securityLevel:
                self._securityLevel = s
                break
        
        self.send(self._securityLevel)
        if self._securityLevel.value == SecurityType.VNC:
            self.expect(16, self.recvVNCChallenge)
        else:
            self.expect(4, self.recvSecurityResult)",2,3
"def addBarcodesToIdentifier(read, UMI, cell):
    

    read_id = read.identifier.split("" "")

    if cell == """":
        read_id[0] = read_id[0] + ""_"" + UMI
    else:
        read_id[0] = read_id[0] + ""_"" + cell + ""_"" + UMI

    identifier = "" "".join(read_id)

    return identifier",1,1
"def _write_reset(self):
        
        data = b'\x00\x00\x00\x00\x00\x00\x00\x8f'
        self._raw_write(data)
        self._waitfor_clear(yubikey_defs.SLOT_WRITE_FLAG)
        return True",0,0
"def receive_datagram(self, data, address):
        

        
        if not self.app:
            logger.debug(""Packet received"", address, data)
            return False

        
        
        try:
            response = self.app.handle_message(data, address)
        except Exception as err:
            logger.error(""Error processing message from "" + str(address) +
                          "":"" + str(data))
            logger.error(traceback.format_exc())
            return False

        
        
        if response:
            self.send_datagram(response, address)",2,3
"def write_files(template, destination='./dxf'):
    
    os.makedirs(destination)
    for key, value in template.items():
        with open(os.path.join(destination, key), 'w') as f:
            f.write(replace_whitespace(value, insert=False))",0,0
"def write_core_register(self, reg, data):
        
        regIndex = register_name_to_index(reg)
        
        if is_single_float_register(regIndex) and type(data) is float:
            data = conversion.float32_to_u32(data)
        elif is_double_float_register(regIndex) and type(data) is float:
            data = conversion.float64_to_u64(data)
        self.write_core_register_raw(regIndex, data)",0,0
"def open(self, number=0):
        
        self.inport = mido.open_input(find_faderport_input_name(number))
        self.outport = mido.open_output(find_faderport_output_name(number))
        self.outport.send(mido.Message.from_bytes([0x91, 0, 0x64]))  
        time.sleep(0.01)
        self.inport.callback = self._message_callback
        self.on_open()",2,2
"def pruned_c2cifft(invec, outvec, indices, pretransposed=False):
    
    N1, N2 = splay(invec)

    if not pretransposed:
        invec = fft_transpose(invec)
    first_phase(invec, outvec, N1=N1, N2=N2)
    out = fast_second_phase(outvec, indices, N1=N1, N2=N2)
    return out",0,0
"def _close_writable(self):
        
        
        for segment in self._write_futures:
            segment['etag'] = segment['etag'].result()

        
        with _handle_client_exception():
            self._client.put_object(self._container, self._object_name, _dumps(
                self._write_futures), query_string='multipart-manifest=put')",0,0
"def raw(self, channel=1):
                
                self.waitOPC()
                self.write('COMM_FORMAT DEF9,WORD,BIN')
                self.write('C%u:WAVEFORM?' % channel)
                return self.read_raw()",1,0
"def read_data(self, variable_instance):
        
        if self.inst is None:
            return
        if variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE':
            return self.parse_value(self.inst.query('?U6P0'))
        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_MANUAL_C_FREQ':
            freq = VariableProperty.objects.get_property(variable=variable_instance, name='VISA:FREQ')
            if freq is None:
                freq = 500
            return self.parse_value(self.inst.query('?MAM1SR9HT3ST2SM%dE'%freq))
        return None",1,1
"def write(self,f):
        
        f.write(""* singular value decomposition\n"")
        f.write(IFMT(self.svdmode)+'\n')
        f.write(IFMT(self.maxsing)+' '+FFMT(self.eigthresh)+""\n"")
        f.write('{0}\n'.format(self.eigwrite))",0,0
"def write(self, pkt):
		
		self.setRTS(self.DD_WRITE)
		self.flushInput()
		
		pkt = bytearray(pkt)
		pkt = bytes(pkt)

		num = self.serial.write(pkt)
		
		
		return num",0,0
"def set_stim(self, signal, fs, attenuation=0):
        

        self.tone_lock.acquire()
        self.stim = signal
        self.fs = fs
        self.atten = attenuation
        self.stim_changed = True

        self.tone_lock.release()",0,2
"def in_template_path(fn):
    
    return os.path.join(
        os.path.abspath(os.path.dirname(__file__)),
        ""../templates"",
        fn,
    )",0,0
"def backup_mediafiles(self):
        
        
        extension = ""tar%s"" % ('.gz' if self.compress else '')
        filename = utils.filename_generate(extension,
                                           servername=self.servername,
                                           content_type=self.content_type)
        tarball = self._create_tar(filename)
        
        if self.encrypt:
            encrypted_file = utils.encrypt_file(tarball, filename)
            tarball, filename = encrypted_file

        self.logger.debug(""Backup size: %s"", utils.handle_size(tarball))
        
        tarball.seek(0)
        if self.path is None:
            self.write_to_storage(tarball, filename)
        else:
            self.write_local_file(tarball, self.path)",0,0
"def _get_timestamp_tuple(ts):
    
    if isinstance(ts, datetime.datetime):    
        return Timestamp.from_datetime(ts).tuple()
    elif isinstance(ts, Timestamp):    
        return ts
    raise TypeError('Timestamp or datetime.datetime required')",3,3
"def open(filename, frame='unspecified'):
        
        data = BagOfPoints.load_data(filename)
        return Direction(data, frame)",1,1
"def get_accessions(cls, entry):
        
        return [models.Accession(accession=x.text) for x in entry.iterfind(""./accession"")]",3,3
"def align(aligner, reads):
    
    i = 0
    for record in SeqIO.parse(reads, ""fastq""):
        try:
            next(aligner.map(str(record.seq)))
            i += 1
        except StopIteration:
            print(record.format(""fastq""), end='')
    sys.stderr.write(""NanoLyse: removed {} reads.\n"".format(i))",0,1
"def scan(self):
        
        found = []
        for addr in range(0,0x80):
            try:
                self._i2c_bus.read_byte(addr)
            except OSError:
                continue
            found.append(addr)
        return found",1,1
"def _compile(self, lines):
        
        m = self.__class__.RE_IF.match(lines.current)
        if m is None:
            raise DefineBlockError(
                'Incorrect block definition at line {}, {}\nShould be '
                'something like: 
        args = m.group(3)
        self._evaluate = m.group(2).replace('.', '-')
        self._isbool = args is None
        if not self._isbool:
            args = args.strip('() \t')
            self._args = [arg.strip('@ \t').replace('.', '-')
                          for arg in args.split(',')]",1,3
"def publish(func):
    

    @wraps(func)
    def wrapper(self, *args, **kwargs):  
        payload = func(self, *args, **kwargs)
        payload.pop('self', None)
        self._publish(func.__name__, payload)
        return None

    wrapper.is_publish = True

    return wrapper",3,2
"def set_status(self, message, scope='status/write'):
        
        return _put(
            token=self.oauth.get_user_token(scope),
            uri='/user/statusmessage',
            data=message
        )",0,0
"def read(stream):
        
        (must_close, stream) = _open_stream(stream, 'read')
        try:
            data = PlyData._parse_header(stream)
            for elt in data:
                elt._read(stream, data.text, data.byte_order)
        finally:
            if must_close:
                stream.close()

        return data",1,1
"def _get_from_send_queue(self):
        
        try:
            packet = self.transmit.get(block=False)
            self.logger.info('Sending packet')
            self.logger.debug(packet)
            return packet
        except queue.Empty:
            pass
        return None",2,3
"def _parse_snapshots(self):
        
        try:
            snap = self._list_snapshots()
        except OSError as err:
            logging.error(""unable to list local snapshots!"")
            return {}
        vols = {}
        for line in snap.splitlines():
            if len(line) == 0:
                continue
            name, used, refer, mountpoint, written = line.split('\t')
            vol_name, snap_name = name.split('@', 1)
            snapshots = vols.setdefault(vol_name, OrderedDict())
            snapshots[snap_name] = {
                'name': name,
                'used': used,
                'refer': refer,
                'mountpoint': mountpoint,
                'written': written,
            }
        return vols",2,3
"def from_token(cls, url, path, token):
        
        source_dict = cls._fetch_secrets(url, path, token)
        return cls(source_dict, url, path, token)",1,3
"def mifare_classic_write_block(self, block_number, data):
        
        assert data is not None and len(data) == 16, 'Data must be an array of 16 bytes!'
        
        params = bytearray(19)
        params[0] = 0x01  
        params[1] = MIFARE_CMD_WRITE
        params[2] = block_number & 0xFF
        params[3:] = data
        
        response = self.call_function(PN532_COMMAND_INDATAEXCHANGE,
                                      params=params,
                                      response_length=1)
        return response[0] == 0x00",0,0
"def write_locked(*args, **kwargs):
    

    def decorator(f):
        attr_name = kwargs.get('lock', '_lock')

        @six.wraps(f)
        def wrapper(self, *args, **kwargs):
            rw_lock = getattr(self, attr_name)
            with rw_lock.write_lock():
                return f(self, *args, **kwargs)

        return wrapper

    
    
    if kwargs or not args:
        return decorator
    else:
        if len(args) == 1:
            return decorator(args[0])
        else:
            return decorator",0,0
"def read(self, source = None, **options):
        
        message = self.read_header(source)
        message.data = self.read_data(message.size, message.is_compressed, **options)

        return message",3,1
"def provider(self, service):
        
        service_provider = default_provider.get_service(str(service.provider.name.name))

        
        module_name = 'th_' + service.provider.name.name.split('Service')[1].lower()
        kwargs = {'trigger_id': str(service.id), 'cache_stack': module_name}
        return getattr(service_provider, 'process_data')(**kwargs)",3,3
"async def get(self, key, param=None, extend_herd_timeout=None):
        
        identity = self._gen_identity(key, param)
        res = await self.client.get(identity)
        if res:
            res, timeout = self._unpack(res)
            now = int(time.time())
            if timeout <= now:
                extend_timeout = extend_herd_timeout or self.extend_herd_timeout
                expected_expired_ts = now + extend_timeout
                value = self._pack([res, expected_expired_ts])
                await self.client.set(identity, value, extend_timeout)
                return None
        return res",3,3
"def dump_to_stream(self, cnf, stream, **opts):
        
        tree = container_to_etree(cnf, **opts)
        etree_write(tree, stream)",0,0
"def fft_transpose_fftw(vec):
    
    global _thetransposeplan
    outvec = pycbc.types.zeros(len(vec), dtype=vec.dtype)
    if _theplan is None:
        N1, N2 = splay(vec)
        _thetransposeplan = plan_transpose(N1, N2)
    ftexecute(_thetransposeplan, vec.ptr, outvec.ptr)
    return  outvec",0,0
"def PassphraseCallback(verify=False,
                       prompt1=""Enter passphrase:"",
                       prompt2=""Verify passphrase:""):
  
  while 1:
    try:
      p1 = getpass.getpass(prompt1)
      if verify:
        p2 = getpass.getpass(prompt2)
        if p1 == p2:
          break
      else:
        break
    except KeyboardInterrupt:
      return None
  return p1",1,3
"def hops(node1, node2):
    
    if node1 == node2:
        return 0
    elif set(node1.interfaces) & set(node2.interfaces):
        
        return 1
    else:
        
        return 0",3,3
"async def send(self, data):
        
        self.writer.write(data)
        await self.writer.drain()",2,2
"def writelog(logfile, contentlist, mode='replace'):
        
        
        if logfile is None:  
            print(UtilClass.print_msg(contentlist))
        else:
            if os.path.exists(logfile):
                if mode == 'replace':
                    os.remove(logfile)
                    log_status = open(logfile, 'w')
                else:
                    log_status = open(logfile, 'a')
            else:
                log_status = open(logfile, 'w')
            log_status.write(UtilClass.print_msg(contentlist))
            log_status.flush()
            log_status.close()",0,0
"def trigger_org_task(task_name, queue=""celery""):
    
    active_orgs = apps.get_model(""orgs"", ""Org"").objects.filter(is_active=True)
    for org in active_orgs:
        sig = signature(task_name, args=[org.pk])
        sig.apply_async(queue=queue)

    logger.info(""Requested task '%s' for %d active orgs"" % (task_name, len(active_orgs)))",2,2
"def onMessage(
        self,
        mid=None,
        author_id=None,
        message=None,
        message_object=None,
        thread_id=None,
        thread_type=ThreadType.USER,
        ts=None,
        metadata=None,
        msg=None,
    ):
        
        log.info(""{} from {} in {}"".format(message_object, thread_id, thread_type.name))",2,3
"def get_value(d, name, field):
    
    multiple = core.is_multiple(field)
    value = d.get(name, core.missing)
    if value is core.missing:
        return core.missing
    if multiple and value is not core.missing:
        return [
            decode_argument(v, name) if isinstance(v, basestring) else v for v in value
        ]
    ret = value
    if value and isinstance(value, (list, tuple)):
        ret = value[0]
    if isinstance(ret, basestring):
        return decode_argument(ret, name)
    else:
        return ret",3,3
"def close(self):
        
        if self.handle:
            self.router.del_handler(self.handle)
            self.handle = None
        self._latch.close()",3,0
"def get_timesheet_collection_for_context(ctx, entries_file=None):
    
    if not entries_file:
        entries_file = ctx.obj['settings'].get_entries_file_path(False)

    parser = TimesheetParser(
        date_format=ctx.obj['settings']['date_format'],
        add_date_to_bottom=ctx.obj['settings'].get_add_to_bottom(),
        flags_repr=ctx.obj['settings'].get_flags(),
    )

    return TimesheetCollection.load(entries_file, ctx.obj['settings']['nb_previous_files'], parser)",3,1
"def load_module(name, original_module):
    
    module = ModuleType(name)
    if PY3:
        import importlib.util
        spec = importlib.util.find_spec(original_module.__name__)
        source = spec.loader.get_code(original_module.__name__)
    else:
        if getattr(sys, ""frozen"", False):
            raise NotImplementedError(""Can't load modules on Python 2 with PyInstaller"")
        path = original_module.__file__
        if path.endswith("".pyc"") or path.endswith("".pyo""):
            path = path[:-1]
        with open(path) as f:
            source = f.read()
    exec_(source, module.__dict__, module.__dict__)
    return module",3,1
"def load(cls, filename):
        r
        fname = cls._parse_filename(filename)
        with open(fname, 'rb') as f:
            dct = pickle.load(f)
        return dct",1,1
"def sendcmd(self, cmd='AT', timeout=1.0):
        
        import time
        if self.write(cmd):
            while self.get_response() == '' and timeout > 0:
                time.sleep(0.1)
                timeout -= 0.1
        return self.get_lines()",2,2
"def broadcast(client, sender, msg_name, dest_name=None, block=None):
    
    dest_name = msg_name if dest_name is None else dest_name
    client[sender].execute('com.publish(%s)'%msg_name, block=None)
    targets = client.ids
    targets.remove(sender)
    return client[targets].execute('%s=com.consume()'%dest_name, block=None)",3,2
"def from_args(cls, args):
        
        
        project = Project.from_args(args)

        
        profile = Profile.from_args(
            args=args,
            project_profile_name=project.profile_name
        )

        return cls.from_parts(
            project=project,
            profile=profile,
            args=args
        )",1,3
"def onReactionRemoved(
        self,
        mid=None,
        author_id=None,
        thread_id=None,
        thread_type=None,
        ts=None,
        msg=None,
    ):
        
        log.info(
            ""{} removed reaction from {} message in {} ({})"".format(
                author_id, mid, thread_id, thread_type
            )
        )",3,3
"def add_tgt(self, as_rep, enc_as_rep_part, override_pp = True): 
		
		c = Credential()
		c.client = CCACHEPrincipal.from_asn1(as_rep['cname'], as_rep['crealm'])
		if override_pp == True:
			self.primary_principal = c.client
		c.server = CCACHEPrincipal.from_asn1(enc_as_rep_part['sname'], enc_as_rep_part['srealm'])
		c.time = Times.from_asn1(enc_as_rep_part)
		c.key = Keyblock.from_asn1(enc_as_rep_part['key'])
		c.is_skey = 0 
		
		c.tktflags = TicketFlags(enc_as_rep_part['flags']).cast(core.IntegerBitString).native
		c.num_address = 0
		c.num_authdata = 0
		c.ticket = CCACHEOctetString.from_asn1(Ticket(as_rep['ticket']).dump())
		c.second_ticket = CCACHEOctetString.empty()
		
		self.credentials.append(c)",2,3
"def confirmation(self, *args, **kwargs):
        
        if not self.current_terminal:
            raise RuntimeError(""no active terminal"")
        if not isinstance(self.current_terminal, Client):
            raise RuntimeError(""current terminal not a client"")

        self.current_terminal.confirmation(*args, **kwargs)",2,2
"def open(self, inp, opts=None):
        
        if isinstance(inp, io.TextIOWrapper):
            self.input = inp
        elif isinstance(inp, 'string'.__class__):  
            self.name  = inp
            self.input = open(inp, 'r')
        else:
            raise IOError(""Invalid input type (%s) for %s"" %
                          (inp.__class__.__name__, inp))
        return",1,1
"def init_app(self, app):
        
        if not hasattr(app, 'extensions'):
            app.extensions = {}
        mailer = BaseMailer(app.config.get('MARROWMAILER_CONFIG') or self.default_config)
        app.extensions['marrowmailer'] = mailer
        app.marrowmailer = self",2,3
"def fix_writemath_answer(results):
    
    new_results = []
    
    translate = _get_translate()

    for i, el in enumerate(results):
        writemathid = get_writemath_id(el, translate)
        if writemathid is None:
            continue
        new_results.append({'symbolnr': el['symbolnr'],
                            'semantics': writemathid,
                            'probability': el['probability']})
        if i >= 10 or (i > 0 and el['probability'] < 0.20):
            break
    return new_results",0,0
"def set_user_permission(rid, uid, action='full'):
    
    rid = rid.replace('/', '%252F')
    
    ensure_resource(rid)

    
    try:
        acl_url = urljoin(_acl_url(), 'acls/{}/users/{}/{}'.format(rid, uid, action))
        r = http.put(acl_url)
        assert r.status_code == 204
    except DCOSHTTPException as e:
        if e.response.status_code != 409:
            raise",0,2
"def read(self, visibility_timeout=None):
        
        rs = self.get_messages(1, visibility_timeout)
        if len(rs) == 1:
            return rs[0]
        else:
            return None",3,1
"def name(function):
    
    if isinstance(function, types.FunctionType):
        return function.__name__
    else:
        return str(function)",3,3
"def log(self, string):
        

        self.log_data.append(string)
        if self.log_function is None:
            print(string)
        else:
            self.log_function(string)",2,0
"def iterread(self, table):
    
    self.log('Reading: %s'%table)
    
    cls = self.FACTORIES[table]
    f = self._open(table)
    
    if unicodecsv:
      data = unicodecsv.reader(f, encoding='utf-8-sig')
    else:
      data = csv.reader(f)
    header = data.next()
    headerlen = len(header)
    ent = collections.namedtuple(
      'EntityNamedTuple',
      map(str, header)
    )
    for row in data:
      if len(row) == 0:
        continue
      
      row = [i.strip() for i in row]
      
      if len(row) < headerlen:
        row += ['']*(headerlen-len(row))
      yield cls.from_row(ent._make(row), self)
    f.close()",1,1
"def sendcommand(self, name, **kwargs):
        
        self.log(""sending command %s(**%s)"" % (name, kwargs))
        self.channel.send((name, kwargs))",2,2
"def get_external_ip(self):
        

        random.shuffle(self.server_list)
        myip = ''
        for server in self.server_list[:3]:
            myip = self.fetch(server)
            if myip != '':
                return myip
            else:
                continue
        return ''",3,3
"def _recv_msg(self):
        
        command = ord(recv_blocking(self._soc, 1))
        msglen = recv_blocking(self._soc, 4)
        msglen = ((msglen[0] << 24) + (msglen[1] << 16) +
                  (msglen[2] << 8) + msglen[3])
        msg = recv_blocking(self._soc, msglen)
        return command, msg",3,3
"def next(self):
        
        if self.__buffered is None:
            
            multiplier = self.__max_in_mem // self.__chunk_size
            self.__buffered = """"
        else:
            multiplier = 1
            self.__buffered = self.__buffered[self.__chunk_size:]

        data = self.__file.read(self.__chunk_size * multiplier)
        
        
        
        data = salt.utils.stringutils.to_str(data)

        if not data:
            self.__file.close()
            raise StopIteration

        self.__buffered += data
        return self.__buffered",1,1
"def check_share_permission(self, user_id):
        

        if _is_admin(user_id):
            return

        if int(self.created_by) == int(user_id):
            return

        for owner in self.owners:
            if owner.user_id == int(user_id):
                if owner.view == 'Y' and owner.share == 'Y':
                    break
        else:
            raise PermissionError(""Permission denied. User %s does not have share""
                             "" access on network %s"" %
                             (user_id, self.id))",0,0
"def add_child(self, child, modify=False):
        
        SceneGraph.add_child(self, child)
        self.notify()
        if modify:
            child._model_matrix_transform[:] = trans.inverse_matrix(self.model_matrix_global)
            child._normal_matrix_transform[:] = trans.inverse_matrix(self.normal_matrix_global)",3,2
"def gather_xml(self):
        
        if self.xml_filepath:
            with open(self.xml_filepath, ""r"") as xml_fh:
                self.raw_xml = xml_fh.read()
            self.bs_xml = BeautifulSoup(self.raw_xml, 'lxml')
        else:
            try:
                req = urlopen('http://%s:%s' % (self.host, self.port))
                self.raw_xml = req.read()
                self.bs_xml = BeautifulSoup(self.raw_xml, 'lxml')
            except URLError as u_error:
                raise XmlError('Unable to query BIND (%s:%s) for statistics. Reason: %s.' %
                               (self.host, self.port, u_error))",1,1
"def spkopa(filename):
    
    filename = stypes.stringToCharP(filename)
    handle = ctypes.c_int()
    libspice.spkopa_c(filename, ctypes.byref(handle))
    return handle.value",0,1
"def _load_neighbors(self) -> None:
        
        if not self.are_neighbors_cached:
            self._load_neighbors_from_external_source()
            db: GraphDatabaseInterface = self._graph.database
            db_node: DBNode = db.Node.find_by_name(self.name)
            db_node.are_neighbors_cached = True
            db.session.commit()
            self.are_neighbors_cached = True
        if not self._are_neighbors_loaded:
            self._load_neighbors_from_database()",1,1
"def file_or_filename(input):
    
    if isinstance(input, string_types):
        
        yield smart_open(input)
    else:
        
        input.seek(0)
        yield input",1,1
"def put(self, segment):
        
        if self.closed:
            return

        if segment is not None:
            future = self.executor.submit(self.fetch, segment,
                                          retries=self.retries)
        else:
            future = None

        self.queue(self.futures, (segment, future))",0,2
"def say(self, message, **options):
        
        


        
	
	
        if hasattr (self, 'voice'):
            if (not 'voice' in options):
                options['voice'] = self.voice


        self._steps.append(Say(message, **options).obj)",2,2
"def create_integration_alert_and_call_send(alert, configured_integration):
    
    integration_alert = IntegrationAlert(
        alert=alert,
        configured_integration=configured_integration,
        status=IntegrationAlertStatuses.PENDING.name,
        retries=configured_integration.integration.max_send_retries
    )

    send_alert_to_configured_integration(integration_alert)",2,2
"def _write_entries(po_files, languages, msgid, msgstrs, metadata, comment):
    
    start = re.compile(r'^[\s]+')
    end = re.compile(r'[\s]+$')
    for i, lang in enumerate(languages):
        meta = ast.literal_eval(metadata)
        entry = polib.POEntry(**meta)
        entry.tcomment = comment
        entry.msgid = msgid
        if msgstrs[i]:
            start_ws = start.search(msgid)
            end_ws = end.search(msgid)
            entry.msgstr = str(start_ws.group() if start_ws else '') + \
                unicode(msgstrs[i].strip()) + \
                str(end_ws.group() if end_ws else '')
        else:
            entry.msgstr = ''
        po_files[lang].append(entry)",1,0
"def get_screen():
    
    img_title = 'screen_' + g.client_id + '.png'
    image_path = STATIC_FILES_PATH + img_title
    if g.driver_status != WhatsAPIDriverStatus.LoggedIn:
        try:
            g.driver.get_qr(image_path)
            return send_file(image_path, mimetype='image/png')
        except Exception as err:
            pass
    g.driver.screenshot(image_path)
    return send_file(image_path, mimetype='image/png')",2,2
"def read_and_redirect(request, notification_id):
    
    notification_page = reverse('notifications:all')
    next_page = request.GET.get('next', notification_page)

    if is_safe_url(next_page):
        target = next_page
    else:
        target = notification_page
    try:
        user_nf = request.user.notifications.get(pk=notification_id)
        if not user_nf.read:
            user_nf.mark_as_read()
    except Notification.DoesNotExist:
        pass

    return HttpResponseRedirect(target)",1,1
"def _try_lookup(table, value, default = """"):
    
    try:
        string = table[ value ]
    except KeyError:
        string = default
    return string",3,3
"def answer_pre_checkout_query(token, pre_checkout_query_id, ok, error_message=None):
    
    method_url = 'answerPreCheckoutQuery'
    payload = {'pre_checkout_query_id': pre_checkout_query_id, 'ok': ok}
    if error_message:
        payload['error_message'] = error_message
    return _make_request(token, method_url, params=payload)",2,2
"def getTWFiles(self):
        

        ddir=""../data/tw/""
        
        
        
        files=os.path.listdir(ddir)
        files=[i for i in files if os.path.getsize(i)]
        files.sort(key=lambda i: os.path.getsize(i))
        filegroups=self.groupTwitterFilesByEquivalents(files)
        filegroups_grouped=self.groupTwitterFileGroupsForPublishing(filegroups)
        return filegroups_grouped",3,1
"def read_file(fname, *args, **kwargs):
    
    if not isstr(fname):
        raise ValueError('reading multiple files not supported, '
                         'please use `pyam.IamDataFrame.append()`')
    logger().info('Reading `{}`'.format(fname))
    format_kwargs = {}
    
    for c in [i for i in IAMC_IDX + ['year', 'time', 'value'] if i in kwargs]:
        format_kwargs[c] = kwargs.pop(c)
    return format_data(read_pandas(fname, *args, **kwargs), **format_kwargs)",1,1
"def run_command(self, codeobj):
        
        try:
            value, stdout = yield from self.attempt_exec(codeobj, self.namespace)
        except Exception:
            yield from self.send_exception()
            return
        else:
            yield from self.send_output(value, stdout)",0,2
"def is_bam_valid(bam_file):
    
    try:
        f = pysam.AlignmentFile(bam_file)
    except ValueError:
        raise
    except:
        raise
    else:
        f.close()

    return True",1,1
"def _backup_bytes(target, offset, length):
    
    click.echo('Backup {l} byes at position {offset} on file {file} to .bytes_backup'.format(
        l=length, offset=offset, file=target))

    with open(target, 'r+b') as f:
        f.seek(offset)

        with open(target + '.bytes_backup', 'w+b') as b:
            for _ in xrange(length):
                byte = f.read(1)
                b.write(byte)
            b.flush()

        f.flush()",0,0
"def get_all_domains(self, max_domains=None, next_token=None):
        
        params = {}
        if max_domains:
            params['MaxNumberOfDomains'] = max_domains
        if next_token:
            params['NextToken'] = next_token
        return self.get_list('ListDomains', params, [('DomainName', Domain)])",3,3
"def write_memory(self, addr, value, transfer_size=32):
        
        self.ap.write_memory(addr, value, transfer_size)",0,0
"def write_array(self, outfile, pixels):
        

        if self.interlace:
            self.write_passes(outfile, self.array_scanlines_interlace(pixels))
        else:
            self.write_passes(outfile, self.array_scanlines(pixels))",0,0
"def write_pdf(self, html):
        
        try:
            f = tempfile.NamedTemporaryFile(delete=False, suffix='.html')
            f.write(html.encode('utf_8', 'xmlcharrefreplace'))
            f.close()
        except Exception:
            raise IOError(u""Unable to create temporary file, aborting"")

        dummy_fh = open(os.path.devnull, 'w')

        try:
            command = [""prince"", f.name, ""-o"", self.destination_file]

            Popen(command, stderr=dummy_fh).communicate()
        except Exception:
            raise EnvironmentError(u""Unable to generate PDF file using ""
                                    ""prince. Is it installed and available?"")
        finally:
            dummy_fh.close()",0,0
"def generate_confusables():
    
    url = 'ftp://ftp.unicode.org/Public/security/latest/confusables.txt'
    file = get(url)
    confusables_matrix = defaultdict(list)
    match = re.compile(r'[0-9A-F ]+\s+;\s*[0-9A-F ]+\s+;\s*\w+\s*
                       r'\*?\s*\( (.+)  (.+) \) (.+)  (.+)\t
                       re.UNICODE)
    for line in file:
        p = re.findall(match, line)
        if p:
            char1, char2, name1, name2 = p[0]
            confusables_matrix[char1].append({
                'c': char2,
                'n': name2,
            })
            confusables_matrix[char2].append({
                'c': char1,
                'n': name1,
            })

    dump('confusables.json', dict(confusables_matrix))",1,1
"def send_exception(self):
        
        self.compiler.reset()

        exc = traceback.format_exc()
        self.writer.write(exc.encode('utf8'))

        yield from self.writer.drain()",0,2
"def _onPublish(self, mqttc, obj, mid):
        
        with self._messagesLock:
            if mid in self._onPublishCallbacks:
                midOnPublish = self._onPublishCallbacks.get(mid)
                del self._onPublishCallbacks[mid]
                if midOnPublish != None:
                    midOnPublish()
            else:
                
                
                self._onPublishCallbacks[mid] = None",3,2
"def _get_node(template, context, name):
    
    for node in template:
        if isinstance(node, BlockNode) and node.name == name:
            return node.nodelist.render(context)
        elif isinstance(node, ExtendsNode):
            return _get_node(node.nodelist, context, name)

    
    return """"",3,3
"def notify_peer_message(self, message, sender_id):
        
        payload = message.SerializeToString()
        self._notify(
            ""consensus_notifier_notify_peer_message"",
            payload,
            len(payload),
            sender_id,
            len(sender_id))",3,3
"def graph_from_file(filename, bidirectional=False, simplify=True,
                    retain_all=False, name='unnamed'):
    
    
    response_jsons = [overpass_json_from_file(filename)]

    
    G = create_graph(response_jsons, bidirectional=bidirectional,
                     retain_all=retain_all, name=name)

    
    if simplify:
        G = simplify_graph(G)

    log('graph_from_file() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))
    return G",1,1
"def change_resource_record_set_writer(connection, change_set, comment=None):
    

    e_root = etree.Element(
        ""ChangeResourceRecordSetsRequest"",
        xmlns=connection._xml_namespace
    )

    e_change_batch = etree.SubElement(e_root, ""ChangeBatch"")

    if comment:
        e_comment = etree.SubElement(e_change_batch, ""Comment"")
        e_comment.text = comment

    e_changes = etree.SubElement(e_change_batch, ""Changes"")

    
    for change in change_set.deletions + change_set.creations:
        e_changes.append(write_change(change))

    e_tree = etree.ElementTree(element=e_root)

    

    fobj = BytesIO()
    
    e_tree.write(fobj, xml_declaration=True, encoding='utf-8', method=""xml"")
    return fobj.getvalue().decode('utf-8')",2,0
"def query_nologin():
        
        time_now = tools.timestamp()
        return TabMember.select().where(
            ((time_now - TabMember.time_login) > 7776000)
            & ((time_now - TabMember.time_email) > 10368000)
        )",2,3
"def get_sequences_from_cluster(c1, c2, data):
    
    seqs1 = data[c1]['seqs']
    seqs2 = data[c2]['seqs']
    seqs = list(set(seqs1 + seqs2))
    names = []
    for s in seqs:
        if s in seqs1 and s in seqs2:
            names.append(""both"")
        elif s in seqs1:
            names.append(c1)
        else:
            names.append(c2)
    return seqs, names",3,3
"def _emit(self, event):
        
        if (event.get('interface') is not iaxiom.IStatEvent and
            'athena_send_messages' not in event and
            'athena_received_messages' not in event):
            return

        out = []
        for k, v in event.iteritems():
            if k in ('system', 'message', 'interface', 'isError'):
                continue
            if not isinstance(v, unicode):
                v = str(v).decode('ascii')
            out.append(dict(key=k.decode('ascii'), value=v))
        self.callRemote(StatUpdate, data=out)",2,2
"def extract(json_object, args, csv_writer):
    
    found = [[]]
    for attribute in args.attributes:
        item = attribute.getElement(json_object)
        if len(item) == 0:
            for row in found:
                row.append(""NA"")
        else:
            found1 = []
            for value in item:
                if value is None:
                    value = ""NA""
                new = copy.deepcopy(found)
                for row in new:
                    row.append(value)
                found1.extend(new)
            found = found1

    for row in found:

        csv_writer.writerow(row)
    return len(found)",0,0
"def error(self, buf, newline=True):
        

        buf = buf or ''

        if self._colored:
            buf = self.ESCAPE_RED + buf + self.ESCAPE_CLEAR
        if newline:
            buf += os.linesep

        try:
            self._error.write(buf)

            if hasattr(self._error, 'flush'):
                self._error.flush()

        except IOError as exc:
            if exc.errno != errno.EPIPE:  
                raise",0,0
"def all_connected_components(i,j):
    
    if len(i) == 0:
        return i
    i1 = np.hstack((i,j))
    j1 = np.hstack((j,i))
    order = np.lexsort((j1,i1))
    i=np.ascontiguousarray(i1[order],np.uint32)
    j=np.ascontiguousarray(j1[order],np.uint32)
    
    
    
    counts = np.ascontiguousarray(np.bincount(i.astype(int)),np.uint32)
    indexes = np.ascontiguousarray(np.cumsum(counts)-counts,np.uint32)
    
    
    
    
    labels = np.zeros(len(counts), np.uint32)
    _all_connected_components(i,j,indexes,counts,labels)
    return labels",2,3
"def report(self, filename=None):
    
    
    filename = filename or self._filename
    if filename:
      
      with safe_open(filename, 'w') as writer:
        writer.write(
          'invocation_id,task_name,targets_hash,target_id,cache_key_id,cache_key_hash,phase,valid'
          + '\n')
        for task_report in self._task_reports.values():
          task_report.report(writer)",0,0
"def bucket_policy_to_dict(policy):
    
    import json

    if not isinstance(policy, dict):
        policy = json.loads(policy)

    statements = {s['Sid']: s for s in policy['Statement']}

    d = {}

    for rw in ('Read', 'Write'):
        for prefix in TOP_LEVEL_DIRS:
            sid = rw.title() + prefix.title()

            if sid in statements:

                if isinstance(statements[sid]['Principal']['AWS'], list):

                    for principal in statements[sid]['Principal']['AWS']:
                        user_name = principal.split('/').pop()
                        d[(user_name, prefix)] = rw[0]
                else:
                    user_name = statements[sid]['Principal']['AWS'].split('/').pop()
                    d[(user_name, prefix)] = rw[0]

    return d",0,1
"def parse(self, fp, headersonly=False):
        
        fp = TextIOWrapper(fp, encoding='ascii', errors='surrogateescape')
        with fp:
            return self.parser.parse(fp, headersonly)",3,1
"def _read_prm_file(prm_filename):
    
    logger.debug(""Reading config-file: %s"" % prm_filename)
    try:
        with open(prm_filename, ""r"") as config_file:
            prm_dict = yaml.load(config_file)
    except yaml.YAMLError:
        raise ConfigFileNotRead
    else:
        _update_prms(prm_dict)",1,1
"def write_data(self,variable_id, value, task):
        
        variable = self._variables[variable_id]
        if task.property_name != '':
            
            vp = VariableProperty.objects.update_or_create_property(variable=variable, name=task.property_name.upper(),
                                                        value=value, value_class='FLOAT64')
            return True
        if variable.visavariable.variable_type == 0:  
            
            pass
        else:
            return False",0,0
"def trim_data_back_to(monthToKeep):
    
    global g_failed_tests_info_dict
    current_time = time.time()      

    oldest_time_allowed = current_time - monthToKeep*30*24*3600 

    clean_up_failed_test_dict(oldest_time_allowed)
    clean_up_summary_text(oldest_time_allowed)",1,0
"def alias_feed(name, alias):
    
    with Database(""aliases"") as db:
        if alias in db:
            print(""Something has gone horribly wrong with your aliases! Try deleting the %s entry."" % name)
            return
        else:
            db[alias] = name",0,3
"def newline(self, node=None, extra=0):
        
        self._new_lines = max(self._new_lines, 1 + extra)
        if node is not None and node.lineno != self._last_line:
            self._write_debug_info = node.lineno
            self._last_line = node.lineno",0,0
"def verify_permitted_to_read(gs_path):
    
    
    
    
    from . import _bucket
    bucket, prefix = _bucket.parse_name(gs_path)
    credentials = None
    if google.datalab.Context._is_signed_in():
      credentials = google.datalab.Context.default().credentials
    args = {
        'maxResults': Api._MAX_RESULTS,
        'projection': 'noAcl'
    }
    if prefix is not None:
      args['prefix'] = prefix
    url = Api._ENDPOINT + (Api._OBJECT_PATH % (bucket, ''))
    try:
      google.datalab.utils.Http.request(url, args=args, credentials=credentials)
    except google.datalab.utils.RequestException as e:
      if e.status == 401:
        raise Exception('Not permitted to read from specified path. '
                        'Please sign in and make sure you have read access.')
      raise e",1,1
"def swap_twitter_subject(subject, body):
    

    if subject.startswith('Tweet from'):
        lines = body.split('\n')
        for idx, line in enumerate(lines):
            if re.match(r'.*, ?\d{2}:\d{2}]]', line) is not None:
                try:
                    subject = lines[idx + 1]
                except IndexError:
                    pass
                break
    return subject, body",3,3
"def flush(self):
        
        self._buffer += self._decoder.decode(b"""", final=True)
        if self._buffer:
            if self._print:
                print(self._buffer)
            if self._callback:
                self._callback(self._buffer)
        self._buffer = """"",2,0
"def download_write_file(self, metadata, out_dir=None):
        
        fileName = metadata['name']
        path = os.path.join(out_dir or wandb_dir(), fileName)
        if self.file_current(fileName, metadata['md5']):
            return path, None

        size, response = self.download_file(metadata['url'])

        with open(path, ""wb"") as file:
            for data in response.iter_content(chunk_size=1024):
                file.write(data)

        return path, response",0,0
"def table_nan_locs(table):
    
    ans = []
    for rownum, row in enumerate(table):
        try:
            if pd.isnull(row).any():
                colnums = pd.isnull(row).nonzero()[0]
                ans += [(rownum, colnum) for colnum in colnums]
        except AttributeError:  
            if pd.isnull(row):
                ans += [(rownum, 0)]
    return ans",3,3
"def _version_from_file(
        path_to_version,
        default_version=DEFAULT_VERSION,
):
    
    version_filepath = os.path.join(path_to_version, 'version.txt')
    if not os.path.isfile(version_filepath):
        warnings.warn(
            'Unable to resolve current version',
            exceptions.ProsperDefaultVersionWarning)
        return default_version

    with open(version_filepath, 'r') as v_fh:
        data = v_fh.read()

    return data",1,1
"def _sendMessage(self, msg):
        
        if not msg:
            return
        msg = self._collapseMsg(msg)
        self.sendStatus(msg)",2,2
"def load_stats(self, fdump):
        
        if isinstance(fdump, type('')):
            fdump = open(fdump, 'rb')
        self.index = pickle.load(fdump)
        self.snapshots = pickle.load(fdump)
        self.sorted = []",1,1
"def unregister(self, matchers, runnable):
        
        for m in matchers:
            self.matchtree.remove(m, runnable)
            if m.indices[0] == PollEvent._classname0 and len(m.indices) >= 2:
                self.polling.onmatch(m.indices[1], None if len(m.indices) <= 2 else m.indices[2], False)
        self.registerIndex.setdefault(runnable, set()).difference_update(matchers)",2,2
"def create_ca_file(anchor_list, filename):
    
    try:
        f = open(filename, ""w"")
        for a in anchor_list:
            s = a.output(fmt=""PEM"")
            f.write(s)
        f.close()
    except:
        return None
    return filename",0,0
"def get_type(type_: Type) -> 'PredicateType':
        
        if is_callable(type_):
            callable_args = type_.__args__
            argument_types = [PredicateType.get_type(t) for t in callable_args[:-1]]
            return_type = PredicateType.get_type(callable_args[-1])
            return FunctionType(argument_types, return_type)
        elif is_generic(type_):
            
            
            name = get_generic_name(type_)
        else:
            name = type_.__name__
        return BasicType(name)",3,3
"def process_messages(self):
        
        try:
            msg = self.msgbackend.pop(self.incoming_message_mailbox)
            self.handle_incoming_message(msg)
        except queue.Empty:
            logger.debug(""Worker message queue currently empty."")",3,3
"def process_next_message(self, timeout):
        
        message = self.worker_manager.receive(timeout)

        if isinstance(message, Acknowledgement):
            self.task_manager.task_start(message.task, message.worker)
        elif isinstance(message, Result):
            self.task_manager.task_done(message.task, message.result)",3,3
"def register(self, make_public=False, cloud=None, api_key=None, version=None, **kwargs):
        
        kwargs['make_public'] = make_public
        url_params = {""batch"": False, ""api_key"": api_key, ""version"": version, ""method"": ""register""}
        return self._api_handler(None, cloud=cloud, api=""custom"", url_params=url_params, **kwargs)",0,2
"def load_data(path, dense=False):
    

    catalog = {'.csv': load_csv, '.sps': load_svmlight_file, '.h5': load_hdf5}

    ext = os.path.splitext(path)[1]
    func = catalog[ext]
    X, y = func(path)

    if dense and sparse.issparse(X):
        X = X.todense()

    return X, y",1,1
"def i2c_write(self, address, data):
        
        data_ = uint8_tVector()
        for i in range(0, len(data)):
            data_.append(int(data[i]))
        Base.i2c_write(self, address, data_)",2,0
"def new_dxfile(mode=None, write_buffer_size=dxfile.DEFAULT_BUFFER_SIZE, expected_file_size=None, file_is_mmapd=False,
               **kwargs):
    
    dx_file = DXFile(mode=mode, write_buffer_size=write_buffer_size, expected_file_size=expected_file_size,
                     file_is_mmapd=file_is_mmapd)
    dx_file.new(**kwargs)
    return dx_file",0,0
"def move(self, target, pos=None):
        
        if self.outline != target.outline:
            raise IntegrityError('Elements must be from the same outline!')
        tree_manipulation.send(
            sender=self.__class__,
            instance=self,
            action='move',
            target_node_type=None,
            target_node=target,
            pos=pos
        )
        return super().move(target, pos)",2,2
"def elcm_session_list(irmc_info):
    
    
    resp = elcm_request(irmc_info,
                        method='GET',
                        path='/sessionInformation/')

    if resp.status_code == 200:
        return _parse_elcm_response_body_as_json(resp)
    else:
        raise scci.SCCIClientError(('Failed to list sessions with '
                                    'error code %s' % resp.status_code))",2,3
"def get_substitutions_from_config(config):
    
    result = []
    pattern_names = config.options(SUBSTITUTION_SECTION)
    pattern_names.sort()
    for name in pattern_names:
        pattern_val = config.get(SUBSTITUTION_SECTION, name)
        list_rep = ast.literal_eval(pattern_val)
        substitution = parse_substitution_from_list(list_rep)
        result.append(substitution)
    return result",3,3
"def _update_proxy(self, change):
        
        if change['type'] == 'event' and self.proxy_is_active:
            self.proxy.set_opened(change['name'] == 'show')
        else:
            super(ActionMenuView, self)._update_proxy(change)",2,2
"async def send_script(self, conn_id, data):
        

        self._ensure_connection(conn_id, True)
        dev = self._get_property(conn_id, 'device')
        conn_string = self._get_property(conn_id, 'connection_string')

        
        await self.notify_progress(conn_string, 'script', 0, len(data))
        await self.notify_progress(conn_string, 'script', len(data) // 2, len(data))
        await self.notify_progress(conn_string, 'script', len(data), len(data))

        dev.script = data",2,2
"def _fetch_messages(self):
        
        try:
            [_, msg] = self.socket.recv_multipart(flags=zmq.NOBLOCK)
            if Global.CONFIG_MANAGER.tracing_mode:
                Global.LOGGER.debug(""fetched a new message"")

            self.fetched = self.fetched + 1
            obj = pickle.loads(msg)
            self._deliver_message(obj)
            return obj
        except zmq.error.Again:
            return None
        except Exception as new_exception:
            Global.LOGGER.error(new_exception)
            raise new_exception",3,3
"def rally_fetch_point_send(self, target_system, target_component, idx, force_mavlink1=False):
                
                return self.send(self.rally_fetch_point_encode(target_system, target_component, idx), force_mavlink1=force_mavlink1)",3,2
"def send_attachment_url(self, recipient_id, attachment_type, attachment_url,
                            notification_type=NotificationType.regular):
        
        return self.send_message(recipient_id, {
            'attachment': {
                'type': attachment_type,
                'payload': {
                    'url': attachment_url
                }
            }
        }, notification_type)",2,2
"def _send_notification(to, subject, template, **ctx):
    
    msg = Message(
        subject,
        sender=current_app.config.get('SUPPORT_EMAIL'),
        recipients=[to]
    )
    msg.body = render_template(template, **ctx)

    send_email.delay(msg.__dict__)",2,2
"def get(self, request=None, timeout=1.0):
        
        if request is None:
            request = nfc.ndef.Message(nfc.ndef.Record())

        if not isinstance(request, nfc.ndef.Message):
            raise TypeError(""request type must be nfc.ndef.Message"")

        response_data = self._get(request, timeout)

        if response_data is not None:
            try:
                response = nfc.ndef.Message(response_data)
            except Exception as error:
                log.error(repr(error))
            else:
                return response",3,3
"def load_go_graph(go_fname):
    
    global _go_graph
    if _go_graph is None:
        _go_graph = rdflib.Graph()
        logger.info(""Parsing GO OWL file"")
        _go_graph.parse(os.path.abspath(go_fname))
    return _go_graph",1,1
"def generate_requirements(output_path=None):
    
    from django.conf import settings
    reqs = set()
    
    for app in settings.INSTALLED_APPS:
        if app in mapping.keys():
            reqs |= set(mapping[app])
    if output_path is None:
        print ""--extra-index-url=http://opensource.washingtontimes.com/pypi/simple/""
        for item in reqs:
            print item
    else:
        try:
            out_file = open(output_path, 'w')
            out_file.write(""--extra-index-url=http://opensource.washingtontimes.com/pypi/simple/\n"")
            for item in reqs:
                out_file.write(""%s\n"" % item)
        finally:
            out_file.close()",0,0
"def begin(self, request, data):
        
        request = self.get_request(
                http_url = self.REQUEST_TOKEN_URL,
                parameters = dict(oauth_callback = self.get_callback(request)))
        content = self.load_request(request)
        if not content:
            return redirect('netauth-login')
        request = self.get_request(token = Token.from_string(content), http_url=self.AUTHORIZE_URL)
        return redirect(request.to_url())",3,3
"def convert_double_to_two_registers(doubleValue):
      
    myList = list()
    myList.append(int(doubleValue & 0x0000FFFF))         
    myList.append(int((doubleValue & 0xFFFF0000)>>16))   
    return myList",2,3
"def copy_file(stream, target, maxread=-1, buffer_size=2*16):
    
    size, read = 0, stream.read
    while 1:
        to_read = buffer_size if maxread < 0 else min(buffer_size, maxread-size)
        part = read(to_read)
        if not part:
            return size
        target.write(part)
        size += len(part)",0,1
"def _send_scp(self, x, y, p, *args, **kwargs):
        
        
        
        
        if self._scp_data_length is None:
            length = consts.SCP_SVER_RECEIVE_LENGTH_MAX
        else:
            length = self._scp_data_length

        connection = self._get_connection(x, y)
        return connection.send_scp(length, x, y, p, *args, **kwargs)",2,2
"def _display_error(normalized_data, stream):
    
    
    error = normalized_data['error']
    if 'error_detail' in normalized_data:
        stream.write(""exit code: {0}\n"".format(normalized_data['error_detail'].get('code'),
                                               'There was no exit code provided'))

        stream.write(normalized_data['error_detail'].get('message', 'There were no message details provided.'))

    raise DockerStreamException(error)",3,0
"async def get(self, public_key):
		
		if settings.SIGNATURE_VERIFICATION:
			super().verify()

		response = await self.account.getnews(public_key=public_key)
		
		if isinstance(response, list):
			self.write(json.dumps(response))
			raise tornado.web.Finish
		
		elif isinstance(response, dict):
			try:
				error_code = response[""error""]
			except:
				del response[""account_id""]

				self.write(response)
			else:
				self.set_status(error_code)
				self.write(response)
				raise tornado.web.Finish",2,0
"async def extended_analog(self, pin, data):
        
        analog_data = [pin, data & 0x7f, (data >> 7) & 0x7f, (data >> 14) & 0x7f]
        await self._send_sysex(PrivateConstants.EXTENDED_ANALOG, analog_data)",0,2
"def get_data_context(context_type, options, *args, **kwargs):
    
    if context_type == ""SqlAlchemy"":
        return SqlAlchemyDataContext(options, *args, **kwargs)
    elif context_type == ""PandasCSV"":
        return PandasCSVDataContext(options, *args, **kwargs)
    else:
        raise ValueError(""Unknown data context."")",3,3
"def setPermissions(self, dbName, access) :
        
        import json

        if not self.URL :
            raise CreationError(""Please save user first"", None, None)

        rights = []
        if access :
            rights.append(""rw"")

        rights = ''.join(rights)

        if not self.connection.hasDatabase(dbName) :
            raise KeyError(""Unknown database: %s"" % dbName)

        url = ""%s/database/%s"" % (self.URL, dbName)
        r = self.connection.session.put(url, data = json.dumps({""grant"": rights}, default=str))
        if r.status_code < 200 or r.status_code > 202 :
            raise CreationError(""Unable to grant rights"", r.content)",0,0
"def _send_request(self, request, headers=None, content=None, **operation_config):
        
        if (TRACE_ENV_VAR in os.environ and os.environ[TRACE_ENV_VAR] == 'true')\
                or (TRACE_ENV_VAR_COMPAT in os.environ and os.environ[TRACE_ENV_VAR_COMPAT] == 'true'):
            print(request.method + ' ' + request.url)
        logger.debug('%s %s', request.method, request.url)
        logger.debug('Request content: %s', content)
        response = self._client.send(request=request, headers=headers,
                                     content=content, **operation_config)
        logger.debug('Response content: %s', response.content)
        if response.status_code < 200 or response.status_code >= 300:
            self._handle_error(request, response)
        return response",2,2
"def service_remove(path, service_name):
    
    compose_result, err = __load_docker_compose(path)
    if err:
        return err
    services = compose_result['compose_content']['services']
    if service_name not in services:
        return __standardize_result(False,
                                    'Service {0} did not exists'.format(service_name),
                                    None, None)
    del services[service_name]
    return __dump_compose_file(path, compose_result,
                               'Service {0} is removed from {1}'.format(service_name, path),
                               already_existed=True)",0,0
"def get_known_read_position(fp, buffered=True):
    
    buffer_size = io.DEFAULT_BUFFER_SIZE if buffered else 0
    return max(fp.tell() - buffer_size, 0)",1,1
"def send_email(self, message):
        
        msg = MIMEMultipart()
        msg['From'] = self.from_address
        msg['To'] = self.to_address
        msg['Subject'] = self.title
        msg.attach(MIMEText('<pre>' + cgi.escape(message) + '</pre>', 'html'))
        smtp = smtplib.SMTP(self.server, self.port,
                            timeout=self.timeout)
        if self.tls_auth:
            smtp.starttls()
            smtp.login(self.user, self.password)
        smtp.sendmail(self.from_address, self.to_address, msg.as_string())
        smtp.quit()",2,2
"def to_html(self, write_to):
        
        page_html = self.get_html()

        with open(write_to, ""wb"") as writefile:
            writefile.write(page_html.encode(""utf-8""))",0,0
"def bin_open(fname: str):
    
    if fname.endswith("".gz""):
        return gzip.open(fname, ""rb"")
    return open(fname, ""rb"")",1,1
"def _tls_mac_verify(alg, p, read_seq_num):
    
    h_size = alg.hash_len
    if p.len < h_size:
        return False
    received_h = p.data[-h_size:]
    p.len -= h_size
    p.data = p.data[:-h_size]

    read_seq_num = struct.pack(""!Q"", read_seq_num)
    h = alg.digest(read_seq_num + bytes(p))
    return h == received_h",3,1
"def msg(self, msg):
        
        if hasattr(self.output, 'writeline'):
            self.output.writeline(msg)
        elif hasattr(self.output, 'writelines'):
            self.output.writelines(msg + ""\n"")
            pass
        return",0,0
"def autocomplete(self):
        
        if self.completion_env_var_name not in os.environ:
            return
        cwords = os.environ['COMP_WORDS'].split()[1:]
        cword = int(os.environ['COMP_CWORD'])
        try:
            current = cwords[cword-1]
        except IndexError:
            current = ''
        cmd_names = self.get_commands().keys()

        if current:
            self.stdout.write(unicode(' '.join(
                [name for name in cmd_names if name.startswith(current)])))

        sys.exit(1)",0,0
"def messageReceived(self, message):
        
        i = message.index(b'')
        assert i > 0
        (routingInfo, msgId, payload) = (
            message[:i - 1], message[i - 1], message[i + 1:])
        msgParts = payload[0:]
        self._routingInfo[msgId] = routingInfo
        self.gotMessage(msgId, *msgParts)",3,3
"def write_value(self, value, offset=0):
        
        bytes = [dbus.Byte(b) for b in value]

        try:
            self._object.WriteValue(
                bytes,
                {'offset': dbus.UInt16(offset, variant_level=1)},
                reply_handler=self._write_value_succeeded,
                error_handler=self._write_value_failed,
                dbus_interface='org.bluez.GattCharacteristic1')
        except dbus.exceptions.DBusException as e:
            self._write_value_failed(self, error=e)",0,0
"def _send_message(session, user_id, message=None, image_files=None):
        
        assert any([message, image_files])

        attachment_items = None
        if image_files:
            attachment_items = Photo._upload_messages_photos_for_group(session, user_id, image_files)

        message_id = session.fetch(""messages.send"", user_id=user_id, message=message, attachment=attachment_items, random_id=random.randint(1, 10**6))
        return message_id",2,2
"def connection_made(self, address):
        
        logger.info(""connection made to {}"".format(address))
        self.count = 0
        self.connected = True
        self.transport.write(b'Echo Me')",2,2
"def create(self, req, **kwargs):
        

        response = requests.post(self.url, json=req, **self.req_args())
        return self.parse_response(response)",2,2
"def initArgosApplicationSettings(app): 
    
    assert app, \
        ""app undefined. Call QtWidgets.QApplication.instance() or QtCor.QApplication.instance() first.""

    logger.debug(""Setting Argos QApplication settings."")
    app.setApplicationName(info.REPO_NAME)
    app.setApplicationVersion(info.VERSION)
    app.setOrganizationName(info.ORGANIZATION_NAME)
    app.setOrganizationDomain(info.ORGANIZATION_DOMAIN)",1,2
"def open(filename, frame='unspecified'):
        
        data = BagOfPoints.load_data(filename)
        return NormalCloud(data, frame)",1,1
"def sendResponse(self, id, result, error):
        
        self.sendMessage({""result"":result, ""error"": error, ""id"":id})",2,2
"def decode():
    
    logger = logging.getLogger('geobuf')
    stdin = click.get_binary_stream('stdin')
    sink = click.get_text_stream('stdout')
    try:
        pbf = stdin.read()
        data = geobuf.decode(pbf)
        json.dump(data, sink)
        sys.exit(0)
    except Exception:
        logger.exception(""Failed. Exception caught"")
        sys.exit(1)",0,1
"def Relay(self, inventory):
        
        inventory = InvPayload(type=inventory.InventoryType, hashes=[inventory.Hash.ToBytes()])
        m = Message(""inv"", inventory)
        self.SendSerializedMessage(m)

        return True",0,2
"def position(self):
        
        dLbl = self._dLbl
        if dLbl is None:
            return None
        dLblPos = dLbl.dLblPos
        if dLblPos is None:
            return None
        return dLblPos.val",0,3
"def register_endpoint(self, endpoint, handler):
        
        return self.event_handler.register_handler((_EventType.Watch, endpoint), handler)",3,2
"def _extract_image_urls(arg: Message_T) -> List[str]:
    
    arg_as_msg = Message(arg)
    return [s.data['url'] for s in arg_as_msg
            if s.type == 'image' and 'url' in s.data]",3,3
"def select(self, table, cols, mode='list', key_filter=True):
        
        if cols is None:
            cols = [c.name for c in self.relations[table]]
        rows = self.read_table(table, key_filter=key_filter)
        for row in select_rows(cols, rows, mode=mode):
            yield row",1,1
"def handle(self, message):
        

        logger.debug(message)
        if Utilities.isNotEmpty(message['metadata']['opts']):
            target = message['metadata']['opts']['target']

            for split_line in Utilities.tokenize(message['text']):
                for truncated_line in Utilities.truncate(split_line):
                    self.botThread.connection.privmsg(target, truncated_line)
                    
                    time.sleep(0.25)",2,3
"def write_to_fil(self, filename_out, *args, **kwargs):
        

        
        t0 = time.time()

        
        self.__update_header()

        if self.container.isheavy():
            self.__write_to_fil_heavy(filename_out)
        else:
            self.__write_to_fil_light(filename_out)

        t1 = time.time()
        logger.info('Conversion time: %2.2fsec' % (t1- t0))",0,0
"def _wrap_callback_parse_link_event(subscription, on_data, message):
    
    if message.type == message.DATA:
        if message.data.type == yamcs_pb2.LINK_EVENT:
            link_message = getattr(message.data, 'linkEvent')
            link_event = LinkEvent(link_message)
            
            subscription._process(link_event)
            if on_data:
                on_data(link_event)",3,3
"def write_response(
        self, status_code: Union[
            int, constants.HttpStatusCode
        ]=constants.HttpStatusCode.BAD_REQUEST, *,
        headers: Optional[_HeaderType]=None
            ) -> ""writers.HttpResponseWriter"":
        
        return self._delegate.write_response(
            constants.HttpStatusCode(status_code),
            headers=headers)",2,0
"def file_chooser(prompt_text = ""Enter File: "", default=None, filearg=[], filekwarg={}):
    
    try:
        import readline, rlcomplete
        completer = rlcomplete.PathCompleter()
        readline.set_completer_delims(completer.delims)
        readline.parse_and_bind(""tab: complete"")
        readline.set_completer(completer.complete)
    except ImportError:
        pass
    while True:
        f = raw_input(prompt_text)
        if f == '': return default
        f = os.path.expanduser(f)
        if len(f) != 0 and f[0] == os.path.sep:
            f = os.path.abspath(f)
        try:
            return open(f, *filearg, **filekwarg)
        except IOError as e:
            stderr.write(ERROR_MESSAGE % (""unable to open %s : %s"" % (f, e)))",3,1
"def _read_json_file(self):
        
        with open(self.json_uri) as metadata_file:
            try:
                metadata = json.load(metadata_file)
                return metadata
            except ValueError:
                message = tr('the file %s does not appear to be valid JSON')
                message = message % self.json_uri
                raise MetadataReadError(message)",1,1
"def remove(self, recv):
        
        try:
            if recv.notify != self._put:
                raise ValueError
            self._receivers.remove(recv)
            recv.notify = None
        except (IndexError, ValueError):
            raise Error(self.not_present_msg)",3,3
"def _create_default_config(self):
        
        
        cfg = { 'site_title': '',
                'site_subtitle': '',
                'default_author': '',
                'site_url': '',
                'default_theme': 'blog1',
                'default_template': 'main.html.tpl',
                'fixed_frontpage': ''
              }


        file_name = os.path.join(self._dirs['s2'],'config.yml')
        f = open(file_name,'w')
        f.write(yaml.dump(cfg,default_flow_style=False))
        f.close()
        return cfg",0,0
"def _send_post(self, blogname, params):
        
        url = ""/v2/blog/{}/post"".format(blogname)
        valid_options = self._post_valid_options(params.get('type', None))

        if len(params.get(""tags"", [])) > 0:
            
            params['tags'] = "","".join(params['tags'])

        return self.send_api_request(""post"", url, params, valid_options)",2,2
"async def pong(self, data: bytes = b"""") -> None:
        
        await self.ensure_open()

        data = encode_data(data)

        await self.write_frame(True, OP_PONG, data)",2,2
"def convert_cluster_dict_keys_to_aliases(self, cluster_dict, alias_hash):
        
        output_dict = {}
        directory_to_index_dict = {os.path.split(item[""output_path""])[0] : key
                            for key, item in alias_hash.iteritems()}
        for key, item in cluster_dict.iteritems():
            cluster_file_directory = os.path.split(key)[0]
            cluster_idx = directory_to_index_dict[cluster_file_directory]
            output_dict[cluster_idx] = item
        return output_dict",1,3
"def untag(self, querystring, tags, afterwards=None):
        
        if self.ro:
            raise DatabaseROError()
        self.writequeue.append(('untag', afterwards, querystring, tags))",1,0
"def _strip_colors(self, message: str) -> str:
        
        for c in self.COLORS:
            message = message.replace(c, """")
        return message",3,3
"def write_int8(self, value, little_endian=True):
        
        if little_endian:
            endian = ""<""
        else:
            endian = "">""
        return self.pack('%sb' % endian, value)",0,0
"def from_pb(cls, policy_pb):
        
        policy = cls(policy_pb.etag, policy_pb.version)

        for binding in policy_pb.bindings:
            policy[binding.role] = sorted(binding.members)

        return policy",3,3
"def send_location(self, loc, to, reply=None):
        
        lat, lon = loc
        payload = dict(chat_id=to, reply_to_message_id=reply,
                       latitude=lat, longitude=lon)
        return Message.from_api(api, **self._get('sendLocation', payload))",2,2
"async def on_connect(self):
        
        if self.db:
            warnings.warn('SELECT DB is not allowed in cluster mode')
            self.db = ''
        await super(ClusterConnection, self).on_connect()
        if self.readonly:
            await self.send_command('READONLY')
            if nativestr(await self.read_response()) != 'OK':
                raise ConnectionError('READONLY command failed')",2,1
"def interrupt(self):
		
		if(self.device.read(9) & 0x01):
			self.handle_request()
		self.device.clear_IR()",0,1
"def save(self, *args, **kwargs):
        
        created = self.pk is None

        
        if self.check_user_settings(medium='web'):
            super(Notification, self).save(*args, **kwargs)

        if created:
            
            self.send_notifications()",2,2
"def process_npdu(self, npdu):
        
        if _debug: DeviceToDeviceServerService._debug(""process_npdu %r"", npdu)

        
        if npdu.pduDestination.addrType == Address.localBroadcastAddr:
            destList = self.connections.keys()
        else:
            if npdu.pduDestination not in self.connections:
                if _debug: DeviceToDeviceServerService._debug(""    - not a connected client"")
                return
            destList = [npdu.pduDestination]
        if _debug: DeviceToDeviceServerService._debug(""    - destList: %r"", destList)

        for dest in destList:
            
            xpdu = DeviceToDeviceAPDU(npdu)
            xpdu.pduDestination = dest

            
            self.service_request(xpdu)",2,2
"def _from(self, line):
        
        self.fromHeader = line
        bot.debug('FROM %s' %self.fromHeader)",3,1
"def reader(self, stream, context):
        
        progress = self.progress
        verbose = self.verbose
        while True:
            s = stream.readline()
            if not s:
                break
            if progress is not None:
                progress(s, context)
            else:
                if not verbose:
                    sys.stderr.write('.')
                else:
                    sys.stderr.write(s.decode('utf-8'))
                sys.stderr.flush()
        stream.close()",0,1
"def call (self, to, **options):
        
        self._steps.append(Call (to, **options).obj)",2,2
"def data_input_and_res_time_analysis(self):
        
        self.topol_data = Data()
        self.topol_data.load_data(self.topology,self.mol_file,self.ligand,self.offset)
        if len(self.trajectory) == 0:
            self.topol_data.analyse_topology(self.topology,self.cutoff)
        else:
            self.res_time = Residence_time(self.topol_data,self.trajectory, self.start, self.end, self.skip,self.topology, self.ligand,self.offset)
            self.res_time.measure_residence_time(self.cutoff)
            self.res_time.define_residues_for_plotting_traj(self.analysis_cutoff)
            self.topol_data.find_the_closest_atoms(self.topology)",1,1
"def _get_data(url):
    
    if urllib_parse.urlparse(url).scheme in ('http', 'https'):
        resp = urllib_request.urlopen(url)
        encoding = resp.headers.get('content-encoding', 'plain')
        data = resp.read()
        if encoding == 'plain':
            data = data.decode('utf-8')
        elif encoding == 'gzip':
            data = BytesIO(data)
            data = gzip.GzipFile(fileobj=data).read().decode('utf-8')
        else:
            raise RuntimeError('unknown encoding')
    else:
        with codecs.open(url, mode='r', encoding='utf-8') as fid:
            data = fid.read()

    return data",1,1
"def open(cls, sock, chunk_type, isatty, chunk_eof_type=None, buf_size=None, select_timeout=None):
    
    with cls.open_multi(sock,
                        (chunk_type,),
                        (isatty,),
                        chunk_eof_type,
                        buf_size,
                        select_timeout) as ctx:
      yield ctx",0,0
"def close(self):
        
        self.sync()
        self.sync = self.create = self.delete = self._closed
        self._write_to_file = self._read_to_file = self._closed
        self._key_to_filename = self._filename_to_key = self._closed
        self.__getitem__ = self.__setitem__ = self.__delitem__ = self._closed
        self.__iter__ = self.__len__ = self.__contains__ = self._closed",0,0
"def get_or_create_element(self, ns, name):
        
        if len(self._node.xpath('%s:%s' % (ns, name), namespaces=SLDNode._nsmap)) == 1:
            return getattr(self, name)

        return self.create_element(ns, name)",3,3
"def get(self, cfg):
        
        collection = cfg[AccessParams.KEY_COLLECTION]
        match_params = cfg[AccessParams.KEY_MATCH_PARAMS] if AccessParams.KEY_MATCH_PARAMS in cfg else None

        target_type = cfg[AccessParams.KEY_TYPE] if AccessParams.KEY_TYPE in cfg else AccessParams.TYPE_MULTI
        if target_type == AccessParams.TYPE_SINGLE:
            result = CRUD.read_single(self.__db, collection, match_params)
        elif target_type == AccessParams.TYPE_MULTI:
            result = CRUD.read_multi(self.__db, collection, match_params)
        return result",1,1
"def get_int(prompt=None):
    
    while True:
        s = get_string(prompt)
        if s is None:
            return None
        if re.search(r""^[+-]?\d+$"", s):
            try:
                i = int(s, 10)
                if type(i) is int:  
                    return i
            except ValueError:
                pass

        
        if prompt is None:
            print(""Retry: "", end="""")",1,3
"def change_email(
        self, user, new_email, base_confirm_url='', send_message=True):
        
        from boiler.user.models import UpdateSchema
        schema = UpdateSchema()
        user.email = new_email
        valid = schema.validate(user)
        if not valid:
            return valid

        db.session.add(user)
        db.session.commit()

        
        if send_message:
            self.send_email_changed_message(user, base_confirm_url)

        events.email_update_requested_event.send(user)
        return user",2,2
"def write_uint16(self, value, little_endian=True):
        
        if little_endian:
            endian = ""<""
        else:
            endian = "">""
        return self.pack('%sH' % endian, value)",0,0
"def raise_db_exception(self):
        
        if not self.messages:
            raise tds_base.Error(""Request failed, server didn't send error message"")
        msg = None
        while True:
            msg = self.messages[-1]
            if msg['msgno'] == 3621:  
                self.messages = self.messages[:-1]
            else:
                break

        error_msg = ' '.join(m['message'] for m in self.messages)
        ex = _create_exception_by_message(msg, error_msg)
        raise ex",3,3
"def read(self, size:int=None):
        
        if size:
            result = self._buffer[0:size]
            self._buffer = self._buffer[size:]
            return result
        else:
            result = self._buffer
            self._buffer = ''
            return result",1,1
"def add_genes(in_file, data, max_distance=10000, work_dir=None):
    
    gene_file = regions.get_sv_bed(data, ""exons"", out_dir=os.path.dirname(in_file))
    if gene_file and utils.file_exists(in_file):
        out_file = ""%s-annotated.bed"" % utils.splitext_plus(in_file)[0]
        if work_dir:
            out_file = os.path.join(work_dir, os.path.basename(out_file))
        if not utils.file_uptodate(out_file, in_file):
            fai_file = ref.fasta_idx(dd.get_ref_file(data))
            with file_transaction(data, out_file) as tx_out_file:
                _add_genes_to_bed(in_file, gene_file, fai_file, tx_out_file, data, max_distance)
        return out_file
    else:
        return in_file",1,1
"def say(self, message, **options):
        
        


        
	
	
        if hasattr (self, 'voice'):
            if (not 'voice' in options):
                options['voice'] = self.voice


        self._steps.append(Say(message, **options).obj)",3,2
"def load_system_host_keys(self, filename=None):
        
        if filename is None:
            
            filename = os.path.expanduser(""~/.ssh/known_hosts"")
            try:
                self._system_host_keys.load(filename)
            except IOError:
                pass
            return
        self._system_host_keys.load(filename)",1,1
"def _SetSocketTimeouts(self):
    
    
    
    timeout = int(self.timeout_seconds * 1000)
    receive_timeout = min(
        self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS, timeout)
    send_timeout = min(self._ZMQ_SOCKET_SEND_TIMEOUT_MILLISECONDS, timeout)

    self._zmq_socket.setsockopt(zmq.RCVTIMEO, receive_timeout)
    self._zmq_socket.setsockopt(zmq.SNDTIMEO, send_timeout)",2,2
"def read_moc_fits(moc, filename, include_meta=False, **kwargs):
    

    hl = fits.open(filename, mode='readonly', **kwargs)

    read_moc_fits_hdu(moc, hl[1], include_meta)",1,1
"def to_environ(self, environ, skip=()):
        
        environ[ensure_unicode(type(self).__name__)] = (
            ensure_unicode(self.to_base64(skip=skip))
        )",0,2
"def get_imports(fname):
    
    txt = ''
    with open(fname, 'r') as f:
        for line in f:
            if line[0:6] == 'import':
                txt += '<PRE>' + strip_text_after_string(line[7:], ' as ') + '</PRE>\n'
    return txt + '<BR>'",3,1
"def _estimate_delta(ntriangles, area):
    
    return np.sqrt(4./np.sqrt(3) * float(area) / float(ntriangles))",2,3
"def _update_proxy(self, change):
        
        if change['name'] in ['row', 'column']:
            super(AbstractWidgetItem, self)._update_proxy(change)
        else:
            self.proxy.data_changed(change)",2,2
"def get_file(self, file_path):
    

    try:
      resp = self._conn.get_object(
        Bucket=self._path.bucket,
        Key=self.get_path_to_file(file_path),
      )

      encoding = ''
      if 'ContentEncoding' in resp:
        encoding = resp['ContentEncoding']

      return resp['Body'].read(), encoding
    except botocore.exceptions.ClientError as err: 
      if err.response['Error']['Code'] == 'NoSuchKey':
        return None, None
      else:
        raise",3,1
"def newFromSites(self, sites, exclude=False):
        
        if exclude:
            sites = set(range(len(self))) - sites

        newSequence = []
        newStructure = []
        for index, (base, structure) in enumerate(zip(self.sequence,
                                                      self.structure)):
            if index in sites:
                newSequence.append(base)
                newStructure.append(structure)
        read = self.__class__(self.id, ''.join(newSequence),
                              ''.join(newStructure))

        return read",1,1
"def search(self, text):
        
        if text is None:
            self._search = None
        else:
            
            self.clear_filters()
            self.clear_order()
            self._search = '""{}""'.format(text)

        return self",3,3
"def on_epoch_end(self, last_metrics, **kwargs):
        ""Finish the computation and sends the result to the Recorder.""
        if not self.nums: return
        metrics = [self.metrics[name]/self.nums for name in self.names]
        return {'last_metrics': last_metrics+metrics}",2,2
"def read_from_ancillary_file(self, custom_xml=None):
        

        if custom_xml and os.path.isfile(self.xml_uri):
            self.read_xml()
        else:
            if not self.read_json():
                self.read_xml()",1,1
"def _get_aliases(parse_info):
        
        return [
            div.string.strip()
            for div in parse_info.find('div', id='editassociated')
            if div.string is not None
        ]",3,3
"def click(self, data):
        
        modules = data.get(""module"")
        for module_name in self.find_modules(modules):
            module = self.py3_wrapper.output_modules[module_name]
            if module[""type""] == ""py3status"":
                name = module[""module""].module_name
                instance = module[""module""].module_inst
            else:
                name = module[""module""].name
                instance = module[""module""].instance
            
            event = {""name"": name, ""instance"": instance}
            for name, message in CLICK_OPTIONS:
                event[name] = data.get(name)

            if self.debug:
                self.py3_wrapper.log(event)
            
            self.py3_wrapper.events_thread.dispatch_event(event)",2,2
"def get_fw_version():
    
    version = 'unknown'
    try:
        pkg = require(get_fw_name())[0]
    except DistributionNotFound:
        
        try:
            setup_path = os.path.abspath(os.path.dirname(__file__)+'/../..')
            with open(os.path.join(setup_path, 'setup.py')) as setup_file:
                lines = setup_file.readlines()
                for line in lines:
                    match = re.search(r""VERSION = \""([\S]{5,})\"""", line)
                    if match:
                        version = match.group(1)
                        break
        except Exception:  
            pass
    else:
        version = ""-rc"".join(pkg.version.split(""rc""))
    return version",1,3
"def focusInEvent(self, event):
        
        self.focus_changed.emit()
        return super(PageControlWidget, self).focusInEvent(event)",2,2
"def _ensure_values(data: Mapping[str, Any]) -> Tuple[Dict[str, Any], bool]:
    
    to_return = {}
    should_write = False
    for keyname, typekind, default in REQUIRED_DATA:
        if keyname not in data:
            LOG.debug(f""Defaulted config value {keyname} to {default}"")
            to_return[keyname] = default
            should_write = True
        elif not isinstance(data[keyname], typekind):
            LOG.warning(
                f""Config value {keyname} was {type(data[keyname])} not""
                f"" {typekind}, defaulted to {default}"")
            to_return[keyname] = default
            should_write = True
        else:
            to_return[keyname] = data[keyname]
    return to_return, should_write",0,0
"def get_subject_with_file_validation(jwt_bu64, cert_path):
    
    cert_obj = d1_common.cert.x509.deserialize_pem_file(cert_path)
    return get_subject_with_local_validation(jwt_bu64, cert_obj)",1,1
"def get_objgrpwr(self, goea_results):
        
        sortobj = self.get_sortobj(goea_results)
        return GrpWr(sortobj, self.pval_fld, ver_list=self.ver_list)",0,3
"def read_member(self):
        
        if self._member_lock is False:
            self._member_lock = True

        if self._new_member:
            try:
                
                BaseGzipFile._read(self, 1)
                assert self._new_member is False
            except EOFError:
                return None
        
        return self",1,1
"def planted(fk_candidate_observations, planted_objects, tolerance=10):
    

    found_pos = []
    detections = fk_candidate_observations.get_sources()
    for detection in detections:
        reading = detection.get_reading(0)
        
        found_pos.append([reading.x, reading.y])

    
    planted_objects_table = planted_objects.table
    planted_pos = numpy.transpose([planted_objects_table['x'].data, planted_objects_table['y'].data])

    
    
    (match_idx, match_fnd) = util.match_lists(numpy.array(planted_pos), numpy.array(found_pos), tolerance=tolerance)
    return match_fnd, match_idx",3,3
"def _actually_populate_keyvals(filename):
        
        assert isinstance(filename, str)
        keyvals = {}
        with open(filename, 'r') as f:
            for line in f:
                line = line.strip(""\r\n"")
                pieces = line.split(_KeyValuePersister._delimiter)
                if len(pieces) >= 2:
                    key = pieces[0]
                    val = _KeyValuePersister._delimiter.join(pieces[1:])
                    keyvals[key] = val
        return keyvals",1,1
"def find_file(path, tgt_env='base', **kwargs):  
    
    return _gitfs().find_file(path, tgt_env=tgt_env, **kwargs)",2,1
"def analog_callback(self, data):
        
        reply = json.dumps({""method"": ""analog_message_reply"", ""params"": [data[0], data[1]]})
        asyncio.ensure_future(self.websocket.send(reply))",3,2
"def expand_env_lazy(loader, node):
    
    val = loader.construct_scalar(node)
    return lazy_once(env_get, val)",1,3
"def write(self, data):
        
        self._check_not_closed()
        if not data:
            return 0
        enc_data = self.encryptor.update(data)
        self.next_fp.write(enc_data)
        self.offset += len(data)
        return len(data)",0,0
"def top_referrers(self, domain_only=True):
        
        referrer = self._referrer_clause(domain_only)
        return (self.get_query()
                .select(referrer, fn.Count(PageView.id))
                .group_by(referrer)
                .order_by(fn.Count(PageView.id).desc())
                .tuples())",2,3
"def _remove_multicast_group(self, datapath, outport, dst):
        
        ofproto = datapath.ofproto
        parser = datapath.ofproto_parser
        dpid = datapath.id

        self._send_event(
            EventMulticastGroupStateChanged(
                MG_GROUP_REMOVED, dst, outport, []))
        self._del_flow_entry(datapath, outport, dst)
        for port in self._to_hosts[dpid][dst]['ports']:
            self._del_flow_entry(datapath, port, dst)
        leave = self._to_hosts[dpid][dst]['leave']
        if leave:
            if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:
                in_port = leave.in_port
            else:
                in_port = leave.match['in_port']
            actions = [parser.OFPActionOutput(outport)]
            self._do_packet_out(
                datapath, leave.data, in_port, actions)",2,2
"def read_transport_message(self, origin, message_type, timeout=15):
        
        return self.event_handler.wait_for_event((_EventType.Transport, origin, message_type), timeout=timeout)",3,3
"def w(self):
        
        if not self._counters_calculated:
            self._counters_calculated = True
            self._extract_counters()

        return self._w",0,0
"def save(self, sender):
        
        um_to_user_list = self.cleaned_data['to']
        body = self.cleaned_data['body']

        msg = Message.objects.send_message(sender,
                                           um_to_user_list,
                                           body)

        return msg",2,2
"def post(self, request, *args, **kwargs):
        
        serializer = self.serializer_class(data=request.data)

        if not serializer.is_valid():
            return response.Response(
                serializer.errors,
                status=status.HTTP_400_BAD_REQUEST,
            )

        serializer.user.send_validation_email()
        msg = _('Email confirmation sent.')
        return response.Response(msg, status=status.HTTP_204_NO_CONTENT)",2,2
"def get_nodedata(self, sort_names=False):
        
        if not self.Node.n:
            return
        if not self.pflow.solved:
            logger.error('Power flow not solved when getting bus data.')
            return tuple([False] * 7)
        idx = self.Node.idx
        names = self.Node.name
        V = [self.dae.y[x] for x in self.Node.v]

        if sort_names:
            ret = (list(x)
                   for x in zip(*sorted(zip(idx, names, V), key=itemgetter(0))))
        else:
            ret = idx, names, V

        return ret",3,3
"def main():
    
    try:
        
        device = AlarmDecoder(SerialDevice(interface=SERIAL_DEVICE))

        
        device.on_alarm += handle_alarm
        with device.open(baudrate=BAUDRATE):
            while True:
                time.sleep(1)

    except Exception as ex:
        print('Exception:', ex)",2,2
"def get_or_set_hash(uri,
        length=8,
        chars='abcdefghijklmnopqrstuvwxyz0123456789!@
    
    return salt.utils.sdb.sdb_get_or_set_hash(uri, __opts__, length, chars, __utils__)",0,3
"def to_epw(self, buffer_or_path=None):
        
        
        df = self._weather_series.copy()
        df[""hour""] += 1
        epw_content = self._headers_to_epw() + df.to_csv(header=False, index=False, line_terminator=""\n"")
        return multi_mode_write(
            lambda buffer: buffer.write(epw_content),
            lambda: epw_content,
            buffer_or_path=buffer_or_path
        )",0,0
"def _default_plugins(self):
        
        plugins = {}
        try:
            with open('entry_points.json') as f:
                entry_points = json.load(f)
            for ep, obj in entry_points.items():
                plugins[ep] = []
                for name, src in obj.items():
                    plugins[ep].append(Plugin(name=name, source=src))
        except Exception as e:
            print(""Failed to load entry points {}"".format(e))
        return plugins",1,1
