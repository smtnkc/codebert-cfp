index,repo,path,url,code,code_tokens,docstring,docstring_tokens,language,cosmic_function,cosmic_function_number,includes_cosmic_function,includes_cosmic_function_number,partition
2336,h2oai/h2o-3,scripts/addjavamessage2ignore.py,https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/scripts/addjavamessage2ignore.py#L122-L137,"def remove_old_message():
    """"""
    Remove java messages from ignored list if users desired it.  It first reads in the java ignored messages
    from user stored in g_old_messages_to_remove and build a dict structure (old_message_dict) out of it.  Next, it removes the
    java messages contained in old_message_dict from g_ok_java_messages.
    :return: none
    """"""
    global g_old_messages_to_remove
    global g_dict_changed

    # extract old java ignored messages to be removed in old_message_dict
    old_message_dict = extract_message_to_dict(g_old_messages_to_remove)

    if old_message_dict:
        g_dict_changed = True
        update_message_dict(old_message_dict,2)","['def', 'remove_old_message', '(', ')', ':', 'global', 'g_old_messages_to_remove', 'global', 'g_dict_changed', '# extract old java ignored messages to be removed in old_message_dict', 'old_message_dict', '=', 'extract_message_to_dict', '(', 'g_old_messages_to_remove', ')', 'if', 'old_message_dict', ':', 'g_dict_changed', '=', 'True', 'update_message_dict', '(', 'old_message_dict', ',', '2', ')']","Remove java messages from ignored list if users desired it.  It first reads in the java ignored messages
    from user stored in g_old_messages_to_remove and build a dict structure (old_message_dict) out of it.  Next, it removes the
    java messages contained in old_message_dict from g_ok_java_messages.
    :return: none","['Remove', 'java', 'messages', 'from', 'ignored', 'list', 'if', 'users', 'desired', 'it', '.', 'It', 'first', 'reads', 'in', 'the', 'java', 'ignored', 'messages', 'from', 'user', 'stored', 'in', 'g_old_messages_to_remove', 'and', 'build', 'a', 'dict', 'structure', '(', 'old_message_dict', ')', 'out', 'of', 'it', '.', 'Next', 'it', 'removes', 'the', 'java', 'messages', 'contained', 'in', 'old_message_dict', 'from', 'g_ok_java_messages', '.', ':', 'return', ':', 'none']",python,R,1,True,1,test
2949,streamlink/streamlink,src/streamlink_cli/main.py,https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/main.py#L270-L298,"def open_stream(stream):
    """"""Opens a stream and reads 8192 bytes from it.

    This is useful to check if a stream actually has data
    before opening the output.

    """"""
    global stream_fd

    # Attempts to open the stream
    try:
        stream_fd = stream.open()
    except StreamError as err:
        raise StreamError(""Could not open stream: {0}"".format(err))

    # Read 8192 bytes before proceeding to check for errors.
    # This is to avoid opening the output unnecessarily.
    try:
        log.debug(""Pre-buffering 8192 bytes"")
        prebuffer = stream_fd.read(8192)
    except IOError as err:
        stream_fd.close()
        raise StreamError(""Failed to read data from stream: {0}"".format(err))

    if not prebuffer:
        stream_fd.close()
        raise StreamError(""No data returned from stream"")

    return stream_fd, prebuffer","['def', 'open_stream', '(', 'stream', ')', ':', 'global', 'stream_fd', '# Attempts to open the stream', 'try', ':', 'stream_fd', '=', 'stream', '.', 'open', '(', ')', 'except', 'StreamError', 'as', 'err', ':', 'raise', 'StreamError', '(', '""Could not open stream: {0}""', '.', 'format', '(', 'err', ')', ')', '# Read 8192 bytes before proceeding to check for errors.', '# This is to avoid opening the output unnecessarily.', 'try', ':', 'log', '.', 'debug', '(', '""Pre-buffering 8192 bytes""', ')', 'prebuffer', '=', 'stream_fd', '.', 'read', '(', '8192', ')', 'except', 'IOError', 'as', 'err', ':', 'stream_fd', '.', 'close', '(', ')', 'raise', 'StreamError', '(', '""Failed to read data from stream: {0}""', '.', 'format', '(', 'err', ')', ')', 'if', 'not', 'prebuffer', ':', 'stream_fd', '.', 'close', '(', ')', 'raise', 'StreamError', '(', '""No data returned from stream""', ')', 'return', 'stream_fd', ',', 'prebuffer']","Opens a stream and reads 8192 bytes from it.

    This is useful to check if a stream actually has data
    before opening the output.","['Opens', 'a', 'stream', 'and', 'reads', '8192', 'bytes', 'from', 'it', '.']",python,R,1,True,1,test
10163,singularityhub/sregistry-cli,sregistry/utils/fileio.py,https://github.com/singularityhub/sregistry-cli/blob/abc96140a1d15b5e96d83432e1e0e1f4f8f36331/sregistry/utils/fileio.py#L323-L329,"def read_json(filename, mode='r'):
    '''read_json reads in a json file and returns
       the data structure as dict.
    '''
    with open(filename, mode) as filey:
        data = json.load(filey)
    return data","['def', 'read_json', '(', 'filename', ',', 'mode', '=', ""'r'"", ')', ':', 'with', 'open', '(', 'filename', ',', 'mode', ')', 'as', 'filey', ':', 'data', '=', 'json', '.', 'load', '(', 'filey', ')', 'return', 'data']","read_json reads in a json file and returns
       the data structure as dict.","['read_json', 'reads', 'in', 'a', 'json', 'file', 'and', 'returns', 'the', 'data', 'structure', 'as', 'dict', '.']",python,R,1,True,1,test
10591,assemblerflow/flowcraft,flowcraft/templates/fastqc_report.py,https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/fastqc_report.py#L324-L359,"def get_sample_trim(p1_data, p2_data):
    """"""Get the optimal read trim range from data files of paired FastQ reads.

    Given the FastQC data report files for paired-end FastQ reads, this
    function will assess the optimal trim range for the 3' and 5' ends of
    the paired-end reads. This assessment will be based on the *'Per sequence
    GC content'*.

    Parameters
    ----------
    p1_data: str
        Path to FastQC data report file from pair 1
    p2_data: str
        Path to FastQC data report file from pair 2

    Returns
    -------
    optimal_5trim: int
        Optimal trim index for the 5' end of the reads
    optima_3trim: int
        Optimal trim index for the 3' end of the reads

    See Also
    --------
    trim_range

    """"""

    sample_ranges = [trim_range(x) for x in [p1_data, p2_data]]

    # Get the optimal trim position for 5' end
    optimal_5trim = max([x[0] for x in sample_ranges])
    # Get optimal trim position for 3' end
    optimal_3trim = min([x[1] for x in sample_ranges])

    return optimal_5trim, optimal_3trim","['def', 'get_sample_trim', '(', 'p1_data', ',', 'p2_data', ')', ':', 'sample_ranges', '=', '[', 'trim_range', '(', 'x', ')', 'for', 'x', 'in', '[', 'p1_data', ',', 'p2_data', ']', ']', ""# Get the optimal trim position for 5' end"", 'optimal_5trim', '=', 'max', '(', '[', 'x', '[', '0', ']', 'for', 'x', 'in', 'sample_ranges', ']', ')', ""# Get optimal trim position for 3' end"", 'optimal_3trim', '=', 'min', '(', '[', 'x', '[', '1', ']', 'for', 'x', 'in', 'sample_ranges', ']', ')', 'return', 'optimal_5trim', ',', 'optimal_3trim']","Get the optimal read trim range from data files of paired FastQ reads.

    Given the FastQC data report files for paired-end FastQ reads, this
    function will assess the optimal trim range for the 3' and 5' ends of
    the paired-end reads. This assessment will be based on the *'Per sequence
    GC content'*.

    Parameters
    ----------
    p1_data: str
        Path to FastQC data report file from pair 1
    p2_data: str
        Path to FastQC data report file from pair 2

    Returns
    -------
    optimal_5trim: int
        Optimal trim index for the 5' end of the reads
    optima_3trim: int
        Optimal trim index for the 3' end of the reads

    See Also
    --------
    trim_range","['Get', 'the', 'optimal', 'read', 'trim', 'range', 'from', 'data', 'files', 'of', 'paired', 'FastQ', 'reads', '.']",python,R,1,True,1,test
11799,UCBerkeleySETI/blimpy,blimpy/guppi.py,https://github.com/UCBerkeleySETI/blimpy/blob/b8822d3e3e911944370d84371a91fa0c29e9772e/blimpy/guppi.py#L194-L206,"def get_data(self):
        """"""
        returns a generator object that reads data a block at a time;
        the generator prints ""File depleted"" and returns nothing when all data in the file has been read.
        :return:
        """"""
        with self as gr:
            while True:
                try:
                    yield gr.read_next_data_block_int8()
                except Exception as e:
                    print(""File depleted"")
                    yield None, None, None","['def', 'get_data', '(', 'self', ')', ':', 'with', 'self', 'as', 'gr', ':', 'while', 'True', ':', 'try', ':', 'yield', 'gr', '.', 'read_next_data_block_int8', '(', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', '""File depleted""', ')', 'yield', 'None', ',', 'None', ',', 'None']","returns a generator object that reads data a block at a time;
        the generator prints ""File depleted"" and returns nothing when all data in the file has been read.
        :return:","['returns', 'a', 'generator', 'object', 'that', 'reads', 'data', 'a', 'block', 'at', 'a', 'time', ';', 'the', 'generator', 'prints', 'File', 'depleted', 'and', 'returns', 'nothing', 'when', 'all', 'data', 'in', 'the', 'file', 'has', 'been', 'read', '.', ':', 'return', ':']",python,R,1,True,1,test
16133,AkihikoITOH/capybara,capybara/virtualenv/lib/python2.7/site-packages/flask/json.py,https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/json.py#L152-L158,"def load(fp, **kwargs):
    """"""Like :func:`loads` but reads from a file object.
    """"""
    _load_arg_defaults(kwargs)
    if not PY2:
        fp = _wrap_reader_for_text(fp, kwargs.pop('encoding', None) or 'utf-8')
    return _json.load(fp, **kwargs)","['def', 'load', '(', 'fp', ',', '*', '*', 'kwargs', ')', ':', '_load_arg_defaults', '(', 'kwargs', ')', 'if', 'not', 'PY2', ':', 'fp', '=', '_wrap_reader_for_text', '(', 'fp', ',', 'kwargs', '.', 'pop', '(', ""'encoding'"", ',', 'None', ')', 'or', ""'utf-8'"", ')', 'return', '_json', '.', 'load', '(', 'fp', ',', '*', '*', 'kwargs', ')']",Like :func:`loads` but reads from a file object.,"['Like', ':', 'func', ':', 'loads', 'but', 'reads', 'from', 'a', 'file', 'object', '.']",python,R,1,True,1,test
16831,cmcginty/PyWeather,weather/stations/davis.py,https://github.com/cmcginty/PyWeather/blob/8c25d9cd1fa921e0a6e460d523656279cac045cb/weather/stations/davis.py#L420-L428,"def _loop_cmd(self):
        '''
        reads a raw string containing data read from the device
        provided (in /dev/XXX) format. all reads are non-blocking.
        '''
        self._cmd('LOOP', 1)
        raw = self.port.read(LoopStruct.size)  # read data
        log_raw('read', raw)
        return raw","['def', '_loop_cmd', '(', 'self', ')', ':', 'self', '.', '_cmd', '(', ""'LOOP'"", ',', '1', ')', 'raw', '=', 'self', '.', 'port', '.', 'read', '(', 'LoopStruct', '.', 'size', ')', '# read data', 'log_raw', '(', ""'read'"", ',', 'raw', ')', 'return', 'raw']","reads a raw string containing data read from the device
        provided (in /dev/XXX) format. all reads are non-blocking.","['reads', 'a', 'raw', 'string', 'containing', 'data', 'read', 'from', 'the', 'device', 'provided', '(', 'in', '/', 'dev', '/', 'XXX', ')', 'format', '.', 'all', 'reads', 'are', 'non', '-', 'blocking', '.']",python,R,1,True,1,test
18177,greenbender/pynntp,nntp/nntp.py,https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/nntp.py#L146-L162,"def __line_gen(self):
        """"""Generator that reads a line of data from the server.

        It first attempts to read from the internal buffer. If there is not
        enough data to read a line it then requests more data from the server
        and adds it to the buffer. This process repeats until a line of data
        can be read from the internal buffer.

        Yields:
            A line of data when it becomes available.
        """"""
        while True:
            line = self.__buffer.readline()
            if not line:
                self.__recv()
                continue
            yield line","['def', '__line_gen', '(', 'self', ')', ':', 'while', 'True', ':', 'line', '=', 'self', '.', '__buffer', '.', 'readline', '(', ')', 'if', 'not', 'line', ':', 'self', '.', '__recv', '(', ')', 'continue', 'yield', 'line']","Generator that reads a line of data from the server.

        It first attempts to read from the internal buffer. If there is not
        enough data to read a line it then requests more data from the server
        and adds it to the buffer. This process repeats until a line of data
        can be read from the internal buffer.

        Yields:
            A line of data when it becomes available.","['Generator', 'that', 'reads', 'a', 'line', 'of', 'data', 'from', 'the', 'server', '.']",python,R,1,True,1,test
18178,greenbender/pynntp,nntp/nntp.py,https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/nntp.py#L164-L189,"def __buf_gen(self, length=0):
        """"""Generator that reads a block of data from the server.

        It first attempts to read from the internal buffer. If there is not
        enough data in the internal buffer it then requests more data from the
        server and adds it to the buffer.

        Args:
            length: An optional amount of data to retrieve. A length of 0 (the
                default) will retrieve a least one buffer of data.

        Yields:
            A block of data when enough data becomes available.

        Note:
            If a length of 0 is supplied then the size of the yielded buffer can
            vary. If there is data in the internal buffer it will yield all of
            that data otherwise it will yield the the data returned by a recv
            on the socket.
        """"""
        while True:
            buf = self.__buffer.read(length)
            if not buf:
                self.__recv()
                continue
            yield buf","['def', '__buf_gen', '(', 'self', ',', 'length', '=', '0', ')', ':', 'while', 'True', ':', 'buf', '=', 'self', '.', '__buffer', '.', 'read', '(', 'length', ')', 'if', 'not', 'buf', ':', 'self', '.', '__recv', '(', ')', 'continue', 'yield', 'buf']","Generator that reads a block of data from the server.

        It first attempts to read from the internal buffer. If there is not
        enough data in the internal buffer it then requests more data from the
        server and adds it to the buffer.

        Args:
            length: An optional amount of data to retrieve. A length of 0 (the
                default) will retrieve a least one buffer of data.

        Yields:
            A block of data when enough data becomes available.

        Note:
            If a length of 0 is supplied then the size of the yielded buffer can
            vary. If there is data in the internal buffer it will yield all of
            that data otherwise it will yield the the data returned by a recv
            on the socket.","['Generator', 'that', 'reads', 'a', 'block', 'of', 'data', 'from', 'the', 'server', '.']",python,R,1,True,1,test
20474,cloud9ers/gurumate,environment/lib/python2.7/site-packages/IPython/external/pexpect/_pexpect.py,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/external/pexpect/_pexpect.py#L879-L907,"def read (self, size = -1):         # File-like object.
        """"""This reads at most ""size"" bytes from the file (less if the read hits
        EOF before obtaining size bytes). If the size argument is negative or
        omitted, read all data until EOF is reached. The bytes are returned as
        a string object. An empty string is returned when EOF is encountered
        immediately. """"""

        if size == 0:
            return self._empty_buffer
        if size < 0:
            self.expect (self.delimiter) # delimiter default is EOF
            return self.before

        # I could have done this more directly by not using expect(), but
        # I deliberately decided to couple read() to expect() so that
        # I would catch any bugs early and ensure consistant behavior.
        # It's a little less efficient, but there is less for me to
        # worry about if I have to later modify read() or expect().
        # Note, it's OK if size==-1 in the regex. That just means it
        # will never match anything in which case we stop only on EOF.
        if self._buffer_type is bytes:
            pat = (u'.{%d}' % size).encode('ascii')
        else:
            pat = u'.{%d}' % size
        cre = re.compile(pat, re.DOTALL)
        index = self.expect ([cre, self.delimiter]) # delimiter default is EOF
        if index == 0:
            return self.after ### self.before should be ''. Should I assert this?
        return self.before","['def', 'read', '(', 'self', ',', 'size', '=', '-', '1', ')', ':', '# File-like object.', 'if', 'size', '==', '0', ':', 'return', 'self', '.', '_empty_buffer', 'if', 'size', '<', '0', ':', 'self', '.', 'expect', '(', 'self', '.', 'delimiter', ')', '# delimiter default is EOF', 'return', 'self', '.', 'before', '# I could have done this more directly by not using expect(), but', '# I deliberately decided to couple read() to expect() so that', '# I would catch any bugs early and ensure consistant behavior.', ""# It's a little less efficient, but there is less for me to"", '# worry about if I have to later modify read() or expect().', ""# Note, it's OK if size==-1 in the regex. That just means it"", '# will never match anything in which case we stop only on EOF.', 'if', 'self', '.', '_buffer_type', 'is', 'bytes', ':', 'pat', '=', '(', ""u'.{%d}'"", '%', 'size', ')', '.', 'encode', '(', ""'ascii'"", ')', 'else', ':', 'pat', '=', ""u'.{%d}'"", '%', 'size', 'cre', '=', 're', '.', 'compile', '(', 'pat', ',', 're', '.', 'DOTALL', ')', 'index', '=', 'self', '.', 'expect', '(', '[', 'cre', ',', 'self', '.', 'delimiter', ']', ')', '# delimiter default is EOF', 'if', 'index', '==', '0', ':', 'return', 'self', '.', 'after', ""### self.before should be ''. Should I assert this?"", 'return', 'self', '.', 'before']","This reads at most ""size"" bytes from the file (less if the read hits
        EOF before obtaining size bytes). If the size argument is negative or
        omitted, read all data until EOF is reached. The bytes are returned as
        a string object. An empty string is returned when EOF is encountered
        immediately.","['This', 'reads', 'at', 'most', 'size', 'bytes', 'from', 'the', 'file', '(', 'less', 'if', 'the', 'read', 'hits', 'EOF', 'before', 'obtaining', 'size', 'bytes', ')', '.', 'If', 'the', 'size', 'argument', 'is', 'negative', 'or', 'omitted', 'read', 'all', 'data', 'until', 'EOF', 'is', 'reached', '.', 'The', 'bytes', 'are', 'returned', 'as', 'a', 'string', 'object', '.', 'An', 'empty', 'string', 'is', 'returned', 'when', 'EOF', 'is', 'encountered', 'immediately', '.']",python,R,1,True,1,test
692,apache/spark,python/pyspark/streaming/context.py,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/context.py#L255-L263,"def textFileStream(self, directory):
        """"""
        Create an input stream that monitors a Hadoop-compatible file system
        for new files and reads them as text files. Files must be wrriten to the
        monitored directory by ""moving"" them from another location within the same
        file system. File names starting with . are ignored.
        The text files must be encoded as UTF-8.
        """"""
        return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())","['def', 'textFileStream', '(', 'self', ',', 'directory', ')', ':', 'return', 'DStream', '(', 'self', '.', '_jssc', '.', 'textFileStream', '(', 'directory', ')', ',', 'self', ',', 'UTF8Deserializer', '(', ')', ')']","Create an input stream that monitors a Hadoop-compatible file system
        for new files and reads them as text files. Files must be wrriten to the
        monitored directory by ""moving"" them from another location within the same
        file system. File names starting with . are ignored.
        The text files must be encoded as UTF-8.","['Create', 'an', 'input', 'stream', 'that', 'monitors', 'a', 'Hadoop', '-', 'compatible', 'file', 'system', 'for', 'new', 'files', 'and', 'reads', 'them', 'as', 'text', 'files', '.', 'Files', 'must', 'be', 'wrriten', 'to', 'the', 'monitored', 'directory', 'by', 'moving', 'them', 'from', 'another', 'location', 'within', 'the', 'same', 'file', 'system', '.', 'File', 'names', 'starting', 'with', '.', 'are', 'ignored', '.', 'The', 'text', 'files', 'must', 'be', 'encoded', 'as', 'UTF', '-', '8', '.']",python,R,1,True,1,train
693,apache/spark,python/pyspark/streaming/context.py,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/context.py#L265-L277,"def binaryRecordsStream(self, directory, recordLength):
        """"""
        Create an input stream that monitors a Hadoop-compatible file system
        for new files and reads them as flat binary files with records of
        fixed length. Files must be written to the monitored directory by ""moving""
        them from another location within the same file system.
        File names starting with . are ignored.

        @param directory:       Directory to load data from
        @param recordLength:    Length of each record in bytes
        """"""
        return DStream(self._jssc.binaryRecordsStream(directory, recordLength), self,
                       NoOpSerializer())","['def', 'binaryRecordsStream', '(', 'self', ',', 'directory', ',', 'recordLength', ')', ':', 'return', 'DStream', '(', 'self', '.', '_jssc', '.', 'binaryRecordsStream', '(', 'directory', ',', 'recordLength', ')', ',', 'self', ',', 'NoOpSerializer', '(', ')', ')']","Create an input stream that monitors a Hadoop-compatible file system
        for new files and reads them as flat binary files with records of
        fixed length. Files must be written to the monitored directory by ""moving""
        them from another location within the same file system.
        File names starting with . are ignored.

        @param directory:       Directory to load data from
        @param recordLength:    Length of each record in bytes","['Create', 'an', 'input', 'stream', 'that', 'monitors', 'a', 'Hadoop', '-', 'compatible', 'file', 'system', 'for', 'new', 'files', 'and', 'reads', 'them', 'as', 'flat', 'binary', 'files', 'with', 'records', 'of', 'fixed', 'length', '.', 'Files', 'must', 'be', 'written', 'to', 'the', 'monitored', 'directory', 'by', 'moving', 'them', 'from', 'another', 'location', 'within', 'the', 'same', 'file', 'system', '.', 'File', 'names', 'starting', 'with', '.', 'are', 'ignored', '.']",python,R,1,True,1,train
9276,pypa/pipenv,pipenv/vendor/pexpect/spawnbase.py,https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/pexpect/spawnbase.py#L157-L180,"def read_nonblocking(self, size=1, timeout=None):
        """"""This reads data from the file descriptor.

        This is a simple implementation suitable for a regular file. Subclasses using ptys or pipes should override it.

        The timeout parameter is ignored.
        """"""

        try:
            s = os.read(self.child_fd, size)
        except OSError as err:
            if err.args[0] == errno.EIO:
                # Linux-style EOF
                self.flag_eof = True
                raise EOF('End Of File (EOF). Exception style platform.')
            raise
        if s == b'':
            # BSD-style EOF
            self.flag_eof = True
            raise EOF('End Of File (EOF). Empty string style platform.')

        s = self._decoder.decode(s, final=False)
        self._log(s, 'read')
        return s","['def', 'read_nonblocking', '(', 'self', ',', 'size', '=', '1', ',', 'timeout', '=', 'None', ')', ':', 'try', ':', 's', '=', 'os', '.', 'read', '(', 'self', '.', 'child_fd', ',', 'size', ')', 'except', 'OSError', 'as', 'err', ':', 'if', 'err', '.', 'args', '[', '0', ']', '==', 'errno', '.', 'EIO', ':', '# Linux-style EOF', 'self', '.', 'flag_eof', '=', 'True', 'raise', 'EOF', '(', ""'End Of File (EOF). Exception style platform.'"", ')', 'raise', 'if', 's', '==', ""b''"", ':', '# BSD-style EOF', 'self', '.', 'flag_eof', '=', 'True', 'raise', 'EOF', '(', ""'End Of File (EOF). Empty string style platform.'"", ')', 's', '=', 'self', '.', '_decoder', '.', 'decode', '(', 's', ',', 'final', '=', 'False', ')', 'self', '.', '_log', '(', 's', ',', ""'read'"", ')', 'return', 's']","This reads data from the file descriptor.

        This is a simple implementation suitable for a regular file. Subclasses using ptys or pipes should override it.

        The timeout parameter is ignored.","['This', 'reads', 'data', 'from', 'the', 'file', 'descriptor', '.']",python,R,1,True,1,train
9282,pypa/pipenv,pipenv/vendor/pexpect/spawnbase.py,https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/pexpect/spawnbase.py#L430-L457,"def read(self, size=-1):
        '''This reads at most ""size"" bytes from the file (less if the read hits
        EOF before obtaining size bytes). If the size argument is negative or
        omitted, read all data until EOF is reached. The bytes are returned as
        a string object. An empty string is returned when EOF is encountered
        immediately. '''

        if size == 0:
            return self.string_type()
        if size < 0:
            # delimiter default is EOF
            self.expect(self.delimiter)
            return self.before

        # I could have done this more directly by not using expect(), but
        # I deliberately decided to couple read() to expect() so that
        # I would catch any bugs early and ensure consistent behavior.
        # It's a little less efficient, but there is less for me to
        # worry about if I have to later modify read() or expect().
        # Note, it's OK if size==-1 in the regex. That just means it
        # will never match anything in which case we stop only on EOF.
        cre = re.compile(self._coerce_expect_string('.{%d}' % size), re.DOTALL)
        # delimiter default is EOF
        index = self.expect([cre, self.delimiter])
        if index == 0:
            ### FIXME self.before should be ''. Should I assert this?
            return self.after
        return self.before","['def', 'read', '(', 'self', ',', 'size', '=', '-', '1', ')', ':', 'if', 'size', '==', '0', ':', 'return', 'self', '.', 'string_type', '(', ')', 'if', 'size', '<', '0', ':', '# delimiter default is EOF', 'self', '.', 'expect', '(', 'self', '.', 'delimiter', ')', 'return', 'self', '.', 'before', '# I could have done this more directly by not using expect(), but', '# I deliberately decided to couple read() to expect() so that', '# I would catch any bugs early and ensure consistent behavior.', ""# It's a little less efficient, but there is less for me to"", '# worry about if I have to later modify read() or expect().', ""# Note, it's OK if size==-1 in the regex. That just means it"", '# will never match anything in which case we stop only on EOF.', 'cre', '=', 're', '.', 'compile', '(', 'self', '.', '_coerce_expect_string', '(', ""'.{%d}'"", '%', 'size', ')', ',', 're', '.', 'DOTALL', ')', '# delimiter default is EOF', 'index', '=', 'self', '.', 'expect', '(', '[', 'cre', ',', 'self', '.', 'delimiter', ']', ')', 'if', 'index', '==', '0', ':', ""### FIXME self.before should be ''. Should I assert this?"", 'return', 'self', '.', 'after', 'return', 'self', '.', 'before']","This reads at most ""size"" bytes from the file (less if the read hits
        EOF before obtaining size bytes). If the size argument is negative or
        omitted, read all data until EOF is reached. The bytes are returned as
        a string object. An empty string is returned when EOF is encountered
        immediately.","['This', 'reads', 'at', 'most', 'size', 'bytes', 'from', 'the', 'file', '(', 'less', 'if', 'the', 'read', 'hits', 'EOF', 'before', 'obtaining', 'size', 'bytes', ')', '.', 'If', 'the', 'size', 'argument', 'is', 'negative', 'or', 'omitted', 'read', 'all', 'data', 'until', 'EOF', 'is', 'reached', '.', 'The', 'bytes', 'are', 'returned', 'as', 'a', 'string', 'object', '.', 'An', 'empty', 'string', 'is', 'returned', 'when', 'EOF', 'is', 'encountered', 'immediately', '.']",python,R,1,True,1,train
12406,tornadoweb/tornado,tornado/iostream.py,https://github.com/tornadoweb/tornado/blob/b8b481770bcdb333a69afde5cce7eaa449128326/tornado/iostream.py#L496-L524,"def read_until_close(self) -> Awaitable[bytes]:
        """"""Asynchronously reads all data from the socket until it is closed.

        This will buffer all available data until ``max_buffer_size``
        is reached. If flow control or cancellation are desired, use a
        loop with `read_bytes(partial=True) <.read_bytes>` instead.

        .. versionchanged:: 4.0
            The callback argument is now optional and a `.Future` will
            be returned if it is omitted.

        .. versionchanged:: 6.0

           The ``callback`` and ``streaming_callback`` arguments have
           been removed. Use the returned `.Future` (and `read_bytes`
           with ``partial=True`` for ``streaming_callback``) instead.

        """"""
        future = self._start_read()
        if self.closed():
            self._finish_read(self._read_buffer_size, False)
            return future
        self._read_until_close = True
        try:
            self._try_inline_read()
        except:
            future.add_done_callback(lambda f: f.exception())
            raise
        return future","['def', 'read_until_close', '(', 'self', ')', '->', 'Awaitable', '[', 'bytes', ']', ':', 'future', '=', 'self', '.', '_start_read', '(', ')', 'if', 'self', '.', 'closed', '(', ')', ':', 'self', '.', '_finish_read', '(', 'self', '.', '_read_buffer_size', ',', 'False', ')', 'return', 'future', 'self', '.', '_read_until_close', '=', 'True', 'try', ':', 'self', '.', '_try_inline_read', '(', ')', 'except', ':', 'future', '.', 'add_done_callback', '(', 'lambda', 'f', ':', 'f', '.', 'exception', '(', ')', ')', 'raise', 'return', 'future']","Asynchronously reads all data from the socket until it is closed.

        This will buffer all available data until ``max_buffer_size``
        is reached. If flow control or cancellation are desired, use a
        loop with `read_bytes(partial=True) <.read_bytes>` instead.

        .. versionchanged:: 4.0
            The callback argument is now optional and a `.Future` will
            be returned if it is omitted.

        .. versionchanged:: 6.0

           The ``callback`` and ``streaming_callback`` arguments have
           been removed. Use the returned `.Future` (and `read_bytes`
           with ``partial=True`` for ``streaming_callback``) instead.","['Asynchronously', 'reads', 'all', 'data', 'from', 'the', 'socket', 'until', 'it', 'is', 'closed', '.']",python,R,1,True,1,train
12604,aio-libs/aiohttp,aiohttp/multipart.py,https://github.com/aio-libs/aiohttp/blob/9504fe2affaaff673fa4f3754c1c44221f8ba47d/aiohttp/multipart.py#L392-L397,"async def release(self) -> None:
        """"""Like read(), but reads all the data to the void.""""""
        if self._at_eof:
            return
        while not self._at_eof:
            await self.read_chunk(self.chunk_size)","['async', 'def', 'release', '(', 'self', ')', '->', 'None', ':', 'if', 'self', '.', '_at_eof', ':', 'return', 'while', 'not', 'self', '.', '_at_eof', ':', 'await', 'self', '.', 'read_chunk', '(', 'self', '.', 'chunk_size', ')']","Like read(), but reads all the data to the void.","['Like', 'read', '()', 'but', 'reads', 'all', 'the', 'data', 'to', 'the', 'void', '.']",python,R,1,True,1,train
17661,awslabs/aws-sam-cli,samcli/local/docker/attach_api.py,https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/local/docker/attach_api.py#L119-L155,"def _read_payload(socket, payload_size):
    """"""
    From the given socket, reads and yields payload of the given size. With sockets, we don't receive all data at
    once. Therefore this method will yield each time we read some data from the socket until the payload_size has
    reached or socket has no more data.

    Parameters
    ----------
    socket
        Socket to read from

    payload_size : int
        Size of the payload to read. Exactly these many bytes are read from the socket before stopping the yield.

    Yields
    -------
    int
        Type of the stream (1 => stdout, 2 => stderr)
    str
        Data in the stream
    """"""

    remaining = payload_size
    while remaining > 0:

        # Try and read as much as possible
        data = read(socket, remaining)
        if data is None:
            # ``read`` will terminate with an empty string. This is just a transient state where we didn't get any data
            continue

        if len(data) == 0:  # pylint: disable=C1801
            # Empty string. Socket does not have any more data. We are done here even if we haven't read full payload
            break

        remaining -= len(data)
        yield data","['def', '_read_payload', '(', 'socket', ',', 'payload_size', ')', ':', 'remaining', '=', 'payload_size', 'while', 'remaining', '>', '0', ':', '# Try and read as much as possible', 'data', '=', 'read', '(', 'socket', ',', 'remaining', ')', 'if', 'data', 'is', 'None', ':', ""# ``read`` will terminate with an empty string. This is just a transient state where we didn't get any data"", 'continue', 'if', 'len', '(', 'data', ')', '==', '0', ':', '# pylint: disable=C1801', ""# Empty string. Socket does not have any more data. We are done here even if we haven't read full payload"", 'break', 'remaining', '-=', 'len', '(', 'data', ')', 'yield', 'data']","From the given socket, reads and yields payload of the given size. With sockets, we don't receive all data at
    once. Therefore this method will yield each time we read some data from the socket until the payload_size has
    reached or socket has no more data.

    Parameters
    ----------
    socket
        Socket to read from

    payload_size : int
        Size of the payload to read. Exactly these many bytes are read from the socket before stopping the yield.

    Yields
    -------
    int
        Type of the stream (1 => stdout, 2 => stderr)
    str
        Data in the stream","['From', 'the', 'given', 'socket', 'reads', 'and', 'yields', 'payload', 'of', 'the', 'given', 'size', '.', 'With', 'sockets', 'we', 'don', 't', 'receive', 'all', 'data', 'at', 'once', '.', 'Therefore', 'this', 'method', 'will', 'yield', 'each', 'time', 'we', 'read', 'some', 'data', 'from', 'the', 'socket', 'until', 'the', 'payload_size', 'has', 'reached', 'or', 'socket', 'has', 'no', 'more', 'data', '.']",python,R,1,True,1,train
17693,awslabs/aws-sam-cli,samcli/commands/local/lib/generated_sample_events/events.py,https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/commands/local/lib/generated_sample_events/events.py#L36-L62,"def encode(self, tags, encoding, values_to_sub):
        """"""
        reads the encoding type from the event-mapping.json
        and determines whether a value needs encoding

        Parameters
        ----------
        tags: dict
            the values of a particular event that can be substituted
            within the event json
        encoding: string
            string that helps navigate to the encoding field of the json
        values_to_sub: dict
            key/value pairs that will be substituted into the json
        Returns
        -------
        values_to_sub: dict
            the encoded (if need be) values to substitute into the json.
        """"""

        for tag in tags:
            if tags[tag].get(encoding) != ""None"":
                if tags[tag].get(encoding) == ""url"":
                    values_to_sub[tag] = self.url_encode(values_to_sub[tag])
                if tags[tag].get(encoding) == ""base64"":
                    values_to_sub[tag] = self.base64_utf_encode(values_to_sub[tag])
        return values_to_sub","['def', 'encode', '(', 'self', ',', 'tags', ',', 'encoding', ',', 'values_to_sub', ')', ':', 'for', 'tag', 'in', 'tags', ':', 'if', 'tags', '[', 'tag', ']', '.', 'get', '(', 'encoding', ')', '!=', '""None""', ':', 'if', 'tags', '[', 'tag', ']', '.', 'get', '(', 'encoding', ')', '==', '""url""', ':', 'values_to_sub', '[', 'tag', ']', '=', 'self', '.', 'url_encode', '(', 'values_to_sub', '[', 'tag', ']', ')', 'if', 'tags', '[', 'tag', ']', '.', 'get', '(', 'encoding', ')', '==', '""base64""', ':', 'values_to_sub', '[', 'tag', ']', '=', 'self', '.', 'base64_utf_encode', '(', 'values_to_sub', '[', 'tag', ']', ')', 'return', 'values_to_sub']","reads the encoding type from the event-mapping.json
        and determines whether a value needs encoding

        Parameters
        ----------
        tags: dict
            the values of a particular event that can be substituted
            within the event json
        encoding: string
            string that helps navigate to the encoding field of the json
        values_to_sub: dict
            key/value pairs that will be substituted into the json
        Returns
        -------
        values_to_sub: dict
            the encoded (if need be) values to substitute into the json.","['reads', 'the', 'encoding', 'type', 'from', 'the', 'event', '-', 'mapping', '.', 'json', 'and', 'determines', 'whether', 'a', 'value', 'needs', 'encoding']",python,R,1,True,1,train
22986,saltstack/salt,salt/utils/gzip_util.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L74-L106,"def compress_file(fh_, compresslevel=9, chunk_size=1048576):
    '''
    Generator that reads chunk_size bytes at a time from a file/filehandle and
    yields the compressed result of each read.

    .. note::
        Each chunk is compressed separately. They cannot be stitched together
        to form a compressed file. This function is designed to break up a file
        into compressed chunks for transport and decompression/reassembly on a
        remote host.
    '''
    try:
        bytes_read = int(chunk_size)
        if bytes_read != chunk_size:
            raise ValueError
    except ValueError:
        raise ValueError('chunk_size must be an integer')
    try:
        while bytes_read == chunk_size:
            buf = BytesIO()
            with open_fileobj(buf, 'wb', compresslevel) as ogz:
                try:
                    bytes_read = ogz.write(fh_.read(chunk_size))
                except AttributeError:
                    # Open the file and re-attempt the read
                    fh_ = salt.utils.files.fopen(fh_, 'rb')
                    bytes_read = ogz.write(fh_.read(chunk_size))
            yield buf.getvalue()
    finally:
        try:
            fh_.close()
        except AttributeError:
            pass","['def', 'compress_file', '(', 'fh_', ',', 'compresslevel', '=', '9', ',', 'chunk_size', '=', '1048576', ')', ':', 'try', ':', 'bytes_read', '=', 'int', '(', 'chunk_size', ')', 'if', 'bytes_read', '!=', 'chunk_size', ':', 'raise', 'ValueError', 'except', 'ValueError', ':', 'raise', 'ValueError', '(', ""'chunk_size must be an integer'"", ')', 'try', ':', 'while', 'bytes_read', '==', 'chunk_size', ':', 'buf', '=', 'BytesIO', '(', ')', 'with', 'open_fileobj', '(', 'buf', ',', ""'wb'"", ',', 'compresslevel', ')', 'as', 'ogz', ':', 'try', ':', 'bytes_read', '=', 'ogz', '.', 'write', '(', 'fh_', '.', 'read', '(', 'chunk_size', ')', ')', 'except', 'AttributeError', ':', '# Open the file and re-attempt the read', 'fh_', '=', 'salt', '.', 'utils', '.', 'files', '.', 'fopen', '(', 'fh_', ',', ""'rb'"", ')', 'bytes_read', '=', 'ogz', '.', 'write', '(', 'fh_', '.', 'read', '(', 'chunk_size', ')', ')', 'yield', 'buf', '.', 'getvalue', '(', ')', 'finally', ':', 'try', ':', 'fh_', '.', 'close', '(', ')', 'except', 'AttributeError', ':', 'pass']","Generator that reads chunk_size bytes at a time from a file/filehandle and
    yields the compressed result of each read.

    .. note::
        Each chunk is compressed separately. They cannot be stitched together
        to form a compressed file. This function is designed to break up a file
        into compressed chunks for transport and decompression/reassembly on a
        remote host.","['Generator', 'that', 'reads', 'chunk_size', 'bytes', 'at', 'a', 'time', 'from', 'a', 'file', '/', 'filehandle', 'and', 'yields', 'the', 'compressed', 'result', 'of', 'each', 'read', '.']",python,R,1,True,1,train
1941,saltstack/salt,salt/utils/itertools.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/itertools.py#L41-L66,"def read_file(fh_, chunk_size=1048576):
    '''
    Generator that reads chunk_size bytes at a time from a file/filehandle and
    yields it.
    '''
    try:
        if chunk_size != int(chunk_size):
            raise ValueError
    except ValueError:
        raise ValueError('chunk_size must be an integer')
    try:
        while True:
            try:
                chunk = fh_.read(chunk_size)
            except AttributeError:
                # Open the file and re-attempt the read
                fh_ = salt.utils.files.fopen(fh_, 'rb')  # pylint: disable=W8470
                chunk = fh_.read(chunk_size)
            if not chunk:
                break
            yield chunk
    finally:
        try:
            fh_.close()
        except AttributeError:
            pass","['def', 'read_file', '(', 'fh_', ',', 'chunk_size', '=', '1048576', ')', ':', 'try', ':', 'if', 'chunk_size', '!=', 'int', '(', 'chunk_size', ')', ':', 'raise', 'ValueError', 'except', 'ValueError', ':', 'raise', 'ValueError', '(', ""'chunk_size must be an integer'"", ')', 'try', ':', 'while', 'True', ':', 'try', ':', 'chunk', '=', 'fh_', '.', 'read', '(', 'chunk_size', ')', 'except', 'AttributeError', ':', '# Open the file and re-attempt the read', 'fh_', '=', 'salt', '.', 'utils', '.', 'files', '.', 'fopen', '(', 'fh_', ',', ""'rb'"", ')', '# pylint: disable=W8470', 'chunk', '=', 'fh_', '.', 'read', '(', 'chunk_size', ')', 'if', 'not', 'chunk', ':', 'break', 'yield', 'chunk', 'finally', ':', 'try', ':', 'fh_', '.', 'close', '(', ')', 'except', 'AttributeError', ':', 'pass']","Generator that reads chunk_size bytes at a time from a file/filehandle and
    yields it.","['Generator', 'that', 'reads', 'chunk_size', 'bytes', 'at', 'a', 'time', 'from', 'a', 'file', '/', 'filehandle', 'and', 'yields', 'it', '.']",python,R,1,True,1,train
7236,deepmind/sonnet,sonnet/examples/brnn_ptb.py,https://github.com/deepmind/sonnet/blob/00612ca3178964d86b556e062694d808ff81fcca/sonnet/examples/brnn_ptb.py#L89-L103,"def _get_raw_data(subset):
  """"""Loads the data or reads it from cache.""""""
  raw_data = _LOADED.get(subset)
  if raw_data is not None:
    return raw_data, _LOADED[""vocab""]
  else:
    train_data, valid_data, test_data, vocab = ptb_reader.ptb_raw_data(
        FLAGS.data_path)
    _LOADED.update({
        ""train"": np.array(train_data),
        ""valid"": np.array(valid_data),
        ""test"": np.array(test_data),
        ""vocab"": vocab
    })
    return _LOADED[subset], vocab","['def', '_get_raw_data', '(', 'subset', ')', ':', 'raw_data', '=', '_LOADED', '.', 'get', '(', 'subset', ')', 'if', 'raw_data', 'is', 'not', 'None', ':', 'return', 'raw_data', ',', '_LOADED', '[', '""vocab""', ']', 'else', ':', 'train_data', ',', 'valid_data', ',', 'test_data', ',', 'vocab', '=', 'ptb_reader', '.', 'ptb_raw_data', '(', 'FLAGS', '.', 'data_path', ')', '_LOADED', '.', 'update', '(', '{', '""train""', ':', 'np', '.', 'array', '(', 'train_data', ')', ',', '""valid""', ':', 'np', '.', 'array', '(', 'valid_data', ')', ',', '""test""', ':', 'np', '.', 'array', '(', 'test_data', ')', ',', '""vocab""', ':', 'vocab', '}', ')', 'return', '_LOADED', '[', 'subset', ']', ',', 'vocab']",Loads the data or reads it from cache.,"['Loads', 'the', 'data', 'or', 'reads', 'it', 'from', 'cache', '.']",python,R,1,True,1,train
16669,google/grr,grr/server/grr_response_server/aff4_objects/standard.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/aff4_objects/standard.py#L154-L175,"def _GetChunkForReading(self, chunk):
    """"""Returns the relevant chunk from the datastore and reads ahead.""""""
    try:
      return self.chunk_cache.Get(chunk)
    except KeyError:
      pass

    # We don't have this chunk already cached. The most common read
    # access pattern is contiguous reading so since we have to go to
    # the data store already, we read ahead to reduce round trips.

    missing_chunks = []
    for chunk_number in range(chunk, chunk + 10):
      if chunk_number not in self.chunk_cache:
        missing_chunks.append(chunk_number)

    self._ReadChunks(missing_chunks)
    # This should work now - otherwise we just give up.
    try:
      return self.chunk_cache.Get(chunk)
    except KeyError:
      raise aff4.ChunkNotFoundError(""Cannot open chunk %s"" % chunk)","['def', '_GetChunkForReading', '(', 'self', ',', 'chunk', ')', ':', 'try', ':', 'return', 'self', '.', 'chunk_cache', '.', 'Get', '(', 'chunk', ')', 'except', 'KeyError', ':', 'pass', ""# We don't have this chunk already cached. The most common read"", '# access pattern is contiguous reading so since we have to go to', '# the data store already, we read ahead to reduce round trips.', 'missing_chunks', '=', '[', ']', 'for', 'chunk_number', 'in', 'range', '(', 'chunk', ',', 'chunk', '+', '10', ')', ':', 'if', 'chunk_number', 'not', 'in', 'self', '.', 'chunk_cache', ':', 'missing_chunks', '.', 'append', '(', 'chunk_number', ')', 'self', '.', '_ReadChunks', '(', 'missing_chunks', ')', '# This should work now - otherwise we just give up.', 'try', ':', 'return', 'self', '.', 'chunk_cache', '.', 'Get', '(', 'chunk', ')', 'except', 'KeyError', ':', 'raise', 'aff4', '.', 'ChunkNotFoundError', '(', '""Cannot open chunk %s""', '%', 'chunk', ')']",Returns the relevant chunk from the datastore and reads ahead.,"['Returns', 'the', 'relevant', 'chunk', 'from', 'the', 'datastore', 'and', 'reads', 'ahead', '.']",python,R,1,True,1,train
24340,amoffat/sh,sh.py,https://github.com/amoffat/sh/blob/858adf0c682af4c40e41f34d6926696b7a5d3b12/sh.py#L2462-L2510,"def output_thread(log, stdout, stderr, timeout_event, is_alive, quit,
        stop_output_event):
    """""" this function is run in a separate thread.  it reads from the
    process's stdout stream (a streamreader), and waits for it to claim that
    its done """"""

    poller = Poller()
    if stdout is not None:
        poller.register_read(stdout)
    if stderr is not None:
        poller.register_read(stderr)

    # this is our poll loop for polling stdout or stderr that is ready to
    # be read and processed.  if one of those streamreaders indicate that it
    # is done altogether being read from, we remove it from our list of
    # things to poll.  when no more things are left to poll, we leave this
    # loop and clean up
    while poller:
        changed = no_interrupt(poller.poll, 0.1)
        for f, events in changed:
            if events & (POLLER_EVENT_READ | POLLER_EVENT_HUP):
                log.debug(""%r ready to be read from"", f)
                done = f.read()
                if done:
                    poller.unregister(f)
            elif events & POLLER_EVENT_ERROR:
                # for some reason, we have to just ignore streams that have had an
                # error.  i'm not exactly sure why, but don't remove this until we
                # figure that out, and create a test for it
                pass

        if timeout_event and timeout_event.is_set():
            break

        if stop_output_event.is_set():
            break

    # we need to wait until the process is guaranteed dead before closing our
    # outputs, otherwise SIGPIPE
    alive, _ = is_alive()
    while alive:
        quit.wait(1)
        alive, _ = is_alive()

    if stdout:
        stdout.close()

    if stderr:
        stderr.close()","['def', 'output_thread', '(', 'log', ',', 'stdout', ',', 'stderr', ',', 'timeout_event', ',', 'is_alive', ',', 'quit', ',', 'stop_output_event', ')', ':', 'poller', '=', 'Poller', '(', ')', 'if', 'stdout', 'is', 'not', 'None', ':', 'poller', '.', 'register_read', '(', 'stdout', ')', 'if', 'stderr', 'is', 'not', 'None', ':', 'poller', '.', 'register_read', '(', 'stderr', ')', '# this is our poll loop for polling stdout or stderr that is ready to', '# be read and processed.  if one of those streamreaders indicate that it', '# is done altogether being read from, we remove it from our list of', '# things to poll.  when no more things are left to poll, we leave this', '# loop and clean up', 'while', 'poller', ':', 'changed', '=', 'no_interrupt', '(', 'poller', '.', 'poll', ',', '0.1', ')', 'for', 'f', ',', 'events', 'in', 'changed', ':', 'if', 'events', '&', '(', 'POLLER_EVENT_READ', '|', 'POLLER_EVENT_HUP', ')', ':', 'log', '.', 'debug', '(', '""%r ready to be read from""', ',', 'f', ')', 'done', '=', 'f', '.', 'read', '(', ')', 'if', 'done', ':', 'poller', '.', 'unregister', '(', 'f', ')', 'elif', 'events', '&', 'POLLER_EVENT_ERROR', ':', '# for some reason, we have to just ignore streams that have had an', ""# error.  i'm not exactly sure why, but don't remove this until we"", '# figure that out, and create a test for it', 'pass', 'if', 'timeout_event', 'and', 'timeout_event', '.', 'is_set', '(', ')', ':', 'break', 'if', 'stop_output_event', '.', 'is_set', '(', ')', ':', 'break', '# we need to wait until the process is guaranteed dead before closing our', '# outputs, otherwise SIGPIPE', 'alive', ',', '_', '=', 'is_alive', '(', ')', 'while', 'alive', ':', 'quit', '.', 'wait', '(', '1', ')', 'alive', ',', '_', '=', 'is_alive', '(', ')', 'if', 'stdout', ':', 'stdout', '.', 'close', '(', ')', 'if', 'stderr', ':', 'stderr', '.', 'close', '(', ')']","this function is run in a separate thread.  it reads from the
    process's stdout stream (a streamreader), and waits for it to claim that
    its done","['this', 'function', 'is', 'run', 'in', 'a', 'separate', 'thread', '.', 'it', 'reads', 'from', 'the', 'process', 's', 'stdout', 'stream', '(', 'a', 'streamreader', ')', 'and', 'waits', 'for', 'it', 'to', 'claim', 'that', 'its', 'done']",python,R,1,True,1,train
27432,ansible/ansible-runner,ansible_runner/utils.py,https://github.com/ansible/ansible-runner/blob/8ce485480a5d0b602428d9d64a752e06fb46cdb8/ansible_runner/utils.py#L337-L344,"def open_fifo_write(path, data):
    '''open_fifo_write opens the fifo named pipe in a new thread.
    This blocks the thread until an external process (such as ssh-agent)
    reads data from the pipe.
    '''
    os.mkfifo(path, stat.S_IRUSR | stat.S_IWUSR)
    threading.Thread(target=lambda p, d: open(p, 'wb').write(d),
                     args=(path, data)).start()","['def', 'open_fifo_write', '(', 'path', ',', 'data', ')', ':', 'os', '.', 'mkfifo', '(', 'path', ',', 'stat', '.', 'S_IRUSR', '|', 'stat', '.', 'S_IWUSR', ')', 'threading', '.', 'Thread', '(', 'target', '=', 'lambda', 'p', ',', 'd', ':', 'open', '(', 'p', ',', ""'wb'"", ')', '.', 'write', '(', 'd', ')', ',', 'args', '=', '(', 'path', ',', 'data', ')', ')', '.', 'start', '(', ')']","open_fifo_write opens the fifo named pipe in a new thread.
    This blocks the thread until an external process (such as ssh-agent)
    reads data from the pipe.","['open_fifo_write', 'opens', 'the', 'fifo', 'named', 'pipe', 'in', 'a', 'new', 'thread', '.', 'This', 'blocks', 'the', 'thread', 'until', 'an', 'external', 'process', '(', 'such', 'as', 'ssh', '-', 'agent', ')', 'reads', 'data', 'from', 'the', 'pipe', '.']",python,R,1,True,1,train
281,christophertbrown/bioscripts,ctbBio/mapped.py,https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/mapped.py#L152-L178,"def get_reads(sam, \
        contigs = False, mismatches = False, mm_option = False, \
        sort_sam = True, req_map = False, region = False, sbuffer = False):
    """"""
    get mapped reads (and their pairs) from an unsorted sam file
    """"""
    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])
    if sort_sam is True:
        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])
        if sam != '-':
            if os.path.exists(mapping) is False:
                os.system(""\
                    sort -k1 --buffer-size=%sG -T %s -o %s %s\
                    "" % (sbuffer, tempdir, mapping, sam))
        else:
            mapping = 'stdin-sam.sorted.sam'
            p = Popen(""sort -k1 --buffer-size=%sG -T %s -o %s"" \
                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True)
            p.communicate()
        mapping = open(mapping)
    else:
        if sam == '-':
            mapping = sys.stdin
        else:
            mapping = open(sam)
    for read in reads_from_mapping(mapping, contigs, mismatches, mm_option, req_map, region):
        yield read","['def', 'get_reads', '(', 'sam', ',', 'contigs', '=', 'False', ',', 'mismatches', '=', 'False', ',', 'mm_option', '=', 'False', ',', 'sort_sam', '=', 'True', ',', 'req_map', '=', 'False', ',', 'region', '=', 'False', ',', 'sbuffer', '=', 'False', ')', ':', 'tempdir', '=', ""'%s/'"", '%', '(', 'os', '.', 'path', '.', 'abspath', '(', 'sam', ')', '.', 'rsplit', '(', ""'/'"", ',', '1', ')', '[', '0', ']', ')', 'if', 'sort_sam', 'is', 'True', ':', 'mapping', '=', ""'%s.sorted.sam'"", '%', '(', 'sam', '.', 'rsplit', '(', ""'.'"", ',', '1', ')', '[', '0', ']', ')', 'if', 'sam', '!=', ""'-'"", ':', 'if', 'os', '.', 'path', '.', 'exists', '(', 'mapping', ')', 'is', 'False', ':', 'os', '.', 'system', '(', '""\\\n                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\n                    ""', '%', '(', 'sbuffer', ',', 'tempdir', ',', 'mapping', ',', 'sam', ')', ')', 'else', ':', 'mapping', '=', ""'stdin-sam.sorted.sam'"", 'p', '=', 'Popen', '(', '""sort -k1 --buffer-size=%sG -T %s -o %s""', '%', '(', 'sbuffer', ',', 'tempdir', ',', 'mapping', ')', ',', 'stdin', '=', 'sys', '.', 'stdin', ',', 'shell', '=', 'True', ')', 'p', '.', 'communicate', '(', ')', 'mapping', '=', 'open', '(', 'mapping', ')', 'else', ':', 'if', 'sam', '==', ""'-'"", ':', 'mapping', '=', 'sys', '.', 'stdin', 'else', ':', 'mapping', '=', 'open', '(', 'sam', ')', 'for', 'read', 'in', 'reads_from_mapping', '(', 'mapping', ',', 'contigs', ',', 'mismatches', ',', 'mm_option', ',', 'req_map', ',', 'region', ')', ':', 'yield', 'read']",get mapped reads (and their pairs) from an unsorted sam file,"['get', 'mapped', 'reads', '(', 'and', 'their', 'pairs', ')', 'from', 'an', 'unsorted', 'sam', 'file']",python,R,1,True,1,train
4393,acutesoftware/AIKIF,aikif/mapper.py,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/mapper.py#L198-L222,"def create_map_from_file(self, data_filename):
        """"""
        reads the data_filename into a matrix and calls the main
        function '' to generate a  .rule file based on the data in the map
        
        For all datafiles mapped, there exists a .rule file to define it
        
        """"""
        
        op_filename = data_filename + '.rule'
        
        dataset = mod_datatable.DataTable(data_filename, ',')
        dataset.load_to_array()
        l_map = self.generate_map_from_dataset(dataset)
        with open(op_filename, 'w') as f:
            f.write('# rules file autogenerated by mapper.py v0.1\n')
            f.write('filename:source=' + data_filename + '\n')
            f.write('filename:rule=' + op_filename + '\n\n')
            for row in l_map:
                #print('ROW = ' , row)
                if type(row) is str:
                    f.write(row + '\n')
                else:
                    for v in row:
                        f.write(v)","['def', 'create_map_from_file', '(', 'self', ',', 'data_filename', ')', ':', 'op_filename', '=', 'data_filename', '+', ""'.rule'"", 'dataset', '=', 'mod_datatable', '.', 'DataTable', '(', 'data_filename', ',', ""','"", ')', 'dataset', '.', 'load_to_array', '(', ')', 'l_map', '=', 'self', '.', 'generate_map_from_dataset', '(', 'dataset', ')', 'with', 'open', '(', 'op_filename', ',', ""'w'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', ""'# rules file autogenerated by mapper.py v0.1\\n'"", ')', 'f', '.', 'write', '(', ""'filename:source='"", '+', 'data_filename', '+', ""'\\n'"", ')', 'f', '.', 'write', '(', ""'filename:rule='"", '+', 'op_filename', '+', ""'\\n\\n'"", ')', 'for', 'row', 'in', 'l_map', ':', ""#print('ROW = ' , row)"", 'if', 'type', '(', 'row', ')', 'is', 'str', ':', 'f', '.', 'write', '(', 'row', '+', ""'\\n'"", ')', 'else', ':', 'for', 'v', 'in', 'row', ':', 'f', '.', 'write', '(', 'v', ')']","reads the data_filename into a matrix and calls the main
        function '' to generate a  .rule file based on the data in the map
        
        For all datafiles mapped, there exists a .rule file to define it","['reads', 'the', 'data_filename', 'into', 'a', 'matrix', 'and', 'calls', 'the', 'main', 'function', 'to', 'generate', 'a', '.', 'rule', 'file', 'based', 'on', 'the', 'data', 'in', 'the', 'map', 'For', 'all', 'datafiles', 'mapped', 'there', 'exists', 'a', '.', 'rule', 'file', 'to', 'define', 'it']",python,R,1,True,1,train
11263,LISE-B26/pylabcontrol,build/lib/pylabcontrol/src/core/probe.py,https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/src/core/probe.py#L58-L66,"def value(self):
        """"""
        reads the value from the instrument
        """"""

        value = getattr(self.instrument, self.probe_name)
        self.buffer.append(value)

        return value","['def', 'value', '(', 'self', ')', ':', 'value', '=', 'getattr', '(', 'self', '.', 'instrument', ',', 'self', '.', 'probe_name', ')', 'self', '.', 'buffer', '.', 'append', '(', 'value', ')', 'return', 'value']",reads the value from the instrument,"['reads', 'the', 'value', 'from', 'the', 'instrument']",python,R,1,True,1,train
11558,koszullab/metaTOR,metator/scripts/network.py,https://github.com/koszullab/metaTOR/blob/0c1203d1dffedfa5ea380c0335b4baa9cfb7e89a/metator/scripts/network.py#L438-L574,"def alignment_to_reads(
    sam_merged,
    output_dir,
    parameters=DEFAULT_PARAMETERS,
    save_memory=True,
    *bin_fasta
):
    """"""Generate reads from ambiguous alignment file

    Extract reads found to be mapping an input FASTA bin.
    If one read maps, the whole pair is extracted and written
    to the output paired-end FASTQ files. Reads that mapped
    and weren't part of a pair are kept in a third 'single'
    file for people who need it (e.g. to get extra paired reads
    by fetching the opposite one from the original FASTQ library).

    Parameters
    ----------
    sam_merged : file, str or pathlib.Path
        The input alignment file in SAM/BAM format to be processed.
    output_dir : str or pathlib.Path
        The output directory to write the network and chunk data into.
    parameters : dict, optional
        Parameters for the network to read conversion, similar to
        alignment_to_network.
    save_memory : bool, optional
        Whether to keep the read names into memory or write them in different
        files, which takes longer but may prevent out-of-memory crashes.
        Default is True.
    `*bin_fasta` : file, str or pathlib.Path
        The bin FASTA files with appropriately named records.

    Returns
    -------
    A dictionary of files with read names for each bin if save_memory is True,
    and a dictionary of the read names lists themselves otherwise.


    Note
    ----
    This will throw an IOError ('close failed in file object destructor') on
    exit with older versions of pysam for some reason. It's harmless but
    you may consider upgrading to a later version of pysam if it comes up in
    a pipeline.
    """"""

    #   Just in case file objects are sent as input
    def get_file_string(file_thing):
        try:
            file_string = file_thing.name
        except AttributeError:
            file_string = str(file_thing)
        return file_string

    #   Global set of chunks against which reads are required to
    #   map - we store them in a tuple that keeps track of the
    #   original bin each chunk came from so we can reattribute the reads later

    bin_chunks = set()
    for bin_file in bin_fasta:
        for record in SeqIO.parse(bin_file, ""fasta""):
            bin_chunks.add((get_file_string(bin_file), record.id))

    chunk_size = int(parameters[""chunk_size""])

    mapq_threshold = int(parameters[""mapq_threshold""])

    def read_name(read):
        return read.query_name.split()[0]

    #   Since reading a huge BAM file can take up a
    #   lot of time and resources, we only do it once
    #   but that requires opening fastq files for writing
    #   as matching reads get detected along the
    #   bam and keeping track of which ones are
    #   currently open.

    def get_base_name(bin_file):

        base_name = ""."".join(os.path.basename(bin_file).split(""."")[:-1])

        output_path = os.path.join(
            output_dir, ""{}.readnames"".format(base_name)
        )

        return output_path

    if save_memory:
        opened_files = dict()
    else:
        read_names = dict()

    with pysam.AlignmentFile(sam_merged, ""rb"") as alignment_merged_handle:

        for (my_read_name, alignment_pool) in itertools.groupby(
            alignment_merged_handle, read_name
        ):

            for my_alignment in alignment_pool:

                relative_position = my_alignment.reference_start
                contig_name = my_alignment.reference_name

                chunk_position = relative_position // chunk_size

                # The 'chunk name' is used to detect macthing positions
                chunk_name = ""{}_{}"".format(contig_name, chunk_position)

                # But such matching positions have to map acceptably
                quality_test = my_alignment.mapping_quality > mapq_threshold

                for bin_file in bin_fasta:
                    chunk_tuple = (bin_file, chunk_name)
                    if chunk_tuple in bin_chunks and quality_test:
                        if save_memory:
                            output_path = get_base_name(bin_file)
                            try:
                                output_handle = opened_files[bin_file]
                            except KeyError:
                                output_handle = open(output_path, ""w"")
                                opened_files[bin_file] = output_handle

                            output_handle.write(""@{}\n"".format(my_read_name))

                        else:
                            try:
                                read_names[my_read_name].append(bin_file)
                            except KeyError:
                                read_names[my_read_name] = [bin_file]

    for file_handle in opened_files.values():
        file_handle.close()
    #   Return unpaired file names for pair_unpaired_reads() to process
    if save_memory:
        return opened_files.keys()
    else:
        return read_names","['def', 'alignment_to_reads', '(', 'sam_merged', ',', 'output_dir', ',', 'parameters', '=', 'DEFAULT_PARAMETERS', ',', 'save_memory', '=', 'True', ',', '*', 'bin_fasta', ')', ':', '#   Just in case file objects are sent as input', 'def', 'get_file_string', '(', 'file_thing', ')', ':', 'try', ':', 'file_string', '=', 'file_thing', '.', 'name', 'except', 'AttributeError', ':', 'file_string', '=', 'str', '(', 'file_thing', ')', 'return', 'file_string', '#   Global set of chunks against which reads are required to', '#   map - we store them in a tuple that keeps track of the', '#   original bin each chunk came from so we can reattribute the reads later', 'bin_chunks', '=', 'set', '(', ')', 'for', 'bin_file', 'in', 'bin_fasta', ':', 'for', 'record', 'in', 'SeqIO', '.', 'parse', '(', 'bin_file', ',', '""fasta""', ')', ':', 'bin_chunks', '.', 'add', '(', '(', 'get_file_string', '(', 'bin_file', ')', ',', 'record', '.', 'id', ')', ')', 'chunk_size', '=', 'int', '(', 'parameters', '[', '""chunk_size""', ']', ')', 'mapq_threshold', '=', 'int', '(', 'parameters', '[', '""mapq_threshold""', ']', ')', 'def', 'read_name', '(', 'read', ')', ':', 'return', 'read', '.', 'query_name', '.', 'split', '(', ')', '[', '0', ']', '#   Since reading a huge BAM file can take up a', '#   lot of time and resources, we only do it once', '#   but that requires opening fastq files for writing', '#   as matching reads get detected along the', '#   bam and keeping track of which ones are', '#   currently open.', 'def', 'get_base_name', '(', 'bin_file', ')', ':', 'base_name', '=', '"".""', '.', 'join', '(', 'os', '.', 'path', '.', 'basename', '(', 'bin_file', ')', '.', 'split', '(', '"".""', ')', '[', ':', '-', '1', ']', ')', 'output_path', '=', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', '""{}.readnames""', '.', 'format', '(', 'base_name', ')', ')', 'return', 'output_path', 'if', 'save_memory', ':', 'opened_files', '=', 'dict', '(', ')', 'else', ':', 'read_names', '=', 'dict', '(', ')', 'with', 'pysam', '.', 'AlignmentFile', '(', 'sam_merged', ',', '""rb""', ')', 'as', 'alignment_merged_handle', ':', 'for', '(', 'my_read_name', ',', 'alignment_pool', ')', 'in', 'itertools', '.', 'groupby', '(', 'alignment_merged_handle', ',', 'read_name', ')', ':', 'for', 'my_alignment', 'in', 'alignment_pool', ':', 'relative_position', '=', 'my_alignment', '.', 'reference_start', 'contig_name', '=', 'my_alignment', '.', 'reference_name', 'chunk_position', '=', 'relative_position', '//', 'chunk_size', ""# The 'chunk name' is used to detect macthing positions"", 'chunk_name', '=', '""{}_{}""', '.', 'format', '(', 'contig_name', ',', 'chunk_position', ')', '# But such matching positions have to map acceptably', 'quality_test', '=', 'my_alignment', '.', 'mapping_quality', '>', 'mapq_threshold', 'for', 'bin_file', 'in', 'bin_fasta', ':', 'chunk_tuple', '=', '(', 'bin_file', ',', 'chunk_name', ')', 'if', 'chunk_tuple', 'in', 'bin_chunks', 'and', 'quality_test', ':', 'if', 'save_memory', ':', 'output_path', '=', 'get_base_name', '(', 'bin_file', ')', 'try', ':', 'output_handle', '=', 'opened_files', '[', 'bin_file', ']', 'except', 'KeyError', ':', 'output_handle', '=', 'open', '(', 'output_path', ',', '""w""', ')', 'opened_files', '[', 'bin_file', ']', '=', 'output_handle', 'output_handle', '.', 'write', '(', '""@{}\\n""', '.', 'format', '(', 'my_read_name', ')', ')', 'else', ':', 'try', ':', 'read_names', '[', 'my_read_name', ']', '.', 'append', '(', 'bin_file', ')', 'except', 'KeyError', ':', 'read_names', '[', 'my_read_name', ']', '=', '[', 'bin_file', ']', 'for', 'file_handle', 'in', 'opened_files', '.', 'values', '(', ')', ':', 'file_handle', '.', 'close', '(', ')', '#   Return unpaired file names for pair_unpaired_reads() to process', 'if', 'save_memory', ':', 'return', 'opened_files', '.', 'keys', '(', ')', 'else', ':', 'return', 'read_names']","Generate reads from ambiguous alignment file

    Extract reads found to be mapping an input FASTA bin.
    If one read maps, the whole pair is extracted and written
    to the output paired-end FASTQ files. Reads that mapped
    and weren't part of a pair are kept in a third 'single'
    file for people who need it (e.g. to get extra paired reads
    by fetching the opposite one from the original FASTQ library).

    Parameters
    ----------
    sam_merged : file, str or pathlib.Path
        The input alignment file in SAM/BAM format to be processed.
    output_dir : str or pathlib.Path
        The output directory to write the network and chunk data into.
    parameters : dict, optional
        Parameters for the network to read conversion, similar to
        alignment_to_network.
    save_memory : bool, optional
        Whether to keep the read names into memory or write them in different
        files, which takes longer but may prevent out-of-memory crashes.
        Default is True.
    `*bin_fasta` : file, str or pathlib.Path
        The bin FASTA files with appropriately named records.

    Returns
    -------
    A dictionary of files with read names for each bin if save_memory is True,
    and a dictionary of the read names lists themselves otherwise.


    Note
    ----
    This will throw an IOError ('close failed in file object destructor') on
    exit with older versions of pysam for some reason. It's harmless but
    you may consider upgrading to a later version of pysam if it comes up in
    a pipeline.","['Generate', 'reads', 'from', 'ambiguous', 'alignment', 'file']",python,R,1,True,1,train
17188,Erotemic/utool,utool/util_grabdata.py,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_grabdata.py#L420-L426,"def url_read_text(url, verbose=True):
    r""""""
    Directly reads text data from url
    """"""
    data = url_read(url, verbose)
    text = data.decode('utf8')
    return text","['def', 'url_read_text', '(', 'url', ',', 'verbose', '=', 'True', ')', ':', 'data', '=', 'url_read', '(', 'url', ',', 'verbose', ')', 'text', '=', 'data', '.', 'decode', '(', ""'utf8'"", ')', 'return', 'text']","r""""""
    Directly reads text data from url","['r', 'Directly', 'reads', 'text', 'data', 'from', 'url']",python,R,1,True,1,train
22282,maljovec/topopy,topopy/TopologicalObject.py,https://github.com/maljovec/topopy/blob/4be598d51c4e4043b73d4ad44beed6d289e2f088/topopy/TopologicalObject.py#L236-L250,"def load_data_and_build(self, filename, delimiter="",""):
        """""" Convenience function for directly working with a data file.
            This opens a file and reads the data into an array, sets the
            data as an nparray and list of dimnames
            @ In, filename, string representing the data file
        """"""
        data = np.genfromtxt(
            filename, dtype=float, delimiter=delimiter, names=True
        )
        data = data.view(np.float64).reshape(data.shape + (-1,))

        X = data[:, 0:-1]
        Y = data[:, -1]

        self.build(X=X, Y=Y)","['def', 'load_data_and_build', '(', 'self', ',', 'filename', ',', 'delimiter', '=', '"",""', ')', ':', 'data', '=', 'np', '.', 'genfromtxt', '(', 'filename', ',', 'dtype', '=', 'float', ',', 'delimiter', '=', 'delimiter', ',', 'names', '=', 'True', ')', 'data', '=', 'data', '.', 'view', '(', 'np', '.', 'float64', ')', '.', 'reshape', '(', 'data', '.', 'shape', '+', '(', '-', '1', ',', ')', ')', 'X', '=', 'data', '[', ':', ',', '0', ':', '-', '1', ']', 'Y', '=', 'data', '[', ':', ',', '-', '1', ']', 'self', '.', 'build', '(', 'X', '=', 'X', ',', 'Y', '=', 'Y', ')']","Convenience function for directly working with a data file.
            This opens a file and reads the data into an array, sets the
            data as an nparray and list of dimnames
            @ In, filename, string representing the data file","['Convenience', 'function', 'for', 'directly', 'working', 'with', 'a', 'data', 'file', '.', 'This', 'opens', 'a', 'file', 'and', 'reads', 'the', 'data', 'into', 'an', 'array', 'sets', 'the', 'data', 'as', 'an', 'nparray', 'and', 'list', 'of', 'dimnames']",python,R,1,True,1,train
22643,NikolayDachev/jadm,lib/paramiko-1.14.1/paramiko/packet.py,https://github.com/NikolayDachev/jadm/blob/12bb550445edfcd87506f7cba7a6a35d413c5511/lib/paramiko-1.14.1/paramiko/packet.py#L264-L277,"def readline(self, timeout):
        """"""
        Read a line from the socket.  We assume no data is pending after the
        line, so it's okay to attempt large reads.
        """"""
        buf = self.__remainder
        while not linefeed_byte in buf:
            buf += self._read_timeout(timeout)
        n = buf.index(linefeed_byte)
        self.__remainder = buf[n + 1:]
        buf = buf[:n]
        if (len(buf) > 0) and (buf[-1] == cr_byte_value):
            buf = buf[:-1]
        return u(buf)","['def', 'readline', '(', 'self', ',', 'timeout', ')', ':', 'buf', '=', 'self', '.', '__remainder', 'while', 'not', 'linefeed_byte', 'in', 'buf', ':', 'buf', '+=', 'self', '.', '_read_timeout', '(', 'timeout', ')', 'n', '=', 'buf', '.', 'index', '(', 'linefeed_byte', ')', 'self', '.', '__remainder', '=', 'buf', '[', 'n', '+', '1', ':', ']', 'buf', '=', 'buf', '[', ':', 'n', ']', 'if', '(', 'len', '(', 'buf', ')', '>', '0', ')', 'and', '(', 'buf', '[', '-', '1', ']', '==', 'cr_byte_value', ')', ':', 'buf', '=', 'buf', '[', ':', '-', '1', ']', 'return', 'u', '(', 'buf', ')']","Read a line from the socket.  We assume no data is pending after the
        line, so it's okay to attempt large reads.","['Read', 'a', 'line', 'from', 'the', 'socket', '.', 'We', 'assume', 'no', 'data', 'is', 'pending', 'after', 'the', 'line', 'so', 'it', 's', 'okay', 'to', 'attempt', 'large', 'reads', '.']",python,R,1,True,1,train
29966,lowandrew/OLCTools,spadespipeline/primer_finder_bbduk.py,https://github.com/lowandrew/OLCTools/blob/88aa90ac85f84d0bbeb03e43c29b0a9d36e4ce2a/spadespipeline/primer_finder_bbduk.py#L201-L237,"def bait(self):
        """"""
        Use bbduk to bait FASTQ reads from input files using the primer file as the target
        """"""
        with progressbar(self.metadata) as bar:
            for sample in bar:
                if sample.general.bestassemblyfile != 'NA':
                    # Only need to perform baiting on FASTQ files
                    if sample[self.analysistype].filetype == 'fastq':
                        # Make the system call - allow for single- or paired-end reads
                        if len(sample.general.fastqfiles) == 2:
                            # Create the command to run the baiting - ref: primer file, k: shortest primer length
                            # in1, in2: paired inputs, hdist: number of mismatches, interleaved: use interleaved output
                            # outm: single, zipped output file of reads that match the target file
                            sample[self.analysistype].bbdukcmd = \
                                'bbduk.sh ref={primerfile} k={klength} in1={forward} in2={reverse} ' \
                                'hdist={mismatches} threads={threads} interleaved=t outm={outfile}' \
                                .format(primerfile=self.formattedprimers,
                                        klength=self.klength,
                                        forward=sample.general.trimmedcorrectedfastqfiles[0],
                                        reverse=sample.general.trimmedcorrectedfastqfiles[1],
                                        mismatches=self.mismatches,
                                        threads=str(self.cpus),
                                        outfile=sample[self.analysistype].baitedfastq)
                        else:
                            sample[self.analysistype].bbdukcmd = \
                                'bbduk.sh ref={primerfile} k={klength} in={fastq} hdist={mismatches} ' \
                                'threads={threads} interleaved=t outm={outfile}' \
                                .format(primerfile=self.formattedprimers,
                                        klength=self.klength,
                                        fastq=sample.general.trimmedcorrectedfastqfiles[0],
                                        mismatches=self.mismatches,
                                        threads=str(self.cpus),
                                        outfile=sample[self.analysistype].baitedfastq)
                        # Run the system call (if necessary)
                        if not os.path.isfile(sample[self.analysistype].baitedfastq):
                            run_subprocess(sample[self.analysistype].bbdukcmd)","['def', 'bait', '(', 'self', ')', ':', 'with', 'progressbar', '(', 'self', '.', 'metadata', ')', 'as', 'bar', ':', 'for', 'sample', 'in', 'bar', ':', 'if', 'sample', '.', 'general', '.', 'bestassemblyfile', '!=', ""'NA'"", ':', '# Only need to perform baiting on FASTQ files', 'if', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'filetype', '==', ""'fastq'"", ':', '# Make the system call - allow for single- or paired-end reads', 'if', 'len', '(', 'sample', '.', 'general', '.', 'fastqfiles', ')', '==', '2', ':', '# Create the command to run the baiting - ref: primer file, k: shortest primer length', '# in1, in2: paired inputs, hdist: number of mismatches, interleaved: use interleaved output', '# outm: single, zipped output file of reads that match the target file', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'bbdukcmd', '=', ""'bbduk.sh ref={primerfile} k={klength} in1={forward} in2={reverse} '"", ""'hdist={mismatches} threads={threads} interleaved=t outm={outfile}'"", '.', 'format', '(', 'primerfile', '=', 'self', '.', 'formattedprimers', ',', 'klength', '=', 'self', '.', 'klength', ',', 'forward', '=', 'sample', '.', 'general', '.', 'trimmedcorrectedfastqfiles', '[', '0', ']', ',', 'reverse', '=', 'sample', '.', 'general', '.', 'trimmedcorrectedfastqfiles', '[', '1', ']', ',', 'mismatches', '=', 'self', '.', 'mismatches', ',', 'threads', '=', 'str', '(', 'self', '.', 'cpus', ')', ',', 'outfile', '=', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'baitedfastq', ')', 'else', ':', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'bbdukcmd', '=', ""'bbduk.sh ref={primerfile} k={klength} in={fastq} hdist={mismatches} '"", ""'threads={threads} interleaved=t outm={outfile}'"", '.', 'format', '(', 'primerfile', '=', 'self', '.', 'formattedprimers', ',', 'klength', '=', 'self', '.', 'klength', ',', 'fastq', '=', 'sample', '.', 'general', '.', 'trimmedcorrectedfastqfiles', '[', '0', ']', ',', 'mismatches', '=', 'self', '.', 'mismatches', ',', 'threads', '=', 'str', '(', 'self', '.', 'cpus', ')', ',', 'outfile', '=', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'baitedfastq', ')', '# Run the system call (if necessary)', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'baitedfastq', ')', ':', 'run_subprocess', '(', 'sample', '[', 'self', '.', 'analysistype', ']', '.', 'bbdukcmd', ')']",Use bbduk to bait FASTQ reads from input files using the primer file as the target,"['Use', 'bbduk', 'to', 'bait', 'FASTQ', 'reads', 'from', 'input', 'files', 'using', 'the', 'primer', 'file', 'as', 'the', 'target']",python,R,1,True,1,train
8140,bimbar/pykwb,pykwb/kwb.py,https://github.com/bimbar/pykwb/blob/3f607c064cc53b8310d22d42506ce817a5b735fe/pykwb/kwb.py#L403-L410,"def run(self):
        """"""Main thread that reads from input and populates the sensors.""""""
        while (self._run_thread):
            (mode, version, packet) = self._read_packet()
            if (mode == PROP_PACKET_SENSE):
                self._decode_sense_packet(version, packet)
            elif (mode == PROP_PACKET_CTRL):
                self._decode_ctrl_packet(version, packet)","['def', 'run', '(', 'self', ')', ':', 'while', '(', 'self', '.', '_run_thread', ')', ':', '(', 'mode', ',', 'version', ',', 'packet', ')', '=', 'self', '.', '_read_packet', '(', ')', 'if', '(', 'mode', '==', 'PROP_PACKET_SENSE', ')', ':', 'self', '.', '_decode_sense_packet', '(', 'version', ',', 'packet', ')', 'elif', '(', 'mode', '==', 'PROP_PACKET_CTRL', ')', ':', 'self', '.', '_decode_ctrl_packet', '(', 'version', ',', 'packet', ')']",Main thread that reads from input and populates the sensors.,"['Main', 'thread', 'that', 'reads', 'from', 'input', 'and', 'populates', 'the', 'sensors', '.']",python,R,1,True,1,train
28032,mushkevych/scheduler,synergy/workers/bash_driver.py,https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/workers/bash_driver.py#L114-L121,"def _mq_callback(self, message):
        """""" reads JSON request from the mq message and delivers it for processing """"""
        while threading.active_count() > settings.settings['bash_runnable_count'] + self.initial_thread_count:
            time.sleep(0.1)

        t = BashRunnable(self.logger, message, self.consumer, self.performance_tracker)
        t.daemon = True
        t.start()","['def', '_mq_callback', '(', 'self', ',', 'message', ')', ':', 'while', 'threading', '.', 'active_count', '(', ')', '>', 'settings', '.', 'settings', '[', ""'bash_runnable_count'"", ']', '+', 'self', '.', 'initial_thread_count', ':', 'time', '.', 'sleep', '(', '0.1', ')', 't', '=', 'BashRunnable', '(', 'self', '.', 'logger', ',', 'message', ',', 'self', '.', 'consumer', ',', 'self', '.', 'performance_tracker', ')', 't', '.', 'daemon', '=', 'True', 't', '.', 'start', '(', ')']",reads JSON request from the mq message and delivers it for processing,"['reads', 'JSON', 'request', 'from', 'the', 'mq', 'message', 'and', 'delivers', 'it', 'for', 'processing']",python,R,1,True,1,train
1258,CivicSpleen/ambry,ambry/util/geocoders.py,https://github.com/CivicSpleen/ambry/blob/d7f2be4bf1f7ffd086f3fadd4fcae60c32473e42/ambry/util/geocoders.py#L35-L64,"def geocode(self):
        """"""A Generator that reads from the address generators and returns
        geocode results.

        The generator yields ( address, geocode_results, object)

        """"""

        submit_set = []
        data_map = {}

        for address, o in self.gen:
            submit_set.append(address)
            data_map[address] = o

            if len(submit_set) >= self.submit_size:
                results = self._send(submit_set)
                submit_set = []

                for k, result in results.items():
                    o = data_map[k]
                    yield (k, result, o)

        if len(submit_set) > 0:
            results = self._send(submit_set)
            # submit_set = []

            for k, result in results.items():
                o = data_map[k]
                yield (k, result, o)","['def', 'geocode', '(', 'self', ')', ':', 'submit_set', '=', '[', ']', 'data_map', '=', '{', '}', 'for', 'address', ',', 'o', 'in', 'self', '.', 'gen', ':', 'submit_set', '.', 'append', '(', 'address', ')', 'data_map', '[', 'address', ']', '=', 'o', 'if', 'len', '(', 'submit_set', ')', '>=', 'self', '.', 'submit_size', ':', 'results', '=', 'self', '.', '_send', '(', 'submit_set', ')', 'submit_set', '=', '[', ']', 'for', 'k', ',', 'result', 'in', 'results', '.', 'items', '(', ')', ':', 'o', '=', 'data_map', '[', 'k', ']', 'yield', '(', 'k', ',', 'result', ',', 'o', ')', 'if', 'len', '(', 'submit_set', ')', '>', '0', ':', 'results', '=', 'self', '.', '_send', '(', 'submit_set', ')', '# submit_set = []', 'for', 'k', ',', 'result', 'in', 'results', '.', 'items', '(', ')', ':', 'o', '=', 'data_map', '[', 'k', ']', 'yield', '(', 'k', ',', 'result', ',', 'o', ')']","A Generator that reads from the address generators and returns
        geocode results.

        The generator yields ( address, geocode_results, object)","['A', 'Generator', 'that', 'reads', 'from', 'the', 'address', 'generators', 'and', 'returns', 'geocode', 'results', '.']",python,R,1,True,1,train
1591,acorg/dark-matter,dark/fasta.py,https://github.com/acorg/dark-matter/blob/c78a1bf262667fa5db3548fa7066c4ec14d0551d/dark/fasta.py#L184-L226,"def combineReads(filename, sequences, readClass=DNARead,
                 upperCase=False, idPrefix='command-line-read-'):
    """"""
    Combine FASTA reads from a file and/or sequence strings.

    @param filename: A C{str} file name containing FASTA reads.
    @param sequences: A C{list} of C{str} sequences. If a sequence
        contains spaces, the last field (after splitting on spaces) will be
        used as the sequence and the first fields will be used as the sequence
        id.
    @param readClass: The class of the individual reads.
    @param upperCase: If C{True}, reads will be converted to upper case.
    @param idPrefix: The C{str} prefix that will be used for the id of the
        sequences in C{sequences} that do not have an id specified. A trailing
        sequence number will be appended to this prefix. Note that
        'command-line-read-', the default id prefix, could collide with ids in
        the FASTA file, if given. So output might be ambiguous. That's why we
        allow the caller to specify a custom prefix.
    @return: A C{FastaReads} instance.
    """"""
    # Read sequences from a FASTA file, if given.
    if filename:
        reads = FastaReads(filename, readClass=readClass, upperCase=upperCase)
    else:
        reads = Reads()

    # Add any individually specified subject sequences.
    if sequences:
        for count, sequence in enumerate(sequences, start=1):
            # Try splitting the sequence on its last space and using the
            # first part of the split as the read id. If there's no space,
            # assign a generic id.
            parts = sequence.rsplit(' ', 1)
            if len(parts) == 2:
                readId, sequence = parts
            else:
                readId = '%s%d' % (idPrefix, count)
            if upperCase:
                sequence = sequence.upper()
            read = readClass(readId, sequence)
            reads.add(read)

    return reads","['def', 'combineReads', '(', 'filename', ',', 'sequences', ',', 'readClass', '=', 'DNARead', ',', 'upperCase', '=', 'False', ',', 'idPrefix', '=', ""'command-line-read-'"", ')', ':', '# Read sequences from a FASTA file, if given.', 'if', 'filename', ':', 'reads', '=', 'FastaReads', '(', 'filename', ',', 'readClass', '=', 'readClass', ',', 'upperCase', '=', 'upperCase', ')', 'else', ':', 'reads', '=', 'Reads', '(', ')', '# Add any individually specified subject sequences.', 'if', 'sequences', ':', 'for', 'count', ',', 'sequence', 'in', 'enumerate', '(', 'sequences', ',', 'start', '=', '1', ')', ':', '# Try splitting the sequence on its last space and using the', ""# first part of the split as the read id. If there's no space,"", '# assign a generic id.', 'parts', '=', 'sequence', '.', 'rsplit', '(', ""' '"", ',', '1', ')', 'if', 'len', '(', 'parts', ')', '==', '2', ':', 'readId', ',', 'sequence', '=', 'parts', 'else', ':', 'readId', '=', ""'%s%d'"", '%', '(', 'idPrefix', ',', 'count', ')', 'if', 'upperCase', ':', 'sequence', '=', 'sequence', '.', 'upper', '(', ')', 'read', '=', 'readClass', '(', 'readId', ',', 'sequence', ')', 'reads', '.', 'add', '(', 'read', ')', 'return', 'reads']","Combine FASTA reads from a file and/or sequence strings.

    @param filename: A C{str} file name containing FASTA reads.
    @param sequences: A C{list} of C{str} sequences. If a sequence
        contains spaces, the last field (after splitting on spaces) will be
        used as the sequence and the first fields will be used as the sequence
        id.
    @param readClass: The class of the individual reads.
    @param upperCase: If C{True}, reads will be converted to upper case.
    @param idPrefix: The C{str} prefix that will be used for the id of the
        sequences in C{sequences} that do not have an id specified. A trailing
        sequence number will be appended to this prefix. Note that
        'command-line-read-', the default id prefix, could collide with ids in
        the FASTA file, if given. So output might be ambiguous. That's why we
        allow the caller to specify a custom prefix.
    @return: A C{FastaReads} instance.","['Combine', 'FASTA', 'reads', 'from', 'a', 'file', 'and', '/', 'or', 'sequence', 'strings', '.']",python,R,1,True,1,train
4702,nickstenning/tagalog,tagalog/io.py,https://github.com/nickstenning/tagalog/blob/c6847a957dc4f96836a5cf13c4eb664fccafaac2/tagalog/io.py#L24-L68,"def lines(fp):
    """"""
    Read lines of UTF-8 from the file-like object given in ``fp``, making sure
    that when reading from STDIN, reads are at most line-buffered.

    UTF-8 decoding errors are handled silently. Invalid characters are
    replaced by U+FFFD REPLACEMENT CHARACTER.

    Line endings are normalised to newlines by Python's universal newlines
    feature.

    Returns an iterator yielding lines.
    """"""
    if fp.fileno() == sys.stdin.fileno():
        close = True

        try: # Python 3
            fp = open(fp.fileno(), mode='r', buffering=BUF_LINEBUFFERED, errors='replace')
            decode = False
        except TypeError:
            fp = os.fdopen(fp.fileno(), 'rU', BUF_LINEBUFFERED)
            decode = True

    else:
        close = False

        try:
            # only decode if the fp doesn't already have an encoding
            decode = (fp.encoding != UTF8)
        except AttributeError:
            # fp has been opened in binary mode
            decode = True

    try:
        while 1:
            l = fp.readline()
            if l:
                if decode:
                    l = l.decode(UTF8, 'replace')
                yield l
            else:
                break
    finally:
        if close:
            fp.close()","['def', 'lines', '(', 'fp', ')', ':', 'if', 'fp', '.', 'fileno', '(', ')', '==', 'sys', '.', 'stdin', '.', 'fileno', '(', ')', ':', 'close', '=', 'True', 'try', ':', '# Python 3', 'fp', '=', 'open', '(', 'fp', '.', 'fileno', '(', ')', ',', 'mode', '=', ""'r'"", ',', 'buffering', '=', 'BUF_LINEBUFFERED', ',', 'errors', '=', ""'replace'"", ')', 'decode', '=', 'False', 'except', 'TypeError', ':', 'fp', '=', 'os', '.', 'fdopen', '(', 'fp', '.', 'fileno', '(', ')', ',', ""'rU'"", ',', 'BUF_LINEBUFFERED', ')', 'decode', '=', 'True', 'else', ':', 'close', '=', 'False', 'try', ':', ""# only decode if the fp doesn't already have an encoding"", 'decode', '=', '(', 'fp', '.', 'encoding', '!=', 'UTF8', ')', 'except', 'AttributeError', ':', '# fp has been opened in binary mode', 'decode', '=', 'True', 'try', ':', 'while', '1', ':', 'l', '=', 'fp', '.', 'readline', '(', ')', 'if', 'l', ':', 'if', 'decode', ':', 'l', '=', 'l', '.', 'decode', '(', 'UTF8', ',', ""'replace'"", ')', 'yield', 'l', 'else', ':', 'break', 'finally', ':', 'if', 'close', ':', 'fp', '.', 'close', '(', ')']","Read lines of UTF-8 from the file-like object given in ``fp``, making sure
    that when reading from STDIN, reads are at most line-buffered.

    UTF-8 decoding errors are handled silently. Invalid characters are
    replaced by U+FFFD REPLACEMENT CHARACTER.

    Line endings are normalised to newlines by Python's universal newlines
    feature.

    Returns an iterator yielding lines.","['Read', 'lines', 'of', 'UTF', '-', '8', 'from', 'the', 'file', '-', 'like', 'object', 'given', 'in', 'fp', 'making', 'sure', 'that', 'when', 'reading', 'from', 'STDIN', 'reads', 'are', 'at', 'most', 'line', '-', 'buffered', '.']",python,R,1,True,1,train
20567,OpenAgInitiative/openag_python,openag/couch.py,https://github.com/OpenAgInitiative/openag_python/blob/f6202340292bbf7185e1a7d4290188c0dacbb8d0/openag/couch.py#L130-L146,"def _folder_to_dict(self, path):
        """"""
        Recursively reads the files from the directory given by `path` and
        writes their contents to a nested dictionary, which is then returned.
        """"""
        res = {}
        for key in os.listdir(path):
            if key.startswith('.'):
                continue
            key_path = os.path.join(path, key)
            if os.path.isfile(key_path):
                val = open(key_path).read()
                key = key.split('.')[0]
                res[key] = val
            else:
                res[key] = self._folder_to_dict(key_path)
        return res","['def', '_folder_to_dict', '(', 'self', ',', 'path', ')', ':', 'res', '=', '{', '}', 'for', 'key', 'in', 'os', '.', 'listdir', '(', 'path', ')', ':', 'if', 'key', '.', 'startswith', '(', ""'.'"", ')', ':', 'continue', 'key_path', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'key', ')', 'if', 'os', '.', 'path', '.', 'isfile', '(', 'key_path', ')', ':', 'val', '=', 'open', '(', 'key_path', ')', '.', 'read', '(', ')', 'key', '=', 'key', '.', 'split', '(', ""'.'"", ')', '[', '0', ']', 'res', '[', 'key', ']', '=', 'val', 'else', ':', 'res', '[', 'key', ']', '=', 'self', '.', '_folder_to_dict', '(', 'key_path', ')', 'return', 'res']","Recursively reads the files from the directory given by `path` and
        writes their contents to a nested dictionary, which is then returned.","['Recursively', 'reads', 'the', 'files', 'from', 'the', 'directory', 'given', 'by', 'path', 'and', 'writes', 'their', 'contents', 'to', 'a', 'nested', 'dictionary', 'which', 'is', 'then', 'returned', '.']",python,R,1,True,1,train
25461,ska-sa/purr,Purr/Plugins/local_pychart/chart_data.py,https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Plugins/local_pychart/chart_data.py#L221-L249,"def read_csv(path, delim=','):
    """"""This function reads comma-separated values from a
    file. Parameter <path> is either a pathname or a file-like object
    that supports the |readline()| method.

    Empty lines and lines
    beginning with ""#"" are ignored.  Parameter <delim> specifies how
    a line is separated into values. If it does not contain the
    letter ""%"", then <delim> marks the end of a value.
    Otherwise, this function acts like scanf in C:

chart_data.read_csv('file', '%d,%s:%d')

    Paramter <delim> currently supports
    only three conversion format specifiers:
    ""d""(int), ""f""(double), and ""s""(string).""""""

    fd = _try_open_file(path, 'r',
                        'The first argument must be a pathname or an object that supports readline() method')

    data = []
    line = fd.readline()
    while line != """":
        if line[0] != '#' and not empty_line_p(line):
            data.append(parse_line(line, delim))
        line = fd.readline()

    _try_close_file(fd, path)
    return data","['def', 'read_csv', '(', 'path', ',', 'delim', '=', ""','"", ')', ':', 'fd', '=', '_try_open_file', '(', 'path', ',', ""'r'"", ',', ""'The first argument must be a pathname or an object that supports readline() method'"", ')', 'data', '=', '[', ']', 'line', '=', 'fd', '.', 'readline', '(', ')', 'while', 'line', '!=', '""""', ':', 'if', 'line', '[', '0', ']', '!=', ""'#'"", 'and', 'not', 'empty_line_p', '(', 'line', ')', ':', 'data', '.', 'append', '(', 'parse_line', '(', 'line', ',', 'delim', ')', ')', 'line', '=', 'fd', '.', 'readline', '(', ')', '_try_close_file', '(', 'fd', ',', 'path', ')', 'return', 'data']","This function reads comma-separated values from a
    file. Parameter <path> is either a pathname or a file-like object
    that supports the |readline()| method.

    Empty lines and lines
    beginning with ""#"" are ignored.  Parameter <delim> specifies how
    a line is separated into values. If it does not contain the
    letter ""%"", then <delim> marks the end of a value.
    Otherwise, this function acts like scanf in C:

chart_data.read_csv('file', '%d,%s:%d')

    Paramter <delim> currently supports
    only three conversion format specifiers:
    ""d""(int), ""f""(double), and ""s""(string).","['This', 'function', 'reads', 'comma', '-', 'separated', 'values', 'from', 'a', 'file', '.', 'Parameter', '<path', '>', 'is', 'either', 'a', 'pathname', 'or', 'a', 'file', '-', 'like', 'object', 'that', 'supports', 'the', '|readline', '()', '|', 'method', '.']",python,R,1,True,1,train
25463,ska-sa/purr,Purr/Plugins/local_pychart/chart_data.py,https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Plugins/local_pychart/chart_data.py#L277-L288,"def read_str(delim=',', *lines):
    """"""This function is similar to read_csv, but it reads data from the
    list of <lines>.

fd = open(""foo"", ""r"")
data = chart_data.read_str("","", fd.readlines())""""""

    data = []
    for line in lines:
        com = parse_line(line, delim)
        data.append(com)
    return data","['def', 'read_str', '(', 'delim', '=', ""','"", ',', '*', 'lines', ')', ':', 'data', '=', '[', ']', 'for', 'line', 'in', 'lines', ':', 'com', '=', 'parse_line', '(', 'line', ',', 'delim', ')', 'data', '.', 'append', '(', 'com', ')', 'return', 'data']","This function is similar to read_csv, but it reads data from the
    list of <lines>.

fd = open(""foo"", ""r"")
data = chart_data.read_str("","", fd.readlines())","['This', 'function', 'is', 'similar', 'to', 'read_csv', 'but', 'it', 'reads', 'data', 'from', 'the', 'list', 'of', '<lines', '>', '.']",python,R,1,True,1,train
28963,ninapavlich/doxie-automator,doxieautomator/settings.py,https://github.com/ninapavlich/doxie-automator/blob/b586c0faecd2cfa3757e9865cebc7dd414e97cba/doxieautomator/settings.py#L21-L45,"def read(env_file="".env""):
    """"""
    Pulled from Honcho code with minor updates, reads local default
    environment variables from a .env file located in the project root
    directory.
    """"""
    try:
        with open(env_file) as f:
            content = f.read()

    except IOError:
        content = ''

    for line in content.splitlines():

        m1 = re.match(r'\A([A-Za-z_0-9]+)=(.*)\Z', line)
        if m1:
            key, val = m1.group(1), m1.group(2)
            m2 = re.match(r""\A'(.*)'\Z"", val)
            if m2:
                val = m2.group(1)
            m3 = re.match(r'\A""(.*)""\Z', val)
            if m3:
                val = re.sub(r'\\(.)', r'\1', m3.group(1))
            os.environ.setdefault(key, val)","['def', 'read', '(', 'env_file', '=', '"".env""', ')', ':', 'try', ':', 'with', 'open', '(', 'env_file', ')', 'as', 'f', ':', 'content', '=', 'f', '.', 'read', '(', ')', 'except', 'IOError', ':', 'content', '=', ""''"", 'for', 'line', 'in', 'content', '.', 'splitlines', '(', ')', ':', 'm1', '=', 're', '.', 'match', '(', ""r'\\A([A-Za-z_0-9]+)=(.*)\\Z'"", ',', 'line', ')', 'if', 'm1', ':', 'key', ',', 'val', '=', 'm1', '.', 'group', '(', '1', ')', ',', 'm1', '.', 'group', '(', '2', ')', 'm2', '=', 're', '.', 'match', '(', 'r""\\A\'(.*)\'\\Z""', ',', 'val', ')', 'if', 'm2', ':', 'val', '=', 'm2', '.', 'group', '(', '1', ')', 'm3', '=', 're', '.', 'match', '(', 'r\'\\A""(.*)""\\Z\'', ',', 'val', ')', 'if', 'm3', ':', 'val', '=', 're', '.', 'sub', '(', ""r'\\\\(.)'"", ',', ""r'\\1'"", ',', 'm3', '.', 'group', '(', '1', ')', ')', 'os', '.', 'environ', '.', 'setdefault', '(', 'key', ',', 'val', ')']","Pulled from Honcho code with minor updates, reads local default
    environment variables from a .env file located in the project root
    directory.","['Pulled', 'from', 'Honcho', 'code', 'with', 'minor', 'updates', 'reads', 'local', 'default', 'environment', 'variables', 'from', 'a', '.', 'env', 'file', 'located', 'in', 'the', 'project', 'root', 'directory', '.']",python,R,1,True,1,train
2431,realestate-com-au/dashmat,dashmat/core_modules/splunk/splunk-sdk-1.3.0/splunklib/data.py,https://github.com/realestate-com-au/dashmat/blob/433886e52698f0ddb9956f087b76041966c3bcd1/dashmat/core_modules/splunk/splunk-sdk-1.3.0/splunklib/data.py#L58-L85,"def load(text, match=None):
    """"""This function reads a string that contains the XML of an Atom Feed, then
    returns the
    data in a native Python structure (a ``dict`` or ``list``). If you also
    provide a tag name or path to match, only the matching sub-elements are
    loaded.

    :param text: The XML text to load.
    :type text: ``string``
    :param match: A tag name or path to match (optional).
    :type match: ``string``
    """"""
    if text is None: return None
    text = text.strip()
    if len(text) == 0: return None
    nametable = {
        'namespaces': [],
        'names': {}
    }
    root = XML(text)
    items = [root] if match is None else root.findall(match)
    count = len(items)
    if count == 0:
        return None
    elif count == 1:
        return load_root(items[0], nametable)
    else:
        return [load_root(item, nametable) for item in items]","['def', 'load', '(', 'text', ',', 'match', '=', 'None', ')', ':', 'if', 'text', 'is', 'None', ':', 'return', 'None', 'text', '=', 'text', '.', 'strip', '(', ')', 'if', 'len', '(', 'text', ')', '==', '0', ':', 'return', 'None', 'nametable', '=', '{', ""'namespaces'"", ':', '[', ']', ',', ""'names'"", ':', '{', '}', '}', 'root', '=', 'XML', '(', 'text', ')', 'items', '=', '[', 'root', ']', 'if', 'match', 'is', 'None', 'else', 'root', '.', 'findall', '(', 'match', ')', 'count', '=', 'len', '(', 'items', ')', 'if', 'count', '==', '0', ':', 'return', 'None', 'elif', 'count', '==', '1', ':', 'return', 'load_root', '(', 'items', '[', '0', ']', ',', 'nametable', ')', 'else', ':', 'return', '[', 'load_root', '(', 'item', ',', 'nametable', ')', 'for', 'item', 'in', 'items', ']']","This function reads a string that contains the XML of an Atom Feed, then
    returns the
    data in a native Python structure (a ``dict`` or ``list``). If you also
    provide a tag name or path to match, only the matching sub-elements are
    loaded.

    :param text: The XML text to load.
    :type text: ``string``
    :param match: A tag name or path to match (optional).
    :type match: ``string``","['This', 'function', 'reads', 'a', 'string', 'that', 'contains', 'the', 'XML', 'of', 'an', 'Atom', 'Feed', 'then', 'returns', 'the', 'data', 'in', 'a', 'native', 'Python', 'structure', '(', 'a', 'dict', 'or', 'list', ')', '.', 'If', 'you', 'also', 'provide', 'a', 'tag', 'name', 'or', 'path', 'to', 'match', 'only', 'the', 'matching', 'sub', '-', 'elements', 'are', 'loaded', '.']",python,R,1,True,1,train
3968,noobermin/lspreader,lspreader/flds.py,https://github.com/noobermin/lspreader/blob/903b9d6427513b07986ffacf76cbca54e18d8be6/lspreader/flds.py#L35-L108,"def read_indexed(i,flds=None,sclr=None,
                 gzip='guess', dir='.', vector_norms=True,
                 keep_xs=False,gettime=False):
    '''
    A smart indexing reader that reads files by names. Looks for files
    like ""<dir>/flds<i>.p4<compression>"" where dir and the index are
    passed, as well as stuff from sclrs and saves them into one
    dictionary. Essentially useful for reading by timestep instead
    of by file. Assumes that both the flds and sclr are in the same
    direction.

    Parameters:
    -----------

    i -- index of file

    Required Keywords:
    ------------------

    flds  -- list of var's to load from the flds file
    sclr  -- list of var's to load from the sclr file
    
    Either one or both are required.

    Keywords:
    ---------

    gzip         -- files are gzipped. If gzip is ""guess"",
                    find a matching file.
    dir          -- Directory to look for files. Default is .
    vector_norms -- save the norm of the flds vectors under the
                    the name of the quantity. Default is True.
    keep_xs      -- Keep the edges. By default, False.
    gettime      -- Get the timestamp.
    '''
    fldsp4  = '{}/flds{}.p4'.format(dir,i);
    sclrp4  = '{}/sclr{}.p4'.format(dir,i);
    fldsgz  = fldsp4 + '.gz';
    sclrgz  = sclrp4 + '.gz';
    if gzip == 'guess':
        fldsname = fldsp4 if os.path.exists(fldsp4) else fldsgz
        sclrname = sclrp4 if os.path.exists(sclrp4) else sclrgz
    else:
        fldsname = fldsgz if gzip else fldsp4;
        sclrname = sclrgz if gzip else sclrp4;
    if not (flds or sclr):
        raise ValueError(""Must specify flds or sclr to read."");
    elif flds is not None and sclr is not None:
        sd,srt=read(sclrname,
                    var=sclr,first_sort=True, gzip='guess',
                    keep_xs=keep_xs);
        fd=read(fldsname,
                var=flds, sort=srt, gzip='guess',
                keep_xs=keep_xs);
        ret = dict(sd=sd,fd=fd);
        ret.update({k:sd[k] for k in sd});
        ret.update({k:fd[k] for k in fd});
        if vector_norms:
            ret.update({k:vector_norm(ret,k) for k in flds})
        if gettime:
            ret['t'] = get_header(sclrname,gzip='guess')['timestamp'];
    else:
        if flds:
            var = flds;
            name= fldsname;
        else:
            var = sclr;
            name= sclrname;
        ret,_ = read(name,var=var,first_sort=True,gzip='guess');
        if flds and vector_norms:
            ret.update({k:vector_norm(ret,k) for k in flds})
        if gettime:
            ret['t'] = get_header(name,gzip='guess')['timestamp'];
    return ret;","['def', 'read_indexed', '(', 'i', ',', 'flds', '=', 'None', ',', 'sclr', '=', 'None', ',', 'gzip', '=', ""'guess'"", ',', 'dir', '=', ""'.'"", ',', 'vector_norms', '=', 'True', ',', 'keep_xs', '=', 'False', ',', 'gettime', '=', 'False', ')', ':', 'fldsp4', '=', ""'{}/flds{}.p4'"", '.', 'format', '(', 'dir', ',', 'i', ')', 'sclrp4', '=', ""'{}/sclr{}.p4'"", '.', 'format', '(', 'dir', ',', 'i', ')', 'fldsgz', '=', 'fldsp4', '+', ""'.gz'"", 'sclrgz', '=', 'sclrp4', '+', ""'.gz'"", 'if', 'gzip', '==', ""'guess'"", ':', 'fldsname', '=', 'fldsp4', 'if', 'os', '.', 'path', '.', 'exists', '(', 'fldsp4', ')', 'else', 'fldsgz', 'sclrname', '=', 'sclrp4', 'if', 'os', '.', 'path', '.', 'exists', '(', 'sclrp4', ')', 'else', 'sclrgz', 'else', ':', 'fldsname', '=', 'fldsgz', 'if', 'gzip', 'else', 'fldsp4', 'sclrname', '=', 'sclrgz', 'if', 'gzip', 'else', 'sclrp4', 'if', 'not', '(', 'flds', 'or', 'sclr', ')', ':', 'raise', 'ValueError', '(', '""Must specify flds or sclr to read.""', ')', 'elif', 'flds', 'is', 'not', 'None', 'and', 'sclr', 'is', 'not', 'None', ':', 'sd', ',', 'srt', '=', 'read', '(', 'sclrname', ',', 'var', '=', 'sclr', ',', 'first_sort', '=', 'True', ',', 'gzip', '=', ""'guess'"", ',', 'keep_xs', '=', 'keep_xs', ')', 'fd', '=', 'read', '(', 'fldsname', ',', 'var', '=', 'flds', ',', 'sort', '=', 'srt', ',', 'gzip', '=', ""'guess'"", ',', 'keep_xs', '=', 'keep_xs', ')', 'ret', '=', 'dict', '(', 'sd', '=', 'sd', ',', 'fd', '=', 'fd', ')', 'ret', '.', 'update', '(', '{', 'k', ':', 'sd', '[', 'k', ']', 'for', 'k', 'in', 'sd', '}', ')', 'ret', '.', 'update', '(', '{', 'k', ':', 'fd', '[', 'k', ']', 'for', 'k', 'in', 'fd', '}', ')', 'if', 'vector_norms', ':', 'ret', '.', 'update', '(', '{', 'k', ':', 'vector_norm', '(', 'ret', ',', 'k', ')', 'for', 'k', 'in', 'flds', '}', ')', 'if', 'gettime', ':', 'ret', '[', ""'t'"", ']', '=', 'get_header', '(', 'sclrname', ',', 'gzip', '=', ""'guess'"", ')', '[', ""'timestamp'"", ']', 'else', ':', 'if', 'flds', ':', 'var', '=', 'flds', 'name', '=', 'fldsname', 'else', ':', 'var', '=', 'sclr', 'name', '=', 'sclrname', 'ret', ',', '_', '=', 'read', '(', 'name', ',', 'var', '=', 'var', ',', 'first_sort', '=', 'True', ',', 'gzip', '=', ""'guess'"", ')', 'if', 'flds', 'and', 'vector_norms', ':', 'ret', '.', 'update', '(', '{', 'k', ':', 'vector_norm', '(', 'ret', ',', 'k', ')', 'for', 'k', 'in', 'flds', '}', ')', 'if', 'gettime', ':', 'ret', '[', ""'t'"", ']', '=', 'get_header', '(', 'name', ',', 'gzip', '=', ""'guess'"", ')', '[', ""'timestamp'"", ']', 'return', 'ret']","A smart indexing reader that reads files by names. Looks for files
    like ""<dir>/flds<i>.p4<compression>"" where dir and the index are
    passed, as well as stuff from sclrs and saves them into one
    dictionary. Essentially useful for reading by timestep instead
    of by file. Assumes that both the flds and sclr are in the same
    direction.

    Parameters:
    -----------

    i -- index of file

    Required Keywords:
    ------------------

    flds  -- list of var's to load from the flds file
    sclr  -- list of var's to load from the sclr file
    
    Either one or both are required.

    Keywords:
    ---------

    gzip         -- files are gzipped. If gzip is ""guess"",
                    find a matching file.
    dir          -- Directory to look for files. Default is .
    vector_norms -- save the norm of the flds vectors under the
                    the name of the quantity. Default is True.
    keep_xs      -- Keep the edges. By default, False.
    gettime      -- Get the timestamp.","['A', 'smart', 'indexing', 'reader', 'that', 'reads', 'files', 'by', 'names', '.', 'Looks', 'for', 'files', 'like', '<dir', '>', '/', 'flds<i', '>', '.', 'p4<compression', '>', 'where', 'dir', 'and', 'the', 'index', 'are', 'passed', 'as', 'well', 'as', 'stuff', 'from', 'sclrs', 'and', 'saves', 'them', 'into', 'one', 'dictionary', '.', 'Essentially', 'useful', 'for', 'reading', 'by', 'timestep', 'instead', 'of', 'by', 'file', '.', 'Assumes', 'that', 'both', 'the', 'flds', 'and', 'sclr', 'are', 'in', 'the', 'same', 'direction', '.']",python,R,1,True,1,train
7470,larsks/thecache,thecache/cache.py,https://github.com/larsks/thecache/blob/e535f91031a7f92f19b5ff6fe2a1a03c7680e9e0/thecache/cache.py#L201-L206,"def load_iter(self, key, chunksize=None, noexpire=None):
        '''Lookup an item in the cache and return an iterator
        that reads chunksize bytes of data at a time.  The underlying
        file will be closed when all data has been read'''
        return chunk_iterator(self.load_fd(key, noexpire=noexpire),
                              chunksize=chunksize)","['def', 'load_iter', '(', 'self', ',', 'key', ',', 'chunksize', '=', 'None', ',', 'noexpire', '=', 'None', ')', ':', 'return', 'chunk_iterator', '(', 'self', '.', 'load_fd', '(', 'key', ',', 'noexpire', '=', 'noexpire', ')', ',', 'chunksize', '=', 'chunksize', ')']","Lookup an item in the cache and return an iterator
        that reads chunksize bytes of data at a time.  The underlying
        file will be closed when all data has been read","['Lookup', 'an', 'item', 'in', 'the', 'cache', 'and', 'return', 'an', 'iterator', 'that', 'reads', 'chunksize', 'bytes', 'of', 'data', 'at', 'a', 'time', '.', 'The', 'underlying', 'file', 'will', 'be', 'closed', 'when', 'all', 'data', 'has', 'been', 'read']",python,R,1,True,1,train
8314,esterhui/pypu,pypu/service_flickr.py,https://github.com/esterhui/pypu/blob/cc3e259d59f024c2c4c0fbb9c8a1547e51de75ec/pypu/service_flickr.py#L259-L330,"def _update_config_sets(self,directory,files=None):
        """"""
        Loads set information from file and updates on flickr, 
        only reads first line. Format is comma separated eg.
        travel, 2010, South Africa, Pretoria
        If files is None, will update all files in DB, otherwise
        will only update files that are in the flickr DB and files list
        """"""
        if not self._connectToFlickr():
            print(""%s - Couldn't connect to flickr""%(directory))
            return False

        # Load sets from SET_FILE
        _sets=self._load_sets(directory)

        # Connect to flickr and get dicionary of photosets
        psets=self._getphotosets()

        db = self._loadDB(directory)

        # To create a set, one needs to pass it the primary
        # photo to use, let's open the DB and load the first
        # photo
        primary_pid=db[db.keys()[0]]['photoid']

        # Loop through all sets, create if it doesn't exist
        for myset in _sets:
            if myset not in psets:
                logger.info('set [%s] not in flickr sets, will create set'%(myset))
                self._createphotoset(myset,primary_pid)

        # Now reaload photosets from flickr
        psets=self._getphotosets()

        # --- Load DB of photos, and update them all with new tags
        for fn in db:
            # --- If file list provided, skip files not in the list
            if files and fn not in files:
                continue

            pid=db[fn]['photoid']

            # Get all the photosets this photo belongs to
            psets_for_photo=self._getphotosets_forphoto(pid)

            for myset in _sets:
                if myset in psets_for_photo:
                    logger.debug(""%s - Already in photoset [%s] - skipping""%(fn,myset))
                    continue
                logger.info(""%s [flickr] Adding to set [%s]"" %(fn,myset))
                psid=psets[myset]['id']
                logger.debug(""%s - Adding to photoset %s""%(fn,psid))
                resp=self.flickr.photosets_addPhoto(photoset_id=psid,photo_id=pid)
                if resp.attrib['stat']!='ok':
                    logger.error(""%s - flickr: photos_addPhoto failed with status: %s"",\
                            resp.attrib['stat']);
                    return False

            # Go through all sets flickr says this photo belongs to and
            # remove from those sets if they don't appear in SET_FILE
            for pset in psets_for_photo:
                if pset not in _sets:
                    psid=psets[pset]['id']
                    logger.info(""%s [flickr] Removing from set [%s]"" %(fn,pset))
                    logger.debug(""%s - Removing from photoset %s""%(fn,psid))
                    resp=self.flickr.photosets_removePhoto(photoset_id=psid,photo_id=pid)
                    if resp.attrib['stat']!='ok':
                        logger.error(""%s - flickr: photossets_removePhoto failed with status: %s"",\
                                resp.attrib['stat']);
                        return False

        return True","['def', '_update_config_sets', '(', 'self', ',', 'directory', ',', 'files', '=', 'None', ')', ':', 'if', 'not', 'self', '.', '_connectToFlickr', '(', ')', ':', 'print', '(', '""%s - Couldn\'t connect to flickr""', '%', '(', 'directory', ')', ')', 'return', 'False', '# Load sets from SET_FILE', '_sets', '=', 'self', '.', '_load_sets', '(', 'directory', ')', '# Connect to flickr and get dicionary of photosets', 'psets', '=', 'self', '.', '_getphotosets', '(', ')', 'db', '=', 'self', '.', '_loadDB', '(', 'directory', ')', '# To create a set, one needs to pass it the primary', ""# photo to use, let's open the DB and load the first"", '# photo', 'primary_pid', '=', 'db', '[', 'db', '.', 'keys', '(', ')', '[', '0', ']', ']', '[', ""'photoid'"", ']', ""# Loop through all sets, create if it doesn't exist"", 'for', 'myset', 'in', '_sets', ':', 'if', 'myset', 'not', 'in', 'psets', ':', 'logger', '.', 'info', '(', ""'set [%s] not in flickr sets, will create set'"", '%', '(', 'myset', ')', ')', 'self', '.', '_createphotoset', '(', 'myset', ',', 'primary_pid', ')', '# Now reaload photosets from flickr', 'psets', '=', 'self', '.', '_getphotosets', '(', ')', '# --- Load DB of photos, and update them all with new tags', 'for', 'fn', 'in', 'db', ':', '# --- If file list provided, skip files not in the list', 'if', 'files', 'and', 'fn', 'not', 'in', 'files', ':', 'continue', 'pid', '=', 'db', '[', 'fn', ']', '[', ""'photoid'"", ']', '# Get all the photosets this photo belongs to', 'psets_for_photo', '=', 'self', '.', '_getphotosets_forphoto', '(', 'pid', ')', 'for', 'myset', 'in', '_sets', ':', 'if', 'myset', 'in', 'psets_for_photo', ':', 'logger', '.', 'debug', '(', '""%s - Already in photoset [%s] - skipping""', '%', '(', 'fn', ',', 'myset', ')', ')', 'continue', 'logger', '.', 'info', '(', '""%s [flickr] Adding to set [%s]""', '%', '(', 'fn', ',', 'myset', ')', ')', 'psid', '=', 'psets', '[', 'myset', ']', '[', ""'id'"", ']', 'logger', '.', 'debug', '(', '""%s - Adding to photoset %s""', '%', '(', 'fn', ',', 'psid', ')', ')', 'resp', '=', 'self', '.', 'flickr', '.', 'photosets_addPhoto', '(', 'photoset_id', '=', 'psid', ',', 'photo_id', '=', 'pid', ')', 'if', 'resp', '.', 'attrib', '[', ""'stat'"", ']', '!=', ""'ok'"", ':', 'logger', '.', 'error', '(', '""%s - flickr: photos_addPhoto failed with status: %s""', ',', 'resp', '.', 'attrib', '[', ""'stat'"", ']', ')', 'return', 'False', '# Go through all sets flickr says this photo belongs to and', ""# remove from those sets if they don't appear in SET_FILE"", 'for', 'pset', 'in', 'psets_for_photo', ':', 'if', 'pset', 'not', 'in', '_sets', ':', 'psid', '=', 'psets', '[', 'pset', ']', '[', ""'id'"", ']', 'logger', '.', 'info', '(', '""%s [flickr] Removing from set [%s]""', '%', '(', 'fn', ',', 'pset', ')', ')', 'logger', '.', 'debug', '(', '""%s - Removing from photoset %s""', '%', '(', 'fn', ',', 'psid', ')', ')', 'resp', '=', 'self', '.', 'flickr', '.', 'photosets_removePhoto', '(', 'photoset_id', '=', 'psid', ',', 'photo_id', '=', 'pid', ')', 'if', 'resp', '.', 'attrib', '[', ""'stat'"", ']', '!=', ""'ok'"", ':', 'logger', '.', 'error', '(', '""%s - flickr: photossets_removePhoto failed with status: %s""', ',', 'resp', '.', 'attrib', '[', ""'stat'"", ']', ')', 'return', 'False', 'return', 'True']","Loads set information from file and updates on flickr, 
        only reads first line. Format is comma separated eg.
        travel, 2010, South Africa, Pretoria
        If files is None, will update all files in DB, otherwise
        will only update files that are in the flickr DB and files list","['Loads', 'set', 'information', 'from', 'file', 'and', 'updates', 'on', 'flickr', 'only', 'reads', 'first', 'line', '.', 'Format', 'is', 'comma', 'separated', 'eg', '.', 'travel', '2010', 'South', 'Africa', 'Pretoria', 'If', 'files', 'is', 'None', 'will', 'update', 'all', 'files', 'in', 'DB', 'otherwise', 'will', 'only', 'update', 'files', 'that', 'are', 'in', 'the', 'flickr', 'DB', 'and', 'files', 'list']",python,R,1,True,1,train
8319,esterhui/pypu,pypu/service_flickr.py,https://github.com/esterhui/pypu/blob/cc3e259d59f024c2c4c0fbb9c8a1547e51de75ec/pypu/service_flickr.py#L469-L502,"def _update_config_tags(self,directory,files=None):
        """"""
        Loads tags information from file and updates on flickr, 
        only reads first line. Format is comma separated eg.
        travel, 2010, South Africa, Pretoria
        If files is None, will update all files in DB, otherwise
        will only update files that are in the flickr DB and files list
        """"""
        if not self._connectToFlickr():
            print(""%s - Couldn't connect to flickr""%(directory))
            return False

        logger.debug(""Updating tags in %s""%(directory))

        _tags=self._load_tags(directory)

        # --- Load DB of photos, and update them all with new tags
        db = self._loadDB(directory)
        for fn in db:
            # --- If file list provided, skip files not in the list
            if files and fn not in files:
                logger.debug('%s [flickr] Skipping, tag update',fn)
                continue

            logger.info(""%s [flickr] Updating tags [%s]"" %(fn,_tags))
            pid=db[fn]['photoid']
            resp=self.flickr.photos_setTags(photo_id=pid,tags=_tags)
            if resp.attrib['stat']!='ok':
                logger.error(""%s - flickr: photos_setTags failed with status: %s"",\
                        resp.attrib['stat']);
                return False
            else:
                return True
        return False","['def', '_update_config_tags', '(', 'self', ',', 'directory', ',', 'files', '=', 'None', ')', ':', 'if', 'not', 'self', '.', '_connectToFlickr', '(', ')', ':', 'print', '(', '""%s - Couldn\'t connect to flickr""', '%', '(', 'directory', ')', ')', 'return', 'False', 'logger', '.', 'debug', '(', '""Updating tags in %s""', '%', '(', 'directory', ')', ')', '_tags', '=', 'self', '.', '_load_tags', '(', 'directory', ')', '# --- Load DB of photos, and update them all with new tags', 'db', '=', 'self', '.', '_loadDB', '(', 'directory', ')', 'for', 'fn', 'in', 'db', ':', '# --- If file list provided, skip files not in the list', 'if', 'files', 'and', 'fn', 'not', 'in', 'files', ':', 'logger', '.', 'debug', '(', ""'%s [flickr] Skipping, tag update'"", ',', 'fn', ')', 'continue', 'logger', '.', 'info', '(', '""%s [flickr] Updating tags [%s]""', '%', '(', 'fn', ',', '_tags', ')', ')', 'pid', '=', 'db', '[', 'fn', ']', '[', ""'photoid'"", ']', 'resp', '=', 'self', '.', 'flickr', '.', 'photos_setTags', '(', 'photo_id', '=', 'pid', ',', 'tags', '=', '_tags', ')', 'if', 'resp', '.', 'attrib', '[', ""'stat'"", ']', '!=', ""'ok'"", ':', 'logger', '.', 'error', '(', '""%s - flickr: photos_setTags failed with status: %s""', ',', 'resp', '.', 'attrib', '[', ""'stat'"", ']', ')', 'return', 'False', 'else', ':', 'return', 'True', 'return', 'False']","Loads tags information from file and updates on flickr, 
        only reads first line. Format is comma separated eg.
        travel, 2010, South Africa, Pretoria
        If files is None, will update all files in DB, otherwise
        will only update files that are in the flickr DB and files list","['Loads', 'tags', 'information', 'from', 'file', 'and', 'updates', 'on', 'flickr', 'only', 'reads', 'first', 'line', '.', 'Format', 'is', 'comma', 'separated', 'eg', '.', 'travel', '2010', 'South', 'Africa', 'Pretoria', 'If', 'files', 'is', 'None', 'will', 'update', 'all', 'files', 'in', 'DB', 'otherwise', 'will', 'only', 'update', 'files', 'that', 'are', 'in', 'the', 'flickr', 'DB', 'and', 'files', 'list']",python,R,1,True,1,train
18689,KnowledgeLinks/rdfframework,rdfframework/search/elasticsearchbase.py,https://github.com/KnowledgeLinks/rdfframework/blob/9ec32dcc4bed51650a4b392cc5c15100fef7923a/rdfframework/search/elasticsearchbase.py#L313-L353,"def _calc_result(self, results, calc):
        """""" parses the calc string and then reads the results and returns
        the new value Elasticsearch no longer allow dynamic in scripting

        args:
            results: the list of results from Elasticsearch
            calc: the calculation sting

        returns:
            refomated results list with calculation added to the '__calc' field
            in _source

        examples:
            concatenation: use + field_names and double quotes to add text
                fld1 +"", "" + fld2 = ""fld1, fld2""
        """"""
        lg = logging.getLogger(""%s.%s"" % (self.ln, inspect.stack()[0][3]))
        lg.setLevel(self.log_level)
        # if the calculation is empty exit
        if calc is None:
            return results
        lg.debug(""calc %s"", calc)
        # perform concatenation
        hits = results.get('hits',{}).get('hits',[])
        for item in hits:
            lg.debug(""\n*** item:\n%s"", pp.pformat(item))
            if ""+"" in calc:
                calc_parts = calc.split(""+"")
                calc_str = """"
                for i, part in enumerate(calc_parts):
                    if '""' in part:
                        calc_parts[i] = part.replace('""','')
                    else:
                        if part.startswith(""_""):
                            calc_parts[i] = item.get(part)
                        else:
                            calc_parts[i] = Dot(item['_source']).get(part)
                lg.debug("" calc result: %s"", """".join(calc_parts))
                item['_source']['__calc'] = """".join(calc_parts)
        lg.debug(""calc %s"", calc)
        return results","['def', '_calc_result', '(', 'self', ',', 'results', ',', 'calc', ')', ':', 'lg', '=', 'logging', '.', 'getLogger', '(', '""%s.%s""', '%', '(', 'self', '.', 'ln', ',', 'inspect', '.', 'stack', '(', ')', '[', '0', ']', '[', '3', ']', ')', ')', 'lg', '.', 'setLevel', '(', 'self', '.', 'log_level', ')', '# if the calculation is empty exit\r', 'if', 'calc', 'is', 'None', ':', 'return', 'results', 'lg', '.', 'debug', '(', '""calc %s""', ',', 'calc', ')', '# perform concatenation\r', 'hits', '=', 'results', '.', 'get', '(', ""'hits'"", ',', '{', '}', ')', '.', 'get', '(', ""'hits'"", ',', '[', ']', ')', 'for', 'item', 'in', 'hits', ':', 'lg', '.', 'debug', '(', '""\\n*** item:\\n%s""', ',', 'pp', '.', 'pformat', '(', 'item', ')', ')', 'if', '""+""', 'in', 'calc', ':', 'calc_parts', '=', 'calc', '.', 'split', '(', '""+""', ')', 'calc_str', '=', '""""', 'for', 'i', ',', 'part', 'in', 'enumerate', '(', 'calc_parts', ')', ':', 'if', '\'""\'', 'in', 'part', ':', 'calc_parts', '[', 'i', ']', '=', 'part', '.', 'replace', '(', '\'""\'', ',', ""''"", ')', 'else', ':', 'if', 'part', '.', 'startswith', '(', '""_""', ')', ':', 'calc_parts', '[', 'i', ']', '=', 'item', '.', 'get', '(', 'part', ')', 'else', ':', 'calc_parts', '[', 'i', ']', '=', 'Dot', '(', 'item', '[', ""'_source'"", ']', ')', '.', 'get', '(', 'part', ')', 'lg', '.', 'debug', '(', '"" calc result: %s""', ',', '""""', '.', 'join', '(', 'calc_parts', ')', ')', 'item', '[', ""'_source'"", ']', '[', ""'__calc'"", ']', '=', '""""', '.', 'join', '(', 'calc_parts', ')', 'lg', '.', 'debug', '(', '""calc %s""', ',', 'calc', ')', 'return', 'results']","parses the calc string and then reads the results and returns
        the new value Elasticsearch no longer allow dynamic in scripting

        args:
            results: the list of results from Elasticsearch
            calc: the calculation sting

        returns:
            refomated results list with calculation added to the '__calc' field
            in _source

        examples:
            concatenation: use + field_names and double quotes to add text
                fld1 +"", "" + fld2 = ""fld1, fld2""","['parses', 'the', 'calc', 'string', 'and', 'then', 'reads', 'the', 'results', 'and', 'returns', 'the', 'new', 'value', 'Elasticsearch', 'no', 'longer', 'allow', 'dynamic', 'in', 'scripting', 'args', ':', 'results', ':', 'the', 'list', 'of', 'results', 'from', 'Elasticsearch', 'calc', ':', 'the', 'calculation', 'sting', 'returns', ':', 'refomated', 'results', 'list', 'with', 'calculation', 'added', 'to', 'the', '__calc', 'field', 'in', '_source', 'examples', ':', 'concatenation', ':', 'use', '+', 'field_names', 'and', 'double', 'quotes', 'to', 'add', 'text', 'fld1', '+', '+', 'fld2', '=', 'fld1', 'fld2']",python,R,1,True,1,train
1947,istresearch/scrapy-cluster,kafka-monitor/kafka_monitor.py,https://github.com/istresearch/scrapy-cluster/blob/13aaed2349af5d792d6bcbfcadc5563158aeb599/kafka-monitor/kafka_monitor.py#L247-L264,"def _main_loop(self):
        '''
        Continuous loop that reads from a kafka topic and tries to validate
        incoming messages
        '''
        self.logger.debug(""Processing messages"")
        old_time = 0
        while True:
            self._process_messages()
            if self.settings['STATS_DUMP'] != 0:
                new_time = int(old_div(time.time(), self.settings['STATS_DUMP']))
                # only log every X seconds
                if new_time != old_time:
                    self._dump_stats()
                    old_time = new_time

            self._report_self()
            time.sleep(self.settings['SLEEP_TIME'])","['def', '_main_loop', '(', 'self', ')', ':', 'self', '.', 'logger', '.', 'debug', '(', '""Processing messages""', ')', 'old_time', '=', '0', 'while', 'True', ':', 'self', '.', '_process_messages', '(', ')', 'if', 'self', '.', 'settings', '[', ""'STATS_DUMP'"", ']', '!=', '0', ':', 'new_time', '=', 'int', '(', 'old_div', '(', 'time', '.', 'time', '(', ')', ',', 'self', '.', 'settings', '[', ""'STATS_DUMP'"", ']', ')', ')', '# only log every X seconds', 'if', 'new_time', '!=', 'old_time', ':', 'self', '.', '_dump_stats', '(', ')', 'old_time', '=', 'new_time', 'self', '.', '_report_self', '(', ')', 'time', '.', 'sleep', '(', 'self', '.', 'settings', '[', ""'SLEEP_TIME'"", ']', ')']","Continuous loop that reads from a kafka topic and tries to validate
        incoming messages","['Continuous', 'loop', 'that', 'reads', 'from', 'a', 'kafka', 'topic', 'and', 'tries', 'to', 'validate', 'incoming', 'messages']",python,R,1,True,1,train
4241,splunk/splunk-sdk-python,splunklib/data.py,https://github.com/splunk/splunk-sdk-python/blob/a245a4eeb93b3621730418008e31715912bcdcd8/splunklib/data.py#L61-L93,"def load(text, match=None):
    """"""This function reads a string that contains the XML of an Atom Feed, then 
    returns the 
    data in a native Python structure (a ``dict`` or ``list``). If you also 
    provide a tag name or path to match, only the matching sub-elements are 
    loaded.

    :param text: The XML text to load.
    :type text: ``string``
    :param match: A tag name or path to match (optional).
    :type match: ``string``
    """"""
    if text is None: return None
    text = text.strip()
    if len(text) == 0: return None
    nametable = {
        'namespaces': [],
        'names': {}
    }

    # Convert to unicode encoding in only python 2 for xml parser
    if(sys.version_info < (3, 0, 0) and isinstance(text, unicode)):
        text = text.encode('utf-8')

    root = XML(text)
    items = [root] if match is None else root.findall(match)
    count = len(items)
    if count == 0: 
        return None
    elif count == 1: 
        return load_root(items[0], nametable)
    else:
        return [load_root(item, nametable) for item in items]","['def', 'load', '(', 'text', ',', 'match', '=', 'None', ')', ':', 'if', 'text', 'is', 'None', ':', 'return', 'None', 'text', '=', 'text', '.', 'strip', '(', ')', 'if', 'len', '(', 'text', ')', '==', '0', ':', 'return', 'None', 'nametable', '=', '{', ""'namespaces'"", ':', '[', ']', ',', ""'names'"", ':', '{', '}', '}', '# Convert to unicode encoding in only python 2 for xml parser', 'if', '(', 'sys', '.', 'version_info', '<', '(', '3', ',', '0', ',', '0', ')', 'and', 'isinstance', '(', 'text', ',', 'unicode', ')', ')', ':', 'text', '=', 'text', '.', 'encode', '(', ""'utf-8'"", ')', 'root', '=', 'XML', '(', 'text', ')', 'items', '=', '[', 'root', ']', 'if', 'match', 'is', 'None', 'else', 'root', '.', 'findall', '(', 'match', ')', 'count', '=', 'len', '(', 'items', ')', 'if', 'count', '==', '0', ':', 'return', 'None', 'elif', 'count', '==', '1', ':', 'return', 'load_root', '(', 'items', '[', '0', ']', ',', 'nametable', ')', 'else', ':', 'return', '[', 'load_root', '(', 'item', ',', 'nametable', ')', 'for', 'item', 'in', 'items', ']']","This function reads a string that contains the XML of an Atom Feed, then 
    returns the 
    data in a native Python structure (a ``dict`` or ``list``). If you also 
    provide a tag name or path to match, only the matching sub-elements are 
    loaded.

    :param text: The XML text to load.
    :type text: ``string``
    :param match: A tag name or path to match (optional).
    :type match: ``string``","['This', 'function', 'reads', 'a', 'string', 'that', 'contains', 'the', 'XML', 'of', 'an', 'Atom', 'Feed', 'then', 'returns', 'the', 'data', 'in', 'a', 'native', 'Python', 'structure', '(', 'a', 'dict', 'or', 'list', ')', '.', 'If', 'you', 'also', 'provide', 'a', 'tag', 'name', 'or', 'path', 'to', 'match', 'only', 'the', 'matching', 'sub', '-', 'elements', 'are', 'loaded', '.']",python,R,1,True,1,train
5817,bcbio/bcbio-nextgen,bcbio/bam/fastq.py,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/bam/fastq.py#L224-L276,"def downsample(f1, f2, data, N, quick=False):
    """""" get N random headers from a fastq file without reading the
    whole thing into memory
    modified from: http://www.biostars.org/p/6544/
    quick=True will just grab the first N reads rather than do a true
    downsampling
    """"""
    if quick:
        rand_records = range(N)
    else:
        records = sum(1 for _ in open(f1)) / 4
        N = records if N > records else N
        rand_records = sorted(random.sample(xrange(records), N))

    fh1 = open_possible_gzip(f1)
    fh2 = open_possible_gzip(f2) if f2 else None
    outf1 = os.path.splitext(f1)[0] + "".subset"" + os.path.splitext(f1)[1]
    outf2 = os.path.splitext(f2)[0] + "".subset"" + os.path.splitext(f2)[1] if f2 else None

    if utils.file_exists(outf1):
        if not outf2:
            return outf1, outf2
        elif utils.file_exists(outf2):
            return outf1, outf2

    out_files = (outf1, outf2) if outf2 else (outf1)

    with file_transaction(out_files) as tx_out_files:
        if isinstance(tx_out_files, six.string_types):
            tx_out_f1 = tx_out_files
        else:
            tx_out_f1, tx_out_f2 = tx_out_files
        sub1 = open_possible_gzip(tx_out_f1, ""w"")
        sub2 = open_possible_gzip(tx_out_f2, ""w"") if outf2 else None
        rec_no = - 1
        for rr in rand_records:
            while rec_no < rr:
                rec_no += 1
                for i in range(4): fh1.readline()
                if fh2:
                    for i in range(4): fh2.readline()
            for i in range(4):
                sub1.write(fh1.readline())
                if sub2:
                    sub2.write(fh2.readline())
            rec_no += 1
        fh1.close()
        sub1.close()
        if f2:
            fh2.close()
            sub2.close()

    return outf1, outf2","['def', 'downsample', '(', 'f1', ',', 'f2', ',', 'data', ',', 'N', ',', 'quick', '=', 'False', ')', ':', 'if', 'quick', ':', 'rand_records', '=', 'range', '(', 'N', ')', 'else', ':', 'records', '=', 'sum', '(', '1', 'for', '_', 'in', 'open', '(', 'f1', ')', ')', '/', '4', 'N', '=', 'records', 'if', 'N', '>', 'records', 'else', 'N', 'rand_records', '=', 'sorted', '(', 'random', '.', 'sample', '(', 'xrange', '(', 'records', ')', ',', 'N', ')', ')', 'fh1', '=', 'open_possible_gzip', '(', 'f1', ')', 'fh2', '=', 'open_possible_gzip', '(', 'f2', ')', 'if', 'f2', 'else', 'None', 'outf1', '=', 'os', '.', 'path', '.', 'splitext', '(', 'f1', ')', '[', '0', ']', '+', '"".subset""', '+', 'os', '.', 'path', '.', 'splitext', '(', 'f1', ')', '[', '1', ']', 'outf2', '=', 'os', '.', 'path', '.', 'splitext', '(', 'f2', ')', '[', '0', ']', '+', '"".subset""', '+', 'os', '.', 'path', '.', 'splitext', '(', 'f2', ')', '[', '1', ']', 'if', 'f2', 'else', 'None', 'if', 'utils', '.', 'file_exists', '(', 'outf1', ')', ':', 'if', 'not', 'outf2', ':', 'return', 'outf1', ',', 'outf2', 'elif', 'utils', '.', 'file_exists', '(', 'outf2', ')', ':', 'return', 'outf1', ',', 'outf2', 'out_files', '=', '(', 'outf1', ',', 'outf2', ')', 'if', 'outf2', 'else', '(', 'outf1', ')', 'with', 'file_transaction', '(', 'out_files', ')', 'as', 'tx_out_files', ':', 'if', 'isinstance', '(', 'tx_out_files', ',', 'six', '.', 'string_types', ')', ':', 'tx_out_f1', '=', 'tx_out_files', 'else', ':', 'tx_out_f1', ',', 'tx_out_f2', '=', 'tx_out_files', 'sub1', '=', 'open_possible_gzip', '(', 'tx_out_f1', ',', '""w""', ')', 'sub2', '=', 'open_possible_gzip', '(', 'tx_out_f2', ',', '""w""', ')', 'if', 'outf2', 'else', 'None', 'rec_no', '=', '-', '1', 'for', 'rr', 'in', 'rand_records', ':', 'while', 'rec_no', '<', 'rr', ':', 'rec_no', '+=', '1', 'for', 'i', 'in', 'range', '(', '4', ')', ':', 'fh1', '.', 'readline', '(', ')', 'if', 'fh2', ':', 'for', 'i', 'in', 'range', '(', '4', ')', ':', 'fh2', '.', 'readline', '(', ')', 'for', 'i', 'in', 'range', '(', '4', ')', ':', 'sub1', '.', 'write', '(', 'fh1', '.', 'readline', '(', ')', ')', 'if', 'sub2', ':', 'sub2', '.', 'write', '(', 'fh2', '.', 'readline', '(', ')', ')', 'rec_no', '+=', '1', 'fh1', '.', 'close', '(', ')', 'sub1', '.', 'close', '(', ')', 'if', 'f2', ':', 'fh2', '.', 'close', '(', ')', 'sub2', '.', 'close', '(', ')', 'return', 'outf1', ',', 'outf2']","get N random headers from a fastq file without reading the
    whole thing into memory
    modified from: http://www.biostars.org/p/6544/
    quick=True will just grab the first N reads rather than do a true
    downsampling","['get', 'N', 'random', 'headers', 'from', 'a', 'fastq', 'file', 'without', 'reading', 'the', 'whole', 'thing', 'into', 'memory', 'modified', 'from', ':', 'http', ':', '//', 'www', '.', 'biostars', '.', 'org', '/', 'p', '/', '6544', '/', 'quick', '=', 'True', 'will', 'just', 'grab', 'the', 'first', 'N', 'reads', 'rather', 'than', 'do', 'a', 'true', 'downsampling']",python,R,1,True,1,train
5835,bcbio/bcbio-nextgen,bcbio/ngsalign/bwa.py,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/ngsalign/bwa.py#L117-L133,"def _can_use_mem(fastq_file, data, read_min_size=None):
    """"""bwa-mem handle longer (> 70bp) reads with improved piping.
    Randomly samples 5000 reads from the first two million.
    Default to no piping if more than 75% of the sampled reads are small.
    If we've previously calculated minimum read sizes (from rtg SDF output)
    we can skip the formal check.
    """"""
    min_size = 70
    if read_min_size and read_min_size >= min_size:
        return True
    thresh = 0.75
    tocheck = 5000
    shorter = 0
    for count, size in fastq_size_output(fastq_file, tocheck):
        if int(size) < min_size:
            shorter += int(count)
    return (float(shorter) / float(tocheck)) <= thresh","['def', '_can_use_mem', '(', 'fastq_file', ',', 'data', ',', 'read_min_size', '=', 'None', ')', ':', 'min_size', '=', '70', 'if', 'read_min_size', 'and', 'read_min_size', '>=', 'min_size', ':', 'return', 'True', 'thresh', '=', '0.75', 'tocheck', '=', '5000', 'shorter', '=', '0', 'for', 'count', ',', 'size', 'in', 'fastq_size_output', '(', 'fastq_file', ',', 'tocheck', ')', ':', 'if', 'int', '(', 'size', ')', '<', 'min_size', ':', 'shorter', '+=', 'int', '(', 'count', ')', 'return', '(', 'float', '(', 'shorter', ')', '/', 'float', '(', 'tocheck', ')', ')', '<=', 'thresh']","bwa-mem handle longer (> 70bp) reads with improved piping.
    Randomly samples 5000 reads from the first two million.
    Default to no piping if more than 75% of the sampled reads are small.
    If we've previously calculated minimum read sizes (from rtg SDF output)
    we can skip the formal check.","['bwa', '-', 'mem', 'handle', 'longer', '(', '>', '70bp', ')', 'reads', 'with', 'improved', 'piping', '.', 'Randomly', 'samples', '5000', 'reads', 'from', 'the', 'first', 'two', 'million', '.', 'Default', 'to', 'no', 'piping', 'if', 'more', 'than', '75%', 'of', 'the', 'sampled', 'reads', 'are', 'small', '.', 'If', 'we', 've', 'previously', 'calculated', 'minimum', 'read', 'sizes', '(', 'from', 'rtg', 'SDF', 'output', ')', 'we', 'can', 'skip', 'the', 'formal', 'check', '.']",python,R,1,True,1,train
6501,bcbio/bcbio-nextgen,bcbio/illumina/machine.py,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/illumina/machine.py#L149-L158,"def _expected_reads(run_info_file):
    """"""Parse the number of expected reads from the RunInfo.xml file.
    """"""
    reads = []
    if os.path.exists(run_info_file):
        tree = ElementTree()
        tree.parse(run_info_file)
        read_elem = tree.find(""Run/Reads"")
        reads = read_elem.findall(""Read"")
    return len(reads)","['def', '_expected_reads', '(', 'run_info_file', ')', ':', 'reads', '=', '[', ']', 'if', 'os', '.', 'path', '.', 'exists', '(', 'run_info_file', ')', ':', 'tree', '=', 'ElementTree', '(', ')', 'tree', '.', 'parse', '(', 'run_info_file', ')', 'read_elem', '=', 'tree', '.', 'find', '(', '""Run/Reads""', ')', 'reads', '=', 'read_elem', '.', 'findall', '(', '""Read""', ')', 'return', 'len', '(', 'reads', ')']",Parse the number of expected reads from the RunInfo.xml file.,"['Parse', 'the', 'number', 'of', 'expected', 'reads', 'from', 'the', 'RunInfo', '.', 'xml', 'file', '.']",python,R,1,True,1,train
6703,bcbio/bcbio-nextgen,bcbio/ngsalign/alignprep.py,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/ngsalign/alignprep.py#L281-L292,"def total_reads_from_grabix(in_file):
    """"""Retrieve total reads in a fastq file from grabix index.
    """"""
    gbi_file = _get_grabix_index(in_file)
    if gbi_file:
        with open(gbi_file) as in_handle:
            next(in_handle)  # throw away
            num_lines = int(next(in_handle).strip())
        assert num_lines % 4 == 0, ""Expected lines to be multiple of 4""
        return num_lines // 4
    else:
        return 0","['def', 'total_reads_from_grabix', '(', 'in_file', ')', ':', 'gbi_file', '=', '_get_grabix_index', '(', 'in_file', ')', 'if', 'gbi_file', ':', 'with', 'open', '(', 'gbi_file', ')', 'as', 'in_handle', ':', 'next', '(', 'in_handle', ')', '# throw away', 'num_lines', '=', 'int', '(', 'next', '(', 'in_handle', ')', '.', 'strip', '(', ')', ')', 'assert', 'num_lines', '%', '4', '==', '0', ',', '""Expected lines to be multiple of 4""', 'return', 'num_lines', '//', '4', 'else', ':', 'return', '0']",Retrieve total reads in a fastq file from grabix index.,"['Retrieve', 'total', 'reads', 'in', 'a', 'fastq', 'file', 'from', 'grabix', 'index', '.']",python,R,1,True,1,train
6880,bcbio/bcbio-nextgen,bcbio/variation/bamprep.py,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/variation/bamprep.py#L23-L34,"def _gatk_extract_reads_cl(data, region, prep_params, tmp_dir):
    """"""Use GATK to extract reads from full BAM file.
    """"""
    args = [""PrintReads"",
            ""-L"", region_to_gatk(region),
            ""-R"", dd.get_ref_file(data),
            ""-I"", data[""work_bam""]]
    # GATK3 back compatibility, need to specify analysis type
    if ""gatk4"" in dd.get_tools_off(data):
        args = [""--analysis_type""] + args
    runner = broad.runner_from_config(data[""config""])
    return runner.cl_gatk(args, tmp_dir)","['def', '_gatk_extract_reads_cl', '(', 'data', ',', 'region', ',', 'prep_params', ',', 'tmp_dir', ')', ':', 'args', '=', '[', '""PrintReads""', ',', '""-L""', ',', 'region_to_gatk', '(', 'region', ')', ',', '""-R""', ',', 'dd', '.', 'get_ref_file', '(', 'data', ')', ',', '""-I""', ',', 'data', '[', '""work_bam""', ']', ']', '# GATK3 back compatibility, need to specify analysis type', 'if', '""gatk4""', 'in', 'dd', '.', 'get_tools_off', '(', 'data', ')', ':', 'args', '=', '[', '""--analysis_type""', ']', '+', 'args', 'runner', '=', 'broad', '.', 'runner_from_config', '(', 'data', '[', '""config""', ']', ')', 'return', 'runner', '.', 'cl_gatk', '(', 'args', ',', 'tmp_dir', ')']",Use GATK to extract reads from full BAM file.,"['Use', 'GATK', 'to', 'extract', 'reads', 'from', 'full', 'BAM', 'file', '.']",python,R,1,True,1,train
12784,atztogo/phonopy,phonopy/interface/siesta.py,https://github.com/atztogo/phonopy/blob/869cc2ba9e7d495d5f4cf6942415ab3fc9e2a10f/phonopy/interface/siesta.py#L147-L207,"def _collect(self, lines):
        """""" This routine reads the following from the Siesta file:
            - atomic positions
            - cell_parameters
            - atomic_species
        """"""
        for tag,value,unit in re.findall(
                '([\.A-Za-z]+)\s+%s\s+([A-Za-z]+)?' %
                self._num_regex,lines):
            tag = tag.lower()
            unit = unit.lower()
            if tag == ""latticeconstant"":
                self._tags['latticeconstantunit'] = unit.capitalize()
                if unit == 'ang':
                    self._tags[tag] = float(value) / Bohr
                elif unit == 'bohr':
                    self._tags[tag] = float(value)
                else:
                    raise ValueError('Unknown LatticeConstant unit: {}'.format(unit))

        for tag,value in re.findall('([\.A-Za-z]+)[ \t]+([a-zA-Z]+)',lines):
            tag = tag.replace('_','').lower()
            if tag == ""atomiccoordinatesformat"":
                self._tags[tag] = value.strip().lower() 

        #check if the necessary tags are present
        self.check_present('atomiccoordinatesformat')
        acell = self._tags['latticeconstant']

        #capture the blocks
        blocks = re.findall(
            '%block\s+([A-Za-z_]+)\s((?:.+\n)+?(?=(?:\s+)?%endblock))',
            lines, re.MULTILINE)
        for tag,block in blocks:
            tag = tag.replace('_','').lower()
            if   tag == ""chemicalspecieslabel"":
                lines = block.split('\n')[:-1]
                self._tags[""atomicnumbers""] = dict([map(int,species.split()[:2])
                                                    for species in lines])
                self._tags[tag] = dict(
                    [(lambda x: (x[2],int(x[0])))(species.split())
                     for species in lines])
            elif tag == ""latticevectors"":
                self._tags[tag] = [[ float(v)*acell for v in vector.split()]
                                   for vector in block.split('\n')[:3]]
            elif tag == ""atomiccoordinatesandatomicspecies"":
                lines = block.split('\n')[:-1]
                self._tags[""atomiccoordinates""] = [
                    [float(x)  for x in atom.split()[:3]] for atom in lines]
                self._tags[""atomicspecies""] = [int(atom.split()[3])
                                               for atom in lines]
       
        #check if the block are present
        self.check_present(""atomicspecies"")
        self.check_present(""atomiccoordinates"")
        self.check_present(""latticevectors"")
        self.check_present(""chemicalspecieslabel"")
            
        #translate the atomicspecies to atomic numbers
        self._tags[""atomicnumbers""] = [self._tags[""atomicnumbers""][atype]
                                       for atype in self._tags[""atomicspecies""]]","['def', '_collect', '(', 'self', ',', 'lines', ')', ':', 'for', 'tag', ',', 'value', ',', 'unit', 'in', 're', '.', 'findall', '(', ""'([\\.A-Za-z]+)\\s+%s\\s+([A-Za-z]+)?'"", '%', 'self', '.', '_num_regex', ',', 'lines', ')', ':', 'tag', '=', 'tag', '.', 'lower', '(', ')', 'unit', '=', 'unit', '.', 'lower', '(', ')', 'if', 'tag', '==', '""latticeconstant""', ':', 'self', '.', '_tags', '[', ""'latticeconstantunit'"", ']', '=', 'unit', '.', 'capitalize', '(', ')', 'if', 'unit', '==', ""'ang'"", ':', 'self', '.', '_tags', '[', 'tag', ']', '=', 'float', '(', 'value', ')', '/', 'Bohr', 'elif', 'unit', '==', ""'bohr'"", ':', 'self', '.', '_tags', '[', 'tag', ']', '=', 'float', '(', 'value', ')', 'else', ':', 'raise', 'ValueError', '(', ""'Unknown LatticeConstant unit: {}'"", '.', 'format', '(', 'unit', ')', ')', 'for', 'tag', ',', 'value', 'in', 're', '.', 'findall', '(', ""'([\\.A-Za-z]+)[ \\t]+([a-zA-Z]+)'"", ',', 'lines', ')', ':', 'tag', '=', 'tag', '.', 'replace', '(', ""'_'"", ',', ""''"", ')', '.', 'lower', '(', ')', 'if', 'tag', '==', '""atomiccoordinatesformat""', ':', 'self', '.', '_tags', '[', 'tag', ']', '=', 'value', '.', 'strip', '(', ')', '.', 'lower', '(', ')', '#check if the necessary tags are present', 'self', '.', 'check_present', '(', ""'atomiccoordinatesformat'"", ')', 'acell', '=', 'self', '.', '_tags', '[', ""'latticeconstant'"", ']', '#capture the blocks', 'blocks', '=', 're', '.', 'findall', '(', ""'%block\\s+([A-Za-z_]+)\\s((?:.+\\n)+?(?=(?:\\s+)?%endblock))'"", ',', 'lines', ',', 're', '.', 'MULTILINE', ')', 'for', 'tag', ',', 'block', 'in', 'blocks', ':', 'tag', '=', 'tag', '.', 'replace', '(', ""'_'"", ',', ""''"", ')', '.', 'lower', '(', ')', 'if', 'tag', '==', '""chemicalspecieslabel""', ':', 'lines', '=', 'block', '.', 'split', '(', ""'\\n'"", ')', '[', ':', '-', '1', ']', 'self', '.', '_tags', '[', '""atomicnumbers""', ']', '=', 'dict', '(', '[', 'map', '(', 'int', ',', 'species', '.', 'split', '(', ')', '[', ':', '2', ']', ')', 'for', 'species', 'in', 'lines', ']', ')', 'self', '.', '_tags', '[', 'tag', ']', '=', 'dict', '(', '[', '(', 'lambda', 'x', ':', '(', 'x', '[', '2', ']', ',', 'int', '(', 'x', '[', '0', ']', ')', ')', ')', '(', 'species', '.', 'split', '(', ')', ')', 'for', 'species', 'in', 'lines', ']', ')', 'elif', 'tag', '==', '""latticevectors""', ':', 'self', '.', '_tags', '[', 'tag', ']', '=', '[', '[', 'float', '(', 'v', ')', '*', 'acell', 'for', 'v', 'in', 'vector', '.', 'split', '(', ')', ']', 'for', 'vector', 'in', 'block', '.', 'split', '(', ""'\\n'"", ')', '[', ':', '3', ']', ']', 'elif', 'tag', '==', '""atomiccoordinatesandatomicspecies""', ':', 'lines', '=', 'block', '.', 'split', '(', ""'\\n'"", ')', '[', ':', '-', '1', ']', 'self', '.', '_tags', '[', '""atomiccoordinates""', ']', '=', '[', '[', 'float', '(', 'x', ')', 'for', 'x', 'in', 'atom', '.', 'split', '(', ')', '[', ':', '3', ']', ']', 'for', 'atom', 'in', 'lines', ']', 'self', '.', '_tags', '[', '""atomicspecies""', ']', '=', '[', 'int', '(', 'atom', '.', 'split', '(', ')', '[', '3', ']', ')', 'for', 'atom', 'in', 'lines', ']', '#check if the block are present', 'self', '.', 'check_present', '(', '""atomicspecies""', ')', 'self', '.', 'check_present', '(', '""atomiccoordinates""', ')', 'self', '.', 'check_present', '(', '""latticevectors""', ')', 'self', '.', 'check_present', '(', '""chemicalspecieslabel""', ')', '#translate the atomicspecies to atomic numbers', 'self', '.', '_tags', '[', '""atomicnumbers""', ']', '=', '[', 'self', '.', '_tags', '[', '""atomicnumbers""', ']', '[', 'atype', ']', 'for', 'atype', 'in', 'self', '.', '_tags', '[', '""atomicspecies""', ']', ']']","This routine reads the following from the Siesta file:
            - atomic positions
            - cell_parameters
            - atomic_species","['This', 'routine', 'reads', 'the', 'following', 'from', 'the', 'Siesta', 'file', ':', '-', 'atomic', 'positions', '-', 'cell_parameters', '-', 'atomic_species']",python,R,1,True,1,train
4335,PMEAL/OpenPNM,openpnm/io/CSV.py,https://github.com/PMEAL/OpenPNM/blob/0547b5724ffedc0a593aae48639d36fe10e0baed/openpnm/io/CSV.py#L83-L130,"def load(cls, filename, project=None, delim=' | '):
        r""""""
        Opens a 'csv' file, reads in the data, and adds it to the **Network**

        Parameters
        ----------
        filename : string (optional)
            The name of the file containing the data to import.  The formatting
            of this file is outlined below.

        project : OpenPNM Project object
            A GenericNetwork is created and added to the specified Project.
            If no Project object is supplied then one will be created and
            returned.

        """"""
        if project is None:
            project = ws.new_project()

        fname = cls._parse_filename(filename, ext='csv')
        a = pd.read_table(filepath_or_buffer=fname,
                          sep=',',
                          skipinitialspace=True,
                          index_col=False,
                          true_values=['T', 't', 'True', 'true', 'TRUE'],
                          false_values=['F', 'f', 'False', 'false', 'FALSE'])

        dct = {}
        # First parse through all the items and re-merge columns
        keys = sorted(list(a.keys()))
        for item in keys:
            m = re.search(r'\[.\]', item)  # The dot '.' is a wildcard
            if m:  # m is None if pattern not found, otherwise merge cols
                pname = re.split(r'\[.\]', item)[0]  # Get base propname
                # Find all other keys with same base propname
                merge_keys = [k for k in a.keys() if k.startswith(pname)]
                # Rerieve and remove arrays with same base propname
                merge_cols = [a.pop(k) for k in merge_keys]
                # Merge arrays into multi-column array and store in DataFrame
                dct[pname] = sp.vstack(merge_cols).T
                # Remove key from list of keys
                [keys.pop(keys.index(k)) for k in keys if k.startswith(pname)]
            else:
                dct[item] = sp.array(a.pop(item))

        project = Dict.from_dict(dct, project=project, delim=delim)

        return project","['def', 'load', '(', 'cls', ',', 'filename', ',', 'project', '=', 'None', ',', 'delim', '=', ""' | '"", ')', ':', 'if', 'project', 'is', 'None', ':', 'project', '=', 'ws', '.', 'new_project', '(', ')', 'fname', '=', 'cls', '.', '_parse_filename', '(', 'filename', ',', 'ext', '=', ""'csv'"", ')', 'a', '=', 'pd', '.', 'read_table', '(', 'filepath_or_buffer', '=', 'fname', ',', 'sep', '=', ""','"", ',', 'skipinitialspace', '=', 'True', ',', 'index_col', '=', 'False', ',', 'true_values', '=', '[', ""'T'"", ',', ""'t'"", ',', ""'True'"", ',', ""'true'"", ',', ""'TRUE'"", ']', ',', 'false_values', '=', '[', ""'F'"", ',', ""'f'"", ',', ""'False'"", ',', ""'false'"", ',', ""'FALSE'"", ']', ')', 'dct', '=', '{', '}', '# First parse through all the items and re-merge columns', 'keys', '=', 'sorted', '(', 'list', '(', 'a', '.', 'keys', '(', ')', ')', ')', 'for', 'item', 'in', 'keys', ':', 'm', '=', 're', '.', 'search', '(', ""r'\\[.\\]'"", ',', 'item', ')', ""# The dot '.' is a wildcard"", 'if', 'm', ':', '# m is None if pattern not found, otherwise merge cols', 'pname', '=', 're', '.', 'split', '(', ""r'\\[.\\]'"", ',', 'item', ')', '[', '0', ']', '# Get base propname', '# Find all other keys with same base propname', 'merge_keys', '=', '[', 'k', 'for', 'k', 'in', 'a', '.', 'keys', '(', ')', 'if', 'k', '.', 'startswith', '(', 'pname', ')', ']', '# Rerieve and remove arrays with same base propname', 'merge_cols', '=', '[', 'a', '.', 'pop', '(', 'k', ')', 'for', 'k', 'in', 'merge_keys', ']', '# Merge arrays into multi-column array and store in DataFrame', 'dct', '[', 'pname', ']', '=', 'sp', '.', 'vstack', '(', 'merge_cols', ')', '.', 'T', '# Remove key from list of keys', '[', 'keys', '.', 'pop', '(', 'keys', '.', 'index', '(', 'k', ')', ')', 'for', 'k', 'in', 'keys', 'if', 'k', '.', 'startswith', '(', 'pname', ')', ']', 'else', ':', 'dct', '[', 'item', ']', '=', 'sp', '.', 'array', '(', 'a', '.', 'pop', '(', 'item', ')', ')', 'project', '=', 'Dict', '.', 'from_dict', '(', 'dct', ',', 'project', '=', 'project', ',', 'delim', '=', 'delim', ')', 'return', 'project']","r""""""
        Opens a 'csv' file, reads in the data, and adds it to the **Network**

        Parameters
        ----------
        filename : string (optional)
            The name of the file containing the data to import.  The formatting
            of this file is outlined below.

        project : OpenPNM Project object
            A GenericNetwork is created and added to the specified Project.
            If no Project object is supplied then one will be created and
            returned.","['r', 'Opens', 'a', 'csv', 'file', 'reads', 'in', 'the', 'data', 'and', 'adds', 'it', 'to', 'the', '**', 'Network', '**']",python,R,1,True,1,train
12089,log2timeline/plaso,plaso/parsers/dsv_parser.py,https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/dsv_parser.py#L125-L166,"def _CreateLineReader(self, file_object):
    """"""Creates an object that reads lines from a text file.

    The line reader is advanced to the beginning of the DSV content, skipping
    any header lines.

    Args:
      file_object (dfvfs.FileIO): file-like object.

    Returns:
      TextFile|BinaryLineReader: an object that implements an iterator
          over lines in a text file.

    Raises:
      UnicodeDecodeError: if the file cannot be read with the specified
          encoding.
    """"""
    # The Python 2 csv module reads bytes and the Python 3 csv module Unicode
    # reads strings.
    if py2to3.PY_3:
      line_reader = text_file.TextFile(
          file_object, encoding=self._encoding, end_of_line=self._end_of_line)

      # pylint: disable=protected-access
      maximum_read_buffer_size = line_reader._MAXIMUM_READ_BUFFER_SIZE

    else:
      line_reader = line_reader_file.BinaryLineReader(
          file_object, end_of_line=self._end_of_line)

      maximum_read_buffer_size = line_reader.MAXIMUM_READ_BUFFER_SIZE

    # Line length is one less than the maximum read buffer size so that we
    # tell if there's a line that doesn't end at the end before the end of
    # the file.
    if self._maximum_line_length > maximum_read_buffer_size:
      self._maximum_line_length = maximum_read_buffer_size - 1

    # If we specifically define a number of lines we should skip, do that here.
    for _ in range(0, self.NUMBER_OF_HEADER_LINES):
      line_reader.readline(self._maximum_line_length)
    return line_reader","['def', '_CreateLineReader', '(', 'self', ',', 'file_object', ')', ':', '# The Python 2 csv module reads bytes and the Python 3 csv module Unicode', '# reads strings.', 'if', 'py2to3', '.', 'PY_3', ':', 'line_reader', '=', 'text_file', '.', 'TextFile', '(', 'file_object', ',', 'encoding', '=', 'self', '.', '_encoding', ',', 'end_of_line', '=', 'self', '.', '_end_of_line', ')', '# pylint: disable=protected-access', 'maximum_read_buffer_size', '=', 'line_reader', '.', '_MAXIMUM_READ_BUFFER_SIZE', 'else', ':', 'line_reader', '=', 'line_reader_file', '.', 'BinaryLineReader', '(', 'file_object', ',', 'end_of_line', '=', 'self', '.', '_end_of_line', ')', 'maximum_read_buffer_size', '=', 'line_reader', '.', 'MAXIMUM_READ_BUFFER_SIZE', '# Line length is one less than the maximum read buffer size so that we', ""# tell if there's a line that doesn't end at the end before the end of"", '# the file.', 'if', 'self', '.', '_maximum_line_length', '>', 'maximum_read_buffer_size', ':', 'self', '.', '_maximum_line_length', '=', 'maximum_read_buffer_size', '-', '1', '# If we specifically define a number of lines we should skip, do that here.', 'for', '_', 'in', 'range', '(', '0', ',', 'self', '.', 'NUMBER_OF_HEADER_LINES', ')', ':', 'line_reader', '.', 'readline', '(', 'self', '.', '_maximum_line_length', ')', 'return', 'line_reader']","Creates an object that reads lines from a text file.

    The line reader is advanced to the beginning of the DSV content, skipping
    any header lines.

    Args:
      file_object (dfvfs.FileIO): file-like object.

    Returns:
      TextFile|BinaryLineReader: an object that implements an iterator
          over lines in a text file.

    Raises:
      UnicodeDecodeError: if the file cannot be read with the specified
          encoding.","['Creates', 'an', 'object', 'that', 'reads', 'lines', 'from', 'a', 'text', 'file', '.']",python,R,1,True,1,train
13059,awslabs/sockeye,sockeye/translate.py,https://github.com/awslabs/sockeye/blob/5d64a1ee1ef3cbba17c6d1d94bc061020c43f6ab/sockeye/translate.py#L158-L200,"def make_inputs(input_file: Optional[str],
                translator: inference.Translator,
                input_is_json: bool,
                input_factors: Optional[List[str]] = None) -> Generator[inference.TranslatorInput, None, None]:
    """"""
    Generates TranslatorInput instances from input. If input is None, reads from stdin. If num_input_factors > 1,
    the function will look for factors attached to each token, separated by '|'.
    If source is not None, reads from the source file. If num_source_factors > 1, num_source_factors source factor
    filenames are required.

    :param input_file: The source file (possibly None).
    :param translator: Translator that will translate each line of input.
    :param input_is_json: Whether the input is in json format.
    :param input_factors: Source factor files.
    :return: TranslatorInput objects.
    """"""
    if input_file is None:
        check_condition(input_factors is None, ""Translating from STDIN, not expecting any factor files."")
        for sentence_id, line in enumerate(sys.stdin, 1):
            if input_is_json:
                yield inference.make_input_from_json_string(sentence_id=sentence_id,
                                                            json_string=line,
                                                            translator=translator)
            else:
                yield inference.make_input_from_factored_string(sentence_id=sentence_id,
                                                                factored_string=line,
                                                                translator=translator)
    else:
        input_factors = [] if input_factors is None else input_factors
        inputs = [input_file] + input_factors
        if not input_is_json:
            check_condition(translator.num_source_factors == len(inputs),
                            ""Model(s) require %d factors, but %d given (through --input and --input-factors)."" % (
                                translator.num_source_factors, len(inputs)))
        with ExitStack() as exit_stack:
            streams = [exit_stack.enter_context(data_io.smart_open(i)) for i in inputs]
            for sentence_id, inputs in enumerate(zip(*streams), 1):
                if input_is_json:
                    yield inference.make_input_from_json_string(sentence_id=sentence_id,
                                                                json_string=inputs[0],
                                                                translator=translator)
                else:
                    yield inference.make_input_from_multiple_strings(sentence_id=sentence_id, strings=list(inputs))","['def', 'make_inputs', '(', 'input_file', ':', 'Optional', '[', 'str', ']', ',', 'translator', ':', 'inference', '.', 'Translator', ',', 'input_is_json', ':', 'bool', ',', 'input_factors', ':', 'Optional', '[', 'List', '[', 'str', ']', ']', '=', 'None', ')', '->', 'Generator', '[', 'inference', '.', 'TranslatorInput', ',', 'None', ',', 'None', ']', ':', 'if', 'input_file', 'is', 'None', ':', 'check_condition', '(', 'input_factors', 'is', 'None', ',', '""Translating from STDIN, not expecting any factor files.""', ')', 'for', 'sentence_id', ',', 'line', 'in', 'enumerate', '(', 'sys', '.', 'stdin', ',', '1', ')', ':', 'if', 'input_is_json', ':', 'yield', 'inference', '.', 'make_input_from_json_string', '(', 'sentence_id', '=', 'sentence_id', ',', 'json_string', '=', 'line', ',', 'translator', '=', 'translator', ')', 'else', ':', 'yield', 'inference', '.', 'make_input_from_factored_string', '(', 'sentence_id', '=', 'sentence_id', ',', 'factored_string', '=', 'line', ',', 'translator', '=', 'translator', ')', 'else', ':', 'input_factors', '=', '[', ']', 'if', 'input_factors', 'is', 'None', 'else', 'input_factors', 'inputs', '=', '[', 'input_file', ']', '+', 'input_factors', 'if', 'not', 'input_is_json', ':', 'check_condition', '(', 'translator', '.', 'num_source_factors', '==', 'len', '(', 'inputs', ')', ',', '""Model(s) require %d factors, but %d given (through --input and --input-factors).""', '%', '(', 'translator', '.', 'num_source_factors', ',', 'len', '(', 'inputs', ')', ')', ')', 'with', 'ExitStack', '(', ')', 'as', 'exit_stack', ':', 'streams', '=', '[', 'exit_stack', '.', 'enter_context', '(', 'data_io', '.', 'smart_open', '(', 'i', ')', ')', 'for', 'i', 'in', 'inputs', ']', 'for', 'sentence_id', ',', 'inputs', 'in', 'enumerate', '(', 'zip', '(', '*', 'streams', ')', ',', '1', ')', ':', 'if', 'input_is_json', ':', 'yield', 'inference', '.', 'make_input_from_json_string', '(', 'sentence_id', '=', 'sentence_id', ',', 'json_string', '=', 'inputs', '[', '0', ']', ',', 'translator', '=', 'translator', ')', 'else', ':', 'yield', 'inference', '.', 'make_input_from_multiple_strings', '(', 'sentence_id', '=', 'sentence_id', ',', 'strings', '=', 'list', '(', 'inputs', ')', ')']","Generates TranslatorInput instances from input. If input is None, reads from stdin. If num_input_factors > 1,
    the function will look for factors attached to each token, separated by '|'.
    If source is not None, reads from the source file. If num_source_factors > 1, num_source_factors source factor
    filenames are required.

    :param input_file: The source file (possibly None).
    :param translator: Translator that will translate each line of input.
    :param input_is_json: Whether the input is in json format.
    :param input_factors: Source factor files.
    :return: TranslatorInput objects.","['Generates', 'TranslatorInput', 'instances', 'from', 'input', '.', 'If', 'input', 'is', 'None', 'reads', 'from', 'stdin', '.', 'If', 'num_input_factors', '>', '1', 'the', 'function', 'will', 'look', 'for', 'factors', 'attached', 'to', 'each', 'token', 'separated', 'by', '|', '.', 'If', 'source', 'is', 'not', 'None', 'reads', 'from', 'the', 'source', 'file', '.', 'If', 'num_source_factors', '>', '1', 'num_source_factors', 'source', 'factor', 'filenames', 'are', 'required', '.']",python,R,1,True,1,train
15440,pantsbuild/pants,src/python/pants/scm/git.py,https://github.com/pantsbuild/pants/blob/b72e650da0df685824ffdcc71988b8c282d0962d/src/python/pants/scm/git.py#L438-L452,"def listdir(self, relpath):
    """"""Like os.listdir, but reads from the git repository.

    :returns: a list of relative filenames
    """"""

    path = self._realpath(relpath)
    if not path.endswith('/'):
      raise self.NotADirException(self.rev, relpath)

    if path[0] == '/' or path.startswith('../'):
      return os.listdir(path)

    tree = self._read_tree(path[:-1])
    return list(tree.keys())","['def', 'listdir', '(', 'self', ',', 'relpath', ')', ':', 'path', '=', 'self', '.', '_realpath', '(', 'relpath', ')', 'if', 'not', 'path', '.', 'endswith', '(', ""'/'"", ')', ':', 'raise', 'self', '.', 'NotADirException', '(', 'self', '.', 'rev', ',', 'relpath', ')', 'if', 'path', '[', '0', ']', '==', ""'/'"", 'or', 'path', '.', 'startswith', '(', ""'../'"", ')', ':', 'return', 'os', '.', 'listdir', '(', 'path', ')', 'tree', '=', 'self', '.', '_read_tree', '(', 'path', '[', ':', '-', '1', ']', ')', 'return', 'list', '(', 'tree', '.', 'keys', '(', ')', ')']","Like os.listdir, but reads from the git repository.

    :returns: a list of relative filenames","['Like', 'os', '.', 'listdir', 'but', 'reads', 'from', 'the', 'git', 'repository', '.']",python,R,1,True,1,train
19486,minio/minio-py,minio/helpers.py,https://github.com/minio/minio-py/blob/7107c84183cf5fb4deff68c0a16ab9f1c0b4c37e/minio/helpers.py#L161-L184,"def read_full(data, size):
    """"""
    read_full reads exactly `size` bytes from reader. returns
    `size` bytes.

    :param data: Input stream to read from.
    :param size: Number of bytes to read from `data`.
    :return: Returns :bytes:`part_data`
    """"""
    default_read_size = 32768 # 32KiB per read operation.
    chunk = io.BytesIO()
    chunk_size = 0

    while chunk_size < size:
        read_size = default_read_size
        if (size - chunk_size) < default_read_size:
            read_size = size - chunk_size
        current_data = data.read(read_size)
        if not current_data or len(current_data) == 0:
            break
        chunk.write(current_data)
        chunk_size+= len(current_data)

    return chunk.getvalue()","['def', 'read_full', '(', 'data', ',', 'size', ')', ':', 'default_read_size', '=', '32768', '# 32KiB per read operation.', 'chunk', '=', 'io', '.', 'BytesIO', '(', ')', 'chunk_size', '=', '0', 'while', 'chunk_size', '<', 'size', ':', 'read_size', '=', 'default_read_size', 'if', '(', 'size', '-', 'chunk_size', ')', '<', 'default_read_size', ':', 'read_size', '=', 'size', '-', 'chunk_size', 'current_data', '=', 'data', '.', 'read', '(', 'read_size', ')', 'if', 'not', 'current_data', 'or', 'len', '(', 'current_data', ')', '==', '0', ':', 'break', 'chunk', '.', 'write', '(', 'current_data', ')', 'chunk_size', '+=', 'len', '(', 'current_data', ')', 'return', 'chunk', '.', 'getvalue', '(', ')']","read_full reads exactly `size` bytes from reader. returns
    `size` bytes.

    :param data: Input stream to read from.
    :param size: Number of bytes to read from `data`.
    :return: Returns :bytes:`part_data`","['read_full', 'reads', 'exactly', 'size', 'bytes', 'from', 'reader', '.', 'returns', 'size', 'bytes', '.']",python,R,1,True,1,train
25084,adubkov/py-zabbix,pyzabbix/sender.py,https://github.com/adubkov/py-zabbix/blob/a26aadcba7c54cb122be8becc3802e2c42562c49/pyzabbix/sender.py#L343-L371,"def _get_response(self, connection):
        """"""Get response from zabbix server, reads from self.socket.

        :type connection: :class:`socket._socketobject`
        :param connection: Socket to read.

        :rtype: dict
        :return: Response from zabbix server or False in case of error.
        """"""

        response_header = self._receive(connection, 13)
        logger.debug('Response header: %s', response_header)

        if (not response_header.startswith(b'ZBXD\x01') or
                len(response_header) != 13):
            logger.debug('Zabbix return not valid response.')
            result = False
        else:
            response_len = struct.unpack('<Q', response_header[5:])[0]
            response_body = connection.recv(response_len)
            result = json.loads(response_body.decode(""utf-8""))
            logger.debug('Data received: %s', result)

        try:
            connection.close()
        except Exception as err:
            pass

        return result","['def', '_get_response', '(', 'self', ',', 'connection', ')', ':', 'response_header', '=', 'self', '.', '_receive', '(', 'connection', ',', '13', ')', 'logger', '.', 'debug', '(', ""'Response header: %s'"", ',', 'response_header', ')', 'if', '(', 'not', 'response_header', '.', 'startswith', '(', ""b'ZBXD\\x01'"", ')', 'or', 'len', '(', 'response_header', ')', '!=', '13', ')', ':', 'logger', '.', 'debug', '(', ""'Zabbix return not valid response.'"", ')', 'result', '=', 'False', 'else', ':', 'response_len', '=', 'struct', '.', 'unpack', '(', ""'<Q'"", ',', 'response_header', '[', '5', ':', ']', ')', '[', '0', ']', 'response_body', '=', 'connection', '.', 'recv', '(', 'response_len', ')', 'result', '=', 'json', '.', 'loads', '(', 'response_body', '.', 'decode', '(', '""utf-8""', ')', ')', 'logger', '.', 'debug', '(', ""'Data received: %s'"", ',', 'result', ')', 'try', ':', 'connection', '.', 'close', '(', ')', 'except', 'Exception', 'as', 'err', ':', 'pass', 'return', 'result']","Get response from zabbix server, reads from self.socket.

        :type connection: :class:`socket._socketobject`
        :param connection: Socket to read.

        :rtype: dict
        :return: Response from zabbix server or False in case of error.","['Get', 'response', 'from', 'zabbix', 'server', 'reads', 'from', 'self', '.', 'socket', '.']",python,R,1,True,1,train
4965,spotify/snakebite,snakebite/channel.py,https://github.com/spotify/snakebite/blob/6a456e6100b0c1be66cc1f7f9d7f50494f369da3/snakebite/channel.py#L364-L373,"def get_length(self, byte_stream):
        ''' In Hadoop protobuf RPC, some parts of the stream are delimited with protobuf varint,
        while others are delimited with 4 byte integers. This reads 4 bytes from the byte stream
        and retruns the length of the delimited part that follows, by unpacking the 4 bytes
        and returning the first element from a tuple. The tuple that is returned from struc.unpack()
        only contains one element.
        '''
        length = struct.unpack(""!i"", byte_stream.read(4))[0]
        log.debug(""4 bytes delimited part length: %d"" % length)
        return length","['def', 'get_length', '(', 'self', ',', 'byte_stream', ')', ':', 'length', '=', 'struct', '.', 'unpack', '(', '""!i""', ',', 'byte_stream', '.', 'read', '(', '4', ')', ')', '[', '0', ']', 'log', '.', 'debug', '(', '""4 bytes delimited part length: %d""', '%', 'length', ')', 'return', 'length']","In Hadoop protobuf RPC, some parts of the stream are delimited with protobuf varint,
        while others are delimited with 4 byte integers. This reads 4 bytes from the byte stream
        and retruns the length of the delimited part that follows, by unpacking the 4 bytes
        and returning the first element from a tuple. The tuple that is returned from struc.unpack()
        only contains one element.","['In', 'Hadoop', 'protobuf', 'RPC', 'some', 'parts', 'of', 'the', 'stream', 'are', 'delimited', 'with', 'protobuf', 'varint', 'while', 'others', 'are', 'delimited', 'with', '4', 'byte', 'integers', '.', 'This', 'reads', '4', 'bytes', 'from', 'the', 'byte', 'stream', 'and', 'retruns', 'the', 'length', 'of', 'the', 'delimited', 'part', 'that', 'follows', 'by', 'unpacking', 'the', '4', 'bytes', 'and', 'returning', 'the', 'first', 'element', 'from', 'a', 'tuple', '.', 'The', 'tuple', 'that', 'is', 'returned', 'from', 'struc', '.', 'unpack', '()', 'only', 'contains', 'one', 'element', '.']",python,R,1,True,1,train
10918,Yelp/kafka-utils,kafka_utils/kafka_check/metadata_file.py,https://github.com/Yelp/kafka-utils/blob/cdb4d64308f3079ee0873250bf7b34d0d94eca50/kafka_utils/kafka_check/metadata_file.py#L9-L29,"def _read_generated_broker_id(meta_properties_path):
    """"""reads broker_id from meta.properties file.

    :param string meta_properties_path: path for meta.properties file
    :returns int: broker_id from meta_properties_path
    """"""
    try:
        with open(meta_properties_path, 'r') as f:
            broker_id = _parse_meta_properties_file(f)
    except IOError:
        raise IOError(
            ""Cannot open meta.properties file: {path}""
            .format(path=meta_properties_path),
        )
    except ValueError:
        raise ValueError(""Broker id not valid"")

    if broker_id is None:
        raise ValueError(""Autogenerated broker id missing from data directory"")

    return broker_id","['def', '_read_generated_broker_id', '(', 'meta_properties_path', ')', ':', 'try', ':', 'with', 'open', '(', 'meta_properties_path', ',', ""'r'"", ')', 'as', 'f', ':', 'broker_id', '=', '_parse_meta_properties_file', '(', 'f', ')', 'except', 'IOError', ':', 'raise', 'IOError', '(', '""Cannot open meta.properties file: {path}""', '.', 'format', '(', 'path', '=', 'meta_properties_path', ')', ',', ')', 'except', 'ValueError', ':', 'raise', 'ValueError', '(', '""Broker id not valid""', ')', 'if', 'broker_id', 'is', 'None', ':', 'raise', 'ValueError', '(', '""Autogenerated broker id missing from data directory""', ')', 'return', 'broker_id']","reads broker_id from meta.properties file.

    :param string meta_properties_path: path for meta.properties file
    :returns int: broker_id from meta_properties_path","['reads', 'broker_id', 'from', 'meta', '.', 'properties', 'file', '.']",python,R,1,True,1,train
13287,markovmodel/PyEMMA,pyemma/coordinates/data/_base/datasource.py,https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/data/_base/datasource.py#L188-L208,"def _data_flow_chain(self):
        """"""
        Get a list of all elements in the data flow graph.
        The first element is the original source, the next one reads from the prior and so on and so forth.

        Returns
        -------
        list: list of data sources

        """"""
        if self.data_producer is None:
            return []

        res = []
        ds = self.data_producer
        while not ds.is_reader:
            res.append(ds)
            ds = ds.data_producer
        res.append(ds)
        res = res[::-1]
        return res","['def', '_data_flow_chain', '(', 'self', ')', ':', 'if', 'self', '.', 'data_producer', 'is', 'None', ':', 'return', '[', ']', 'res', '=', '[', ']', 'ds', '=', 'self', '.', 'data_producer', 'while', 'not', 'ds', '.', 'is_reader', ':', 'res', '.', 'append', '(', 'ds', ')', 'ds', '=', 'ds', '.', 'data_producer', 'res', '.', 'append', '(', 'ds', ')', 'res', '=', 'res', '[', ':', ':', '-', '1', ']', 'return', 'res']","Get a list of all elements in the data flow graph.
        The first element is the original source, the next one reads from the prior and so on and so forth.

        Returns
        -------
        list: list of data sources","['Get', 'a', 'list', 'of', 'all', 'elements', 'in', 'the', 'data', 'flow', 'graph', '.', 'The', 'first', 'element', 'is', 'the', 'original', 'source', 'the', 'next', 'one', 'reads', 'from', 'the', 'prior', 'and', 'so', 'on', 'and', 'so', 'forth', '.']",python,R,1,True,1,train
13491,markovmodel/PyEMMA,pyemma/util/_config.py,https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/util/_config.py#L418-L436,"def _cfgs_to_read(self):
        """"""
        reads config files from various locations to build final config.
        """"""
        # use these files to extend/overwrite the conf_values.
        # Last red file always overwrites existing values!
        cfg = Config.DEFAULT_CONFIG_FILE_NAME
        filenames = [
            self.default_config_file,
            cfg,  # conf_values in current directory
            os.path.join(os.path.expanduser('~' + os.path.sep), cfg),  # config in user dir
            '.pyemma.cfg',
        ]

        # look for user defined files
        if self.cfg_dir:
            from glob import glob
            filenames.extend(glob(self.cfg_dir + os.path.sep + ""*.cfg""))
        return filenames","['def', '_cfgs_to_read', '(', 'self', ')', ':', '# use these files to extend/overwrite the conf_values.', '# Last red file always overwrites existing values!', 'cfg', '=', 'Config', '.', 'DEFAULT_CONFIG_FILE_NAME', 'filenames', '=', '[', 'self', '.', 'default_config_file', ',', 'cfg', ',', '# conf_values in current directory', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'expanduser', '(', ""'~'"", '+', 'os', '.', 'path', '.', 'sep', ')', ',', 'cfg', ')', ',', '# config in user dir', ""'.pyemma.cfg'"", ',', ']', '# look for user defined files', 'if', 'self', '.', 'cfg_dir', ':', 'from', 'glob', 'import', 'glob', 'filenames', '.', 'extend', '(', 'glob', '(', 'self', '.', 'cfg_dir', '+', 'os', '.', 'path', '.', 'sep', '+', '""*.cfg""', ')', ')', 'return', 'filenames']",reads config files from various locations to build final config.,"['reads', 'config', 'files', 'from', 'various', 'locations', 'to', 'build', 'final', 'config', '.']",python,R,1,True,1,train
15845,itamarst/eliot,eliot/prettyprint.py,https://github.com/itamarst/eliot/blob/c03c96520c5492fadfc438b4b0f6336e2785ba2d/eliot/prettyprint.py#L100-L121,"def _main():
    """"""
    Command-line program that reads in JSON from stdin and writes out
    pretty-printed messages to stdout.
    """"""
    if argv[1:]:
        stdout.write(_CLI_HELP)
        raise SystemExit()
    for line in stdin:
        try:
            message = loads(line)
        except ValueError:
            stdout.write(""Not JSON: {}\n\n"".format(line.rstrip(b""\n"")))
            continue
        if REQUIRED_FIELDS - set(message.keys()):
            stdout.write(
                ""Not an Eliot message: {}\n\n"".format(line.rstrip(b""\n"")))
            continue
        result = pretty_format(message) + ""\n""
        if PY2:
            result = result.encode(""utf-8"")
        stdout.write(result)","['def', '_main', '(', ')', ':', 'if', 'argv', '[', '1', ':', ']', ':', 'stdout', '.', 'write', '(', '_CLI_HELP', ')', 'raise', 'SystemExit', '(', ')', 'for', 'line', 'in', 'stdin', ':', 'try', ':', 'message', '=', 'loads', '(', 'line', ')', 'except', 'ValueError', ':', 'stdout', '.', 'write', '(', '""Not JSON: {}\\n\\n""', '.', 'format', '(', 'line', '.', 'rstrip', '(', 'b""\\n""', ')', ')', ')', 'continue', 'if', 'REQUIRED_FIELDS', '-', 'set', '(', 'message', '.', 'keys', '(', ')', ')', ':', 'stdout', '.', 'write', '(', '""Not an Eliot message: {}\\n\\n""', '.', 'format', '(', 'line', '.', 'rstrip', '(', 'b""\\n""', ')', ')', ')', 'continue', 'result', '=', 'pretty_format', '(', 'message', ')', '+', '""\\n""', 'if', 'PY2', ':', 'result', '=', 'result', '.', 'encode', '(', '""utf-8""', ')', 'stdout', '.', 'write', '(', 'result', ')']","Command-line program that reads in JSON from stdin and writes out
    pretty-printed messages to stdout.","['Command', '-', 'line', 'program', 'that', 'reads', 'in', 'JSON', 'from', 'stdin', 'and', 'writes', 'out', 'pretty', '-', 'printed', 'messages', 'to', 'stdout', '.']",python,R,1,True,1,train
18532,wandb/client,wandb/io_wrap.py,https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/io_wrap.py#L267-L291,"def spawn_reader_writer(get_data_fn, put_data_fn):
    """"""Spawn a thread that reads from a data source and writes to a sink.

    The thread will terminate if it receives a Falsey value from the source.

    Args:
        get_data_fn: Data-reading function. Called repeatedly until it returns
            False-y to indicate that the thread should terminate.
        put_data_fn: Data-writing function.
    Returns: threading.Thread
    """"""
    def _reader_thread():
        while True:
            out = get_data_fn()
            put_data_fn(out)
            if not out:
                # EOF.
                # We've passed this on so things farther down the pipeline will
                # know to shut down.
                break

    t = threading.Thread(target=_reader_thread)
    t.daemon = True
    t.start()
    return t","['def', 'spawn_reader_writer', '(', 'get_data_fn', ',', 'put_data_fn', ')', ':', 'def', '_reader_thread', '(', ')', ':', 'while', 'True', ':', 'out', '=', 'get_data_fn', '(', ')', 'put_data_fn', '(', 'out', ')', 'if', 'not', 'out', ':', '# EOF.', ""# We've passed this on so things farther down the pipeline will"", '# know to shut down.', 'break', 't', '=', 'threading', '.', 'Thread', '(', 'target', '=', '_reader_thread', ')', 't', '.', 'daemon', '=', 'True', 't', '.', 'start', '(', ')', 'return', 't']","Spawn a thread that reads from a data source and writes to a sink.

    The thread will terminate if it receives a Falsey value from the source.

    Args:
        get_data_fn: Data-reading function. Called repeatedly until it returns
            False-y to indicate that the thread should terminate.
        put_data_fn: Data-writing function.
    Returns: threading.Thread","['Spawn', 'a', 'thread', 'that', 'reads', 'from', 'a', 'data', 'source', 'and', 'writes', 'to', 'a', 'sink', '.']",python,R,1,True,1,train
20980,jilljenn/tryalgo,tryalgo/horn_sat.py,https://github.com/jilljenn/tryalgo/blob/89a4dd9655e7b6b0a176f72b4c60d0196420dfe1/tryalgo/horn_sat.py#L24-L52,"def read(filename):
    """""" reads a Horn SAT formula from a text file

    :file format:
        # comment
        A     # clause with unique positive literal
        :- A  # clause with unique negative literal
        A :- B, C, D # clause where A is positive and B,C,D negative
        # variables are strings without spaces
    """"""
    formula = []
    for line in open(filename, 'r'):
        line = line.strip()
        if line[0] == ""#"":
            continue
        lit = line.split("":-"")
        if len(lit) == 1:
            posvar = lit[0]
            negvars = []
        else:
            assert len(lit) == 2
            posvar = lit[0].strip()
            if posvar == '':
                posvar = None
            negvars = lit[1].split(',')
            for i in range(len(negvars)):
                negvars[i] = negvars[i].strip()
        formula.append((posvar, negvars))
    return formula","['def', 'read', '(', 'filename', ')', ':', 'formula', '=', '[', ']', 'for', 'line', 'in', 'open', '(', 'filename', ',', ""'r'"", ')', ':', 'line', '=', 'line', '.', 'strip', '(', ')', 'if', 'line', '[', '0', ']', '==', '""#""', ':', 'continue', 'lit', '=', 'line', '.', 'split', '(', '"":-""', ')', 'if', 'len', '(', 'lit', ')', '==', '1', ':', 'posvar', '=', 'lit', '[', '0', ']', 'negvars', '=', '[', ']', 'else', ':', 'assert', 'len', '(', 'lit', ')', '==', '2', 'posvar', '=', 'lit', '[', '0', ']', '.', 'strip', '(', ')', 'if', 'posvar', '==', ""''"", ':', 'posvar', '=', 'None', 'negvars', '=', 'lit', '[', '1', ']', '.', 'split', '(', ""','"", ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'negvars', ')', ')', ':', 'negvars', '[', 'i', ']', '=', 'negvars', '[', 'i', ']', '.', 'strip', '(', ')', 'formula', '.', 'append', '(', '(', 'posvar', ',', 'negvars', ')', ')', 'return', 'formula']","reads a Horn SAT formula from a text file

    :file format:
        # comment
        A     # clause with unique positive literal
        :- A  # clause with unique negative literal
        A :- B, C, D # clause where A is positive and B,C,D negative
        # variables are strings without spaces","['reads', 'a', 'Horn', 'SAT', 'formula', 'from', 'a', 'text', 'file']",python,R,1,True,1,train
21265,developmentseed/landsat-util,landsat/ndvi.py,https://github.com/developmentseed/landsat-util/blob/92dc81771ddaa64a8a9124a89a6516b52485374b/landsat/ndvi.py#L20-L50,"def _read_cmap(self):
        """"""
        reads the colormap from a text file given in settings.py.
        See colormap_cubehelix.txt. File must contain 256 RGB values
        """"""

        try:
            i = 0
            colormap = {0: (0, 0, 0)}
            with open(settings.COLORMAP) as cmap:
                lines = cmap.readlines()
                for line in lines:
                    if i == 0 and 'mode = ' in line:
                        i = 1
                        maxval = float(line.replace('mode = ', ''))
                    elif i > 0:
                        str = line.split()
                        if str == []:  # when there are empty lines at the end of the file
                            break
                        colormap.update(
                            {
                                i: (int(round(float(str[0]) * 255 / maxval)),
                                    int(round(float(str[1]) * 255 / maxval)),
                                    int(round(float(str[2]) * 255 / maxval)))
                            }
                        )
                        i += 1
        except IOError:
            pass

        self.cmap = {k: v[:4] for k, v in colormap.items()}","['def', '_read_cmap', '(', 'self', ')', ':', 'try', ':', 'i', '=', '0', 'colormap', '=', '{', '0', ':', '(', '0', ',', '0', ',', '0', ')', '}', 'with', 'open', '(', 'settings', '.', 'COLORMAP', ')', 'as', 'cmap', ':', 'lines', '=', 'cmap', '.', 'readlines', '(', ')', 'for', 'line', 'in', 'lines', ':', 'if', 'i', '==', '0', 'and', ""'mode = '"", 'in', 'line', ':', 'i', '=', '1', 'maxval', '=', 'float', '(', 'line', '.', 'replace', '(', ""'mode = '"", ',', ""''"", ')', ')', 'elif', 'i', '>', '0', ':', 'str', '=', 'line', '.', 'split', '(', ')', 'if', 'str', '==', '[', ']', ':', '# when there are empty lines at the end of the file', 'break', 'colormap', '.', 'update', '(', '{', 'i', ':', '(', 'int', '(', 'round', '(', 'float', '(', 'str', '[', '0', ']', ')', '*', '255', '/', 'maxval', ')', ')', ',', 'int', '(', 'round', '(', 'float', '(', 'str', '[', '1', ']', ')', '*', '255', '/', 'maxval', ')', ')', ',', 'int', '(', 'round', '(', 'float', '(', 'str', '[', '2', ']', ')', '*', '255', '/', 'maxval', ')', ')', ')', '}', ')', 'i', '+=', '1', 'except', 'IOError', ':', 'pass', 'self', '.', 'cmap', '=', '{', 'k', ':', 'v', '[', ':', '4', ']', 'for', 'k', ',', 'v', 'in', 'colormap', '.', 'items', '(', ')', '}']","reads the colormap from a text file given in settings.py.
        See colormap_cubehelix.txt. File must contain 256 RGB values","['reads', 'the', 'colormap', 'from', 'a', 'text', 'file', 'given', 'in', 'settings', '.', 'py', '.', 'See', 'colormap_cubehelix', '.', 'txt', '.', 'File', 'must', 'contain', '256', 'RGB', 'values']",python,R,1,True,1,train
22431,fabioz/PyDev.Debugger,pydevd_attach_to_process/winappdbg/thread.py,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L1529-L1565,"def peek_pointers_in_registers(self, peekSize = 16, context = None):
        """"""
        Tries to guess which values in the registers are valid pointers,
        and reads some data from them.

        @type  peekSize: int
        @param peekSize: Number of bytes to read from each pointer found.

        @type  context: dict( str S{->} int )
        @param context: (Optional)
            Dictionary mapping register names to their values.
            If not given, the current thread context will be used.

        @rtype:  dict( str S{->} str )
        @return: Dictionary mapping register names to the data they point to.
        """"""
        peekable_registers = (
            'Eax', 'Ebx', 'Ecx', 'Edx', 'Esi', 'Edi', 'Ebp'
        )
        if not context:
            context = self.get_context(win32.CONTEXT_CONTROL | \
                                       win32.CONTEXT_INTEGER)
        aProcess    = self.get_process()
        data        = dict()
        for (reg_name, reg_value) in compat.iteritems(context):
            if reg_name not in peekable_registers:
                continue
##            if reg_name == 'Ebp':
##                stack_begin, stack_end = self.get_stack_range()
##                print hex(stack_end), hex(reg_value), hex(stack_begin)
##                if stack_begin and stack_end and stack_end < stack_begin and \
##                   stack_begin <= reg_value <= stack_end:
##                      continue
            reg_data = aProcess.peek(reg_value, peekSize)
            if reg_data:
                data[reg_name] = reg_data
        return data","['def', 'peek_pointers_in_registers', '(', 'self', ',', 'peekSize', '=', '16', ',', 'context', '=', 'None', ')', ':', 'peekable_registers', '=', '(', ""'Eax'"", ',', ""'Ebx'"", ',', ""'Ecx'"", ',', ""'Edx'"", ',', ""'Esi'"", ',', ""'Edi'"", ',', ""'Ebp'"", ')', 'if', 'not', 'context', ':', 'context', '=', 'self', '.', 'get_context', '(', 'win32', '.', 'CONTEXT_CONTROL', '|', 'win32', '.', 'CONTEXT_INTEGER', ')', 'aProcess', '=', 'self', '.', 'get_process', '(', ')', 'data', '=', 'dict', '(', ')', 'for', '(', 'reg_name', ',', 'reg_value', ')', 'in', 'compat', '.', 'iteritems', '(', 'context', ')', ':', 'if', 'reg_name', 'not', 'in', 'peekable_registers', ':', 'continue', ""##            if reg_name == 'Ebp':"", '##                stack_begin, stack_end = self.get_stack_range()', '##                print hex(stack_end), hex(reg_value), hex(stack_begin)', '##                if stack_begin and stack_end and stack_end < stack_begin and \\', '##                   stack_begin <= reg_value <= stack_end:', '##                      continue', 'reg_data', '=', 'aProcess', '.', 'peek', '(', 'reg_value', ',', 'peekSize', ')', 'if', 'reg_data', ':', 'data', '[', 'reg_name', ']', '=', 'reg_data', 'return', 'data']","Tries to guess which values in the registers are valid pointers,
        and reads some data from them.

        @type  peekSize: int
        @param peekSize: Number of bytes to read from each pointer found.

        @type  context: dict( str S{->} int )
        @param context: (Optional)
            Dictionary mapping register names to their values.
            If not given, the current thread context will be used.

        @rtype:  dict( str S{->} str )
        @return: Dictionary mapping register names to the data they point to.","['Tries', 'to', 'guess', 'which', 'values', 'in', 'the', 'registers', 'are', 'valid', 'pointers', 'and', 'reads', 'some', 'data', 'from', 'them', '.']",python,R,1,True,1,train
22432,fabioz/PyDev.Debugger,pydevd_attach_to_process/winappdbg/thread.py,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L1569-L1590,"def peek_pointers_in_data(self, data, peekSize = 16, peekStep = 1):
        """"""
        Tries to guess which values in the given data are valid pointers,
        and reads some data from them.

        @type  data: str
        @param data: Binary data to find pointers in.

        @type  peekSize: int
        @param peekSize: Number of bytes to read from each pointer found.

        @type  peekStep: int
        @param peekStep: Expected data alignment.
            Tipically you specify 1 when data alignment is unknown,
            or 4 when you expect data to be DWORD aligned.
            Any other value may be specified.

        @rtype:  dict( str S{->} str )
        @return: Dictionary mapping stack offsets to the data they point to.
        """"""
        aProcess = self.get_process()
        return aProcess.peek_pointers_in_data(data, peekSize, peekStep)","['def', 'peek_pointers_in_data', '(', 'self', ',', 'data', ',', 'peekSize', '=', '16', ',', 'peekStep', '=', '1', ')', ':', 'aProcess', '=', 'self', '.', 'get_process', '(', ')', 'return', 'aProcess', '.', 'peek_pointers_in_data', '(', 'data', ',', 'peekSize', ',', 'peekStep', ')']","Tries to guess which values in the given data are valid pointers,
        and reads some data from them.

        @type  data: str
        @param data: Binary data to find pointers in.

        @type  peekSize: int
        @param peekSize: Number of bytes to read from each pointer found.

        @type  peekStep: int
        @param peekStep: Expected data alignment.
            Tipically you specify 1 when data alignment is unknown,
            or 4 when you expect data to be DWORD aligned.
            Any other value may be specified.

        @rtype:  dict( str S{->} str )
        @return: Dictionary mapping stack offsets to the data they point to.","['Tries', 'to', 'guess', 'which', 'values', 'in', 'the', 'given', 'data', 'are', 'valid', 'pointers', 'and', 'reads', 'some', 'data', 'from', 'them', '.']",python,R,1,True,1,train
22641,fabioz/PyDev.Debugger,pydevd_attach_to_process/winappdbg/process.py,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/process.py#L2430-L2467,"def peek_pointers_in_data(self, data, peekSize = 16, peekStep = 1):
        """"""
        Tries to guess which values in the given data are valid pointers,
        and reads some data from them.

        @see: L{peek}

        @type  data: str
        @param data: Binary data to find pointers in.

        @type  peekSize: int
        @param peekSize: Number of bytes to read from each pointer found.

        @type  peekStep: int
        @param peekStep: Expected data alignment.
            Tipically you specify 1 when data alignment is unknown,
            or 4 when you expect data to be DWORD aligned.
            Any other value may be specified.

        @rtype:  dict( str S{->} str )
        @return: Dictionary mapping stack offsets to the data they point to.
        """"""
        result = dict()
        ptrSize = win32.sizeof(win32.LPVOID)
        if ptrSize == 4:
            ptrFmt = '<L'
        else:
            ptrFmt = '<Q'
        if len(data) > 0:
            for i in compat.xrange(0, len(data), peekStep):
                packed          = data[i:i+ptrSize]
                if len(packed) == ptrSize:
                    address     = struct.unpack(ptrFmt, packed)[0]
##                    if not address & (~0xFFFF): continue
                    peek_data   = self.peek(address, peekSize)
                    if peek_data:
                        result[i] = peek_data
        return result","['def', 'peek_pointers_in_data', '(', 'self', ',', 'data', ',', 'peekSize', '=', '16', ',', 'peekStep', '=', '1', ')', ':', 'result', '=', 'dict', '(', ')', 'ptrSize', '=', 'win32', '.', 'sizeof', '(', 'win32', '.', 'LPVOID', ')', 'if', 'ptrSize', '==', '4', ':', 'ptrFmt', '=', ""'<L'"", 'else', ':', 'ptrFmt', '=', ""'<Q'"", 'if', 'len', '(', 'data', ')', '>', '0', ':', 'for', 'i', 'in', 'compat', '.', 'xrange', '(', '0', ',', 'len', '(', 'data', ')', ',', 'peekStep', ')', ':', 'packed', '=', 'data', '[', 'i', ':', 'i', '+', 'ptrSize', ']', 'if', 'len', '(', 'packed', ')', '==', 'ptrSize', ':', 'address', '=', 'struct', '.', 'unpack', '(', 'ptrFmt', ',', 'packed', ')', '[', '0', ']', '##                    if not address & (~0xFFFF): continue', 'peek_data', '=', 'self', '.', 'peek', '(', 'address', ',', 'peekSize', ')', 'if', 'peek_data', ':', 'result', '[', 'i', ']', '=', 'peek_data', 'return', 'result']","Tries to guess which values in the given data are valid pointers,
        and reads some data from them.

        @see: L{peek}

        @type  data: str
        @param data: Binary data to find pointers in.

        @type  peekSize: int
        @param peekSize: Number of bytes to read from each pointer found.

        @type  peekStep: int
        @param peekStep: Expected data alignment.
            Tipically you specify 1 when data alignment is unknown,
            or 4 when you expect data to be DWORD aligned.
            Any other value may be specified.

        @rtype:  dict( str S{->} str )
        @return: Dictionary mapping stack offsets to the data they point to.","['Tries', 'to', 'guess', 'which', 'values', 'in', 'the', 'given', 'data', 'are', 'valid', 'pointers', 'and', 'reads', 'some', 'data', 'from', 'them', '.']",python,R,1,True,1,train
24980,sanger-pathogens/circlator,circlator/bamfilter.py,https://github.com/sanger-pathogens/circlator/blob/a4befb8c9dbbcd4b3ad1899a95aa3e689d58b638/circlator/bamfilter.py#L73-L77,"def _all_reads_from_contig(self, contig, fout):
        '''Gets all reads from contig called ""contig"" and writes to fout'''
        sam_reader = pysam.Samfile(self.bam, ""rb"")
        for read in sam_reader.fetch(contig):
            print(mapping.aligned_read_to_read(read, ignore_quality=not self.fastq_out), file=fout)","['def', '_all_reads_from_contig', '(', 'self', ',', 'contig', ',', 'fout', ')', ':', 'sam_reader', '=', 'pysam', '.', 'Samfile', '(', 'self', '.', 'bam', ',', '""rb""', ')', 'for', 'read', 'in', 'sam_reader', '.', 'fetch', '(', 'contig', ')', ':', 'print', '(', 'mapping', '.', 'aligned_read_to_read', '(', 'read', ',', 'ignore_quality', '=', 'not', 'self', '.', 'fastq_out', ')', ',', 'file', '=', 'fout', ')']","Gets all reads from contig called ""contig"" and writes to fout","['Gets', 'all', 'reads', 'from', 'contig', 'called', 'contig', 'and', 'writes', 'to', 'fout']",python,R,1,True,1,train
28287,knipknap/exscript,Exscript/util/template.py,https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/template.py#L187-L200,"def paste_file(conn, filename, **kwargs):
    """"""
    Convenience wrapper around paste() that reads the template from a file
    instead.

    :type  conn: Exscript.protocols.Protocol
    :param conn: The connection on which to run the template.
    :type  filename: string
    :param filename: The name of the template file.
    :type  kwargs: dict
    :param kwargs: Variables to define in the template.
    """"""
    with open(filename, 'r') as fp:
        return _run(conn, None, fp.read(), {'no_prompt': True}, **kwargs)","['def', 'paste_file', '(', 'conn', ',', 'filename', ',', '*', '*', 'kwargs', ')', ':', 'with', 'open', '(', 'filename', ',', ""'r'"", ')', 'as', 'fp', ':', 'return', '_run', '(', 'conn', ',', 'None', ',', 'fp', '.', 'read', '(', ')', ',', '{', ""'no_prompt'"", ':', 'True', '}', ',', '*', '*', 'kwargs', ')']","Convenience wrapper around paste() that reads the template from a file
    instead.

    :type  conn: Exscript.protocols.Protocol
    :param conn: The connection on which to run the template.
    :type  filename: string
    :param filename: The name of the template file.
    :type  kwargs: dict
    :param kwargs: Variables to define in the template.","['Convenience', 'wrapper', 'around', 'paste', '()', 'that', 'reads', 'the', 'template', 'from', 'a', 'file', 'instead', '.']",python,R,1,True,1,train
28310,knipknap/exscript,Exscript/emulators/vdevice.py,https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/emulators/vdevice.py#L144-L160,"def add_commands_from_file(self, filename, autoprompt=True):
        """"""
        Wrapper around add_command_handler that reads the handlers from the
        file with the given name. The file is a Python script containing
        a list named 'commands' of tuples that map command names to
        handlers.

        :type  filename: str
        :param filename: The name of the file containing the tuples.
        :type  autoprompt: bool
        :param autoprompt: Whether to append a prompt to each response.
        """"""
        if autoprompt:
            deco = self._create_autoprompt_handler
        else:
            deco = None
        self.commands.add_from_file(filename, deco)","['def', 'add_commands_from_file', '(', 'self', ',', 'filename', ',', 'autoprompt', '=', 'True', ')', ':', 'if', 'autoprompt', ':', 'deco', '=', 'self', '.', '_create_autoprompt_handler', 'else', ':', 'deco', '=', 'None', 'self', '.', 'commands', '.', 'add_from_file', '(', 'filename', ',', 'deco', ')']","Wrapper around add_command_handler that reads the handlers from the
        file with the given name. The file is a Python script containing
        a list named 'commands' of tuples that map command names to
        handlers.

        :type  filename: str
        :param filename: The name of the file containing the tuples.
        :type  autoprompt: bool
        :param autoprompt: Whether to append a prompt to each response.","['Wrapper', 'around', 'add_command_handler', 'that', 'reads', 'the', 'handlers', 'from', 'the', 'file', 'with', 'the', 'given', 'name', '.', 'The', 'file', 'is', 'a', 'Python', 'script', 'containing', 'a', 'list', 'named', 'commands', 'of', 'tuples', 'that', 'map', 'command', 'names', 'to', 'handlers', '.']",python,R,1,True,1,train
28340,knipknap/exscript,Exscript/emulators/command.py,https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/emulators/command.py#L73-L96,"def add_from_file(self, filename, handler_decorator=None):
        """"""
        Wrapper around add() that reads the handlers from the
        file with the given name. The file is a Python script containing
        a list named 'commands' of tuples that map command names to
        handlers.

        :type  filename: str
        :param filename: The name of the file containing the tuples.
        :type  handler_decorator: function
        :param handler_decorator: A function that is used to decorate
               each of the handlers in the file.
        """"""
        args = {}
        execfile(filename, args)
        commands = args.get('commands')
        if commands is None:
            raise Exception(filename + ' has no variable named ""commands""')
        elif not hasattr(commands, '__iter__'):
            raise Exception(filename + ': ""commands"" is not iterable')
        for key, handler in commands:
            if handler_decorator:
                handler = handler_decorator(handler)
            self.add(key, handler)","['def', 'add_from_file', '(', 'self', ',', 'filename', ',', 'handler_decorator', '=', 'None', ')', ':', 'args', '=', '{', '}', 'execfile', '(', 'filename', ',', 'args', ')', 'commands', '=', 'args', '.', 'get', '(', ""'commands'"", ')', 'if', 'commands', 'is', 'None', ':', 'raise', 'Exception', '(', 'filename', '+', '\' has no variable named ""commands""\'', ')', 'elif', 'not', 'hasattr', '(', 'commands', ',', ""'__iter__'"", ')', ':', 'raise', 'Exception', '(', 'filename', '+', '\': ""commands"" is not iterable\'', ')', 'for', 'key', ',', 'handler', 'in', 'commands', ':', 'if', 'handler_decorator', ':', 'handler', '=', 'handler_decorator', '(', 'handler', ')', 'self', '.', 'add', '(', 'key', ',', 'handler', ')']","Wrapper around add() that reads the handlers from the
        file with the given name. The file is a Python script containing
        a list named 'commands' of tuples that map command names to
        handlers.

        :type  filename: str
        :param filename: The name of the file containing the tuples.
        :type  handler_decorator: function
        :param handler_decorator: A function that is used to decorate
               each of the handlers in the file.","['Wrapper', 'around', 'add', '()', 'that', 'reads', 'the', 'handlers', 'from', 'the', 'file', 'with', 'the', 'given', 'name', '.', 'The', 'file', 'is', 'a', 'Python', 'script', 'containing', 'a', 'list', 'named', 'commands', 'of', 'tuples', 'that', 'map', 'command', 'names', 'to', 'handlers', '.']",python,R,1,True,1,train
28375,knipknap/exscript,Exscript/util/mail.py,https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/mail.py#L417-L430,"def from_template(filename, **kwargs):
    """"""
    Like from_template_string(), but reads the template from the file with
    the given name instead.

    :type  filename: string
    :param filename: The name of the template file.
    :type  kwargs: str
    :param kwargs: Variables to replace in the template.
    :rtype:  Mail
    :return: The resulting mail.
    """"""
    with open(filename) as fp:
        return from_template_string(fp.read(), **kwargs)","['def', 'from_template', '(', 'filename', ',', '*', '*', 'kwargs', ')', ':', 'with', 'open', '(', 'filename', ')', 'as', 'fp', ':', 'return', 'from_template_string', '(', 'fp', '.', 'read', '(', ')', ',', '*', '*', 'kwargs', ')']","Like from_template_string(), but reads the template from the file with
    the given name instead.

    :type  filename: string
    :param filename: The name of the template file.
    :type  kwargs: str
    :param kwargs: Variables to replace in the template.
    :rtype:  Mail
    :return: The resulting mail.","['Like', 'from_template_string', '()', 'but', 'reads', 'the', 'template', 'from', 'the', 'file', 'with', 'the', 'given', 'name', 'instead', '.']",python,R,1,True,1,train
28783,jonhadfield/python-hosts,python_hosts/hosts.py,https://github.com/jonhadfield/python-hosts/blob/9ccaa8edc63418a91f10bf732b26070f21dd2ad0/python_hosts/hosts.py#L410-L439,"def populate_entries(self):
        """"""
        Called by the initialiser of Hosts. This reads the entries from the local hosts file,
        converts them into instances of HostsEntry and adds them to the Hosts list of entries.
        :return: None
        """"""
        try:
            with open(self.hosts_path, 'r') as hosts_file:
                hosts_entries = [line for line in hosts_file]
                for hosts_entry in hosts_entries:
                    entry_type = HostsEntry.get_entry_type(hosts_entry)
                    if entry_type == ""comment"":
                        hosts_entry = hosts_entry.replace(""\r"", """")
                        hosts_entry = hosts_entry.replace(""\n"", """")
                        self.entries.append(HostsEntry(entry_type=""comment"",
                                                       comment=hosts_entry))
                    elif entry_type == ""blank"":
                        self.entries.append(HostsEntry(entry_type=""blank""))
                    elif entry_type in (""ipv4"", ""ipv6""):
                        chunked_entry = hosts_entry.split()
                        stripped_name_list = [name.strip() for name in chunked_entry[1:]]

                        self.entries.append(
                            HostsEntry(
                                entry_type=entry_type,
                                address=chunked_entry[0].strip(),
                                names=stripped_name_list))
        except IOError:
            return {'result': 'failed',
                    'message': 'Cannot read: {0}.'.format(self.hosts_path)}","['def', 'populate_entries', '(', 'self', ')', ':', 'try', ':', 'with', 'open', '(', 'self', '.', 'hosts_path', ',', ""'r'"", ')', 'as', 'hosts_file', ':', 'hosts_entries', '=', '[', 'line', 'for', 'line', 'in', 'hosts_file', ']', 'for', 'hosts_entry', 'in', 'hosts_entries', ':', 'entry_type', '=', 'HostsEntry', '.', 'get_entry_type', '(', 'hosts_entry', ')', 'if', 'entry_type', '==', '""comment""', ':', 'hosts_entry', '=', 'hosts_entry', '.', 'replace', '(', '""\\r""', ',', '""""', ')', 'hosts_entry', '=', 'hosts_entry', '.', 'replace', '(', '""\\n""', ',', '""""', ')', 'self', '.', 'entries', '.', 'append', '(', 'HostsEntry', '(', 'entry_type', '=', '""comment""', ',', 'comment', '=', 'hosts_entry', ')', ')', 'elif', 'entry_type', '==', '""blank""', ':', 'self', '.', 'entries', '.', 'append', '(', 'HostsEntry', '(', 'entry_type', '=', '""blank""', ')', ')', 'elif', 'entry_type', 'in', '(', '""ipv4""', ',', '""ipv6""', ')', ':', 'chunked_entry', '=', 'hosts_entry', '.', 'split', '(', ')', 'stripped_name_list', '=', '[', 'name', '.', 'strip', '(', ')', 'for', 'name', 'in', 'chunked_entry', '[', '1', ':', ']', ']', 'self', '.', 'entries', '.', 'append', '(', 'HostsEntry', '(', 'entry_type', '=', 'entry_type', ',', 'address', '=', 'chunked_entry', '[', '0', ']', '.', 'strip', '(', ')', ',', 'names', '=', 'stripped_name_list', ')', ')', 'except', 'IOError', ':', 'return', '{', ""'result'"", ':', ""'failed'"", ',', ""'message'"", ':', ""'Cannot read: {0}.'"", '.', 'format', '(', 'self', '.', 'hosts_path', ')', '}']","Called by the initialiser of Hosts. This reads the entries from the local hosts file,
        converts them into instances of HostsEntry and adds them to the Hosts list of entries.
        :return: None","['Called', 'by', 'the', 'initialiser', 'of', 'Hosts', '.', 'This', 'reads', 'the', 'entries', 'from', 'the', 'local', 'hosts', 'file', 'converts', 'them', 'into', 'instances', 'of', 'HostsEntry', 'and', 'adds', 'them', 'to', 'the', 'Hosts', 'list', 'of', 'entries', '.', ':', 'return', ':', 'None']",python,R,1,True,1,train
3899,crate/crash,src/crate/crash/command.py,https://github.com/crate/crash/blob/32d3ddc78fd2f7848ed2b99d9cd8889e322528d9/src/crate/crash/command.py#L121-L178,"def get_parser(output_formats=[], conf=None):
    """"""
    Create an argument parser that reads default values from a
    configuration file if provided.
    """"""

    def _conf_or_default(key, value):
        return value if conf is None else conf.get_or_set(key, value)

    parser = ArgumentParser(description='crate shell')
    parser.add_argument('-v', '--verbose', action='count',
                        dest='verbose', default=_conf_or_default('verbosity', 0),
                        help='print debug information to STDOUT')
    parser.add_argument('-A', '--no-autocomplete', action='store_false',
                        dest='autocomplete',
                        default=_conf_or_default('autocomplete', True),
                        help='disable SQL keywords autocompletion')
    parser.add_argument('-a', '--autocapitalize', action='store_true',
                        dest='autocapitalize',
                        default=False,
                        help='enable automatic capitalization of SQL keywords while typing')
    parser.add_argument('-U', '--username', type=str, metavar='USERNAME',
                        help='Authenticate as USERNAME.')
    parser.add_argument('-W', '--password', action='store_true',
                        dest='force_passwd_prompt', default=_conf_or_default('force_passwd_prompt', False),
                        help='force a password prompt')
    parser.add_argument('--schema', type=str,
                        help='default schema for statements if schema is not explicitly stated in queries')
    parser.add_argument('--history', type=str, metavar='FILENAME',
                        help='Use FILENAME as a history file', default=HISTORY_PATH)
    parser.add_argument('--config', type=str, metavar='FILENAME',
                        help='use FILENAME as a configuration file', default=CONFIG_PATH)

    group = parser.add_mutually_exclusive_group()
    group.add_argument('-c', '--command', type=str, metavar='STATEMENT',
                       help='Execute the STATEMENT and exit.')
    group.add_argument('--sysinfo', action='store_true', default=False,
                       help='print system and cluster information')

    parser.add_argument('--hosts', type=str, nargs='*',
                        default=_conf_or_default('hosts', ['localhost:4200']),
                        help='connect to HOSTS.', metavar='HOSTS')
    parser.add_argument('--verify-ssl', type=boolean, default=True,
                        help='force the verification of the server SSL certificate')
    parser.add_argument('--cert-file', type=file_with_permissions, metavar='FILENAME',
                        help='use FILENAME as the client certificate file')
    parser.add_argument('--key-file', type=file_with_permissions, metavar='FILENAME',
                        help='Use FILENAME as the client certificate key file')
    parser.add_argument('--ca-cert-file', type=file_with_permissions, metavar='FILENAME',
                        help='use FILENAME as the CA certificate file')
    parser.add_argument('--format', type=str,
                        default=_conf_or_default('format', 'tabular'),
                        choices=output_formats, metavar='FORMAT',
                        help='the output FORMAT of the SQL response')
    parser.add_argument('--version', action='store_true', default=False,
                        help='print the Crash version and exit')

    return parser","['def', 'get_parser', '(', 'output_formats', '=', '[', ']', ',', 'conf', '=', 'None', ')', ':', 'def', '_conf_or_default', '(', 'key', ',', 'value', ')', ':', 'return', 'value', 'if', 'conf', 'is', 'None', 'else', 'conf', '.', 'get_or_set', '(', 'key', ',', 'value', ')', 'parser', '=', 'ArgumentParser', '(', 'description', '=', ""'crate shell'"", ')', 'parser', '.', 'add_argument', '(', ""'-v'"", ',', ""'--verbose'"", ',', 'action', '=', ""'count'"", ',', 'dest', '=', ""'verbose'"", ',', 'default', '=', '_conf_or_default', '(', ""'verbosity'"", ',', '0', ')', ',', 'help', '=', ""'print debug information to STDOUT'"", ')', 'parser', '.', 'add_argument', '(', ""'-A'"", ',', ""'--no-autocomplete'"", ',', 'action', '=', ""'store_false'"", ',', 'dest', '=', ""'autocomplete'"", ',', 'default', '=', '_conf_or_default', '(', ""'autocomplete'"", ',', 'True', ')', ',', 'help', '=', ""'disable SQL keywords autocompletion'"", ')', 'parser', '.', 'add_argument', '(', ""'-a'"", ',', ""'--autocapitalize'"", ',', 'action', '=', ""'store_true'"", ',', 'dest', '=', ""'autocapitalize'"", ',', 'default', '=', 'False', ',', 'help', '=', ""'enable automatic capitalization of SQL keywords while typing'"", ')', 'parser', '.', 'add_argument', '(', ""'-U'"", ',', ""'--username'"", ',', 'type', '=', 'str', ',', 'metavar', '=', ""'USERNAME'"", ',', 'help', '=', ""'Authenticate as USERNAME.'"", ')', 'parser', '.', 'add_argument', '(', ""'-W'"", ',', ""'--password'"", ',', 'action', '=', ""'store_true'"", ',', 'dest', '=', ""'force_passwd_prompt'"", ',', 'default', '=', '_conf_or_default', '(', ""'force_passwd_prompt'"", ',', 'False', ')', ',', 'help', '=', ""'force a password prompt'"", ')', 'parser', '.', 'add_argument', '(', ""'--schema'"", ',', 'type', '=', 'str', ',', 'help', '=', ""'default schema for statements if schema is not explicitly stated in queries'"", ')', 'parser', '.', 'add_argument', '(', ""'--history'"", ',', 'type', '=', 'str', ',', 'metavar', '=', ""'FILENAME'"", ',', 'help', '=', ""'Use FILENAME as a history file'"", ',', 'default', '=', 'HISTORY_PATH', ')', 'parser', '.', 'add_argument', '(', ""'--config'"", ',', 'type', '=', 'str', ',', 'metavar', '=', ""'FILENAME'"", ',', 'help', '=', ""'use FILENAME as a configuration file'"", ',', 'default', '=', 'CONFIG_PATH', ')', 'group', '=', 'parser', '.', 'add_mutually_exclusive_group', '(', ')', 'group', '.', 'add_argument', '(', ""'-c'"", ',', ""'--command'"", ',', 'type', '=', 'str', ',', 'metavar', '=', ""'STATEMENT'"", ',', 'help', '=', ""'Execute the STATEMENT and exit.'"", ')', 'group', '.', 'add_argument', '(', ""'--sysinfo'"", ',', 'action', '=', ""'store_true'"", ',', 'default', '=', 'False', ',', 'help', '=', ""'print system and cluster information'"", ')', 'parser', '.', 'add_argument', '(', ""'--hosts'"", ',', 'type', '=', 'str', ',', 'nargs', '=', ""'*'"", ',', 'default', '=', '_conf_or_default', '(', ""'hosts'"", ',', '[', ""'localhost:4200'"", ']', ')', ',', 'help', '=', ""'connect to HOSTS.'"", ',', 'metavar', '=', ""'HOSTS'"", ')', 'parser', '.', 'add_argument', '(', ""'--verify-ssl'"", ',', 'type', '=', 'boolean', ',', 'default', '=', 'True', ',', 'help', '=', ""'force the verification of the server SSL certificate'"", ')', 'parser', '.', 'add_argument', '(', ""'--cert-file'"", ',', 'type', '=', 'file_with_permissions', ',', 'metavar', '=', ""'FILENAME'"", ',', 'help', '=', ""'use FILENAME as the client certificate file'"", ')', 'parser', '.', 'add_argument', '(', ""'--key-file'"", ',', 'type', '=', 'file_with_permissions', ',', 'metavar', '=', ""'FILENAME'"", ',', 'help', '=', ""'Use FILENAME as the client certificate key file'"", ')', 'parser', '.', 'add_argument', '(', ""'--ca-cert-file'"", ',', 'type', '=', 'file_with_permissions', ',', 'metavar', '=', ""'FILENAME'"", ',', 'help', '=', ""'use FILENAME as the CA certificate file'"", ')', 'parser', '.', 'add_argument', '(', ""'--format'"", ',', 'type', '=', 'str', ',', 'default', '=', '_conf_or_default', '(', ""'format'"", ',', ""'tabular'"", ')', ',', 'choices', '=', 'output_formats', ',', 'metavar', '=', ""'FORMAT'"", ',', 'help', '=', ""'the output FORMAT of the SQL response'"", ')', 'parser', '.', 'add_argument', '(', ""'--version'"", ',', 'action', '=', ""'store_true'"", ',', 'default', '=', 'False', ',', 'help', '=', ""'print the Crash version and exit'"", ')', 'return', 'parser']","Create an argument parser that reads default values from a
    configuration file if provided.","['Create', 'an', 'argument', 'parser', 'that', 'reads', 'default', 'values', 'from', 'a', 'configuration', 'file', 'if', 'provided', '.']",python,R,1,True,1,train
7655,googledatalab/pydatalab,solutionbox/ml_workbench/tensorflow/trainer/feature_transforms.py,https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/solutionbox/ml_workbench/tensorflow/trainer/feature_transforms.py#L598-L698,"def build_csv_transforming_training_input_fn(schema,
                                             features,
                                             stats,
                                             analysis_output_dir,
                                             raw_data_file_pattern,
                                             training_batch_size,
                                             num_epochs=None,
                                             randomize_input=False,
                                             min_after_dequeue=1,
                                             reader_num_threads=1,
                                             allow_smaller_final_batch=True):
  """"""Creates training input_fn that reads raw csv data and applies transforms.

  Args:
    schema: schema list
    features: features dict
    stats: stats dict
    analysis_output_dir: output folder from analysis
    raw_data_file_pattern: file path, or list of files
    training_batch_size: An int specifying the batch size to use.
    num_epochs: numer of epochs to read from the files. Use None to read forever.
    randomize_input: If true, the input rows are read out of order. This
        randomness is limited by the min_after_dequeue value.
    min_after_dequeue: Minimum number elements in the reading queue after a
        dequeue, used to ensure a level of mixing of elements. Only used if
        randomize_input is True.
    reader_num_threads: The number of threads enqueuing data.
    allow_smaller_final_batch: If false, fractional batches at the end of
        training or evaluation are not used.

  Returns:
    An input_fn suitable for training that reads raw csv training data and
    applies transforms.

  """"""

  def raw_training_input_fn():
    """"""Training input function that reads raw data and applies transforms.""""""

    if isinstance(raw_data_file_pattern, six.string_types):
      filepath_list = [raw_data_file_pattern]
    else:
      filepath_list = raw_data_file_pattern

    files = []
    for path in filepath_list:
      files.extend(file_io.get_matching_files(path))

    filename_queue = tf.train.string_input_producer(
        files, num_epochs=num_epochs, shuffle=randomize_input)

    csv_id, csv_lines = tf.TextLineReader().read_up_to(filename_queue, training_batch_size)

    queue_capacity = (reader_num_threads + 3) * training_batch_size + min_after_dequeue
    if randomize_input:
      _, batch_csv_lines = tf.train.shuffle_batch(
          tensors=[csv_id, csv_lines],
          batch_size=training_batch_size,
          capacity=queue_capacity,
          min_after_dequeue=min_after_dequeue,
          enqueue_many=True,
          num_threads=reader_num_threads,
          allow_smaller_final_batch=allow_smaller_final_batch)

    else:
      _, batch_csv_lines = tf.train.batch(
          tensors=[csv_id, csv_lines],
          batch_size=training_batch_size,
          capacity=queue_capacity,
          enqueue_many=True,
          num_threads=reader_num_threads,
          allow_smaller_final_batch=allow_smaller_final_batch)

    csv_header, record_defaults = csv_header_and_defaults(features, schema, stats, keep_target=True)
    parsed_tensors = tf.decode_csv(batch_csv_lines, record_defaults, name='csv_to_tensors')
    raw_features = dict(zip(csv_header, parsed_tensors))

    transform_fn = make_preprocessing_fn(analysis_output_dir, features, keep_target=True)
    transformed_tensors = transform_fn(raw_features)

    # Expand the dims of non-sparse tensors. This is needed by tf.learn.
    transformed_features = {}
    for k, v in six.iteritems(transformed_tensors):
      if isinstance(v, tf.Tensor) and v.get_shape().ndims == 1:
        transformed_features[k] = tf.expand_dims(v, -1)
      else:
        transformed_features[k] = v

    # image_feature_engineering does not need to be called as images are not
    # supported in raw csv for training.

    # Remove the target tensor, and return it directly
    target_name = get_target_name(features)
    if not target_name or target_name not in transformed_features:
      raise ValueError('Cannot find target transform in features')

    transformed_target = transformed_features.pop(target_name)

    return transformed_features, transformed_target

  return raw_training_input_fn","['def', 'build_csv_transforming_training_input_fn', '(', 'schema', ',', 'features', ',', 'stats', ',', 'analysis_output_dir', ',', 'raw_data_file_pattern', ',', 'training_batch_size', ',', 'num_epochs', '=', 'None', ',', 'randomize_input', '=', 'False', ',', 'min_after_dequeue', '=', '1', ',', 'reader_num_threads', '=', '1', ',', 'allow_smaller_final_batch', '=', 'True', ')', ':', 'def', 'raw_training_input_fn', '(', ')', ':', '""""""Training input function that reads raw data and applies transforms.""""""', 'if', 'isinstance', '(', 'raw_data_file_pattern', ',', 'six', '.', 'string_types', ')', ':', 'filepath_list', '=', '[', 'raw_data_file_pattern', ']', 'else', ':', 'filepath_list', '=', 'raw_data_file_pattern', 'files', '=', '[', ']', 'for', 'path', 'in', 'filepath_list', ':', 'files', '.', 'extend', '(', 'file_io', '.', 'get_matching_files', '(', 'path', ')', ')', 'filename_queue', '=', 'tf', '.', 'train', '.', 'string_input_producer', '(', 'files', ',', 'num_epochs', '=', 'num_epochs', ',', 'shuffle', '=', 'randomize_input', ')', 'csv_id', ',', 'csv_lines', '=', 'tf', '.', 'TextLineReader', '(', ')', '.', 'read_up_to', '(', 'filename_queue', ',', 'training_batch_size', ')', 'queue_capacity', '=', '(', 'reader_num_threads', '+', '3', ')', '*', 'training_batch_size', '+', 'min_after_dequeue', 'if', 'randomize_input', ':', '_', ',', 'batch_csv_lines', '=', 'tf', '.', 'train', '.', 'shuffle_batch', '(', 'tensors', '=', '[', 'csv_id', ',', 'csv_lines', ']', ',', 'batch_size', '=', 'training_batch_size', ',', 'capacity', '=', 'queue_capacity', ',', 'min_after_dequeue', '=', 'min_after_dequeue', ',', 'enqueue_many', '=', 'True', ',', 'num_threads', '=', 'reader_num_threads', ',', 'allow_smaller_final_batch', '=', 'allow_smaller_final_batch', ')', 'else', ':', '_', ',', 'batch_csv_lines', '=', 'tf', '.', 'train', '.', 'batch', '(', 'tensors', '=', '[', 'csv_id', ',', 'csv_lines', ']', ',', 'batch_size', '=', 'training_batch_size', ',', 'capacity', '=', 'queue_capacity', ',', 'enqueue_many', '=', 'True', ',', 'num_threads', '=', 'reader_num_threads', ',', 'allow_smaller_final_batch', '=', 'allow_smaller_final_batch', ')', 'csv_header', ',', 'record_defaults', '=', 'csv_header_and_defaults', '(', 'features', ',', 'schema', ',', 'stats', ',', 'keep_target', '=', 'True', ')', 'parsed_tensors', '=', 'tf', '.', 'decode_csv', '(', 'batch_csv_lines', ',', 'record_defaults', ',', 'name', '=', ""'csv_to_tensors'"", ')', 'raw_features', '=', 'dict', '(', 'zip', '(', 'csv_header', ',', 'parsed_tensors', ')', ')', 'transform_fn', '=', 'make_preprocessing_fn', '(', 'analysis_output_dir', ',', 'features', ',', 'keep_target', '=', 'True', ')', 'transformed_tensors', '=', 'transform_fn', '(', 'raw_features', ')', '# Expand the dims of non-sparse tensors. This is needed by tf.learn.', 'transformed_features', '=', '{', '}', 'for', 'k', ',', 'v', 'in', 'six', '.', 'iteritems', '(', 'transformed_tensors', ')', ':', 'if', 'isinstance', '(', 'v', ',', 'tf', '.', 'Tensor', ')', 'and', 'v', '.', 'get_shape', '(', ')', '.', 'ndims', '==', '1', ':', 'transformed_features', '[', 'k', ']', '=', 'tf', '.', 'expand_dims', '(', 'v', ',', '-', '1', ')', 'else', ':', 'transformed_features', '[', 'k', ']', '=', 'v', '# image_feature_engineering does not need to be called as images are not', '# supported in raw csv for training.', '# Remove the target tensor, and return it directly', 'target_name', '=', 'get_target_name', '(', 'features', ')', 'if', 'not', 'target_name', 'or', 'target_name', 'not', 'in', 'transformed_features', ':', 'raise', 'ValueError', '(', ""'Cannot find target transform in features'"", ')', 'transformed_target', '=', 'transformed_features', '.', 'pop', '(', 'target_name', ')', 'return', 'transformed_features', ',', 'transformed_target', 'return', 'raw_training_input_fn']","Creates training input_fn that reads raw csv data and applies transforms.

  Args:
    schema: schema list
    features: features dict
    stats: stats dict
    analysis_output_dir: output folder from analysis
    raw_data_file_pattern: file path, or list of files
    training_batch_size: An int specifying the batch size to use.
    num_epochs: numer of epochs to read from the files. Use None to read forever.
    randomize_input: If true, the input rows are read out of order. This
        randomness is limited by the min_after_dequeue value.
    min_after_dequeue: Minimum number elements in the reading queue after a
        dequeue, used to ensure a level of mixing of elements. Only used if
        randomize_input is True.
    reader_num_threads: The number of threads enqueuing data.
    allow_smaller_final_batch: If false, fractional batches at the end of
        training or evaluation are not used.

  Returns:
    An input_fn suitable for training that reads raw csv training data and
    applies transforms.","['Creates', 'training', 'input_fn', 'that', 'reads', 'raw', 'csv', 'data', 'and', 'applies', 'transforms', '.']",python,R,1,True,1,train
20928,msoulier/tftpy,tftpy/TftpClient.py,https://github.com/msoulier/tftpy/blob/af2f2fe89a3bf45748b78703820efb0986a8207a/tftpy/TftpClient.py#L74-L107,"def upload(self, filename, input, packethook=None, timeout=SOCK_TIMEOUT):
        """"""This method initiates a tftp upload to the configured remote host,
        uploading the filename passed. It reads the file from input, which
        can be a file-like object or a path to a local file. If a packethook
        is provided, it must be a function that takes a single parameter,
        which will be a copy of each DAT packet sent in the form of a
        TftpPacketDAT object. The timeout parameter may be used to override
        the default SOCK_TIMEOUT setting, which is the amount of time that
        the client will wait for a DAT packet to be ACKd by the server.

        Note: If input is a hyphen, stdin is used.""""""
        self.context = TftpContextClientUpload(self.host,
                                               self.iport,
                                               filename,
                                               input,
                                               self.options,
                                               packethook,
                                               timeout,
                                               localip = self.localip)
        self.context.start()
        # Upload happens here
        self.context.end()

        metrics = self.context.metrics

        log.info('')
        log.info(""Upload complete."")
        if metrics.duration == 0:
            log.info(""Duration too short, rate undetermined"")
        else:
            log.info(""Uploaded %d bytes in %.2f seconds"" % (metrics.bytes, metrics.duration))
            log.info(""Average rate: %.2f kbps"" % metrics.kbps)
        log.info(""%.2f bytes in resent data"" % metrics.resent_bytes)
        log.info(""Resent %d packets"" % metrics.dupcount)","['def', 'upload', '(', 'self', ',', 'filename', ',', 'input', ',', 'packethook', '=', 'None', ',', 'timeout', '=', 'SOCK_TIMEOUT', ')', ':', 'self', '.', 'context', '=', 'TftpContextClientUpload', '(', 'self', '.', 'host', ',', 'self', '.', 'iport', ',', 'filename', ',', 'input', ',', 'self', '.', 'options', ',', 'packethook', ',', 'timeout', ',', 'localip', '=', 'self', '.', 'localip', ')', 'self', '.', 'context', '.', 'start', '(', ')', '# Upload happens here', 'self', '.', 'context', '.', 'end', '(', ')', 'metrics', '=', 'self', '.', 'context', '.', 'metrics', 'log', '.', 'info', '(', ""''"", ')', 'log', '.', 'info', '(', '""Upload complete.""', ')', 'if', 'metrics', '.', 'duration', '==', '0', ':', 'log', '.', 'info', '(', '""Duration too short, rate undetermined""', ')', 'else', ':', 'log', '.', 'info', '(', '""Uploaded %d bytes in %.2f seconds""', '%', '(', 'metrics', '.', 'bytes', ',', 'metrics', '.', 'duration', ')', ')', 'log', '.', 'info', '(', '""Average rate: %.2f kbps""', '%', 'metrics', '.', 'kbps', ')', 'log', '.', 'info', '(', '""%.2f bytes in resent data""', '%', 'metrics', '.', 'resent_bytes', ')', 'log', '.', 'info', '(', '""Resent %d packets""', '%', 'metrics', '.', 'dupcount', ')']","This method initiates a tftp upload to the configured remote host,
        uploading the filename passed. It reads the file from input, which
        can be a file-like object or a path to a local file. If a packethook
        is provided, it must be a function that takes a single parameter,
        which will be a copy of each DAT packet sent in the form of a
        TftpPacketDAT object. The timeout parameter may be used to override
        the default SOCK_TIMEOUT setting, which is the amount of time that
        the client will wait for a DAT packet to be ACKd by the server.

        Note: If input is a hyphen, stdin is used.","['This', 'method', 'initiates', 'a', 'tftp', 'upload', 'to', 'the', 'configured', 'remote', 'host', 'uploading', 'the', 'filename', 'passed', '.', 'It', 'reads', 'the', 'file', 'from', 'input', 'which', 'can', 'be', 'a', 'file', '-', 'like', 'object', 'or', 'a', 'path', 'to', 'a', 'local', 'file', '.', 'If', 'a', 'packethook', 'is', 'provided', 'it', 'must', 'be', 'a', 'function', 'that', 'takes', 'a', 'single', 'parameter', 'which', 'will', 'be', 'a', 'copy', 'of', 'each', 'DAT', 'packet', 'sent', 'in', 'the', 'form', 'of', 'a', 'TftpPacketDAT', 'object', '.', 'The', 'timeout', 'parameter', 'may', 'be', 'used', 'to', 'override', 'the', 'default', 'SOCK_TIMEOUT', 'setting', 'which', 'is', 'the', 'amount', 'of', 'time', 'that', 'the', 'client', 'will', 'wait', 'for', 'a', 'DAT', 'packet', 'to', 'be', 'ACKd', 'by', 'the', 'server', '.']",python,R,1,True,1,train
24331,cmbruns/pyopenvr,src/openvr/__init__.py,https://github.com/cmbruns/pyopenvr/blob/68395d26bb3df6ab1f0f059c38d441f962938be6/src/openvr/__init__.py#L6205-L6211,"def read(self, ulBuffer, pDst, unBytes):
        """"""reads up to unBytes from buffer into *pDst, returning number of bytes read in *punRead""""""

        fn = self.function_table.read
        punRead = c_uint32()
        result = fn(ulBuffer, pDst, unBytes, byref(punRead))
        return result, punRead.value","['def', 'read', '(', 'self', ',', 'ulBuffer', ',', 'pDst', ',', 'unBytes', ')', ':', 'fn', '=', 'self', '.', 'function_table', '.', 'read', 'punRead', '=', 'c_uint32', '(', ')', 'result', '=', 'fn', '(', 'ulBuffer', ',', 'pDst', ',', 'unBytes', ',', 'byref', '(', 'punRead', ')', ')', 'return', 'result', ',', 'punRead', '.', 'value']","reads up to unBytes from buffer into *pDst, returning number of bytes read in *punRead","['reads', 'up', 'to', 'unBytes', 'from', 'buffer', 'into', '*', 'pDst', 'returning', 'number', 'of', 'bytes', 'read', 'in', '*', 'punRead']",python,R,1,True,1,train
27361,Kozea/cairocffi,cairocffi/surfaces.py,https://github.com/Kozea/cairocffi/blob/450853add7e32eea20985b6aa5f54d9cb3cd04fe/cairocffi/surfaces.py#L24-L33,"def _make_read_func(file_obj):
    """"""Return a CFFI callback that reads from a file-like object.""""""
    @ffi.callback(""cairo_read_func_t"", error=constants.STATUS_READ_ERROR)
    def read_func(_closure, data, length):
        string = file_obj.read(length)
        if len(string) < length:  # EOF too early
            return constants.STATUS_READ_ERROR
        ffi.buffer(data, length)[:len(string)] = string
        return constants.STATUS_SUCCESS
    return read_func","['def', '_make_read_func', '(', 'file_obj', ')', ':', '@', 'ffi', '.', 'callback', '(', '""cairo_read_func_t""', ',', 'error', '=', 'constants', '.', 'STATUS_READ_ERROR', ')', 'def', 'read_func', '(', '_closure', ',', 'data', ',', 'length', ')', ':', 'string', '=', 'file_obj', '.', 'read', '(', 'length', ')', 'if', 'len', '(', 'string', ')', '<', 'length', ':', '# EOF too early', 'return', 'constants', '.', 'STATUS_READ_ERROR', 'ffi', '.', 'buffer', '(', 'data', ',', 'length', ')', '[', ':', 'len', '(', 'string', ')', ']', '=', 'string', 'return', 'constants', '.', 'STATUS_SUCCESS', 'return', 'read_func']",Return a CFFI callback that reads from a file-like object.,"['Return', 'a', 'CFFI', 'callback', 'that', 'reads', 'from', 'a', 'file', '-', 'like', 'object', '.']",python,R,1,True,1,train
13492,keepkey/python-keepkey,keepkeylib/transport.py,https://github.com/keepkey/python-keepkey/blob/8318e3a8c4025d499342130ce4305881a325c013/keepkeylib/transport.py#L71-L83,"def read(self):
        """"""
        If there is data available to be read from the transport, reads the data and tries to parse it as a protobuf message.  If the parsing succeeds, return a protobuf object.
        Otherwise, returns None.
        """"""
        if not self.ready_to_read():
            return None

        data = self._read()
        if data is None:
            return None

        return self._parse_message(data)","['def', 'read', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'ready_to_read', '(', ')', ':', 'return', 'None', 'data', '=', 'self', '.', '_read', '(', ')', 'if', 'data', 'is', 'None', ':', 'return', 'None', 'return', 'self', '.', '_parse_message', '(', 'data', ')']","If there is data available to be read from the transport, reads the data and tries to parse it as a protobuf message.  If the parsing succeeds, return a protobuf object.
        Otherwise, returns None.","['If', 'there', 'is', 'data', 'available', 'to', 'be', 'read', 'from', 'the', 'transport', 'reads', 'the', 'data', 'and', 'tries', 'to', 'parse', 'it', 'as', 'a', 'protobuf', 'message', '.', 'If', 'the', 'parsing', 'succeeds', 'return', 'a', 'protobuf', 'object', '.', 'Otherwise', 'returns', 'None', '.']",python,R,1,True,1,train
27869,ValvePython/vpk,vpk/__init__.py,https://github.com/ValvePython/vpk/blob/cc522fc7febbf53efa5d58fcd1ad2103dae37ac8/vpk/__init__.py#L375-L422,"def read_index_iter(self):
        """"""Generator function that reads the file index from the vpk file

        yeilds (file_path, metadata)
        """"""

        with fopen(self.vpk_path, 'rb') as f:
            f.seek(self.header_length)

            while True:
                if self.version > 0 and f.tell() > self.tree_length + self.header_length:
                    raise ValueError(""Error parsing index (out of bounds)"")

                ext = _read_cstring(f)
                if ext == '':
                    break

                while True:
                    path = _read_cstring(f)
                    if path == '':
                        break
                    if path != ' ':
                        path = os.path.join(path, '')
                    else:
                        path = ''

                    while True:
                        name = _read_cstring(f)
                        if name == '':
                            break

                        (crc32,
                         preload_length,
                         archive_index,
                         archive_offset,
                         file_length,
                         suffix,
                         ) = metadata = list(struct.unpack(""IHHIIH"", f.read(18)))

                        if suffix != 0xffff:
                            raise ValueError(""Error while parsing index"")

                        if archive_index == 0x7fff:
                            metadata[3] = self.header_length + self.tree_length + archive_offset

                        metadata = (f.read(preload_length),) + tuple(metadata[:-1])

                        yield path + name + '.' + ext, metadata","['def', 'read_index_iter', '(', 'self', ')', ':', 'with', 'fopen', '(', 'self', '.', 'vpk_path', ',', ""'rb'"", ')', 'as', 'f', ':', 'f', '.', 'seek', '(', 'self', '.', 'header_length', ')', 'while', 'True', ':', 'if', 'self', '.', 'version', '>', '0', 'and', 'f', '.', 'tell', '(', ')', '>', 'self', '.', 'tree_length', '+', 'self', '.', 'header_length', ':', 'raise', 'ValueError', '(', '""Error parsing index (out of bounds)""', ')', 'ext', '=', '_read_cstring', '(', 'f', ')', 'if', 'ext', '==', ""''"", ':', 'break', 'while', 'True', ':', 'path', '=', '_read_cstring', '(', 'f', ')', 'if', 'path', '==', ""''"", ':', 'break', 'if', 'path', '!=', ""' '"", ':', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', ""''"", ')', 'else', ':', 'path', '=', ""''"", 'while', 'True', ':', 'name', '=', '_read_cstring', '(', 'f', ')', 'if', 'name', '==', ""''"", ':', 'break', '(', 'crc32', ',', 'preload_length', ',', 'archive_index', ',', 'archive_offset', ',', 'file_length', ',', 'suffix', ',', ')', '=', 'metadata', '=', 'list', '(', 'struct', '.', 'unpack', '(', '""IHHIIH""', ',', 'f', '.', 'read', '(', '18', ')', ')', ')', 'if', 'suffix', '!=', '0xffff', ':', 'raise', 'ValueError', '(', '""Error while parsing index""', ')', 'if', 'archive_index', '==', '0x7fff', ':', 'metadata', '[', '3', ']', '=', 'self', '.', 'header_length', '+', 'self', '.', 'tree_length', '+', 'archive_offset', 'metadata', '=', '(', 'f', '.', 'read', '(', 'preload_length', ')', ',', ')', '+', 'tuple', '(', 'metadata', '[', ':', '-', '1', ']', ')', 'yield', 'path', '+', 'name', '+', ""'.'"", '+', 'ext', ',', 'metadata']","Generator function that reads the file index from the vpk file

        yeilds (file_path, metadata)","['Generator', 'function', 'that', 'reads', 'the', 'file', 'index', 'from', 'the', 'vpk', 'file']",python,R,1,True,1,train
6472,alexandrovteam/pyimzML,pyimzml/ImzMLWriter.py,https://github.com/alexandrovteam/pyimzML/blob/baae0bea7279f9439113d6b2f61be528c0462b3f/pyimzml/ImzMLWriter.py#L241-L247,"def _read_mz(self, mz_offset, mz_len, mz_enc_len):
        '''reads a mz array from the currently open ibd file'''
        self.ibd.seek(mz_offset)
        data = self.ibd.read(mz_enc_len)
        self.ibd.seek(0, 2)
        data = self.mz_compression.decompress(data)
        return tuple(np.fromstring(data, dtype=self.mz_dtype))","['def', '_read_mz', '(', 'self', ',', 'mz_offset', ',', 'mz_len', ',', 'mz_enc_len', ')', ':', 'self', '.', 'ibd', '.', 'seek', '(', 'mz_offset', ')', 'data', '=', 'self', '.', 'ibd', '.', 'read', '(', 'mz_enc_len', ')', 'self', '.', 'ibd', '.', 'seek', '(', '0', ',', '2', ')', 'data', '=', 'self', '.', 'mz_compression', '.', 'decompress', '(', 'data', ')', 'return', 'tuple', '(', 'np', '.', 'fromstring', '(', 'data', ',', 'dtype', '=', 'self', '.', 'mz_dtype', ')', ')']",reads a mz array from the currently open ibd file,"['reads', 'a', 'mz', 'array', 'from', 'the', 'currently', 'open', 'ibd', 'file']",python,R,1,True,1,train
7023,peterjc/backports.lzma,backports/lzma/__init__.py,https://github.com/peterjc/backports.lzma/blob/6555d8b8e493a35159025b4cfc204dfb54c33d3e/backports/lzma/__init__.py#L297-L321,"def read1(self, size=-1):
        """"""Read up to size uncompressed bytes, while trying to avoid
        making multiple reads from the underlying stream.

        Returns b"""" if the file is at EOF.
        """"""
        # Usually, read1() calls _fp.read() at most once. However, sometimes
        # this does not give enough data for the decompressor to make progress.
        # In this case we make multiple reads, to avoid returning b"""".
        self._check_can_read()
        if size is None:
            #This is not needed on Python 3 where the comparison to zero
            #will fail with a TypeError. 
            raise TypeError(""Read size should be an integer, not None"")
        if (size == 0 or self._mode == _MODE_READ_EOF or
            not self._fill_buffer()):
            return b""""
        if 0 < size < len(self._buffer):
            data = self._buffer[:size]
            self._buffer = self._buffer[size:]
        else:
            data = self._buffer
            self._buffer = None
        self._pos += len(data)
        return data","['def', 'read1', '(', 'self', ',', 'size', '=', '-', '1', ')', ':', '# Usually, read1() calls _fp.read() at most once. However, sometimes', '# this does not give enough data for the decompressor to make progress.', '# In this case we make multiple reads, to avoid returning b"""".', 'self', '.', '_check_can_read', '(', ')', 'if', 'size', 'is', 'None', ':', '#This is not needed on Python 3 where the comparison to zero', '#will fail with a TypeError. ', 'raise', 'TypeError', '(', '""Read size should be an integer, not None""', ')', 'if', '(', 'size', '==', '0', 'or', 'self', '.', '_mode', '==', '_MODE_READ_EOF', 'or', 'not', 'self', '.', '_fill_buffer', '(', ')', ')', ':', 'return', 'b""""', 'if', '0', '<', 'size', '<', 'len', '(', 'self', '.', '_buffer', ')', ':', 'data', '=', 'self', '.', '_buffer', '[', ':', 'size', ']', 'self', '.', '_buffer', '=', 'self', '.', '_buffer', '[', 'size', ':', ']', 'else', ':', 'data', '=', 'self', '.', '_buffer', 'self', '.', '_buffer', '=', 'None', 'self', '.', '_pos', '+=', 'len', '(', 'data', ')', 'return', 'data']","Read up to size uncompressed bytes, while trying to avoid
        making multiple reads from the underlying stream.

        Returns b"""" if the file is at EOF.","['Read', 'up', 'to', 'size', 'uncompressed', 'bytes', 'while', 'trying', 'to', 'avoid', 'making', 'multiple', 'reads', 'from', 'the', 'underlying', 'stream', '.']",python,R,1,True,1,train
7900,cloudmesh/cloudmesh-common,cloudmesh/common/BaseConfigDict.py,https://github.com/cloudmesh/cloudmesh-common/blob/ae4fae09cd78205d179ea692dc58f0b0c8fea2b8/cloudmesh/common/BaseConfigDict.py#L129-L185,"def read_yaml_config(filename, check=True, osreplace=True, exit=True):
    """"""
    reads in a yaml file from the specified filename. If check is set to true
    the code will fail if the file does not exist. However if it is set to
    false and the file does not exist, None is returned.

    :param exit: if true is exist with sys exit
    :param osreplace: if true replaces environment variables from the OS
    :param filename: the file name
    :param check: if True fails if the file does not exist,
                  if False and the file does not exist return will be None
    """"""
    location = filename
    if location is not None:
        location = path_expand(location)

    if not os.path.exists(location) and not check:
        return None

    if check and os.path.exists(location):

        # test for tab in yaml file
        if check_file_for_tabs(location):
            log.error(""The file {0} contains tabs. yaml ""
                      ""Files are not allowed to contain tabs"".format(location))
            sys.exit()
        result = None
        try:

            if osreplace:
                result = open(location, 'r').read()
                t = Template(result)
                result = t.substitute(os.environ)

                # data = yaml.safe_load(result)
                data = ordered_load(result, yaml.SafeLoader)
            else:
                f = open(location, ""r"")

                # data = yaml.safe_load(f)

                data = ordered_load(result, yaml.SafeLoader)
                f.close()

            return data
        except Exception as e:
            log.error(
                ""The file {0} fails with a yaml read error"".format(filename))
            Error.traceback(e)
            sys.exit()

    else:
        log.error(""The file {0} does not exist."".format(filename))
        if exit:
            sys.exit()

    return None","['def', 'read_yaml_config', '(', 'filename', ',', 'check', '=', 'True', ',', 'osreplace', '=', 'True', ',', 'exit', '=', 'True', ')', ':', 'location', '=', 'filename', 'if', 'location', 'is', 'not', 'None', ':', 'location', '=', 'path_expand', '(', 'location', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'location', ')', 'and', 'not', 'check', ':', 'return', 'None', 'if', 'check', 'and', 'os', '.', 'path', '.', 'exists', '(', 'location', ')', ':', '# test for tab in yaml file', 'if', 'check_file_for_tabs', '(', 'location', ')', ':', 'log', '.', 'error', '(', '""The file {0} contains tabs. yaml ""', '""Files are not allowed to contain tabs""', '.', 'format', '(', 'location', ')', ')', 'sys', '.', 'exit', '(', ')', 'result', '=', 'None', 'try', ':', 'if', 'osreplace', ':', 'result', '=', 'open', '(', 'location', ',', ""'r'"", ')', '.', 'read', '(', ')', 't', '=', 'Template', '(', 'result', ')', 'result', '=', 't', '.', 'substitute', '(', 'os', '.', 'environ', ')', '# data = yaml.safe_load(result)', 'data', '=', 'ordered_load', '(', 'result', ',', 'yaml', '.', 'SafeLoader', ')', 'else', ':', 'f', '=', 'open', '(', 'location', ',', '""r""', ')', '# data = yaml.safe_load(f)', 'data', '=', 'ordered_load', '(', 'result', ',', 'yaml', '.', 'SafeLoader', ')', 'f', '.', 'close', '(', ')', 'return', 'data', 'except', 'Exception', 'as', 'e', ':', 'log', '.', 'error', '(', '""The file {0} fails with a yaml read error""', '.', 'format', '(', 'filename', ')', ')', 'Error', '.', 'traceback', '(', 'e', ')', 'sys', '.', 'exit', '(', ')', 'else', ':', 'log', '.', 'error', '(', '""The file {0} does not exist.""', '.', 'format', '(', 'filename', ')', ')', 'if', 'exit', ':', 'sys', '.', 'exit', '(', ')', 'return', 'None']","reads in a yaml file from the specified filename. If check is set to true
    the code will fail if the file does not exist. However if it is set to
    false and the file does not exist, None is returned.

    :param exit: if true is exist with sys exit
    :param osreplace: if true replaces environment variables from the OS
    :param filename: the file name
    :param check: if True fails if the file does not exist,
                  if False and the file does not exist return will be None","['reads', 'in', 'a', 'yaml', 'file', 'from', 'the', 'specified', 'filename', '.', 'If', 'check', 'is', 'set', 'to', 'true', 'the', 'code', 'will', 'fail', 'if', 'the', 'file', 'does', 'not', 'exist', '.', 'However', 'if', 'it', 'is', 'set', 'to', 'false', 'and', 'the', 'file', 'does', 'not', 'exist', 'None', 'is', 'returned', '.']",python,R,1,True,1,train
10607,denisenkom/pytds,src/pytds/tds_base.py,https://github.com/denisenkom/pytds/blob/7d875cab29134afdef719406831c1c6a0d7af48a/src/pytds/tds_base.py#L567-L583,"def readall_fast(stm, size):
    """"""
    Slightly faster version of readall, it reads no more than two chunks.
    Meaning that it can only be used to read small data that doesn't span
    more that two packets.

    :param stm: Stream to read from, should have read method.
    :param size: Number of bytes to read.
    :return:
    """"""
    buf, offset = stm.read_fast(size)
    if len(buf) - offset < size:
        # slow case
        buf = buf[offset:]
        buf += stm.recv(size - len(buf))
        return buf, 0
    return buf, offset","['def', 'readall_fast', '(', 'stm', ',', 'size', ')', ':', 'buf', ',', 'offset', '=', 'stm', '.', 'read_fast', '(', 'size', ')', 'if', 'len', '(', 'buf', ')', '-', 'offset', '<', 'size', ':', '# slow case', 'buf', '=', 'buf', '[', 'offset', ':', ']', 'buf', '+=', 'stm', '.', 'recv', '(', 'size', '-', 'len', '(', 'buf', ')', ')', 'return', 'buf', ',', '0', 'return', 'buf', ',', 'offset']","Slightly faster version of readall, it reads no more than two chunks.
    Meaning that it can only be used to read small data that doesn't span
    more that two packets.

    :param stm: Stream to read from, should have read method.
    :param size: Number of bytes to read.
    :return:","['Slightly', 'faster', 'version', 'of', 'readall', 'it', 'reads', 'no', 'more', 'than', 'two', 'chunks', '.', 'Meaning', 'that', 'it', 'can', 'only', 'be', 'used', 'to', 'read', 'small', 'data', 'that', 'doesn', 't', 'span', 'more', 'that', 'two', 'packets', '.']",python,R,1,True,1,train
14296,rakanalh/pocket-api,pocket/__init__.py,https://github.com/rakanalh/pocket-api/blob/d8222dd34e3aa5e545f9b8ba407fa277c734ab82/pocket/__init__.py#L291-L307,"def _get_method_params(self):
        """"""
        This method makes reading and filtering each method implemented
        in this class a more general approach. It reads the previous
        frame from Python and filters the params passed to the caller
        of _make_request.
        :return: a dictionary of caller's parameters and values
        :rtype: dict
        """"""
        caller = sys._getframe(2)
        var_names = list(caller.f_code.co_varnames)
        caller_locals = caller.f_locals

        var_names.remove('self')
        kwargs = {key: value for key, value in caller_locals.items()
                  if key in var_names and value is not None}
        return kwargs","['def', '_get_method_params', '(', 'self', ')', ':', 'caller', '=', 'sys', '.', '_getframe', '(', '2', ')', 'var_names', '=', 'list', '(', 'caller', '.', 'f_code', '.', 'co_varnames', ')', 'caller_locals', '=', 'caller', '.', 'f_locals', 'var_names', '.', 'remove', '(', ""'self'"", ')', 'kwargs', '=', '{', 'key', ':', 'value', 'for', 'key', ',', 'value', 'in', 'caller_locals', '.', 'items', '(', ')', 'if', 'key', 'in', 'var_names', 'and', 'value', 'is', 'not', 'None', '}', 'return', 'kwargs']","This method makes reading and filtering each method implemented
        in this class a more general approach. It reads the previous
        frame from Python and filters the params passed to the caller
        of _make_request.
        :return: a dictionary of caller's parameters and values
        :rtype: dict","['This', 'method', 'makes', 'reading', 'and', 'filtering', 'each', 'method', 'implemented', 'in', 'this', 'class', 'a', 'more', 'general', 'approach', '.', 'It', 'reads', 'the', 'previous', 'frame', 'from', 'Python', 'and', 'filters', 'the', 'params', 'passed', 'to', 'the', 'caller', 'of', '_make_request', '.', ':', 'return', ':', 'a', 'dictionary', 'of', 'caller', 's', 'parameters', 'and', 'values', ':', 'rtype', ':', 'dict']",python,R,1,True,1,train
19091,PmagPy/PmagPy,SPD/new_lj_thellier_gui_spd.py,https://github.com/PmagPy/PmagPy/blob/c7984f8809bf40fe112e53dcc311a33293b62d0b/SPD/new_lj_thellier_gui_spd.py#L1102-L1155,"def magic_read(self,infile):
        """"""
        reads  a Magic template file, puts data in a list of dictionaries
        """"""
#        print ""calling magic_read(self, infile)"", infile
        hold,magic_data,magic_record,magic_keys=[],[],{},[]
        try:
            f=open(infile,""r"")
        except:
            return [],'bad_file'
        d = f.readline()[:-1].strip('\n')
        if d[0]==""s"" or d[1]==""s"":
            delim='space'
        elif d[0]==""t"" or d[1]==""t"":
            delim='tab'
        else:
            print('error reading ', infile)
            sys.exit()
        if delim=='space':file_type=d.split()[1]
        if delim=='tab':file_type=d.split('\t')[1]
        if file_type=='delimited':
            if delim=='space':file_type=d.split()[2]
            if delim=='tab':file_type=d.split('\t')[2]
        if delim=='space':line =f.readline()[:-1].split()
        if delim=='tab':line =f.readline()[:-1].split('\t')
        for key in line:
            magic_keys.append(key)
        lines=f.readlines()
        for line in lines[:-1]:
            line.replace('\n','')
            if delim=='space':rec=line[:-1].split()
            if delim=='tab':rec=line[:-1].split('\t')
            hold.append(rec)
        line = lines[-1].replace('\n','')
        if delim=='space':rec=line[:-1].split()
        if delim=='tab':rec=line.split('\t')
        hold.append(rec)
        for rec in hold:
            magic_record={}
            if len(magic_keys) != len(rec):

                print(""Warning: Uneven record lengths detected: "")
                #print magic_keys
                #print rec
            for k in range(len(rec)):
               magic_record[magic_keys[k]]=rec[k].strip('\n')
            magic_data.append(magic_record)
        magictype=file_type.lower().split(""_"")
        Types=['er','magic','pmag','rmag']
        if magictype in Types:file_type=file_type.lower()
#        print ""magic data from magic_read:""
#        print str(magic_data)[:500] + ""...""
#        print ""file_type"", file_type
        return magic_data,file_type","['def', 'magic_read', '(', 'self', ',', 'infile', ')', ':', '#        print ""calling magic_read(self, infile)"", infile', 'hold', ',', 'magic_data', ',', 'magic_record', ',', 'magic_keys', '=', '[', ']', ',', '[', ']', ',', '{', '}', ',', '[', ']', 'try', ':', 'f', '=', 'open', '(', 'infile', ',', '""r""', ')', 'except', ':', 'return', '[', ']', ',', ""'bad_file'"", 'd', '=', 'f', '.', 'readline', '(', ')', '[', ':', '-', '1', ']', '.', 'strip', '(', ""'\\n'"", ')', 'if', 'd', '[', '0', ']', '==', '""s""', 'or', 'd', '[', '1', ']', '==', '""s""', ':', 'delim', '=', ""'space'"", 'elif', 'd', '[', '0', ']', '==', '""t""', 'or', 'd', '[', '1', ']', '==', '""t""', ':', 'delim', '=', ""'tab'"", 'else', ':', 'print', '(', ""'error reading '"", ',', 'infile', ')', 'sys', '.', 'exit', '(', ')', 'if', 'delim', '==', ""'space'"", ':', 'file_type', '=', 'd', '.', 'split', '(', ')', '[', '1', ']', 'if', 'delim', '==', ""'tab'"", ':', 'file_type', '=', 'd', '.', 'split', '(', ""'\\t'"", ')', '[', '1', ']', 'if', 'file_type', '==', ""'delimited'"", ':', 'if', 'delim', '==', ""'space'"", ':', 'file_type', '=', 'd', '.', 'split', '(', ')', '[', '2', ']', 'if', 'delim', '==', ""'tab'"", ':', 'file_type', '=', 'd', '.', 'split', '(', ""'\\t'"", ')', '[', '2', ']', 'if', 'delim', '==', ""'space'"", ':', 'line', '=', 'f', '.', 'readline', '(', ')', '[', ':', '-', '1', ']', '.', 'split', '(', ')', 'if', 'delim', '==', ""'tab'"", ':', 'line', '=', 'f', '.', 'readline', '(', ')', '[', ':', '-', '1', ']', '.', 'split', '(', ""'\\t'"", ')', 'for', 'key', 'in', 'line', ':', 'magic_keys', '.', 'append', '(', 'key', ')', 'lines', '=', 'f', '.', 'readlines', '(', ')', 'for', 'line', 'in', 'lines', '[', ':', '-', '1', ']', ':', 'line', '.', 'replace', '(', ""'\\n'"", ',', ""''"", ')', 'if', 'delim', '==', ""'space'"", ':', 'rec', '=', 'line', '[', ':', '-', '1', ']', '.', 'split', '(', ')', 'if', 'delim', '==', ""'tab'"", ':', 'rec', '=', 'line', '[', ':', '-', '1', ']', '.', 'split', '(', ""'\\t'"", ')', 'hold', '.', 'append', '(', 'rec', ')', 'line', '=', 'lines', '[', '-', '1', ']', '.', 'replace', '(', ""'\\n'"", ',', ""''"", ')', 'if', 'delim', '==', ""'space'"", ':', 'rec', '=', 'line', '[', ':', '-', '1', ']', '.', 'split', '(', ')', 'if', 'delim', '==', ""'tab'"", ':', 'rec', '=', 'line', '.', 'split', '(', ""'\\t'"", ')', 'hold', '.', 'append', '(', 'rec', ')', 'for', 'rec', 'in', 'hold', ':', 'magic_record', '=', '{', '}', 'if', 'len', '(', 'magic_keys', ')', '!=', 'len', '(', 'rec', ')', ':', 'print', '(', '""Warning: Uneven record lengths detected: ""', ')', '#print magic_keys', '#print rec', 'for', 'k', 'in', 'range', '(', 'len', '(', 'rec', ')', ')', ':', 'magic_record', '[', 'magic_keys', '[', 'k', ']', ']', '=', 'rec', '[', 'k', ']', '.', 'strip', '(', ""'\\n'"", ')', 'magic_data', '.', 'append', '(', 'magic_record', ')', 'magictype', '=', 'file_type', '.', 'lower', '(', ')', '.', 'split', '(', '""_""', ')', 'Types', '=', '[', ""'er'"", ',', ""'magic'"", ',', ""'pmag'"", ',', ""'rmag'"", ']', 'if', 'magictype', 'in', 'Types', ':', 'file_type', '=', 'file_type', '.', 'lower', '(', ')', '#        print ""magic data from magic_read:""', '#        print str(magic_data)[:500] + ""...""', '#        print ""file_type"", file_type', 'return', 'magic_data', ',', 'file_type']","reads  a Magic template file, puts data in a list of dictionaries","['reads', 'a', 'Magic', 'template', 'file', 'puts', 'data', 'in', 'a', 'list', 'of', 'dictionaries']",python,R,1,True,1,train
19236,PmagPy/PmagPy,programs/demag_gui.py,https://github.com/PmagPy/PmagPy/blob/c7984f8809bf40fe112e53dcc311a33293b62d0b/programs/demag_gui.py#L2909-L2960,"def read_criteria_file(self, criteria_file_name=None):
        """"""
        reads 2.5 or 3.0 formatted PmagPy criteria file and returns a set of
        nested dictionary 2.5 formated criteria data that can be passed into
        pmag.grade to filter data.

        Parameters
        ----------
        criteria_file : name of criteria file to read in

        Returns
        -------
        nested dictionary 2.5 formated criteria data
        """"""
#        import pdb; pdb.set_trace()
        acceptance_criteria = pmag.initialize_acceptance_criteria()
        if self.data_model == 3:
            if criteria_file_name == None:
                criteria_file_name = ""criteria.txt""
            contribution = cb.Contribution(self.WD, read_tables=[
                                           'criteria'], custom_filenames={'criteria': criteria_file_name})
            if 'criteria' in contribution.tables:
                crit_container = contribution.tables['criteria']
                crit_data = crit_container.df
                crit_data = crit_data.to_dict('records')
                for crit in crit_data:
                    m2_name = map_magic.convert_direction_criteria(
                        'magic2', crit['table_column'])
                    if m2_name != """":
                        try:
                            if crit['criterion_value'] == 'True':
                                acceptance_criteria[m2_name]['value'] = 1
                            else:
                                acceptance_criteria[m2_name]['value'] = 0
                            acceptance_criteria[m2_name]['value'] = float(
                                crit['criterion_value'])
                        except ValueError:
                            self.user_warning(""%s is not a valid comparitor for %s, skipping this criteria"" % (
                                str(crit['criterion_value']), m2_name))
                            continue
                        acceptance_criteria[m2_name]['pmag_criteria_code'] = crit['criterion']
            return acceptance_criteria
        else:
            if criteria_file_name == None:
                criteria_file_name = ""pmag_criteria.txt""
            try:
                acceptance_criteria = pmag.read_criteria_from_file(
                    os.path.join(self.WD, criteria_file_name), acceptance_criteria)
            except (IOError, OSError) as e:
                self.user_warning(""File %s not found in directory %s aborting opperation"" % (
                    criteria_file_name, self.WD))
            return acceptance_criteria","['def', 'read_criteria_file', '(', 'self', ',', 'criteria_file_name', '=', 'None', ')', ':', '#        import pdb; pdb.set_trace()', 'acceptance_criteria', '=', 'pmag', '.', 'initialize_acceptance_criteria', '(', ')', 'if', 'self', '.', 'data_model', '==', '3', ':', 'if', 'criteria_file_name', '==', 'None', ':', 'criteria_file_name', '=', '""criteria.txt""', 'contribution', '=', 'cb', '.', 'Contribution', '(', 'self', '.', 'WD', ',', 'read_tables', '=', '[', ""'criteria'"", ']', ',', 'custom_filenames', '=', '{', ""'criteria'"", ':', 'criteria_file_name', '}', ')', 'if', ""'criteria'"", 'in', 'contribution', '.', 'tables', ':', 'crit_container', '=', 'contribution', '.', 'tables', '[', ""'criteria'"", ']', 'crit_data', '=', 'crit_container', '.', 'df', 'crit_data', '=', 'crit_data', '.', 'to_dict', '(', ""'records'"", ')', 'for', 'crit', 'in', 'crit_data', ':', 'm2_name', '=', 'map_magic', '.', 'convert_direction_criteria', '(', ""'magic2'"", ',', 'crit', '[', ""'table_column'"", ']', ')', 'if', 'm2_name', '!=', '""""', ':', 'try', ':', 'if', 'crit', '[', ""'criterion_value'"", ']', '==', ""'True'"", ':', 'acceptance_criteria', '[', 'm2_name', ']', '[', ""'value'"", ']', '=', '1', 'else', ':', 'acceptance_criteria', '[', 'm2_name', ']', '[', ""'value'"", ']', '=', '0', 'acceptance_criteria', '[', 'm2_name', ']', '[', ""'value'"", ']', '=', 'float', '(', 'crit', '[', ""'criterion_value'"", ']', ')', 'except', 'ValueError', ':', 'self', '.', 'user_warning', '(', '""%s is not a valid comparitor for %s, skipping this criteria""', '%', '(', 'str', '(', 'crit', '[', ""'criterion_value'"", ']', ')', ',', 'm2_name', ')', ')', 'continue', 'acceptance_criteria', '[', 'm2_name', ']', '[', ""'pmag_criteria_code'"", ']', '=', 'crit', '[', ""'criterion'"", ']', 'return', 'acceptance_criteria', 'else', ':', 'if', 'criteria_file_name', '==', 'None', ':', 'criteria_file_name', '=', '""pmag_criteria.txt""', 'try', ':', 'acceptance_criteria', '=', 'pmag', '.', 'read_criteria_from_file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'WD', ',', 'criteria_file_name', ')', ',', 'acceptance_criteria', ')', 'except', '(', 'IOError', ',', 'OSError', ')', 'as', 'e', ':', 'self', '.', 'user_warning', '(', '""File %s not found in directory %s aborting opperation""', '%', '(', 'criteria_file_name', ',', 'self', '.', 'WD', ')', ')', 'return', 'acceptance_criteria']","reads 2.5 or 3.0 formatted PmagPy criteria file and returns a set of
        nested dictionary 2.5 formated criteria data that can be passed into
        pmag.grade to filter data.

        Parameters
        ----------
        criteria_file : name of criteria file to read in

        Returns
        -------
        nested dictionary 2.5 formated criteria data","['reads', '2', '.', '5', 'or', '3', '.', '0', 'formatted', 'PmagPy', 'criteria', 'file', 'and', 'returns', 'a', 'set', 'of', 'nested', 'dictionary', '2', '.', '5', 'formated', 'criteria', 'data', 'that', 'can', 'be', 'passed', 'into', 'pmag', '.', 'grade', 'to', 'filter', 'data', '.']",python,R,1,True,1,train
19254,PmagPy/PmagPy,programs/demag_gui.py,https://github.com/PmagPy/PmagPy/blob/c7984f8809bf40fe112e53dcc311a33293b62d0b/programs/demag_gui.py#L3861-L4422,"def get_data(self):
        """"""
        reads data from current WD measurement.txt or magic_measurements.txt
        depending on data model and sorts it into main measurements data
        structures given bellow:
        Data - {specimen: {
                zijdblock:[[treatment temp-str,dec-float, inc-float,
                        mag_moment-float, ZI-float, meas_flag-str ('b','g'),
                        method_codes-str]],
                zijdblock_geo:[[treatment temp-str,dec-float, inc-float,
                        mag_moment-float, ZI-float, meas_flag-str ('b','g'),
                        method_codes-str]],
                zijdblock_tilt:[[treatment temp-str,dec-float, inc-float,
                        mag_moment-float, ZI-float, meas_flag-str ('b','g'),
                        method_codes-str]],
                zijdblock_lab_treatments: [str],
                zijdblock_steps: [str],
                measurement_flag: [str ('b','g')],
                mag_meas_data_index: [int],
                csds: [float],
                pars: {},
                zdata: array.shape = 2x2 (float),
                zdata_geo: array.shape = 2x2 (float),
                zdata_tilt: array.shape = 2x2 (float),
                vector_diffs: [float],
                vds: float }}
        Data_hierarchy - {specimen: {
                            study: {}
                            locations: {}
                            sites: {}
                            samples: {}
                            specimens: {}
                            sample_of_specimen: {}
                            site_of_specimen: {}
                            site_of_sample: {}
                            location_of_site: {}
                            location_of_specimen: {}
                            study_of_specimen: {}
                            expedition_name_of_specimen: {} }}
        """"""
        # ------------------------------------------------
        # Read magic measurement file and sort to blocks
        # ------------------------------------------------

        # All meas data information is stored in Data[secimen]={}
        Data = {}
        Data_hierarchy = {}
        Data_hierarchy['study'] = {}
        Data_hierarchy['locations'] = {}
        Data_hierarchy['sites'] = {}
        Data_hierarchy['samples'] = {}
        Data_hierarchy['specimens'] = {}
        Data_hierarchy['sample_of_specimen'] = {}
        Data_hierarchy['site_of_specimen'] = {}
        Data_hierarchy['site_of_sample'] = {}
        Data_hierarchy['location_of_site'] = {}
        Data_hierarchy['location_of_specimen'] = {}
        Data_hierarchy['study_of_specimen'] = {}
        Data_hierarchy['expedition_name_of_specimen'] = {}

        if self.data_model == 3:

            if 'measurements' not in self.con.tables:
                self.user_warning(
                    ""Measurement data file is empty and the GUI cannot start, aborting"")
                return Data, Data_hierarchy

            if self.con.tables['measurements'].df.empty:
                self.user_warning(
                    ""Measurement data file is empty and the GUI cannot start, aborting"")
                return Data, Data_hierarchy

            # extract specimen data from measurements table
            if not len(self.spec_data):
                specs = self.con.tables['measurements'].df['specimen'].unique()
                df = pd.DataFrame(index=specs, columns=['specimen'])
                df.index.name = 'specimen_name'
                df['specimen'] = specs
                self.con.tables['specimens'].df = df
                self.spec_data = df

            if not len(self.spec_data):
                self.user_warning(
                    ""Measurement data file does not seem to have specimen data and the GUI cannot start, aborting"")
                return Data, Data_hierarchy


            if 'sample' not in self.spec_data.columns or 'sample' not in self.samp_data.columns:
                if 'specimen' not in self.spec_data.columns:
                    self.spec_data['specimen'] = self.con.tables['measurements'].df['specimen']
                    self.spec_data.set_index('specimen', inplace=True)
                    self.spec_data['specimen'] = self.spec_data.index

                ui_dialog = demag_dialogs.user_input(
                    self, [""# of characters to remove""], heading=""Sample data could not be found attempting to generate sample names by removing characters from specimen names"")
                self.show_dlg(ui_dialog)
                ui_data = ui_dialog.get_values()
                try:
                    samp_ncr = int(ui_data[1][""# of characters to remove""])
                except ValueError:
                    self.user_warning(
                        ""Invalid input, specimen names will be used for sample names instead"")
                    samp_ncr = 0
                self.spec_data['sample'] = [x[:-samp_ncr]
                                            for x in self.spec_data['specimen']]

                self.samp_data['sample'] = self.spec_data['sample']
                self.samp_data.set_index('sample', inplace=True)
                self.samp_data['sample'] = self.samp_data.index

            if 'site' not in self.samp_data.columns or 'site' not in self.site_data.columns:
                ui_dialog = demag_dialogs.user_input(
                    self, [""# of characters to remove"", ""site delimiter""], heading=""No Site Data found attempting to create site names from specimen names"")
                self.show_dlg(ui_dialog)
                ui_data = ui_dialog.get_values()
                try:
                    site_ncr = int(ui_data[1][""# of characters to remove""])
                    self.samp_data['site'] = [x[:-site_ncr]
                                              for x in self.spec_data['specimen']]
                except ValueError:
                    try:
                        sd = ui_data[1][""site delimiter""]
                        self.samp_data['site'] = [
                            x.split(sd)[0] for x in self.spec_data['specimen']]
                    except ValueError:
                        self.samp_data['site'] = [
                            x for x in self.spec_data['specimen']]

                self.site_data['site'] = self.samp_data['site']
                self.site_data.drop_duplicates(inplace=True)
                self.site_data.set_index('site', inplace=True)
                self.site_data['site'] = self.site_data.index

            if 'location' not in self.site_data.columns or 'location' not in self.loc_data.columns:
                ui_dialog = demag_dialogs.user_input(
                    self, [""location name for all sites""], heading=""No Location found"")
                self.show_dlg(ui_dialog)
                ui_data = ui_dialog.get_values()
                self.site_data['location'] = ui_data[1][""location name for all sites""]

                self.loc_data['location'] = self.site_data['location']
                self.loc_data.drop_duplicates(inplace=True)
                self.loc_data.set_index('location', inplace=True)
                self.loc_data['location'] = self.loc_data.index

            # add data to other dataframes
            self.con.propagate_location_to_measurements()
            self.con.propagate_location_to_specimens()

            # get measurement data from contribution object
            meas_container = self.con.tables['measurements']
            meas_data3_0 = meas_container.df

            meas_data3_0.replace({'specimen': {nan: 'unknown'}, 'sample': {nan: 'unknown'}, 'site': {
                                 nan: 'unknown'}, 'location': {nan: 'unknown'}}, inplace=True)
            meas_data3_0['specimen'] = meas_data3_0['specimen'].apply(str)
            meas_data3_0['sample'] = meas_data3_0['sample'].apply(str)
            meas_data3_0['site'] = meas_data3_0['site'].apply(str)
            meas_data3_0['location'] = meas_data3_0['location'].apply(str)

            # do some filtering
#            if 'location' in meas_data3_0.columns:
#                if any(meas_data3_0['location'].isnull()):
#                    print(""-W- Some measurements are missing location data, and will not be used"")
#                meas_data3_0 = meas_data3_0[meas_data3_0['location'].notnull()]
# meas_data3_0.replace({'location':float('nan')},'unknown',inplace=True)
#            if 'site' in meas_data3_0.columns:
#                if any(meas_data3_0['site'].isnull()):
#                    print(""-W- Some measurements are missing site data, and will not be used"")
#                meas_data3_0 = meas_data3_0[meas_data3_0['site'].notnull()]
# meas_data3_0.replace({'site':float('nan')},'unknown',inplace=True)
#            if 'sample' in meas_data3_0.columns:
#                if any(meas_data3_0['sample'].isnull()):
#                    print(""-W- Some measurements are missing sample data, and will not be used"")
#                meas_data3_0 = meas_data3_0[meas_data3_0['sample'].notnull()]
# meas_data3_0.replace({'sample':float('nan')},'unknown',inplace=True)
#            if 'specimen' in meas_data3_0.columns:
#                missing = meas_data3_0[meas_data3_0['specimen'].isnull()]
#                if len(missing):
#                    print(""-W- {} measurements are missing specimen data, and will not be used"".format(missing))
#                meas_data3_0 = meas_data3_0[meas_data3_0['specimen'].notnull()]
# meas_data3_0.replace({'specimen':float('nan')},'unknown',inplace=True)

#            col_names = ['specimen', 'sample', 'site', 'location']
#            for col_name in col_names:
#                if col_name in meas_data3_0.columns:
#                    pruned = meas_data3_0[meas_data3_0[col_name].apply(cb.not_null)]
#                    num_missing = len(meas_data3_0) - len(pruned)
#                    if num_missing:
#                        msg = ""{} measurements cannot be associated with a {} and will be excluded\nTry using Pmag GUI (step 3) to make sure you have provided the full chain from specimen to location."".format(num_missing, col_name)
#                        pw.simple_warning(msg)
#                        print(""-W- {} measurements are missing {} data and will be excluded"".format(num_missing, col_name))
#                        meas_data3_0 = pruned
            Mkeys = ['magn_moment', 'magn_volume', 'magn_mass']
# fish out all the relavent data
            meas_data3_0 = meas_data3_0[meas_data3_0['method_codes'].str.contains(
                'LT-NO|LT-AF-Z|LT-T-Z|LT-M-Z|LT-LT-Z') == True]
            if not len(meas_data3_0):
                self.user_warning(""Your measurements table contains none of the required method codes to run Demag GUI: [LT-NO, LT-AF-Z, LT-T-Z, LT-M-Z, LT-LT-Z]"")
                return {}, {}
# now convert back to 2.5  changing only those keys that are necessary for thellier_gui
            meas_con_dict = map_magic.get_thellier_gui_meas_mapping(
                meas_data3_0, output=2)
            intensity_col = cb.get_intensity_col(meas_data3_0)
            if not intensity_col:
                self.user_warning(""Your measurements table must have one of the following columns to run Demag GUI: 'magn_moment', 'magn_volume', 'magn_mass',or 'magn_uncal'"")
                return {}, {}

            print('-I- Using {} for intensity'.format(intensity_col))
            self.intensity_col = meas_con_dict[intensity_col]
            meas_data2_5 = meas_data3_0.rename(columns=meas_con_dict)
            # make a list of dictionaries to maintain backward compatibility
            mag_meas_data = meas_data2_5.to_dict(""records"")

        else:  # data model 2.5
            try:
                print((""-I- Read magic file %s"" % self.magic_file))
            except ValueError:
                self.magic_measurement = self.choose_meas_file()
                print((""-I- Read magic file %s"" % self.magic_file))
            mag_meas_data, file_type = pmag.magic_read(self.magic_file)
            if file_type != ""magic_measurements"":
                self.user_warning(""You have selected data model 2.5, but your measurements file is either not in 2.5, or is not a measurements file.\n{} has file type: {}"".format(self.magic_file, file_type))
                return {}, {}


        self.mag_meas_data = self.merge_pmag_recs(mag_meas_data)

        # get list of unique specimen names with measurement data
        CurrRec = []
        sids = pmag.get_specs(self.mag_meas_data)  # specimen ID's
        for s in sids:
            if s not in list(Data.keys()):
                Data[s] = {}
                Data[s]['zijdblock'] = []
                Data[s]['zijdblock_geo'] = []
                Data[s]['zijdblock_tilt'] = []
                Data[s]['zijdblock_lab_treatments'] = []
                Data[s]['pars'] = {}
                Data[s]['csds'] = []
                Data[s]['zijdblock_steps'] = []
                Data[s]['measurement_flag'] = []  # a list of points 'g' or 'b'
                # index in original magic_measurements.txt
                Data[s]['mag_meas_data_index'] = []
                Data[s]['measurement_names'] = []

        prev_s = None
        cnt = -1
        # list of excluded lab protocols. copied from pmag.find_dmag_rec(s,data)
        self.excluded_methods = [""LP-AN-ARM"", ""LP-AN-TRM"", ""LP-ARM-AFD"",
                                 ""LP-ARM2-AFD"", ""LP-TRM-AFD"", ""LP-TRM"", ""LP-TRM-TD"", ""LP-X"", ""LP-PI-ARM""]
        self.included_methods = [
            ""LT-NO"", ""LT-AF-Z"", ""LT-T-Z"", ""LT-M-Z"", ""LT-LT-Z""]
#        self.mag_meas_data.sort(key=meas_key)
        # asiigned default values for NRM
        if len(self.mag_meas_data) > 0 and self.intensity_col in list(self.mag_meas_data[0].keys()):
            NRM = float(self.mag_meas_data[0][self.intensity_col])
        for rec in self.mag_meas_data:
            # if ""measurement_number"" in rec.keys() and str(rec['measurement_number']) == '1' and ""magic_method_codes"" in rec.keys() and ""LT-NO"" not in rec[""magic_method_codes""].split(':'):
            #    NRM = 1 #not really sure how to handle this case but assume that data is already normalized
            cnt += 1  # index counter
            s = rec[""er_specimen_name""]
            if ""er_sample_name"" in list(rec.keys()):
                sample = rec[""er_sample_name""]
            else:
                sample = ''
            if ""er_site_name"" in list(rec.keys()):
                site = rec[""er_site_name""]
            else:
                site = ''
            if ""er_location_name"" in list(rec.keys()):
                location = rec[""er_location_name""]
            else:
                location = ''
            expedition_name = """"
            if ""er_expedition_name"" in list(rec.keys()):
                expedition_name = rec[""er_expedition_name""]

            methods = rec[""magic_method_codes""].replace(
                "" "", """").strip(""\n"").split("":"")
            LP_methods = []
            LT_methods = []

            for k in ['zdata', 'zdata_geo', 'zdata_tilt', 'vector_diffs']:
                if k not in Data[s]:
                    Data[s][k] = []

            for i in range(len(methods)):
                methods[i] = methods[i].strip()
            if 'measurement_flag' not in list(rec.keys()):
                rec['measurement_flag'] = 'g'
            SKIP = True
            lab_treatment = """"
            for meth in methods:
                if 'DIR' in meth:
                    SKIP = False
                if meth in self.included_methods:
                    lab_treatment = meth
                    SKIP = False
                if ""LP"" in meth:
                    LP_methods.append(meth)
            for meth in self.excluded_methods:
                if meth in methods:
                    SKIP = True
                    break
            if SKIP:
                continue
            tr, LPcode, measurement_step_unit = """", """", """"
            if ""LT-NO"" in methods:
                tr = 0
                if prev_s != s and self.intensity_col in rec:
                    try:
                        NRM = float(rec[self.intensity_col])
                    except ValueError:
                        NRM = 1
                for method in methods:
                    if ""AF"" in method:
                        LPcode = ""LP-DIR-AF""
                        measurement_step_unit = ""mT""
                    if ""TRM"" in method:
                        LPcode = ""LP-DIR-T""
                        measurement_step_unit = ""C""
            elif ""LT-AF-Z"" in methods:
                try:
                    tr = float(rec[""treatment_ac_field""])*1e3  # (mT)
                except ValueError:
                    print((""Could not convert ac field for measurement, was given %s, skipping"" %
                           rec[""treatment_ac_field""]))
                    continue
                measurement_step_unit = ""mT""  # in magic its T in GUI its mT
                LPcode = ""LP-DIR-AF""
            elif ""LT-T-Z"" in methods or ""LT-LT-Z"" in methods:
                try:
                    tr = float(rec[""treatment_temp""])-273.  # celsius
                except ValueError:
                    print(
                        (""Could not convert temperature for measurement, was given %s, skipping"" % rec[""treatment_temp""]))
                    continue
                measurement_step_unit = ""C""  # in magic its K in GUI its C
                LPcode = ""LP-DIR-T""
            elif ""LT-M-Z"" in methods:
                # temporary for microwave
                tr = float(rec[""measurement_number""])
            else:
                # attempt to determine from treatment data
                if all(im not in methods for im in self.included_methods):
                    if 'treatment_temp' in list(rec.keys()) and not str(rec['treatment_temp']).isalpha() and rec['treatment_temp'] != '' and float(rec['treatment_temp']) > 0:
                        tr = float(rec[""treatment_temp""])-273.  # celsius
                        measurement_step_unit = ""C""  # in magic its K in GUI its C
                        LPcode = ""LP-DIR-T""
                    elif 'treatment_ac_field' in list(rec.keys()) and not str(rec['treatment_ac_field']).isalpha() and rec['treatment_ac_field'] != '' and float(rec['treatment_ac_field']) > 0:
                        tr = float(rec[""treatment_ac_field""])*1e3  # (mT)
                        measurement_step_unit = ""mT""  # in magic its T in GUI its mT
                        LPcode = ""LP-DIR-AF""
                    else:
                        tr = 0
                        if prev_s != s and self.intensity_col in rec:
                            try:
                                NRM = float(rec[self.intensity_col])
                            except ValueError:
                                NRM = 1
                        for method in methods:
                            if ""AF"" in method:
                                LPcode = ""LP-DIR-AF""
                                measurement_step_unit = ""mT""
                            if ""TRM"" in method:
                                LPcode = ""LP-DIR-T""
                                measurement_step_unit = ""C""
                else:
                    tr = float(rec[""measurement_number""])
            if prev_s != s and len(Data[s]['zijdblock']) > 0:
                NRM = Data[s]['zijdblock'][0][3]

            ZI = 0
            if tr != """":
                Data[s]['mag_meas_data_index'].append(
                    cnt)  # magic_measurement file intex
                if not int(self.data_model) == 2:
                    try:
                        Data[s]['measurement_names'].append(rec['measurement'])
                    except KeyError:
                        Data[s]['measurement_names'].append(rec['measurement_number'])
                Data[s]['zijdblock_lab_treatments'].append(lab_treatment)
                if measurement_step_unit != """":
                    if 'measurement_step_unit' in list(Data[s].keys()):
                        if measurement_step_unit not in Data[s]['measurement_step_unit'].split("":""):
                            Data[s]['measurement_step_unit'] = Data[s]['measurement_step_unit'] + \
                                "":""+measurement_step_unit
                    else:
                        Data[s]['measurement_step_unit'] = measurement_step_unit
                dec, inc, inten = """", """", """"
                if ""measurement_dec"" in list(rec.keys()) and cb.not_null(rec[""measurement_dec""], False):
                    dec = float(rec[""measurement_dec""])
                else:
                    continue
                if ""measurement_inc"" in list(rec.keys()) and cb.not_null(rec[""measurement_inc""], False):
                    inc = float(rec[""measurement_inc""])
                else:
                    continue
                if self.intensity_col in list(rec.keys()) and cb.not_null(rec[self.intensity_col], False):
                    intensity = float(rec[self.intensity_col])
                else:
                    intensity = 1.  # just assume a normal vector
                if 'magic_instrument_codes' not in list(rec.keys()):
                    rec['magic_instrument_codes'] = ''
                if 'measurement_csd' in list(rec.keys()):
                    csd = str(rec['measurement_csd'])
                else:
                    csd = ''
                Data[s]['zijdblock'].append(
                    [tr, dec, inc, intensity, ZI, rec['measurement_flag'], rec['magic_instrument_codes']])
                Data[s]['csds'].append(csd)
                DIR = [dec, inc, intensity/NRM]
                cart = pmag.dir2cart(DIR)
                Data[s]['zdata'].append(array([cart[0], cart[1], cart[2]]))

                if 'magic_experiment_name' in list(Data[s].keys()) and Data[s]['magic_experiment_name'] != rec[""magic_experiment_name""]:
                    print((""-E- ERROR: specimen %s has more than one demagnetization experiment name. You need to merge them to one experiment-name?"" % (s)))
                if float(tr) == 0 or float(tr) == 273:
                    Data[s]['zijdblock_steps'].append(""0"")
                elif measurement_step_unit == ""C"":
                    Data[s]['zijdblock_steps'].append(
                        ""%.0f%s"" % (tr, measurement_step_unit))
                else:
                    Data[s]['zijdblock_steps'].append(
                        ""%.1f%s"" % (tr, measurement_step_unit))
                # --------------
                if 'magic_experiment_name' in list(rec.keys()):
                    Data[s]['magic_experiment_name'] = rec[""magic_experiment_name""]
                if ""magic_instrument_codes"" in list(rec.keys()):
                    Data[s]['magic_instrument_codes'] = rec['magic_instrument_codes']
                Data[s][""magic_method_codes""] = LPcode

                # --------------
                # """"good"" or ""bad"" data
                # --------------

                flag = 'g'
                if 'measurement_flag' in list(rec.keys()):
                    if str(rec[""measurement_flag""]) == 'b':
                        flag = 'b'
                Data[s]['measurement_flag'].append(flag)

                # gegraphic coordinates
                try:
                    sample_azimuth = float(
                        self.Data_info[""er_samples""][sample]['sample_azimuth'])
                    sample_dip = float(
                        self.Data_info[""er_samples""][sample]['sample_dip'])
                    d_geo, i_geo = pmag.dogeo(
                        dec, inc, sample_azimuth, sample_dip)
                    # if d_geo or i_geo is null, we can't do geographic coordinates
                    # otherwise, go ahead
                    if not any([np.isnan(val) for val in [d_geo, i_geo]]):
                        Data[s]['zijdblock_geo'].append(
                            [tr, d_geo, i_geo, intensity, ZI, rec['measurement_flag'], rec['magic_instrument_codes']])
                        DIR = [d_geo, i_geo, intensity/NRM]
                        cart = pmag.dir2cart(DIR)
                        Data[s]['zdata_geo'].append([cart[0], cart[1], cart[2]])
                except (IOError, KeyError, ValueError, TypeError) as e:
                    pass
                #                    if prev_s != s:
                #                        print( ""-W- can't find sample_azimuth,sample_dip for sample %s""%sample)

                # tilt-corrected coordinates
                try:
                    sample_bed_dip_direction = float(
                        self.Data_info[""er_samples""][sample]['sample_bed_dip_direction'])
                    sample_bed_dip = float(
                        self.Data_info[""er_samples""][sample]['sample_bed_dip'])
                    d_tilt, i_tilt = pmag.dotilt(
                        d_geo, i_geo, sample_bed_dip_direction, sample_bed_dip)
                    Data[s]['zijdblock_tilt'].append(
                        [tr, d_tilt, i_tilt, intensity, ZI, rec['measurement_flag'], rec['magic_instrument_codes']])
                    DIR = [d_tilt, i_tilt, intensity/NRM]
                    cart = pmag.dir2cart(DIR)
                    Data[s]['zdata_tilt'].append([cart[0], cart[1], cart[2]])
                except (IOError, KeyError, TypeError, ValueError, UnboundLocalError) as e:
                    pass
                #                    if prev_s != s:
                #                        printd(""-W- can't find tilt-corrected data for sample %s""%sample)

                if len(Data[s]['zdata']) > 1:
                    Data[s]['vector_diffs'].append(
                        sqrt(sum((array(Data[s]['zdata'][-2])-array(Data[s]['zdata'][-1]))**2)))

            # ---------------------
            # hierarchy is determined from magic_measurements.txt
            # ---------------------

            if sample not in list(Data_hierarchy['samples'].keys()):
                Data_hierarchy['samples'][sample] = {}
                Data_hierarchy['samples'][sample]['specimens'] = []

            if site not in list(Data_hierarchy['sites'].keys()):
                Data_hierarchy['sites'][site] = {}
                Data_hierarchy['sites'][site]['samples'] = []
                Data_hierarchy['sites'][site]['specimens'] = []

            if location not in list(Data_hierarchy['locations'].keys()):
                Data_hierarchy['locations'][location] = {}
                Data_hierarchy['locations'][location]['sites'] = []
                Data_hierarchy['locations'][location]['samples'] = []
                Data_hierarchy['locations'][location]['specimens'] = []

            if 'this study' not in list(Data_hierarchy['study'].keys()):
                Data_hierarchy['study']['this study'] = {}
                Data_hierarchy['study']['this study']['sites'] = []
                Data_hierarchy['study']['this study']['samples'] = []
                Data_hierarchy['study']['this study']['specimens'] = []

            if s not in Data_hierarchy['samples'][sample]['specimens']:
                Data_hierarchy['samples'][sample]['specimens'].append(s)

            if s not in Data_hierarchy['sites'][site]['specimens']:
                Data_hierarchy['sites'][site]['specimens'].append(s)

            if s not in Data_hierarchy['locations'][location]['specimens']:
                Data_hierarchy['locations'][location]['specimens'].append(s)

            if s not in Data_hierarchy['study']['this study']['specimens']:
                Data_hierarchy['study']['this study']['specimens'].append(s)

            if sample not in Data_hierarchy['sites'][site]['samples']:
                Data_hierarchy['sites'][site]['samples'].append(sample)

            if sample not in Data_hierarchy['locations'][location]['samples']:
                Data_hierarchy['locations'][location]['samples'].append(sample)

            if sample not in Data_hierarchy['study']['this study']['samples']:
                Data_hierarchy['study']['this study']['samples'].append(sample)

            if site not in Data_hierarchy['locations'][location]['sites']:
                Data_hierarchy['locations'][location]['sites'].append(site)

            if site not in Data_hierarchy['study']['this study']['sites']:
                Data_hierarchy['study']['this study']['sites'].append(site)

            # Data_hierarchy['specimens'][s]=sample
            Data_hierarchy['sample_of_specimen'][s] = sample
            Data_hierarchy['site_of_specimen'][s] = site
            Data_hierarchy['site_of_sample'][sample] = site
            Data_hierarchy['location_of_site'][site] = location
            Data_hierarchy['location_of_specimen'][s] = location
            if expedition_name != """":
                Data_hierarchy['expedition_name_of_specimen'][s] = expedition_name
            prev_s = s

        print(""-I- done sorting meas data"")
        self.specimens = list(Data.keys())

        for s in self.specimens:
            if len(Data[s]['zdata']) > 0:
                Data[s]['vector_diffs'].append(
                    sqrt(sum(array(Data[s]['zdata'][-1])**2)))  # last vector of the vds
            vds = sum(Data[s]['vector_diffs'])  # vds calculation
            Data[s]['vector_diffs'] = array(Data[s]['vector_diffs'])
            Data[s]['vds'] = vds
            Data[s]['zdata'] = array(Data[s]['zdata'])
            Data[s]['zdata_geo'] = array(Data[s]['zdata_geo'])
            Data[s]['zdata_tilt'] = array(Data[s]['zdata_tilt'])
        return(Data, Data_hierarchy)","['def', 'get_data', '(', 'self', ')', ':', '# ------------------------------------------------', '# Read magic measurement file and sort to blocks', '# ------------------------------------------------', '# All meas data information is stored in Data[secimen]={}', 'Data', '=', '{', '}', 'Data_hierarchy', '=', '{', '}', 'Data_hierarchy', '[', ""'study'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'locations'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'sites'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'samples'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'specimens'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'sample_of_specimen'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'site_of_specimen'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'site_of_sample'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'location_of_site'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'location_of_specimen'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'study_of_specimen'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'expedition_name_of_specimen'"", ']', '=', '{', '}', 'if', 'self', '.', 'data_model', '==', '3', ':', 'if', ""'measurements'"", 'not', 'in', 'self', '.', 'con', '.', 'tables', ':', 'self', '.', 'user_warning', '(', '""Measurement data file is empty and the GUI cannot start, aborting""', ')', 'return', 'Data', ',', 'Data_hierarchy', 'if', 'self', '.', 'con', '.', 'tables', '[', ""'measurements'"", ']', '.', 'df', '.', 'empty', ':', 'self', '.', 'user_warning', '(', '""Measurement data file is empty and the GUI cannot start, aborting""', ')', 'return', 'Data', ',', 'Data_hierarchy', '# extract specimen data from measurements table', 'if', 'not', 'len', '(', 'self', '.', 'spec_data', ')', ':', 'specs', '=', 'self', '.', 'con', '.', 'tables', '[', ""'measurements'"", ']', '.', 'df', '[', ""'specimen'"", ']', '.', 'unique', '(', ')', 'df', '=', 'pd', '.', 'DataFrame', '(', 'index', '=', 'specs', ',', 'columns', '=', '[', ""'specimen'"", ']', ')', 'df', '.', 'index', '.', 'name', '=', ""'specimen_name'"", 'df', '[', ""'specimen'"", ']', '=', 'specs', 'self', '.', 'con', '.', 'tables', '[', ""'specimens'"", ']', '.', 'df', '=', 'df', 'self', '.', 'spec_data', '=', 'df', 'if', 'not', 'len', '(', 'self', '.', 'spec_data', ')', ':', 'self', '.', 'user_warning', '(', '""Measurement data file does not seem to have specimen data and the GUI cannot start, aborting""', ')', 'return', 'Data', ',', 'Data_hierarchy', 'if', ""'sample'"", 'not', 'in', 'self', '.', 'spec_data', '.', 'columns', 'or', ""'sample'"", 'not', 'in', 'self', '.', 'samp_data', '.', 'columns', ':', 'if', ""'specimen'"", 'not', 'in', 'self', '.', 'spec_data', '.', 'columns', ':', 'self', '.', 'spec_data', '[', ""'specimen'"", ']', '=', 'self', '.', 'con', '.', 'tables', '[', ""'measurements'"", ']', '.', 'df', '[', ""'specimen'"", ']', 'self', '.', 'spec_data', '.', 'set_index', '(', ""'specimen'"", ',', 'inplace', '=', 'True', ')', 'self', '.', 'spec_data', '[', ""'specimen'"", ']', '=', 'self', '.', 'spec_data', '.', 'index', 'ui_dialog', '=', 'demag_dialogs', '.', 'user_input', '(', 'self', ',', '[', '""# of characters to remove""', ']', ',', 'heading', '=', '""Sample data could not be found attempting to generate sample names by removing characters from specimen names""', ')', 'self', '.', 'show_dlg', '(', 'ui_dialog', ')', 'ui_data', '=', 'ui_dialog', '.', 'get_values', '(', ')', 'try', ':', 'samp_ncr', '=', 'int', '(', 'ui_data', '[', '1', ']', '[', '""# of characters to remove""', ']', ')', 'except', 'ValueError', ':', 'self', '.', 'user_warning', '(', '""Invalid input, specimen names will be used for sample names instead""', ')', 'samp_ncr', '=', '0', 'self', '.', 'spec_data', '[', ""'sample'"", ']', '=', '[', 'x', '[', ':', '-', 'samp_ncr', ']', 'for', 'x', 'in', 'self', '.', 'spec_data', '[', ""'specimen'"", ']', ']', 'self', '.', 'samp_data', '[', ""'sample'"", ']', '=', 'self', '.', 'spec_data', '[', ""'sample'"", ']', 'self', '.', 'samp_data', '.', 'set_index', '(', ""'sample'"", ',', 'inplace', '=', 'True', ')', 'self', '.', 'samp_data', '[', ""'sample'"", ']', '=', 'self', '.', 'samp_data', '.', 'index', 'if', ""'site'"", 'not', 'in', 'self', '.', 'samp_data', '.', 'columns', 'or', ""'site'"", 'not', 'in', 'self', '.', 'site_data', '.', 'columns', ':', 'ui_dialog', '=', 'demag_dialogs', '.', 'user_input', '(', 'self', ',', '[', '""# of characters to remove""', ',', '""site delimiter""', ']', ',', 'heading', '=', '""No Site Data found attempting to create site names from specimen names""', ')', 'self', '.', 'show_dlg', '(', 'ui_dialog', ')', 'ui_data', '=', 'ui_dialog', '.', 'get_values', '(', ')', 'try', ':', 'site_ncr', '=', 'int', '(', 'ui_data', '[', '1', ']', '[', '""# of characters to remove""', ']', ')', 'self', '.', 'samp_data', '[', ""'site'"", ']', '=', '[', 'x', '[', ':', '-', 'site_ncr', ']', 'for', 'x', 'in', 'self', '.', 'spec_data', '[', ""'specimen'"", ']', ']', 'except', 'ValueError', ':', 'try', ':', 'sd', '=', 'ui_data', '[', '1', ']', '[', '""site delimiter""', ']', 'self', '.', 'samp_data', '[', ""'site'"", ']', '=', '[', 'x', '.', 'split', '(', 'sd', ')', '[', '0', ']', 'for', 'x', 'in', 'self', '.', 'spec_data', '[', ""'specimen'"", ']', ']', 'except', 'ValueError', ':', 'self', '.', 'samp_data', '[', ""'site'"", ']', '=', '[', 'x', 'for', 'x', 'in', 'self', '.', 'spec_data', '[', ""'specimen'"", ']', ']', 'self', '.', 'site_data', '[', ""'site'"", ']', '=', 'self', '.', 'samp_data', '[', ""'site'"", ']', 'self', '.', 'site_data', '.', 'drop_duplicates', '(', 'inplace', '=', 'True', ')', 'self', '.', 'site_data', '.', 'set_index', '(', ""'site'"", ',', 'inplace', '=', 'True', ')', 'self', '.', 'site_data', '[', ""'site'"", ']', '=', 'self', '.', 'site_data', '.', 'index', 'if', ""'location'"", 'not', 'in', 'self', '.', 'site_data', '.', 'columns', 'or', ""'location'"", 'not', 'in', 'self', '.', 'loc_data', '.', 'columns', ':', 'ui_dialog', '=', 'demag_dialogs', '.', 'user_input', '(', 'self', ',', '[', '""location name for all sites""', ']', ',', 'heading', '=', '""No Location found""', ')', 'self', '.', 'show_dlg', '(', 'ui_dialog', ')', 'ui_data', '=', 'ui_dialog', '.', 'get_values', '(', ')', 'self', '.', 'site_data', '[', ""'location'"", ']', '=', 'ui_data', '[', '1', ']', '[', '""location name for all sites""', ']', 'self', '.', 'loc_data', '[', ""'location'"", ']', '=', 'self', '.', 'site_data', '[', ""'location'"", ']', 'self', '.', 'loc_data', '.', 'drop_duplicates', '(', 'inplace', '=', 'True', ')', 'self', '.', 'loc_data', '.', 'set_index', '(', ""'location'"", ',', 'inplace', '=', 'True', ')', 'self', '.', 'loc_data', '[', ""'location'"", ']', '=', 'self', '.', 'loc_data', '.', 'index', '# add data to other dataframes', 'self', '.', 'con', '.', 'propagate_location_to_measurements', '(', ')', 'self', '.', 'con', '.', 'propagate_location_to_specimens', '(', ')', '# get measurement data from contribution object', 'meas_container', '=', 'self', '.', 'con', '.', 'tables', '[', ""'measurements'"", ']', 'meas_data3_0', '=', 'meas_container', '.', 'df', 'meas_data3_0', '.', 'replace', '(', '{', ""'specimen'"", ':', '{', 'nan', ':', ""'unknown'"", '}', ',', ""'sample'"", ':', '{', 'nan', ':', ""'unknown'"", '}', ',', ""'site'"", ':', '{', 'nan', ':', ""'unknown'"", '}', ',', ""'location'"", ':', '{', 'nan', ':', ""'unknown'"", '}', '}', ',', 'inplace', '=', 'True', ')', 'meas_data3_0', '[', ""'specimen'"", ']', '=', 'meas_data3_0', '[', ""'specimen'"", ']', '.', 'apply', '(', 'str', ')', 'meas_data3_0', '[', ""'sample'"", ']', '=', 'meas_data3_0', '[', ""'sample'"", ']', '.', 'apply', '(', 'str', ')', 'meas_data3_0', '[', ""'site'"", ']', '=', 'meas_data3_0', '[', ""'site'"", ']', '.', 'apply', '(', 'str', ')', 'meas_data3_0', '[', ""'location'"", ']', '=', 'meas_data3_0', '[', ""'location'"", ']', '.', 'apply', '(', 'str', ')', '# do some filtering', ""#            if 'location' in meas_data3_0.columns:"", ""#                if any(meas_data3_0['location'].isnull()):"", '#                    print(""-W- Some measurements are missing location data, and will not be used"")', ""#                meas_data3_0 = meas_data3_0[meas_data3_0['location'].notnull()]"", ""# meas_data3_0.replace({'location':float('nan')},'unknown',inplace=True)"", ""#            if 'site' in meas_data3_0.columns:"", ""#                if any(meas_data3_0['site'].isnull()):"", '#                    print(""-W- Some measurements are missing site data, and will not be used"")', ""#                meas_data3_0 = meas_data3_0[meas_data3_0['site'].notnull()]"", ""# meas_data3_0.replace({'site':float('nan')},'unknown',inplace=True)"", ""#            if 'sample' in meas_data3_0.columns:"", ""#                if any(meas_data3_0['sample'].isnull()):"", '#                    print(""-W- Some measurements are missing sample data, and will not be used"")', ""#                meas_data3_0 = meas_data3_0[meas_data3_0['sample'].notnull()]"", ""# meas_data3_0.replace({'sample':float('nan')},'unknown',inplace=True)"", ""#            if 'specimen' in meas_data3_0.columns:"", ""#                missing = meas_data3_0[meas_data3_0['specimen'].isnull()]"", '#                if len(missing):', '#                    print(""-W- {} measurements are missing specimen data, and will not be used"".format(missing))', ""#                meas_data3_0 = meas_data3_0[meas_data3_0['specimen'].notnull()]"", ""# meas_data3_0.replace({'specimen':float('nan')},'unknown',inplace=True)"", ""#            col_names = ['specimen', 'sample', 'site', 'location']"", '#            for col_name in col_names:', '#                if col_name in meas_data3_0.columns:', '#                    pruned = meas_data3_0[meas_data3_0[col_name].apply(cb.not_null)]', '#                    num_missing = len(meas_data3_0) - len(pruned)', '#                    if num_missing:', '#                        msg = ""{} measurements cannot be associated with a {} and will be excluded\\nTry using Pmag GUI (step 3) to make sure you have provided the full chain from specimen to location."".format(num_missing, col_name)', '#                        pw.simple_warning(msg)', '#                        print(""-W- {} measurements are missing {} data and will be excluded"".format(num_missing, col_name))', '#                        meas_data3_0 = pruned', 'Mkeys', '=', '[', ""'magn_moment'"", ',', ""'magn_volume'"", ',', ""'magn_mass'"", ']', '# fish out all the relavent data', 'meas_data3_0', '=', 'meas_data3_0', '[', 'meas_data3_0', '[', ""'method_codes'"", ']', '.', 'str', '.', 'contains', '(', ""'LT-NO|LT-AF-Z|LT-T-Z|LT-M-Z|LT-LT-Z'"", ')', '==', 'True', ']', 'if', 'not', 'len', '(', 'meas_data3_0', ')', ':', 'self', '.', 'user_warning', '(', '""Your measurements table contains none of the required method codes to run Demag GUI: [LT-NO, LT-AF-Z, LT-T-Z, LT-M-Z, LT-LT-Z]""', ')', 'return', '{', '}', ',', '{', '}', '# now convert back to 2.5  changing only those keys that are necessary for thellier_gui', 'meas_con_dict', '=', 'map_magic', '.', 'get_thellier_gui_meas_mapping', '(', 'meas_data3_0', ',', 'output', '=', '2', ')', 'intensity_col', '=', 'cb', '.', 'get_intensity_col', '(', 'meas_data3_0', ')', 'if', 'not', 'intensity_col', ':', 'self', '.', 'user_warning', '(', '""Your measurements table must have one of the following columns to run Demag GUI: \'magn_moment\', \'magn_volume\', \'magn_mass\',or \'magn_uncal\'""', ')', 'return', '{', '}', ',', '{', '}', 'print', '(', ""'-I- Using {} for intensity'"", '.', 'format', '(', 'intensity_col', ')', ')', 'self', '.', 'intensity_col', '=', 'meas_con_dict', '[', 'intensity_col', ']', 'meas_data2_5', '=', 'meas_data3_0', '.', 'rename', '(', 'columns', '=', 'meas_con_dict', ')', '# make a list of dictionaries to maintain backward compatibility', 'mag_meas_data', '=', 'meas_data2_5', '.', 'to_dict', '(', '""records""', ')', 'else', ':', '# data model 2.5', 'try', ':', 'print', '(', '(', '""-I- Read magic file %s""', '%', 'self', '.', 'magic_file', ')', ')', 'except', 'ValueError', ':', 'self', '.', 'magic_measurement', '=', 'self', '.', 'choose_meas_file', '(', ')', 'print', '(', '(', '""-I- Read magic file %s""', '%', 'self', '.', 'magic_file', ')', ')', 'mag_meas_data', ',', 'file_type', '=', 'pmag', '.', 'magic_read', '(', 'self', '.', 'magic_file', ')', 'if', 'file_type', '!=', '""magic_measurements""', ':', 'self', '.', 'user_warning', '(', '""You have selected data model 2.5, but your measurements file is either not in 2.5, or is not a measurements file.\\n{} has file type: {}""', '.', 'format', '(', 'self', '.', 'magic_file', ',', 'file_type', ')', ')', 'return', '{', '}', ',', '{', '}', 'self', '.', 'mag_meas_data', '=', 'self', '.', 'merge_pmag_recs', '(', 'mag_meas_data', ')', '# get list of unique specimen names with measurement data', 'CurrRec', '=', '[', ']', 'sids', '=', 'pmag', '.', 'get_specs', '(', 'self', '.', 'mag_meas_data', ')', ""# specimen ID's"", 'for', 's', 'in', 'sids', ':', 'if', 's', 'not', 'in', 'list', '(', 'Data', '.', 'keys', '(', ')', ')', ':', 'Data', '[', 's', ']', '=', '{', '}', 'Data', '[', 's', ']', '[', ""'zijdblock'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'zijdblock_geo'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'zijdblock_tilt'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'zijdblock_lab_treatments'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'pars'"", ']', '=', '{', '}', 'Data', '[', 's', ']', '[', ""'csds'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'zijdblock_steps'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'measurement_flag'"", ']', '=', '[', ']', ""# a list of points 'g' or 'b'"", '# index in original magic_measurements.txt', 'Data', '[', 's', ']', '[', ""'mag_meas_data_index'"", ']', '=', '[', ']', 'Data', '[', 's', ']', '[', ""'measurement_names'"", ']', '=', '[', ']', 'prev_s', '=', 'None', 'cnt', '=', '-', '1', '# list of excluded lab protocols. copied from pmag.find_dmag_rec(s,data)', 'self', '.', 'excluded_methods', '=', '[', '""LP-AN-ARM""', ',', '""LP-AN-TRM""', ',', '""LP-ARM-AFD""', ',', '""LP-ARM2-AFD""', ',', '""LP-TRM-AFD""', ',', '""LP-TRM""', ',', '""LP-TRM-TD""', ',', '""LP-X""', ',', '""LP-PI-ARM""', ']', 'self', '.', 'included_methods', '=', '[', '""LT-NO""', ',', '""LT-AF-Z""', ',', '""LT-T-Z""', ',', '""LT-M-Z""', ',', '""LT-LT-Z""', ']', '#        self.mag_meas_data.sort(key=meas_key)', '# asiigned default values for NRM', 'if', 'len', '(', 'self', '.', 'mag_meas_data', ')', '>', '0', 'and', 'self', '.', 'intensity_col', 'in', 'list', '(', 'self', '.', 'mag_meas_data', '[', '0', ']', '.', 'keys', '(', ')', ')', ':', 'NRM', '=', 'float', '(', 'self', '.', 'mag_meas_data', '[', '0', ']', '[', 'self', '.', 'intensity_col', ']', ')', 'for', 'rec', 'in', 'self', '.', 'mag_meas_data', ':', '# if ""measurement_number"" in rec.keys() and str(rec[\'measurement_number\']) == \'1\' and ""magic_method_codes"" in rec.keys() and ""LT-NO"" not in rec[""magic_method_codes""].split(\':\'):', '#    NRM = 1 #not really sure how to handle this case but assume that data is already normalized', 'cnt', '+=', '1', '# index counter', 's', '=', 'rec', '[', '""er_specimen_name""', ']', 'if', '""er_sample_name""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'sample', '=', 'rec', '[', '""er_sample_name""', ']', 'else', ':', 'sample', '=', ""''"", 'if', '""er_site_name""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'site', '=', 'rec', '[', '""er_site_name""', ']', 'else', ':', 'site', '=', ""''"", 'if', '""er_location_name""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'location', '=', 'rec', '[', '""er_location_name""', ']', 'else', ':', 'location', '=', ""''"", 'expedition_name', '=', '""""', 'if', '""er_expedition_name""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'expedition_name', '=', 'rec', '[', '""er_expedition_name""', ']', 'methods', '=', 'rec', '[', '""magic_method_codes""', ']', '.', 'replace', '(', '"" ""', ',', '""""', ')', '.', 'strip', '(', '""\\n""', ')', '.', 'split', '(', '"":""', ')', 'LP_methods', '=', '[', ']', 'LT_methods', '=', '[', ']', 'for', 'k', 'in', '[', ""'zdata'"", ',', ""'zdata_geo'"", ',', ""'zdata_tilt'"", ',', ""'vector_diffs'"", ']', ':', 'if', 'k', 'not', 'in', 'Data', '[', 's', ']', ':', 'Data', '[', 's', ']', '[', 'k', ']', '=', '[', ']', 'for', 'i', 'in', 'range', '(', 'len', '(', 'methods', ')', ')', ':', 'methods', '[', 'i', ']', '=', 'methods', '[', 'i', ']', '.', 'strip', '(', ')', 'if', ""'measurement_flag'"", 'not', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'rec', '[', ""'measurement_flag'"", ']', '=', ""'g'"", 'SKIP', '=', 'True', 'lab_treatment', '=', '""""', 'for', 'meth', 'in', 'methods', ':', 'if', ""'DIR'"", 'in', 'meth', ':', 'SKIP', '=', 'False', 'if', 'meth', 'in', 'self', '.', 'included_methods', ':', 'lab_treatment', '=', 'meth', 'SKIP', '=', 'False', 'if', '""LP""', 'in', 'meth', ':', 'LP_methods', '.', 'append', '(', 'meth', ')', 'for', 'meth', 'in', 'self', '.', 'excluded_methods', ':', 'if', 'meth', 'in', 'methods', ':', 'SKIP', '=', 'True', 'break', 'if', 'SKIP', ':', 'continue', 'tr', ',', 'LPcode', ',', 'measurement_step_unit', '=', '""""', ',', '""""', ',', '""""', 'if', '""LT-NO""', 'in', 'methods', ':', 'tr', '=', '0', 'if', 'prev_s', '!=', 's', 'and', 'self', '.', 'intensity_col', 'in', 'rec', ':', 'try', ':', 'NRM', '=', 'float', '(', 'rec', '[', 'self', '.', 'intensity_col', ']', ')', 'except', 'ValueError', ':', 'NRM', '=', '1', 'for', 'method', 'in', 'methods', ':', 'if', '""AF""', 'in', 'method', ':', 'LPcode', '=', '""LP-DIR-AF""', 'measurement_step_unit', '=', '""mT""', 'if', '""TRM""', 'in', 'method', ':', 'LPcode', '=', '""LP-DIR-T""', 'measurement_step_unit', '=', '""C""', 'elif', '""LT-AF-Z""', 'in', 'methods', ':', 'try', ':', 'tr', '=', 'float', '(', 'rec', '[', '""treatment_ac_field""', ']', ')', '*', '1e3', '# (mT)', 'except', 'ValueError', ':', 'print', '(', '(', '""Could not convert ac field for measurement, was given %s, skipping""', '%', 'rec', '[', '""treatment_ac_field""', ']', ')', ')', 'continue', 'measurement_step_unit', '=', '""mT""', '# in magic its T in GUI its mT', 'LPcode', '=', '""LP-DIR-AF""', 'elif', '""LT-T-Z""', 'in', 'methods', 'or', '""LT-LT-Z""', 'in', 'methods', ':', 'try', ':', 'tr', '=', 'float', '(', 'rec', '[', '""treatment_temp""', ']', ')', '-', '273.', '# celsius', 'except', 'ValueError', ':', 'print', '(', '(', '""Could not convert temperature for measurement, was given %s, skipping""', '%', 'rec', '[', '""treatment_temp""', ']', ')', ')', 'continue', 'measurement_step_unit', '=', '""C""', '# in magic its K in GUI its C', 'LPcode', '=', '""LP-DIR-T""', 'elif', '""LT-M-Z""', 'in', 'methods', ':', '# temporary for microwave', 'tr', '=', 'float', '(', 'rec', '[', '""measurement_number""', ']', ')', 'else', ':', '# attempt to determine from treatment data', 'if', 'all', '(', 'im', 'not', 'in', 'methods', 'for', 'im', 'in', 'self', '.', 'included_methods', ')', ':', 'if', ""'treatment_temp'"", 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', 'and', 'not', 'str', '(', 'rec', '[', ""'treatment_temp'"", ']', ')', '.', 'isalpha', '(', ')', 'and', 'rec', '[', ""'treatment_temp'"", ']', '!=', ""''"", 'and', 'float', '(', 'rec', '[', ""'treatment_temp'"", ']', ')', '>', '0', ':', 'tr', '=', 'float', '(', 'rec', '[', '""treatment_temp""', ']', ')', '-', '273.', '# celsius', 'measurement_step_unit', '=', '""C""', '# in magic its K in GUI its C', 'LPcode', '=', '""LP-DIR-T""', 'elif', ""'treatment_ac_field'"", 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', 'and', 'not', 'str', '(', 'rec', '[', ""'treatment_ac_field'"", ']', ')', '.', 'isalpha', '(', ')', 'and', 'rec', '[', ""'treatment_ac_field'"", ']', '!=', ""''"", 'and', 'float', '(', 'rec', '[', ""'treatment_ac_field'"", ']', ')', '>', '0', ':', 'tr', '=', 'float', '(', 'rec', '[', '""treatment_ac_field""', ']', ')', '*', '1e3', '# (mT)', 'measurement_step_unit', '=', '""mT""', '# in magic its T in GUI its mT', 'LPcode', '=', '""LP-DIR-AF""', 'else', ':', 'tr', '=', '0', 'if', 'prev_s', '!=', 's', 'and', 'self', '.', 'intensity_col', 'in', 'rec', ':', 'try', ':', 'NRM', '=', 'float', '(', 'rec', '[', 'self', '.', 'intensity_col', ']', ')', 'except', 'ValueError', ':', 'NRM', '=', '1', 'for', 'method', 'in', 'methods', ':', 'if', '""AF""', 'in', 'method', ':', 'LPcode', '=', '""LP-DIR-AF""', 'measurement_step_unit', '=', '""mT""', 'if', '""TRM""', 'in', 'method', ':', 'LPcode', '=', '""LP-DIR-T""', 'measurement_step_unit', '=', '""C""', 'else', ':', 'tr', '=', 'float', '(', 'rec', '[', '""measurement_number""', ']', ')', 'if', 'prev_s', '!=', 's', 'and', 'len', '(', 'Data', '[', 's', ']', '[', ""'zijdblock'"", ']', ')', '>', '0', ':', 'NRM', '=', 'Data', '[', 's', ']', '[', ""'zijdblock'"", ']', '[', '0', ']', '[', '3', ']', 'ZI', '=', '0', 'if', 'tr', '!=', '""""', ':', 'Data', '[', 's', ']', '[', ""'mag_meas_data_index'"", ']', '.', 'append', '(', 'cnt', ')', '# magic_measurement file intex', 'if', 'not', 'int', '(', 'self', '.', 'data_model', ')', '==', '2', ':', 'try', ':', 'Data', '[', 's', ']', '[', ""'measurement_names'"", ']', '.', 'append', '(', 'rec', '[', ""'measurement'"", ']', ')', 'except', 'KeyError', ':', 'Data', '[', 's', ']', '[', ""'measurement_names'"", ']', '.', 'append', '(', 'rec', '[', ""'measurement_number'"", ']', ')', 'Data', '[', 's', ']', '[', ""'zijdblock_lab_treatments'"", ']', '.', 'append', '(', 'lab_treatment', ')', 'if', 'measurement_step_unit', '!=', '""""', ':', 'if', ""'measurement_step_unit'"", 'in', 'list', '(', 'Data', '[', 's', ']', '.', 'keys', '(', ')', ')', ':', 'if', 'measurement_step_unit', 'not', 'in', 'Data', '[', 's', ']', '[', ""'measurement_step_unit'"", ']', '.', 'split', '(', '"":""', ')', ':', 'Data', '[', 's', ']', '[', ""'measurement_step_unit'"", ']', '=', 'Data', '[', 's', ']', '[', ""'measurement_step_unit'"", ']', '+', '"":""', '+', 'measurement_step_unit', 'else', ':', 'Data', '[', 's', ']', '[', ""'measurement_step_unit'"", ']', '=', 'measurement_step_unit', 'dec', ',', 'inc', ',', 'inten', '=', '""""', ',', '""""', ',', '""""', 'if', '""measurement_dec""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', 'and', 'cb', '.', 'not_null', '(', 'rec', '[', '""measurement_dec""', ']', ',', 'False', ')', ':', 'dec', '=', 'float', '(', 'rec', '[', '""measurement_dec""', ']', ')', 'else', ':', 'continue', 'if', '""measurement_inc""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', 'and', 'cb', '.', 'not_null', '(', 'rec', '[', '""measurement_inc""', ']', ',', 'False', ')', ':', 'inc', '=', 'float', '(', 'rec', '[', '""measurement_inc""', ']', ')', 'else', ':', 'continue', 'if', 'self', '.', 'intensity_col', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', 'and', 'cb', '.', 'not_null', '(', 'rec', '[', 'self', '.', 'intensity_col', ']', ',', 'False', ')', ':', 'intensity', '=', 'float', '(', 'rec', '[', 'self', '.', 'intensity_col', ']', ')', 'else', ':', 'intensity', '=', '1.', '# just assume a normal vector', 'if', ""'magic_instrument_codes'"", 'not', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'rec', '[', ""'magic_instrument_codes'"", ']', '=', ""''"", 'if', ""'measurement_csd'"", 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'csd', '=', 'str', '(', 'rec', '[', ""'measurement_csd'"", ']', ')', 'else', ':', 'csd', '=', ""''"", 'Data', '[', 's', ']', '[', ""'zijdblock'"", ']', '.', 'append', '(', '[', 'tr', ',', 'dec', ',', 'inc', ',', 'intensity', ',', 'ZI', ',', 'rec', '[', ""'measurement_flag'"", ']', ',', 'rec', '[', ""'magic_instrument_codes'"", ']', ']', ')', 'Data', '[', 's', ']', '[', ""'csds'"", ']', '.', 'append', '(', 'csd', ')', 'DIR', '=', '[', 'dec', ',', 'inc', ',', 'intensity', '/', 'NRM', ']', 'cart', '=', 'pmag', '.', 'dir2cart', '(', 'DIR', ')', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', '.', 'append', '(', 'array', '(', '[', 'cart', '[', '0', ']', ',', 'cart', '[', '1', ']', ',', 'cart', '[', '2', ']', ']', ')', ')', 'if', ""'magic_experiment_name'"", 'in', 'list', '(', 'Data', '[', 's', ']', '.', 'keys', '(', ')', ')', 'and', 'Data', '[', 's', ']', '[', ""'magic_experiment_name'"", ']', '!=', 'rec', '[', '""magic_experiment_name""', ']', ':', 'print', '(', '(', '""-E- ERROR: specimen %s has more than one demagnetization experiment name. You need to merge them to one experiment-name?""', '%', '(', 's', ')', ')', ')', 'if', 'float', '(', 'tr', ')', '==', '0', 'or', 'float', '(', 'tr', ')', '==', '273', ':', 'Data', '[', 's', ']', '[', ""'zijdblock_steps'"", ']', '.', 'append', '(', '""0""', ')', 'elif', 'measurement_step_unit', '==', '""C""', ':', 'Data', '[', 's', ']', '[', ""'zijdblock_steps'"", ']', '.', 'append', '(', '""%.0f%s""', '%', '(', 'tr', ',', 'measurement_step_unit', ')', ')', 'else', ':', 'Data', '[', 's', ']', '[', ""'zijdblock_steps'"", ']', '.', 'append', '(', '""%.1f%s""', '%', '(', 'tr', ',', 'measurement_step_unit', ')', ')', '# --------------', 'if', ""'magic_experiment_name'"", 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'Data', '[', 's', ']', '[', ""'magic_experiment_name'"", ']', '=', 'rec', '[', '""magic_experiment_name""', ']', 'if', '""magic_instrument_codes""', 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'Data', '[', 's', ']', '[', ""'magic_instrument_codes'"", ']', '=', 'rec', '[', ""'magic_instrument_codes'"", ']', 'Data', '[', 's', ']', '[', '""magic_method_codes""', ']', '=', 'LPcode', '# --------------', '# """"good"" or ""bad"" data', '# --------------', 'flag', '=', ""'g'"", 'if', ""'measurement_flag'"", 'in', 'list', '(', 'rec', '.', 'keys', '(', ')', ')', ':', 'if', 'str', '(', 'rec', '[', '""measurement_flag""', ']', ')', '==', ""'b'"", ':', 'flag', '=', ""'b'"", 'Data', '[', 's', ']', '[', ""'measurement_flag'"", ']', '.', 'append', '(', 'flag', ')', '# gegraphic coordinates', 'try', ':', 'sample_azimuth', '=', 'float', '(', 'self', '.', 'Data_info', '[', '""er_samples""', ']', '[', 'sample', ']', '[', ""'sample_azimuth'"", ']', ')', 'sample_dip', '=', 'float', '(', 'self', '.', 'Data_info', '[', '""er_samples""', ']', '[', 'sample', ']', '[', ""'sample_dip'"", ']', ')', 'd_geo', ',', 'i_geo', '=', 'pmag', '.', 'dogeo', '(', 'dec', ',', 'inc', ',', 'sample_azimuth', ',', 'sample_dip', ')', ""# if d_geo or i_geo is null, we can't do geographic coordinates"", '# otherwise, go ahead', 'if', 'not', 'any', '(', '[', 'np', '.', 'isnan', '(', 'val', ')', 'for', 'val', 'in', '[', 'd_geo', ',', 'i_geo', ']', ']', ')', ':', 'Data', '[', 's', ']', '[', ""'zijdblock_geo'"", ']', '.', 'append', '(', '[', 'tr', ',', 'd_geo', ',', 'i_geo', ',', 'intensity', ',', 'ZI', ',', 'rec', '[', ""'measurement_flag'"", ']', ',', 'rec', '[', ""'magic_instrument_codes'"", ']', ']', ')', 'DIR', '=', '[', 'd_geo', ',', 'i_geo', ',', 'intensity', '/', 'NRM', ']', 'cart', '=', 'pmag', '.', 'dir2cart', '(', 'DIR', ')', 'Data', '[', 's', ']', '[', ""'zdata_geo'"", ']', '.', 'append', '(', '[', 'cart', '[', '0', ']', ',', 'cart', '[', '1', ']', ',', 'cart', '[', '2', ']', ']', ')', 'except', '(', 'IOError', ',', 'KeyError', ',', 'ValueError', ',', 'TypeError', ')', 'as', 'e', ':', 'pass', '#                    if prev_s != s:', '#                        print( ""-W- can\'t find sample_azimuth,sample_dip for sample %s""%sample)', '# tilt-corrected coordinates', 'try', ':', 'sample_bed_dip_direction', '=', 'float', '(', 'self', '.', 'Data_info', '[', '""er_samples""', ']', '[', 'sample', ']', '[', ""'sample_bed_dip_direction'"", ']', ')', 'sample_bed_dip', '=', 'float', '(', 'self', '.', 'Data_info', '[', '""er_samples""', ']', '[', 'sample', ']', '[', ""'sample_bed_dip'"", ']', ')', 'd_tilt', ',', 'i_tilt', '=', 'pmag', '.', 'dotilt', '(', 'd_geo', ',', 'i_geo', ',', 'sample_bed_dip_direction', ',', 'sample_bed_dip', ')', 'Data', '[', 's', ']', '[', ""'zijdblock_tilt'"", ']', '.', 'append', '(', '[', 'tr', ',', 'd_tilt', ',', 'i_tilt', ',', 'intensity', ',', 'ZI', ',', 'rec', '[', ""'measurement_flag'"", ']', ',', 'rec', '[', ""'magic_instrument_codes'"", ']', ']', ')', 'DIR', '=', '[', 'd_tilt', ',', 'i_tilt', ',', 'intensity', '/', 'NRM', ']', 'cart', '=', 'pmag', '.', 'dir2cart', '(', 'DIR', ')', 'Data', '[', 's', ']', '[', ""'zdata_tilt'"", ']', '.', 'append', '(', '[', 'cart', '[', '0', ']', ',', 'cart', '[', '1', ']', ',', 'cart', '[', '2', ']', ']', ')', 'except', '(', 'IOError', ',', 'KeyError', ',', 'TypeError', ',', 'ValueError', ',', 'UnboundLocalError', ')', 'as', 'e', ':', 'pass', '#                    if prev_s != s:', '#                        printd(""-W- can\'t find tilt-corrected data for sample %s""%sample)', 'if', 'len', '(', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', ')', '>', '1', ':', 'Data', '[', 's', ']', '[', ""'vector_diffs'"", ']', '.', 'append', '(', 'sqrt', '(', 'sum', '(', '(', 'array', '(', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', '[', '-', '2', ']', ')', '-', 'array', '(', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', '[', '-', '1', ']', ')', ')', '**', '2', ')', ')', ')', '# ---------------------', '# hierarchy is determined from magic_measurements.txt', '# ---------------------', 'if', 'sample', 'not', 'in', 'list', '(', 'Data_hierarchy', '[', ""'samples'"", ']', '.', 'keys', '(', ')', ')', ':', 'Data_hierarchy', '[', ""'samples'"", ']', '[', 'sample', ']', '=', '{', '}', 'Data_hierarchy', '[', ""'samples'"", ']', '[', 'sample', ']', '[', ""'specimens'"", ']', '=', '[', ']', 'if', 'site', 'not', 'in', 'list', '(', 'Data_hierarchy', '[', ""'sites'"", ']', '.', 'keys', '(', ')', ')', ':', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '=', '{', '}', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '[', ""'samples'"", ']', '=', '[', ']', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '[', ""'specimens'"", ']', '=', '[', ']', 'if', 'location', 'not', 'in', 'list', '(', 'Data_hierarchy', '[', ""'locations'"", ']', '.', 'keys', '(', ')', ')', ':', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '=', '{', '}', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'sites'"", ']', '=', '[', ']', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'samples'"", ']', '=', '[', ']', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'specimens'"", ']', '=', '[', ']', 'if', ""'this study'"", 'not', 'in', 'list', '(', 'Data_hierarchy', '[', ""'study'"", ']', '.', 'keys', '(', ')', ')', ':', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '=', '{', '}', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'sites'"", ']', '=', '[', ']', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'samples'"", ']', '=', '[', ']', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'specimens'"", ']', '=', '[', ']', 'if', 's', 'not', 'in', 'Data_hierarchy', '[', ""'samples'"", ']', '[', 'sample', ']', '[', ""'specimens'"", ']', ':', 'Data_hierarchy', '[', ""'samples'"", ']', '[', 'sample', ']', '[', ""'specimens'"", ']', '.', 'append', '(', 's', ')', 'if', 's', 'not', 'in', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '[', ""'specimens'"", ']', ':', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '[', ""'specimens'"", ']', '.', 'append', '(', 's', ')', 'if', 's', 'not', 'in', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'specimens'"", ']', ':', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'specimens'"", ']', '.', 'append', '(', 's', ')', 'if', 's', 'not', 'in', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'specimens'"", ']', ':', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'specimens'"", ']', '.', 'append', '(', 's', ')', 'if', 'sample', 'not', 'in', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '[', ""'samples'"", ']', ':', 'Data_hierarchy', '[', ""'sites'"", ']', '[', 'site', ']', '[', ""'samples'"", ']', '.', 'append', '(', 'sample', ')', 'if', 'sample', 'not', 'in', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'samples'"", ']', ':', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'samples'"", ']', '.', 'append', '(', 'sample', ')', 'if', 'sample', 'not', 'in', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'samples'"", ']', ':', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'samples'"", ']', '.', 'append', '(', 'sample', ')', 'if', 'site', 'not', 'in', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'sites'"", ']', ':', 'Data_hierarchy', '[', ""'locations'"", ']', '[', 'location', ']', '[', ""'sites'"", ']', '.', 'append', '(', 'site', ')', 'if', 'site', 'not', 'in', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'sites'"", ']', ':', 'Data_hierarchy', '[', ""'study'"", ']', '[', ""'this study'"", ']', '[', ""'sites'"", ']', '.', 'append', '(', 'site', ')', ""# Data_hierarchy['specimens'][s]=sample"", 'Data_hierarchy', '[', ""'sample_of_specimen'"", ']', '[', 's', ']', '=', 'sample', 'Data_hierarchy', '[', ""'site_of_specimen'"", ']', '[', 's', ']', '=', 'site', 'Data_hierarchy', '[', ""'site_of_sample'"", ']', '[', 'sample', ']', '=', 'site', 'Data_hierarchy', '[', ""'location_of_site'"", ']', '[', 'site', ']', '=', 'location', 'Data_hierarchy', '[', ""'location_of_specimen'"", ']', '[', 's', ']', '=', 'location', 'if', 'expedition_name', '!=', '""""', ':', 'Data_hierarchy', '[', ""'expedition_name_of_specimen'"", ']', '[', 's', ']', '=', 'expedition_name', 'prev_s', '=', 's', 'print', '(', '""-I- done sorting meas data""', ')', 'self', '.', 'specimens', '=', 'list', '(', 'Data', '.', 'keys', '(', ')', ')', 'for', 's', 'in', 'self', '.', 'specimens', ':', 'if', 'len', '(', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', ')', '>', '0', ':', 'Data', '[', 's', ']', '[', ""'vector_diffs'"", ']', '.', 'append', '(', 'sqrt', '(', 'sum', '(', 'array', '(', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', '[', '-', '1', ']', ')', '**', '2', ')', ')', ')', '# last vector of the vds', 'vds', '=', 'sum', '(', 'Data', '[', 's', ']', '[', ""'vector_diffs'"", ']', ')', '# vds calculation', 'Data', '[', 's', ']', '[', ""'vector_diffs'"", ']', '=', 'array', '(', 'Data', '[', 's', ']', '[', ""'vector_diffs'"", ']', ')', 'Data', '[', 's', ']', '[', ""'vds'"", ']', '=', 'vds', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', '=', 'array', '(', 'Data', '[', 's', ']', '[', ""'zdata'"", ']', ')', 'Data', '[', 's', ']', '[', ""'zdata_geo'"", ']', '=', 'array', '(', 'Data', '[', 's', ']', '[', ""'zdata_geo'"", ']', ')', 'Data', '[', 's', ']', '[', ""'zdata_tilt'"", ']', '=', 'array', '(', 'Data', '[', 's', ']', '[', ""'zdata_tilt'"", ']', ')', 'return', '(', 'Data', ',', 'Data_hierarchy', ')']","reads data from current WD measurement.txt or magic_measurements.txt
        depending on data model and sorts it into main measurements data
        structures given bellow:
        Data - {specimen: {
                zijdblock:[[treatment temp-str,dec-float, inc-float,
                        mag_moment-float, ZI-float, meas_flag-str ('b','g'),
                        method_codes-str]],
                zijdblock_geo:[[treatment temp-str,dec-float, inc-float,
                        mag_moment-float, ZI-float, meas_flag-str ('b','g'),
                        method_codes-str]],
                zijdblock_tilt:[[treatment temp-str,dec-float, inc-float,
                        mag_moment-float, ZI-float, meas_flag-str ('b','g'),
                        method_codes-str]],
                zijdblock_lab_treatments: [str],
                zijdblock_steps: [str],
                measurement_flag: [str ('b','g')],
                mag_meas_data_index: [int],
                csds: [float],
                pars: {},
                zdata: array.shape = 2x2 (float),
                zdata_geo: array.shape = 2x2 (float),
                zdata_tilt: array.shape = 2x2 (float),
                vector_diffs: [float],
                vds: float }}
        Data_hierarchy - {specimen: {
                            study: {}
                            locations: {}
                            sites: {}
                            samples: {}
                            specimens: {}
                            sample_of_specimen: {}
                            site_of_specimen: {}
                            site_of_sample: {}
                            location_of_site: {}
                            location_of_specimen: {}
                            study_of_specimen: {}
                            expedition_name_of_specimen: {} }}","['reads', 'data', 'from', 'current', 'WD', 'measurement', '.', 'txt', 'or', 'magic_measurements', '.', 'txt', 'depending', 'on', 'data', 'model', 'and', 'sorts', 'it', 'into', 'main', 'measurements', 'data', 'structures', 'given', 'bellow', ':', 'Data', '-', '{', 'specimen', ':', '{', 'zijdblock', ':', '[[', 'treatment', 'temp', '-', 'str', 'dec', '-', 'float', 'inc', '-', 'float', 'mag_moment', '-', 'float', 'ZI', '-', 'float', 'meas_flag', '-', 'str', '(', 'b', 'g', ')', 'method_codes', '-', 'str', ']]', 'zijdblock_geo', ':', '[[', 'treatment', 'temp', '-', 'str', 'dec', '-', 'float', 'inc', '-', 'float', 'mag_moment', '-', 'float', 'ZI', '-', 'float', 'meas_flag', '-', 'str', '(', 'b', 'g', ')', 'method_codes', '-', 'str', ']]', 'zijdblock_tilt', ':', '[[', 'treatment', 'temp', '-', 'str', 'dec', '-', 'float', 'inc', '-', 'float', 'mag_moment', '-', 'float', 'ZI', '-', 'float', 'meas_flag', '-', 'str', '(', 'b', 'g', ')', 'method_codes', '-', 'str', ']]', 'zijdblock_lab_treatments', ':', '[', 'str', ']', 'zijdblock_steps', ':', '[', 'str', ']', 'measurement_flag', ':', '[', 'str', '(', 'b', 'g', ')', ']', 'mag_meas_data_index', ':', '[', 'int', ']', 'csds', ':', '[', 'float', ']', 'pars', ':', '{}', 'zdata', ':', 'array', '.', 'shape', '=', '2x2', '(', 'float', ')', 'zdata_geo', ':', 'array', '.', 'shape', '=', '2x2', '(', 'float', ')', 'zdata_tilt', ':', 'array', '.', 'shape', '=', '2x2', '(', 'float', ')', 'vector_diffs', ':', '[', 'float', ']', 'vds', ':', 'float', '}}', 'Data_hierarchy', '-', '{', 'specimen', ':', '{', 'study', ':', '{}', 'locations', ':', '{}', 'sites', ':', '{}', 'samples', ':', '{}', 'specimens', ':', '{}', 'sample_of_specimen', ':', '{}', 'site_of_specimen', ':', '{}', 'site_of_sample', ':', '{}', 'location_of_site', ':', '{}', 'location_of_specimen', ':', '{}', 'study_of_specimen', ':', '{}', 'expedition_name_of_specimen', ':', '{}', '}}']",python,R,1,True,1,train
19258,PmagPy/PmagPy,programs/demag_gui.py,https://github.com/PmagPy/PmagPy/blob/c7984f8809bf40fe112e53dcc311a33293b62d0b/programs/demag_gui.py#L4657-L4693,"def read_magic_file(self, path, sort_by_this_name):
        """"""
        reads a magic formated data file from path and sorts the keys
        according to sort_by_this_name

        Parameters
        ----------
        path : path to file to read
        sort_by_this_name : variable to sort data by
        """"""
        DATA = {}
        try:
            with open(path, 'r') as finput:
                lines = list(finput.readlines()[1:])
        except FileNotFoundError:
            return []
        # fin=open(path,'r')
        # fin.readline()
        line = lines[0]
        header = line.strip('\n').split('\t')
        error_strings = []
        for line in lines[1:]:
            tmp_data = {}
            tmp_line = line.strip('\n').split('\t')
            for i in range(len(tmp_line)):
                tmp_data[header[i]] = tmp_line[i]
            if tmp_data[sort_by_this_name] in list(DATA.keys()):
                error_string = ""-E- ERROR: magic file %s has more than one line for %s %s"" % (
                    path, sort_by_this_name, tmp_data[sort_by_this_name])
                # only print each error message once
                if error_string not in error_strings:
                    print(error_string)
                    error_strings.append(error_string)
            DATA[tmp_data[sort_by_this_name]] = tmp_data
        # fin.close()
        finput.close()
        return(DATA)","['def', 'read_magic_file', '(', 'self', ',', 'path', ',', 'sort_by_this_name', ')', ':', 'DATA', '=', '{', '}', 'try', ':', 'with', 'open', '(', 'path', ',', ""'r'"", ')', 'as', 'finput', ':', 'lines', '=', 'list', '(', 'finput', '.', 'readlines', '(', ')', '[', '1', ':', ']', ')', 'except', 'FileNotFoundError', ':', 'return', '[', ']', ""# fin=open(path,'r')"", '# fin.readline()', 'line', '=', 'lines', '[', '0', ']', 'header', '=', 'line', '.', 'strip', '(', ""'\\n'"", ')', '.', 'split', '(', ""'\\t'"", ')', 'error_strings', '=', '[', ']', 'for', 'line', 'in', 'lines', '[', '1', ':', ']', ':', 'tmp_data', '=', '{', '}', 'tmp_line', '=', 'line', '.', 'strip', '(', ""'\\n'"", ')', '.', 'split', '(', ""'\\t'"", ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'tmp_line', ')', ')', ':', 'tmp_data', '[', 'header', '[', 'i', ']', ']', '=', 'tmp_line', '[', 'i', ']', 'if', 'tmp_data', '[', 'sort_by_this_name', ']', 'in', 'list', '(', 'DATA', '.', 'keys', '(', ')', ')', ':', 'error_string', '=', '""-E- ERROR: magic file %s has more than one line for %s %s""', '%', '(', 'path', ',', 'sort_by_this_name', ',', 'tmp_data', '[', 'sort_by_this_name', ']', ')', '# only print each error message once', 'if', 'error_string', 'not', 'in', 'error_strings', ':', 'print', '(', 'error_string', ')', 'error_strings', '.', 'append', '(', 'error_string', ')', 'DATA', '[', 'tmp_data', '[', 'sort_by_this_name', ']', ']', '=', 'tmp_data', '# fin.close()', 'finput', '.', 'close', '(', ')', 'return', '(', 'DATA', ')']","reads a magic formated data file from path and sorts the keys
        according to sort_by_this_name

        Parameters
        ----------
        path : path to file to read
        sort_by_this_name : variable to sort data by","['reads', 'a', 'magic', 'formated', 'data', 'file', 'from', 'path', 'and', 'sorts', 'the', 'keys', 'according', 'to', 'sort_by_this_name']",python,R,1,True,1,train
21306,singularityhub/singularity-cli,setup.py,https://github.com/singularityhub/singularity-cli/blob/cb36b4504812ca87e29c6a40b222a545d1865799/setup.py#L32-L51,"def get_requirements(lookup=None):
    '''get_requirements reads in requirements and versions from
    the lookup obtained with get_lookup'''

    if lookup == None:
        lookup = get_lookup()

    install_requires = []
    for module in lookup['INSTALL_REQUIRES']:
        module_name = module[0]
        module_meta = module[1]
        if ""exact_version"" in module_meta:
            dependency = ""%s==%s"" %(module_name,module_meta['exact_version'])
        elif ""min_version"" in module_meta:
            if module_meta['min_version'] == None:
                dependency = module_name
            else:
                dependency = ""%s>=%s"" %(module_name,module_meta['min_version'])
        install_requires.append(dependency)
    return install_requires","['def', 'get_requirements', '(', 'lookup', '=', 'None', ')', ':', 'if', 'lookup', '==', 'None', ':', 'lookup', '=', 'get_lookup', '(', ')', 'install_requires', '=', '[', ']', 'for', 'module', 'in', 'lookup', '[', ""'INSTALL_REQUIRES'"", ']', ':', 'module_name', '=', 'module', '[', '0', ']', 'module_meta', '=', 'module', '[', '1', ']', 'if', '""exact_version""', 'in', 'module_meta', ':', 'dependency', '=', '""%s==%s""', '%', '(', 'module_name', ',', 'module_meta', '[', ""'exact_version'"", ']', ')', 'elif', '""min_version""', 'in', 'module_meta', ':', 'if', 'module_meta', '[', ""'min_version'"", ']', '==', 'None', ':', 'dependency', '=', 'module_name', 'else', ':', 'dependency', '=', '""%s>=%s""', '%', '(', 'module_name', ',', 'module_meta', '[', ""'min_version'"", ']', ')', 'install_requires', '.', 'append', '(', 'dependency', ')', 'return', 'install_requires']","get_requirements reads in requirements and versions from
    the lookup obtained with get_lookup","['get_requirements', 'reads', 'in', 'requirements', 'and', 'versions', 'from', 'the', 'lookup', 'obtained', 'with', 'get_lookup']",python,R,1,True,1,train
694,spacetelescope/drizzlepac,drizzlepac/tweakutils.py,https://github.com/spacetelescope/drizzlepac/blob/15bec3c929a6a869d9e71b9398ced43ede0620f1/drizzlepac/tweakutils.py#L393-L421,"def readcols(infile, cols=None):
    """""" Function which reads specified columns from either FITS tables or
        ASCII files

        This function reads in the columns specified by the user into numpy
        arrays regardless of the format of the input table (ASCII or FITS
        table).

        Parameters
        ----------
        infile : string
            Filename of the input file
        cols   : string or list of strings
            Columns to be read into arrays

        Returns
        -------
        outarr : array
            Numpy array or arrays of columns from the table

    """"""
    if _is_str_none(infile) is None:
        return None

    if infile.endswith('.fits'):
        outarr = read_FITS_cols(infile, cols=cols)
    else:
        outarr = read_ASCII_cols(infile, cols=cols)
    return outarr","['def', 'readcols', '(', 'infile', ',', 'cols', '=', 'None', ')', ':', 'if', '_is_str_none', '(', 'infile', ')', 'is', 'None', ':', 'return', 'None', 'if', 'infile', '.', 'endswith', '(', ""'.fits'"", ')', ':', 'outarr', '=', 'read_FITS_cols', '(', 'infile', ',', 'cols', '=', 'cols', ')', 'else', ':', 'outarr', '=', 'read_ASCII_cols', '(', 'infile', ',', 'cols', '=', 'cols', ')', 'return', 'outarr']","Function which reads specified columns from either FITS tables or
        ASCII files

        This function reads in the columns specified by the user into numpy
        arrays regardless of the format of the input table (ASCII or FITS
        table).

        Parameters
        ----------
        infile : string
            Filename of the input file
        cols   : string or list of strings
            Columns to be read into arrays

        Returns
        -------
        outarr : array
            Numpy array or arrays of columns from the table","['Function', 'which', 'reads', 'specified', 'columns', 'from', 'either', 'FITS', 'tables', 'or', 'ASCII', 'files']",python,R,1,True,1,train
2356,twisted/txaws,txaws/client/_producers.py,https://github.com/twisted/txaws/blob/5c3317376cd47e536625027e38c3b37840175ce0/txaws/client/_producers.py#L93-L104,"def _writeloop(self, consumer):
        """"""
        Return an iterator which reads one chunk of bytes from the input file
        and writes them to the consumer for each time it is iterated.
        """"""
        while True:
            bytes = self._inputFile.read(self._readSize)
            if not bytes:
                self._inputFile.close()
                break
            consumer.write(bytes)
            yield None","['def', '_writeloop', '(', 'self', ',', 'consumer', ')', ':', 'while', 'True', ':', 'bytes', '=', 'self', '.', '_inputFile', '.', 'read', '(', 'self', '.', '_readSize', ')', 'if', 'not', 'bytes', ':', 'self', '.', '_inputFile', '.', 'close', '(', ')', 'break', 'consumer', '.', 'write', '(', 'bytes', ')', 'yield', 'None']","Return an iterator which reads one chunk of bytes from the input file
        and writes them to the consumer for each time it is iterated.","['Return', 'an', 'iterator', 'which', 'reads', 'one', 'chunk', 'of', 'bytes', 'from', 'the', 'input', 'file', 'and', 'writes', 'them', 'to', 'the', 'consumer', 'for', 'each', 'time', 'it', 'is', 'iterated', '.']",python,R,1,True,1,train
4587,codelv/enaml-native,src/enamlnative/android/android_view.py,https://github.com/codelv/enaml-native/blob/c33986e9eda468c508806e0a3e73c771401e5718/src/enamlnative/android/android_view.py#L147-L168,"def init_widget(self):
        """""" Initialize the underlying widget.
        
        This reads all items declared in the enamldef block for this node 
        and sets only the values that have been specified. All other values 
        will be left as default. Doing it this way makes atom to only create 
        the properties that need to be overridden from defaults thus greatly 
        reducing the number of initialization checks, saving time and memory.
        
        If you don't want this to happen override `get_declared_keys` 
        to return an empty list. 

        """"""
        super(AndroidView, self).init_widget()

        # Initialize the widget by updating only the members that
        # have read expressions declared. This saves a lot of time and
        # simplifies widget initialization code
        for k, v in self.get_declared_items():
            handler = getattr(self, 'set_'+k, None)
            if handler:
                handler(v)","['def', 'init_widget', '(', 'self', ')', ':', 'super', '(', 'AndroidView', ',', 'self', ')', '.', 'init_widget', '(', ')', '# Initialize the widget by updating only the members that', '# have read expressions declared. This saves a lot of time and', '# simplifies widget initialization code', 'for', 'k', ',', 'v', 'in', 'self', '.', 'get_declared_items', '(', ')', ':', 'handler', '=', 'getattr', '(', 'self', ',', ""'set_'"", '+', 'k', ',', 'None', ')', 'if', 'handler', ':', 'handler', '(', 'v', ')']","Initialize the underlying widget.
        
        This reads all items declared in the enamldef block for this node 
        and sets only the values that have been specified. All other values 
        will be left as default. Doing it this way makes atom to only create 
        the properties that need to be overridden from defaults thus greatly 
        reducing the number of initialization checks, saving time and memory.
        
        If you don't want this to happen override `get_declared_keys` 
        to return an empty list.","['Initialize', 'the', 'underlying', 'widget', '.', 'This', 'reads', 'all', 'items', 'declared', 'in', 'the', 'enamldef', 'block', 'for', 'this', 'node', 'and', 'sets', 'only', 'the', 'values', 'that', 'have', 'been', 'specified', '.', 'All', 'other', 'values', 'will', 'be', 'left', 'as', 'default', '.', 'Doing', 'it', 'this', 'way', 'makes', 'atom', 'to', 'only', 'create', 'the', 'properties', 'that', 'need', 'to', 'be', 'overridden', 'from', 'defaults', 'thus', 'greatly', 'reducing', 'the', 'number', 'of', 'initialization', 'checks', 'saving', 'time', 'and', 'memory', '.', 'If', 'you', 'don', 't', 'want', 'this', 'to', 'happen', 'override', 'get_declared_keys', 'to', 'return', 'an', 'empty', 'list', '.']",python,R,1,True,1,train
7160,Spinmob/spinmob,egg/_gui.py,https://github.com/Spinmob/spinmob/blob/f037f5df07f194bcd4a01f4d9916e57b9e8fb45a/egg/_gui.py#L248-L286,"def _load_gui_setting(self, key, d=None):
        """"""
        Safely reads the header setting and sets the appropriate control
        value. 
        
        Parameters
        ----------
        key
            Key string of the format 'self.controlname'.
        d = None
            Databox instance or dictionary, presumably containing the aforementioned
            key. If d=None, pops the value from self._lazy_load.
        """"""
        # Whether we should pop the value from the dictionary when we set it.
        pop_value = False
        
        # If d is None, assume we're using the lazy load settings.
        if d == None: 
            d = self._lazy_load
            pop_value = True
        
        # If we have a databox, take the header dictionary
        if not type(d) == dict: d = d.headers
        
        # only do this if the key exists
        if key in d:
            
            try:
                # Try to set the value
                eval(key).set_value(d[key])

                # If this fails, perhaps the element does not yet exist
                # For example, TabArea may not have all the tabs created
                # and cannot set the active tab until later.
                # If it's here, it worked, so pop if necessary
                if pop_value: d.pop(key)
           
            except: 
                print(""ERROR: Could not load gui setting ""+repr(key))","['def', '_load_gui_setting', '(', 'self', ',', 'key', ',', 'd', '=', 'None', ')', ':', '# Whether we should pop the value from the dictionary when we set it.', 'pop_value', '=', 'False', ""# If d is None, assume we're using the lazy load settings."", 'if', 'd', '==', 'None', ':', 'd', '=', 'self', '.', '_lazy_load', 'pop_value', '=', 'True', '# If we have a databox, take the header dictionary', 'if', 'not', 'type', '(', 'd', ')', '==', 'dict', ':', 'd', '=', 'd', '.', 'headers', '# only do this if the key exists', 'if', 'key', 'in', 'd', ':', 'try', ':', '# Try to set the value', 'eval', '(', 'key', ')', '.', 'set_value', '(', 'd', '[', 'key', ']', ')', '# If this fails, perhaps the element does not yet exist', '# For example, TabArea may not have all the tabs created', '# and cannot set the active tab until later.', ""# If it's here, it worked, so pop if necessary"", 'if', 'pop_value', ':', 'd', '.', 'pop', '(', 'key', ')', 'except', ':', 'print', '(', '""ERROR: Could not load gui setting ""', '+', 'repr', '(', 'key', ')', ')']","Safely reads the header setting and sets the appropriate control
        value. 
        
        Parameters
        ----------
        key
            Key string of the format 'self.controlname'.
        d = None
            Databox instance or dictionary, presumably containing the aforementioned
            key. If d=None, pops the value from self._lazy_load.","['Safely', 'reads', 'the', 'header', 'setting', 'and', 'sets', 'the', 'appropriate', 'control', 'value', '.', 'Parameters', '----------', 'key', 'Key', 'string', 'of', 'the', 'format', 'self', '.', 'controlname', '.', 'd', '=', 'None', 'Databox', 'instance', 'or', 'dictionary', 'presumably', 'containing', 'the', 'aforementioned', 'key', '.', 'If', 'd', '=', 'None', 'pops', 'the', 'value', 'from', 'self', '.', '_lazy_load', '.']",python,R,1,True,1,train
10240,NuGrid/NuGridPy,nugridpy/ascii_table.py,https://github.com/NuGrid/NuGridPy/blob/eee8047446e398be77362d82c1d8b3310054fab0/nugridpy/ascii_table.py#L248-L346,"def _readFile(self, sldir, fileName, sep):
        '''
        Private method that reads in the header and column data.

        '''

        if sldir.endswith(os.sep):
            fileName = str(sldir)+str(fileName)
        else:
            fileName = str(sldir)+os.sep+str(fileName)


        fileLines=[] #list of lines in the file
        header=[]    #list of Header lines
        dataCols=[]  #Dictionary of data column names
        data=[]      #List of Data lists
        cols=[]      #List of column names

        f=open(fileName,'r')
        fileLines=f.readlines()
        i=0
        if self.datatype != 'trajectory':

            while i<len(fileLines):
                if fileLines[i].startswith(self.header_char):
                    tmp=fileLines[i].lstrip(self.header_char)
                    header.append(tmp.strip())
                else:
                    break
                i+=1

            cols=fileLines[i].split(sep)

            tmp=[]
            tmp1=[]
            for j in range(len(cols)):
                tmp1=cols[j].strip()
                if tmp1 !='':
                    tmp.append(tmp1)
            cols=tmp
            i+=1
        else:
            header={}
            while fileLines[i].startswith('#') or '=' in fileLines[i]:
                if fileLines[i].startswith('#') and cols==[]:
                    cols=fileLines[i].strip('#')
                    cols=cols.strip()
                    cols=cols.split()
                elif fileLines[i].startswith('#'):
                    tmp1=fileLines[i].strip('#')
                    tmp1=tmp1.strip()
                    self.headerLines.append(tmp1)
                elif not fileLines[i].startswith('#'):
                    tmp=fileLines[i].split('=')
                    tmp[0]=tmp[0].strip()
                    tmp[1]=tmp[1].strip()
                    if header=={}:
                        header={str(tmp[0]):str(tmp[1])}
                    else:
                        header[str(tmp[0])]=str(tmp[1])
                i+=1
        while i<len(fileLines):
            if fileLines[i].startswith('#'):
                i=i+1
            else:
                tmp=fileLines[i].split()
                for j in range(len(tmp)):
                    tmp[j]=tmp[j].strip()
                data.append(tmp)
                i+=1
        tmp=[]
        tmp1=[]
        for j in range(len(data)):
            for k in range(len(data[j])):
                tmp1=data[j][k].strip()
                if tmp1 !='':
                    tmp.append(tmp1)
            data[j]=tmp
            tmp=[]
        tmp=[]

        for j in range(len(cols)):
            for k in range(len(data)):
                try:
                    a=float(data[k][j])
                    tmp.append(a)
                except ValueError:
                    tmp.append(data[k][j])
                #else:
                 #   tmp.append(float(data[k][j])) # previously tmp.append(float(data[k][j]))
            tmp=array(tmp)

            if j == 0:
                dataCols={cols[j]:tmp}
            else:
                dataCols[cols[j]]=tmp
            tmp=[]

        return header,dataCols","['def', '_readFile', '(', 'self', ',', 'sldir', ',', 'fileName', ',', 'sep', ')', ':', 'if', 'sldir', '.', 'endswith', '(', 'os', '.', 'sep', ')', ':', 'fileName', '=', 'str', '(', 'sldir', ')', '+', 'str', '(', 'fileName', ')', 'else', ':', 'fileName', '=', 'str', '(', 'sldir', ')', '+', 'os', '.', 'sep', '+', 'str', '(', 'fileName', ')', 'fileLines', '=', '[', ']', '#list of lines in the file', 'header', '=', '[', ']', '#list of Header lines', 'dataCols', '=', '[', ']', '#Dictionary of data column names', 'data', '=', '[', ']', '#List of Data lists', 'cols', '=', '[', ']', '#List of column names', 'f', '=', 'open', '(', 'fileName', ',', ""'r'"", ')', 'fileLines', '=', 'f', '.', 'readlines', '(', ')', 'i', '=', '0', 'if', 'self', '.', 'datatype', '!=', ""'trajectory'"", ':', 'while', 'i', '<', 'len', '(', 'fileLines', ')', ':', 'if', 'fileLines', '[', 'i', ']', '.', 'startswith', '(', 'self', '.', 'header_char', ')', ':', 'tmp', '=', 'fileLines', '[', 'i', ']', '.', 'lstrip', '(', 'self', '.', 'header_char', ')', 'header', '.', 'append', '(', 'tmp', '.', 'strip', '(', ')', ')', 'else', ':', 'break', 'i', '+=', '1', 'cols', '=', 'fileLines', '[', 'i', ']', '.', 'split', '(', 'sep', ')', 'tmp', '=', '[', ']', 'tmp1', '=', '[', ']', 'for', 'j', 'in', 'range', '(', 'len', '(', 'cols', ')', ')', ':', 'tmp1', '=', 'cols', '[', 'j', ']', '.', 'strip', '(', ')', 'if', 'tmp1', '!=', ""''"", ':', 'tmp', '.', 'append', '(', 'tmp1', ')', 'cols', '=', 'tmp', 'i', '+=', '1', 'else', ':', 'header', '=', '{', '}', 'while', 'fileLines', '[', 'i', ']', '.', 'startswith', '(', ""'#'"", ')', 'or', ""'='"", 'in', 'fileLines', '[', 'i', ']', ':', 'if', 'fileLines', '[', 'i', ']', '.', 'startswith', '(', ""'#'"", ')', 'and', 'cols', '==', '[', ']', ':', 'cols', '=', 'fileLines', '[', 'i', ']', '.', 'strip', '(', ""'#'"", ')', 'cols', '=', 'cols', '.', 'strip', '(', ')', 'cols', '=', 'cols', '.', 'split', '(', ')', 'elif', 'fileLines', '[', 'i', ']', '.', 'startswith', '(', ""'#'"", ')', ':', 'tmp1', '=', 'fileLines', '[', 'i', ']', '.', 'strip', '(', ""'#'"", ')', 'tmp1', '=', 'tmp1', '.', 'strip', '(', ')', 'self', '.', 'headerLines', '.', 'append', '(', 'tmp1', ')', 'elif', 'not', 'fileLines', '[', 'i', ']', '.', 'startswith', '(', ""'#'"", ')', ':', 'tmp', '=', 'fileLines', '[', 'i', ']', '.', 'split', '(', ""'='"", ')', 'tmp', '[', '0', ']', '=', 'tmp', '[', '0', ']', '.', 'strip', '(', ')', 'tmp', '[', '1', ']', '=', 'tmp', '[', '1', ']', '.', 'strip', '(', ')', 'if', 'header', '==', '{', '}', ':', 'header', '=', '{', 'str', '(', 'tmp', '[', '0', ']', ')', ':', 'str', '(', 'tmp', '[', '1', ']', ')', '}', 'else', ':', 'header', '[', 'str', '(', 'tmp', '[', '0', ']', ')', ']', '=', 'str', '(', 'tmp', '[', '1', ']', ')', 'i', '+=', '1', 'while', 'i', '<', 'len', '(', 'fileLines', ')', ':', 'if', 'fileLines', '[', 'i', ']', '.', 'startswith', '(', ""'#'"", ')', ':', 'i', '=', 'i', '+', '1', 'else', ':', 'tmp', '=', 'fileLines', '[', 'i', ']', '.', 'split', '(', ')', 'for', 'j', 'in', 'range', '(', 'len', '(', 'tmp', ')', ')', ':', 'tmp', '[', 'j', ']', '=', 'tmp', '[', 'j', ']', '.', 'strip', '(', ')', 'data', '.', 'append', '(', 'tmp', ')', 'i', '+=', '1', 'tmp', '=', '[', ']', 'tmp1', '=', '[', ']', 'for', 'j', 'in', 'range', '(', 'len', '(', 'data', ')', ')', ':', 'for', 'k', 'in', 'range', '(', 'len', '(', 'data', '[', 'j', ']', ')', ')', ':', 'tmp1', '=', 'data', '[', 'j', ']', '[', 'k', ']', '.', 'strip', '(', ')', 'if', 'tmp1', '!=', ""''"", ':', 'tmp', '.', 'append', '(', 'tmp1', ')', 'data', '[', 'j', ']', '=', 'tmp', 'tmp', '=', '[', ']', 'tmp', '=', '[', ']', 'for', 'j', 'in', 'range', '(', 'len', '(', 'cols', ')', ')', ':', 'for', 'k', 'in', 'range', '(', 'len', '(', 'data', ')', ')', ':', 'try', ':', 'a', '=', 'float', '(', 'data', '[', 'k', ']', '[', 'j', ']', ')', 'tmp', '.', 'append', '(', 'a', ')', 'except', 'ValueError', ':', 'tmp', '.', 'append', '(', 'data', '[', 'k', ']', '[', 'j', ']', ')', '#else:', '#   tmp.append(float(data[k][j])) # previously tmp.append(float(data[k][j]))', 'tmp', '=', 'array', '(', 'tmp', ')', 'if', 'j', '==', '0', ':', 'dataCols', '=', '{', 'cols', '[', 'j', ']', ':', 'tmp', '}', 'else', ':', 'dataCols', '[', 'cols', '[', 'j', ']', ']', '=', 'tmp', 'tmp', '=', '[', ']', 'return', 'header', ',', 'dataCols']",Private method that reads in the header and column data.,"['Private', 'method', 'that', 'reads', 'in', 'the', 'header', 'and', 'column', 'data', '.']",python,R,1,True,1,train
367,apache/airflow,airflow/utils/log/wasb_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L154-L178,"def wasb_write(self, log, remote_log_location, append=True):
        """"""
        Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool
        """"""
        if append and self.wasb_log_exists(remote_log_location):
            old_log = self.wasb_read(remote_log_location)
            log = '\n'.join([old_log, log]) if old_log else log

        try:
            self.hook.load_string(
                log,
                self.wasb_container,
                remote_log_location,
            )
        except AzureHttpError:
            self.log.exception('Could not write logs to %s',
                               remote_log_location)","['def', 'wasb_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'True', ')', ':', 'if', 'append', 'and', 'self', '.', 'wasb_log_exists', '(', 'remote_log_location', ')', ':', 'old_log', '=', 'self', '.', 'wasb_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'try', ':', 'self', '.', 'hook', '.', 'load_string', '(', 'log', ',', 'self', '.', 'wasb_container', ',', 'remote_log_location', ',', ')', 'except', 'AzureHttpError', ':', 'self', '.', 'log', '.', 'exception', '(', ""'Could not write logs to %s'"", ',', 'remote_log_location', ')']","Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool","['Writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'Fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'False', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'If', 'True', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",python,W,0,True,1,test
439,apache/airflow,airflow/utils/log/gcs_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/gcs_task_handler.py#L132-L163,"def gcs_write(self, log, remote_log_location, append=True):
        """"""
        Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool
        """"""
        if append:
            try:
                old_log = self.gcs_read(remote_log_location)
                log = '\n'.join([old_log, log]) if old_log else log
            except Exception as e:
                if not hasattr(e, 'resp') or e.resp.get('status') != '404':
                    log = '*** Previous log discarded: {}\n\n'.format(str(e)) + log

        try:
            bkt, blob = self.parse_gcs_url(remote_log_location)
            from tempfile import NamedTemporaryFile
            with NamedTemporaryFile(mode='w+') as tmpfile:
                tmpfile.write(log)
                # Force the file to be flushed, since we're doing the
                # upload from within the file context (it hasn't been
                # closed).
                tmpfile.flush()
                self.hook.upload(bkt, blob, tmpfile.name)
        except Exception as e:
            self.log.error('Could not write logs to %s: %s', remote_log_location, e)","['def', 'gcs_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'True', ')', ':', 'if', 'append', ':', 'try', ':', 'old_log', '=', 'self', '.', 'gcs_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'except', 'Exception', 'as', 'e', ':', 'if', 'not', 'hasattr', '(', 'e', ',', ""'resp'"", ')', 'or', 'e', '.', 'resp', '.', 'get', '(', ""'status'"", ')', '!=', ""'404'"", ':', 'log', '=', ""'*** Previous log discarded: {}\\n\\n'"", '.', 'format', '(', 'str', '(', 'e', ')', ')', '+', 'log', 'try', ':', 'bkt', ',', 'blob', '=', 'self', '.', 'parse_gcs_url', '(', 'remote_log_location', ')', 'from', 'tempfile', 'import', 'NamedTemporaryFile', 'with', 'NamedTemporaryFile', '(', 'mode', '=', ""'w+'"", ')', 'as', 'tmpfile', ':', 'tmpfile', '.', 'write', '(', 'log', ')', ""# Force the file to be flushed, since we're doing the"", ""# upload from within the file context (it hasn't been"", '# closed).', 'tmpfile', '.', 'flush', '(', ')', 'self', '.', 'hook', '.', 'upload', '(', 'bkt', ',', 'blob', ',', 'tmpfile', '.', 'name', ')', 'except', 'Exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'error', '(', ""'Could not write logs to %s: %s'"", ',', 'remote_log_location', ',', 'e', ')']","Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool","['Writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'Fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'False', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'If', 'True', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",python,W,0,True,1,test
555,apache/airflow,airflow/hooks/hive_hooks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L858-L918,"def to_csv(
            self,
            hql,
            csv_filepath,
            schema='default',
            delimiter=',',
            lineterminator='\r\n',
            output_header=True,
            fetch_size=1000,
            hive_conf=None):
        """"""
        Execute hql in target schema and write results to a csv file.

        :param hql: hql to be executed.
        :type hql: str or list
        :param csv_filepath: filepath of csv to write results into.
        :type csv_filepath: str
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param delimiter: delimiter of the csv file, default to ','.
        :type delimiter: str
        :param lineterminator: lineterminator of the csv file.
        :type lineterminator: str
        :param output_header: header of the csv file, default to True.
        :type output_header: bool
        :param fetch_size: number of result rows to write into the csv file, default to 1000.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict

        """"""

        results_iter = self._get_results(hql, schema,
                                         fetch_size=fetch_size, hive_conf=hive_conf)
        header = next(results_iter)
        message = None

        i = 0
        with open(csv_filepath, 'wb') as f:
            writer = csv.writer(f,
                                delimiter=delimiter,
                                lineterminator=lineterminator,
                                encoding='utf-8')
            try:
                if output_header:
                    self.log.debug('Cursor description is %s', header)
                    writer.writerow([c[0] for c in header])

                for i, row in enumerate(results_iter, 1):
                    writer.writerow(row)
                    if i % fetch_size == 0:
                        self.log.info(""Written %s rows so far."", i)
            except ValueError as exception:
                message = str(exception)

        if message:
            # need to clean up the file first
            os.remove(csv_filepath)
            raise ValueError(message)

        self.log.info(""Done. Loaded a total of %s rows."", i)","['def', 'to_csv', '(', 'self', ',', 'hql', ',', 'csv_filepath', ',', 'schema', '=', ""'default'"", ',', 'delimiter', '=', ""','"", ',', 'lineterminator', '=', ""'\\r\\n'"", ',', 'output_header', '=', 'True', ',', 'fetch_size', '=', '1000', ',', 'hive_conf', '=', 'None', ')', ':', 'results_iter', '=', 'self', '.', '_get_results', '(', 'hql', ',', 'schema', ',', 'fetch_size', '=', 'fetch_size', ',', 'hive_conf', '=', 'hive_conf', ')', 'header', '=', 'next', '(', 'results_iter', ')', 'message', '=', 'None', 'i', '=', '0', 'with', 'open', '(', 'csv_filepath', ',', ""'wb'"", ')', 'as', 'f', ':', 'writer', '=', 'csv', '.', 'writer', '(', 'f', ',', 'delimiter', '=', 'delimiter', ',', 'lineterminator', '=', 'lineterminator', ',', 'encoding', '=', ""'utf-8'"", ')', 'try', ':', 'if', 'output_header', ':', 'self', '.', 'log', '.', 'debug', '(', ""'Cursor description is %s'"", ',', 'header', ')', 'writer', '.', 'writerow', '(', '[', 'c', '[', '0', ']', 'for', 'c', 'in', 'header', ']', ')', 'for', 'i', ',', 'row', 'in', 'enumerate', '(', 'results_iter', ',', '1', ')', ':', 'writer', '.', 'writerow', '(', 'row', ')', 'if', 'i', '%', 'fetch_size', '==', '0', ':', 'self', '.', 'log', '.', 'info', '(', '""Written %s rows so far.""', ',', 'i', ')', 'except', 'ValueError', 'as', 'exception', ':', 'message', '=', 'str', '(', 'exception', ')', 'if', 'message', ':', '# need to clean up the file first', 'os', '.', 'remove', '(', 'csv_filepath', ')', 'raise', 'ValueError', '(', 'message', ')', 'self', '.', 'log', '.', 'info', '(', '""Done. Loaded a total of %s rows.""', ',', 'i', ')']","Execute hql in target schema and write results to a csv file.

        :param hql: hql to be executed.
        :type hql: str or list
        :param csv_filepath: filepath of csv to write results into.
        :type csv_filepath: str
        :param schema: target schema, default to 'default'.
        :type schema: str
        :param delimiter: delimiter of the csv file, default to ','.
        :type delimiter: str
        :param lineterminator: lineterminator of the csv file.
        :type lineterminator: str
        :param output_header: header of the csv file, default to True.
        :type output_header: bool
        :param fetch_size: number of result rows to write into the csv file, default to 1000.
        :type fetch_size: int
        :param hive_conf: hive_conf to execute alone with the hql.
        :type hive_conf: dict","['Execute', 'hql', 'in', 'target', 'schema', 'and', 'write', 'results', 'to', 'a', 'csv', 'file', '.']",python,W,0,True,1,test
701,apache/airflow,airflow/utils/log/s3_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/s3_task_handler.py#L146-L170,"def s3_write(self, log, remote_log_location, append=True):
        """"""
        Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool
        """"""
        if append and self.s3_log_exists(remote_log_location):
            old_log = self.s3_read(remote_log_location)
            log = '\n'.join([old_log, log]) if old_log else log

        try:
            self.hook.load_string(
                log,
                key=remote_log_location,
                replace=True,
                encrypt=configuration.conf.getboolean('core', 'ENCRYPT_S3_LOGS'),
            )
        except Exception:
            self.log.exception('Could not write logs to %s', remote_log_location)","['def', 's3_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'True', ')', ':', 'if', 'append', 'and', 'self', '.', 's3_log_exists', '(', 'remote_log_location', ')', ':', 'old_log', '=', 'self', '.', 's3_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'try', ':', 'self', '.', 'hook', '.', 'load_string', '(', 'log', ',', 'key', '=', 'remote_log_location', ',', 'replace', '=', 'True', ',', 'encrypt', '=', 'configuration', '.', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'ENCRYPT_S3_LOGS'"", ')', ',', ')', 'except', 'Exception', ':', 'self', '.', 'log', '.', 'exception', '(', ""'Could not write logs to %s'"", ',', 'remote_log_location', ')']","Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool","['Writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'Fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'False', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'If', 'True', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",python,W,0,True,1,test
2262,h2oai/h2o-3,scripts/logscrapedaily.py,https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/scripts/logscrapedaily.py#L885-L916,"def write_java_message(key,val,text_file):
    """"""
    Loop through all java messages that are not associated with a unit test and
    write them into a log file.

    Parameters
    ----------
    key :  str
        9.general_bad_java_messages
    val : list of list of str
        contains the bad java messages and the message types.


    :return: none
    """"""

    text_file.write(key)
    text_file.write('\n')

    if (len(val[0]) > 0) and (len(val) >= 3):
        for index in range(len(val[0])):
            text_file.write(""Java Message Type: "")
            text_file.write(val[1][index])
            text_file.write('\n')

            text_file.write(""Java Message: "")

            for jmess in val[2][index]:
                text_file.write(jmess)
                text_file.write('\n')

        text_file.write('\n \n')","['def', 'write_java_message', '(', 'key', ',', 'val', ',', 'text_file', ')', ':', 'text_file', '.', 'write', '(', 'key', ')', 'text_file', '.', 'write', '(', ""'\\n'"", ')', 'if', '(', 'len', '(', 'val', '[', '0', ']', ')', '>', '0', ')', 'and', '(', 'len', '(', 'val', ')', '>=', '3', ')', ':', 'for', 'index', 'in', 'range', '(', 'len', '(', 'val', '[', '0', ']', ')', ')', ':', 'text_file', '.', 'write', '(', '""Java Message Type: ""', ')', 'text_file', '.', 'write', '(', 'val', '[', '1', ']', '[', 'index', ']', ')', 'text_file', '.', 'write', '(', ""'\\n'"", ')', 'text_file', '.', 'write', '(', '""Java Message: ""', ')', 'for', 'jmess', 'in', 'val', '[', '2', ']', '[', 'index', ']', ':', 'text_file', '.', 'write', '(', 'jmess', ')', 'text_file', '.', 'write', '(', ""'\\n'"", ')', 'text_file', '.', 'write', '(', ""'\\n \\n'"", ')']","Loop through all java messages that are not associated with a unit test and
    write them into a log file.

    Parameters
    ----------
    key :  str
        9.general_bad_java_messages
    val : list of list of str
        contains the bad java messages and the message types.


    :return: none","['Loop', 'through', 'all', 'java', 'messages', 'that', 'are', 'not', 'associated', 'with', 'a', 'unit', 'test', 'and', 'write', 'them', 'into', 'a', 'log', 'file', '.']",python,W,0,True,1,test
2950,streamlink/streamlink,src/streamlink_cli/main.py,https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/main.py#L301-L333,"def output_stream(plugin, stream):
    """"""Open stream, create output and finally write the stream to output.""""""
    global output

    success_open = False
    for i in range(args.retry_open):
        try:
            stream_fd, prebuffer = open_stream(stream)
            success_open = True
            break
        except StreamError as err:
            log.error(""Try {0}/{1}: Could not open stream {2} ({3})"", i + 1, args.retry_open, stream, err)

    if not success_open:
        console.exit(""Could not open stream {0}, tried {1} times, exiting"", stream, args.retry_open)

    output = create_output(plugin)

    try:
        output.open()
    except (IOError, OSError) as err:
        if isinstance(output, PlayerOutput):
            console.exit(""Failed to start player: {0} ({1})"",
                         args.player, err)
        else:
            console.exit(""Failed to open output: {0} ({1})"",
                         args.output, err)

    with closing(output):
        log.debug(""Writing stream to output"")
        read_stream(stream_fd, output, prebuffer)

    return True","['def', 'output_stream', '(', 'plugin', ',', 'stream', ')', ':', 'global', 'output', 'success_open', '=', 'False', 'for', 'i', 'in', 'range', '(', 'args', '.', 'retry_open', ')', ':', 'try', ':', 'stream_fd', ',', 'prebuffer', '=', 'open_stream', '(', 'stream', ')', 'success_open', '=', 'True', 'break', 'except', 'StreamError', 'as', 'err', ':', 'log', '.', 'error', '(', '""Try {0}/{1}: Could not open stream {2} ({3})""', ',', 'i', '+', '1', ',', 'args', '.', 'retry_open', ',', 'stream', ',', 'err', ')', 'if', 'not', 'success_open', ':', 'console', '.', 'exit', '(', '""Could not open stream {0}, tried {1} times, exiting""', ',', 'stream', ',', 'args', '.', 'retry_open', ')', 'output', '=', 'create_output', '(', 'plugin', ')', 'try', ':', 'output', '.', 'open', '(', ')', 'except', '(', 'IOError', ',', 'OSError', ')', 'as', 'err', ':', 'if', 'isinstance', '(', 'output', ',', 'PlayerOutput', ')', ':', 'console', '.', 'exit', '(', '""Failed to start player: {0} ({1})""', ',', 'args', '.', 'player', ',', 'err', ')', 'else', ':', 'console', '.', 'exit', '(', '""Failed to open output: {0} ({1})""', ',', 'args', '.', 'output', ',', 'err', ')', 'with', 'closing', '(', 'output', ')', ':', 'log', '.', 'debug', '(', '""Writing stream to output""', ')', 'read_stream', '(', 'stream_fd', ',', 'output', ',', 'prebuffer', ')', 'return', 'True']","Open stream, create output and finally write the stream to output.","['Open', 'stream', 'create', 'output', 'and', 'finally', 'write', 'the', 'stream', 'to', 'output', '.']",python,W,0,True,1,test
4302,Qiskit/qiskit-terra,qiskit/transpiler/passes/commutation_analysis.py,https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/transpiler/passes/commutation_analysis.py#L38-L76,"def run(self, dag):
        """"""
        Run the pass on the DAG, and write the discovered commutation relations
        into the property_set.
        """"""
        # Initiate the commutation set
        self.property_set['commutation_set'] = defaultdict(list)

        # Build a dictionary to keep track of the gates on each qubit
        for wire in dag.wires:
            wire_name = ""{0}[{1}]"".format(str(wire[0].name), str(wire[1]))
            self.property_set['commutation_set'][wire_name] = []

        # Add edges to the dictionary for each qubit
        for node in dag.topological_op_nodes():
            for (_, _, edge_data) in dag.edges(node):

                edge_name = edge_data['name']
                self.property_set['commutation_set'][(node, edge_name)] = -1

        for wire in dag.wires:
            wire_name = ""{0}[{1}]"".format(str(wire[0].name), str(wire[1]))

            for current_gate in dag.nodes_on_wire(wire):

                current_comm_set = self.property_set['commutation_set'][wire_name]
                if not current_comm_set:
                    current_comm_set.append([current_gate])

                if current_gate not in current_comm_set[-1]:
                    prev_gate = current_comm_set[-1][-1]
                    if _commute(current_gate, prev_gate):
                        current_comm_set[-1].append(current_gate)

                    else:
                        current_comm_set.append([current_gate])

                temp_len = len(current_comm_set)
                self.property_set['commutation_set'][(current_gate, wire_name)] = temp_len - 1","['def', 'run', '(', 'self', ',', 'dag', ')', ':', '# Initiate the commutation set', 'self', '.', 'property_set', '[', ""'commutation_set'"", ']', '=', 'defaultdict', '(', 'list', ')', '# Build a dictionary to keep track of the gates on each qubit', 'for', 'wire', 'in', 'dag', '.', 'wires', ':', 'wire_name', '=', '""{0}[{1}]""', '.', 'format', '(', 'str', '(', 'wire', '[', '0', ']', '.', 'name', ')', ',', 'str', '(', 'wire', '[', '1', ']', ')', ')', 'self', '.', 'property_set', '[', ""'commutation_set'"", ']', '[', 'wire_name', ']', '=', '[', ']', '# Add edges to the dictionary for each qubit', 'for', 'node', 'in', 'dag', '.', 'topological_op_nodes', '(', ')', ':', 'for', '(', '_', ',', '_', ',', 'edge_data', ')', 'in', 'dag', '.', 'edges', '(', 'node', ')', ':', 'edge_name', '=', 'edge_data', '[', ""'name'"", ']', 'self', '.', 'property_set', '[', ""'commutation_set'"", ']', '[', '(', 'node', ',', 'edge_name', ')', ']', '=', '-', '1', 'for', 'wire', 'in', 'dag', '.', 'wires', ':', 'wire_name', '=', '""{0}[{1}]""', '.', 'format', '(', 'str', '(', 'wire', '[', '0', ']', '.', 'name', ')', ',', 'str', '(', 'wire', '[', '1', ']', ')', ')', 'for', 'current_gate', 'in', 'dag', '.', 'nodes_on_wire', '(', 'wire', ')', ':', 'current_comm_set', '=', 'self', '.', 'property_set', '[', ""'commutation_set'"", ']', '[', 'wire_name', ']', 'if', 'not', 'current_comm_set', ':', 'current_comm_set', '.', 'append', '(', '[', 'current_gate', ']', ')', 'if', 'current_gate', 'not', 'in', 'current_comm_set', '[', '-', '1', ']', ':', 'prev_gate', '=', 'current_comm_set', '[', '-', '1', ']', '[', '-', '1', ']', 'if', '_commute', '(', 'current_gate', ',', 'prev_gate', ')', ':', 'current_comm_set', '[', '-', '1', ']', '.', 'append', '(', 'current_gate', ')', 'else', ':', 'current_comm_set', '.', 'append', '(', '[', 'current_gate', ']', ')', 'temp_len', '=', 'len', '(', 'current_comm_set', ')', 'self', '.', 'property_set', '[', ""'commutation_set'"", ']', '[', '(', 'current_gate', ',', 'wire_name', ')', ']', '=', 'temp_len', '-', '1']","Run the pass on the DAG, and write the discovered commutation relations
        into the property_set.","['Run', 'the', 'pass', 'on', 'the', 'DAG', 'and', 'write', 'the', 'discovered', 'commutation', 'relations', 'into', 'the', 'property_set', '.']",python,W,0,True,1,test
5475,PyCQA/pylint,pylint/checkers/imports.py,https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/imports.py#L911-L928,"def _report_dependencies_graph(self, sect, _, _dummy):
        """"""write dependencies as a dot (graphviz) file""""""
        dep_info = self.stats[""dependencies""]
        if not dep_info or not (
            self.config.import_graph
            or self.config.ext_import_graph
            or self.config.int_import_graph
        ):
            raise EmptyReportError()
        filename = self.config.import_graph
        if filename:
            _make_graph(filename, dep_info, sect, """")
        filename = self.config.ext_import_graph
        if filename:
            _make_graph(filename, self._external_dependencies_info(), sect, ""external "")
        filename = self.config.int_import_graph
        if filename:
            _make_graph(filename, self._internal_dependencies_info(), sect, ""internal "")","['def', '_report_dependencies_graph', '(', 'self', ',', 'sect', ',', '_', ',', '_dummy', ')', ':', 'dep_info', '=', 'self', '.', 'stats', '[', '""dependencies""', ']', 'if', 'not', 'dep_info', 'or', 'not', '(', 'self', '.', 'config', '.', 'import_graph', 'or', 'self', '.', 'config', '.', 'ext_import_graph', 'or', 'self', '.', 'config', '.', 'int_import_graph', ')', ':', 'raise', 'EmptyReportError', '(', ')', 'filename', '=', 'self', '.', 'config', '.', 'import_graph', 'if', 'filename', ':', '_make_graph', '(', 'filename', ',', 'dep_info', ',', 'sect', ',', '""""', ')', 'filename', '=', 'self', '.', 'config', '.', 'ext_import_graph', 'if', 'filename', ':', '_make_graph', '(', 'filename', ',', 'self', '.', '_external_dependencies_info', '(', ')', ',', 'sect', ',', '""external ""', ')', 'filename', '=', 'self', '.', 'config', '.', 'int_import_graph', 'if', 'filename', ':', '_make_graph', '(', 'filename', ',', 'self', '.', '_internal_dependencies_info', '(', ')', ',', 'sect', ',', '""internal ""', ')']",write dependencies as a dot (graphviz) file,"['write', 'dependencies', 'as', 'a', 'dot', '(', 'graphviz', ')', 'file']",python,W,0,True,1,test
5618,PyCQA/pylint,pylint/reporters/ureports/__init__.py,https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/reporters/ureports/__init__.py#L22-L37,"def format(self, layout, stream=None, encoding=None):
        """"""format and write the given layout into the stream object

        unicode policy: unicode strings may be found in the layout;
        try to call stream.write with it, but give it back encoded using
        the given encoding if it fails
        """"""
        if stream is None:
            stream = sys.stdout
        if not encoding:
            encoding = getattr(stream, ""encoding"", ""UTF-8"")
        self.encoding = encoding or ""UTF-8""
        self.out = stream
        self.begin_format()
        layout.accept(self)
        self.end_format()","['def', 'format', '(', 'self', ',', 'layout', ',', 'stream', '=', 'None', ',', 'encoding', '=', 'None', ')', ':', 'if', 'stream', 'is', 'None', ':', 'stream', '=', 'sys', '.', 'stdout', 'if', 'not', 'encoding', ':', 'encoding', '=', 'getattr', '(', 'stream', ',', '""encoding""', ',', '""UTF-8""', ')', 'self', '.', 'encoding', '=', 'encoding', 'or', '""UTF-8""', 'self', '.', 'out', '=', 'stream', 'self', '.', 'begin_format', '(', ')', 'layout', '.', 'accept', '(', 'self', ')', 'self', '.', 'end_format', '(', ')']","format and write the given layout into the stream object

        unicode policy: unicode strings may be found in the layout;
        try to call stream.write with it, but give it back encoded using
        the given encoding if it fails","['format', 'and', 'write', 'the', 'given', 'layout', 'into', 'the', 'stream', 'object']",python,W,0,True,1,test
5752,PyCQA/pylint,pylint/pyreverse/writer.py,https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/pyreverse/writer.py#L30-L41,"def write(self, diadefs):
        """"""write files for <project> according to <diadefs>
        """"""
        for diagram in diadefs:
            basename = diagram.title.strip().replace("" "", ""_"")
            file_name = ""%s.%s"" % (basename, self.config.output_format)
            self.set_printer(file_name, basename)
            if diagram.TYPE == ""class"":
                self.write_classes(diagram)
            else:
                self.write_packages(diagram)
            self.close_graph()","['def', 'write', '(', 'self', ',', 'diadefs', ')', ':', 'for', 'diagram', 'in', 'diadefs', ':', 'basename', '=', 'diagram', '.', 'title', '.', 'strip', '(', ')', '.', 'replace', '(', '"" ""', ',', '""_""', ')', 'file_name', '=', '""%s.%s""', '%', '(', 'basename', ',', 'self', '.', 'config', '.', 'output_format', ')', 'self', '.', 'set_printer', '(', 'file_name', ',', 'basename', ')', 'if', 'diagram', '.', 'TYPE', '==', '""class""', ':', 'self', '.', 'write_classes', '(', 'diagram', ')', 'else', ':', 'self', '.', 'write_packages', '(', 'diagram', ')', 'self', '.', 'close_graph', '(', ')']",write files for <project> according to <diadefs>,"['write', 'files', 'for', '<project', '>', 'according', 'to', '<diadefs', '>']",python,W,0,True,1,test
5819,PyCQA/pylint,pylint/config.py,https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/config.py#L647-L678,"def generate_config(self, stream=None, skipsections=(), encoding=None):
        """"""write a configuration file according to the current configuration
        into the given stream or stdout
        """"""
        options_by_section = {}
        sections = []
        for provider in self.options_providers:
            for section, options in provider.options_by_section():
                if section is None:
                    section = provider.name
                if section in skipsections:
                    continue
                options = [
                    (n, d, v)
                    for (n, d, v) in options
                    if d.get(""type"") is not None and not d.get(""deprecated"")
                ]
                if not options:
                    continue
                if section not in sections:
                    sections.append(section)
                alloptions = options_by_section.setdefault(section, [])
                alloptions += options
        stream = stream or sys.stdout
        printed = False
        for section in sections:
            if printed:
                print(""\n"", file=stream)
            utils.format_section(
                stream, section.upper(), sorted(options_by_section[section])
            )
            printed = True","['def', 'generate_config', '(', 'self', ',', 'stream', '=', 'None', ',', 'skipsections', '=', '(', ')', ',', 'encoding', '=', 'None', ')', ':', 'options_by_section', '=', '{', '}', 'sections', '=', '[', ']', 'for', 'provider', 'in', 'self', '.', 'options_providers', ':', 'for', 'section', ',', 'options', 'in', 'provider', '.', 'options_by_section', '(', ')', ':', 'if', 'section', 'is', 'None', ':', 'section', '=', 'provider', '.', 'name', 'if', 'section', 'in', 'skipsections', ':', 'continue', 'options', '=', '[', '(', 'n', ',', 'd', ',', 'v', ')', 'for', '(', 'n', ',', 'd', ',', 'v', ')', 'in', 'options', 'if', 'd', '.', 'get', '(', '""type""', ')', 'is', 'not', 'None', 'and', 'not', 'd', '.', 'get', '(', '""deprecated""', ')', ']', 'if', 'not', 'options', ':', 'continue', 'if', 'section', 'not', 'in', 'sections', ':', 'sections', '.', 'append', '(', 'section', ')', 'alloptions', '=', 'options_by_section', '.', 'setdefault', '(', 'section', ',', '[', ']', ')', 'alloptions', '+=', 'options', 'stream', '=', 'stream', 'or', 'sys', '.', 'stdout', 'printed', '=', 'False', 'for', 'section', 'in', 'sections', ':', 'if', 'printed', ':', 'print', '(', '""\\n""', ',', 'file', '=', 'stream', ')', 'utils', '.', 'format_section', '(', 'stream', ',', 'section', '.', 'upper', '(', ')', ',', 'sorted', '(', 'options_by_section', '[', 'section', ']', ')', ')', 'printed', '=', 'True']","write a configuration file according to the current configuration
        into the given stream or stdout","['write', 'a', 'configuration', 'file', 'according', 'to', 'the', 'current', 'configuration', 'into', 'the', 'given', 'stream', 'or', 'stdout']",python,W,0,True,1,test
6358,3DLIRIOUS/MeshLabXML,meshlabxml/mlx.py,https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/mlx.py#L623-L691,"def begin(script='TEMP3D_default.mlx', file_in=None, mlp_in=None):
    """"""Create new mlx script and write opening tags.

    Performs special processing on stl files.

    If no input files are provided this will create a dummy
    file and delete it as the first filter. This works around
    the meshlab limitation that it must be provided an input
    file, even if you will be creating a mesh as the first
    filter.

    """"""
    script_file = open(script, 'w')
    script_file.write(''.join(['<!DOCTYPE FilterScript>\n',
                               '<FilterScript>\n']))
    script_file.close()

    current_layer = -1
    last_layer = -1
    stl = False

    # Process project files first
    if mlp_in is not None:
        # make a list if it isn't already
        if not isinstance(mlp_in, list):
            mlp_in = [mlp_in]
        for val in mlp_in:
            tree = ET.parse(val)
            #root = tree.getroot()
            for elem in tree.iter(tag='MLMesh'):
                filename = (elem.attrib['filename'])
                current_layer += 1
                last_layer += 1
                # If the mesh file extension is stl, change to that layer and
                # run clean.merge_vert
                if os.path.splitext(filename)[1][1:].strip().lower() == 'stl':
                    layers.change(script, current_layer)
                    clean.merge_vert(script)
                    stl = True

    # Process separate input files next
    if file_in is not None:
        # make a list if it isn't already
        if not isinstance(file_in, list):
            file_in = [file_in]
        for val in file_in:
            current_layer += 1
            last_layer += 1
            # If the mesh file extension is stl, change to that layer and
            # run clean.merge_vert
            if os.path.splitext(val)[1][1:].strip().lower() == 'stl':
                layers.change(script, current_layer)
                clean.merge_vert(script)
                stl = True

    # If some input files were stl, we need to change back to the last layer
    if stl:
        layers.change(script, last_layer)  # Change back to the last layer
    elif last_layer == -1:
        # If no input files are provided, create a dummy file
        # with a single vertex and delete it first in the script.
        # This works around the fact that meshlabserver will
        # not run without an input file.
        file_in = ['TEMP3D.xyz']
        file_in_descriptor = open(file_in[0], 'w')
        file_in_descriptor.write('0 0 0')
        file_in_descriptor.close()
        layers.delete(script)
    return current_layer, last_layer","['def', 'begin', '(', 'script', '=', ""'TEMP3D_default.mlx'"", ',', 'file_in', '=', 'None', ',', 'mlp_in', '=', 'None', ')', ':', 'script_file', '=', 'open', '(', 'script', ',', ""'w'"", ')', 'script_file', '.', 'write', '(', ""''"", '.', 'join', '(', '[', ""'<!DOCTYPE FilterScript>\\n'"", ',', ""'<FilterScript>\\n'"", ']', ')', ')', 'script_file', '.', 'close', '(', ')', 'current_layer', '=', '-', '1', 'last_layer', '=', '-', '1', 'stl', '=', 'False', '# Process project files first', 'if', 'mlp_in', 'is', 'not', 'None', ':', ""# make a list if it isn't already"", 'if', 'not', 'isinstance', '(', 'mlp_in', ',', 'list', ')', ':', 'mlp_in', '=', '[', 'mlp_in', ']', 'for', 'val', 'in', 'mlp_in', ':', 'tree', '=', 'ET', '.', 'parse', '(', 'val', ')', '#root = tree.getroot()', 'for', 'elem', 'in', 'tree', '.', 'iter', '(', 'tag', '=', ""'MLMesh'"", ')', ':', 'filename', '=', '(', 'elem', '.', 'attrib', '[', ""'filename'"", ']', ')', 'current_layer', '+=', '1', 'last_layer', '+=', '1', '# If the mesh file extension is stl, change to that layer and', '# run clean.merge_vert', 'if', 'os', '.', 'path', '.', 'splitext', '(', 'filename', ')', '[', '1', ']', '[', '1', ':', ']', '.', 'strip', '(', ')', '.', 'lower', '(', ')', '==', ""'stl'"", ':', 'layers', '.', 'change', '(', 'script', ',', 'current_layer', ')', 'clean', '.', 'merge_vert', '(', 'script', ')', 'stl', '=', 'True', '# Process separate input files next', 'if', 'file_in', 'is', 'not', 'None', ':', ""# make a list if it isn't already"", 'if', 'not', 'isinstance', '(', 'file_in', ',', 'list', ')', ':', 'file_in', '=', '[', 'file_in', ']', 'for', 'val', 'in', 'file_in', ':', 'current_layer', '+=', '1', 'last_layer', '+=', '1', '# If the mesh file extension is stl, change to that layer and', '# run clean.merge_vert', 'if', 'os', '.', 'path', '.', 'splitext', '(', 'val', ')', '[', '1', ']', '[', '1', ':', ']', '.', 'strip', '(', ')', '.', 'lower', '(', ')', '==', ""'stl'"", ':', 'layers', '.', 'change', '(', 'script', ',', 'current_layer', ')', 'clean', '.', 'merge_vert', '(', 'script', ')', 'stl', '=', 'True', '# If some input files were stl, we need to change back to the last layer', 'if', 'stl', ':', 'layers', '.', 'change', '(', 'script', ',', 'last_layer', ')', '# Change back to the last layer', 'elif', 'last_layer', '==', '-', '1', ':', '# If no input files are provided, create a dummy file', '# with a single vertex and delete it first in the script.', '# This works around the fact that meshlabserver will', '# not run without an input file.', 'file_in', '=', '[', ""'TEMP3D.xyz'"", ']', 'file_in_descriptor', '=', 'open', '(', 'file_in', '[', '0', ']', ',', ""'w'"", ')', 'file_in_descriptor', '.', 'write', '(', ""'0 0 0'"", ')', 'file_in_descriptor', '.', 'close', '(', ')', 'layers', '.', 'delete', '(', 'script', ')', 'return', 'current_layer', ',', 'last_layer']","Create new mlx script and write opening tags.

    Performs special processing on stl files.

    If no input files are provided this will create a dummy
    file and delete it as the first filter. This works around
    the meshlab limitation that it must be provided an input
    file, even if you will be creating a mesh as the first
    filter.","['Create', 'new', 'mlx', 'script', 'and', 'write', 'opening', 'tags', '.']",python,W,0,True,1,test
394,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L156-L180,"def _write_local_data_files(self, cursor):
        """"""
        Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.
        """"""
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}
        for row in cursor:
            row_dict = self.generate_data_dict(row._fields, row)
            s = json.dumps(row_dict).encode('utf-8')
            tmp_file_handle.write(s)

            # Append newline to make dumps BigQuery compatible.
            tmp_file_handle.write(b'\n')

            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle

        return tmp_file_handles","['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'file_no', '=', '0', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '=', '{', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ':', 'tmp_file_handle', '}', 'for', 'row', 'in', 'cursor', ':', 'row_dict', '=', 'self', '.', 'generate_data_dict', '(', 'row', '.', '_fields', ',', 'row', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# Append newline to make dumps BigQuery compatible.', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '[', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ']', '=', 'tmp_file_handle', 'return', 'tmp_file_handles']","Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.","['Takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",python,W,0,True,1,test
395,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L182-L199,"def _write_local_schema_file(self, cursor):
        """"""
        Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.
        """"""
        schema = []
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)

        for name, type in zip(cursor.column_names, cursor.column_types):
            schema.append(self.generate_schema_dict(name, type))
        json_serialized_schema = json.dumps(schema).encode('utf-8')

        tmp_schema_file_handle.write(json_serialized_schema)
        return {self.schema_filename: tmp_schema_file_handle}","['def', '_write_local_schema_file', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', '[', ']', 'tmp_schema_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'for', 'name', ',', 'type', 'in', 'zip', '(', 'cursor', '.', 'column_names', ',', 'cursor', '.', 'column_types', ')', ':', 'schema', '.', 'append', '(', 'self', '.', 'generate_schema_dict', '(', 'name', ',', 'type', ')', ')', 'json_serialized_schema', '=', 'json', '.', 'dumps', '(', 'schema', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_schema_file_handle', '.', 'write', '(', 'json_serialized_schema', ')', 'return', '{', 'self', '.', 'schema_filename', ':', 'tmp_schema_file_handle', '}']","Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.","['Takes', 'a', 'cursor', 'and', 'writes', 'the', 'BigQuery', 'schema', 'for', 'the', 'results', 'to', 'a', 'local', 'file', 'system', '.']",python,W,0,True,1,test
419,apache/airflow,airflow/contrib/operators/mssql_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mssql_to_gcs.py#L139-L170,"def _write_local_data_files(self, cursor):
        """"""
        Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.
        """"""
        schema = list(map(lambda schema_tuple: schema_tuple[0].replace(' ', '_'), cursor.description))
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}

        for row in cursor:
            # Convert if needed
            row = map(self.convert_types, row)
            row_dict = dict(zip(schema, row))

            s = json.dumps(row_dict, sort_keys=True)
            s = s.encode('utf-8')
            tmp_file_handle.write(s)

            # Append newline to make dumps BQ compatible
            tmp_file_handle.write(b'\n')

            # Stop if the file exceeds the file size limit
            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle

        return tmp_file_handles","['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', 'list', '(', 'map', '(', 'lambda', 'schema_tuple', ':', 'schema_tuple', '[', '0', ']', '.', 'replace', '(', ""' '"", ',', ""'_'"", ')', ',', 'cursor', '.', 'description', ')', ')', 'file_no', '=', '0', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '=', '{', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ':', 'tmp_file_handle', '}', 'for', 'row', 'in', 'cursor', ':', '# Convert if needed', 'row', '=', 'map', '(', 'self', '.', 'convert_types', ',', 'row', ')', 'row_dict', '=', 'dict', '(', 'zip', '(', 'schema', ',', 'row', ')', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ',', 'sort_keys', '=', 'True', ')', 's', '=', 's', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# Append newline to make dumps BQ compatible', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', '# Stop if the file exceeds the file size limit', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '[', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ']', '=', 'tmp_file_handle', 'return', 'tmp_file_handles']","Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.","['Takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",python,W,0,True,1,test
611,apache/airflow,airflow/contrib/operators/postgres_to_gcs_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/postgres_to_gcs_operator.py#L124-L164,"def _write_local_data_files(self, cursor):
        """"""
        Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.
        """"""
        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))
        tmp_file_handles = {}
        row_no = 0

        def _create_new_file():
            handle = NamedTemporaryFile(delete=True)
            filename = self.filename.format(len(tmp_file_handles))
            tmp_file_handles[filename] = handle
            return handle

        # Don't create a file if there is nothing to write
        if cursor.rowcount > 0:
            tmp_file_handle = _create_new_file()

            for row in cursor:
                # Convert datetime objects to utc seconds, and decimals to floats
                row = map(self.convert_types, row)
                row_dict = dict(zip(schema, row))

                s = json.dumps(row_dict, sort_keys=True).encode('utf-8')
                tmp_file_handle.write(s)

                # Append newline to make dumps BigQuery compatible.
                tmp_file_handle.write(b'\n')

                # Stop if the file exceeds the file size limit.
                if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                    tmp_file_handle = _create_new_file()
                row_no += 1

        self.log.info('Received %s rows over %s files', row_no, len(tmp_file_handles))

        return tmp_file_handles","['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', 'list', '(', 'map', '(', 'lambda', 'schema_tuple', ':', 'schema_tuple', '[', '0', ']', ',', 'cursor', '.', 'description', ')', ')', 'tmp_file_handles', '=', '{', '}', 'row_no', '=', '0', 'def', '_create_new_file', '(', ')', ':', 'handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'filename', '=', 'self', '.', 'filename', '.', 'format', '(', 'len', '(', 'tmp_file_handles', ')', ')', 'tmp_file_handles', '[', 'filename', ']', '=', 'handle', 'return', 'handle', ""# Don't create a file if there is nothing to write"", 'if', 'cursor', '.', 'rowcount', '>', '0', ':', 'tmp_file_handle', '=', '_create_new_file', '(', ')', 'for', 'row', 'in', 'cursor', ':', '# Convert datetime objects to utc seconds, and decimals to floats', 'row', '=', 'map', '(', 'self', '.', 'convert_types', ',', 'row', ')', 'row_dict', '=', 'dict', '(', 'zip', '(', 'schema', ',', 'row', ')', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ',', 'sort_keys', '=', 'True', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# Append newline to make dumps BigQuery compatible.', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', '# Stop if the file exceeds the file size limit.', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'tmp_file_handle', '=', '_create_new_file', '(', ')', 'row_no', '+=', '1', 'self', '.', 'log', '.', 'info', '(', ""'Received %s rows over %s files'"", ',', 'row_no', ',', 'len', '(', 'tmp_file_handles', ')', ')', 'return', 'tmp_file_handles']","Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.","['Takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",python,W,0,True,1,test
612,apache/airflow,airflow/contrib/operators/postgres_to_gcs_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/postgres_to_gcs_operator.py#L166-L192,"def _write_local_schema_file(self, cursor):
        """"""
        Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.
        """"""
        schema = []
        for field in cursor.description:
            # See PEP 249 for details about the description tuple.
            field_name = field[0]
            field_type = self.type_map(field[1])
            field_mode = 'REPEATED' if field[1] in (1009, 1005, 1007,
                                                    1016) else 'NULLABLE'
            schema.append({
                'name': field_name,
                'type': field_type,
                'mode': field_mode,
            })

        self.log.info('Using schema for %s: %s', self.schema_filename, schema)
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)
        s = json.dumps(schema, sort_keys=True).encode('utf-8')
        tmp_schema_file_handle.write(s)
        return {self.schema_filename: tmp_schema_file_handle}","['def', '_write_local_schema_file', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', '[', ']', 'for', 'field', 'in', 'cursor', '.', 'description', ':', '# See PEP 249 for details about the description tuple.', 'field_name', '=', 'field', '[', '0', ']', 'field_type', '=', 'self', '.', 'type_map', '(', 'field', '[', '1', ']', ')', 'field_mode', '=', ""'REPEATED'"", 'if', 'field', '[', '1', ']', 'in', '(', '1009', ',', '1005', ',', '1007', ',', '1016', ')', 'else', ""'NULLABLE'"", 'schema', '.', 'append', '(', '{', ""'name'"", ':', 'field_name', ',', ""'type'"", ':', 'field_type', ',', ""'mode'"", ':', 'field_mode', ',', '}', ')', 'self', '.', 'log', '.', 'info', '(', ""'Using schema for %s: %s'"", ',', 'self', '.', 'schema_filename', ',', 'schema', ')', 'tmp_schema_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 's', '=', 'json', '.', 'dumps', '(', 'schema', ',', 'sort_keys', '=', 'True', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_schema_file_handle', '.', 'write', '(', 's', ')', 'return', '{', 'self', '.', 'schema_filename', ':', 'tmp_schema_file_handle', '}']","Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.","['Takes', 'a', 'cursor', 'and', 'writes', 'the', 'BigQuery', 'schema', 'for', 'the', 'results', 'to', 'a', 'local', 'file', 'system', '.']",python,W,0,True,1,test
768,apache/airflow,airflow/contrib/operators/mysql_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L144-L199,"def _write_local_data_files(self, cursor):
        """"""
        Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.
        """"""
        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))
        col_type_dict = self._get_col_type_dict()
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        if self.export_format == 'csv':
            file_mime_type = 'text/csv'
        else:
            file_mime_type = 'application/json'
        files_to_upload = [{
            'file_name': self.filename.format(file_no),
            'file_handle': tmp_file_handle,
            'file_mime_type': file_mime_type
        }]

        if self.export_format == 'csv':
            csv_writer = self._configure_csv_file(tmp_file_handle, schema)

        for row in cursor:
            # Convert datetime objects to utc seconds, and decimals to floats.
            # Convert binary type object to string encoded with base64.
            row = self._convert_types(schema, col_type_dict, row)

            if self.export_format == 'csv':
                csv_writer.writerow(row)
            else:
                row_dict = dict(zip(schema, row))

                # TODO validate that row isn't > 2MB. BQ enforces a hard row size of 2MB.
                s = json.dumps(row_dict, sort_keys=True).encode('utf-8')
                tmp_file_handle.write(s)

                # Append newline to make dumps BigQuery compatible.
                tmp_file_handle.write(b'\n')

            # Stop if the file exceeds the file size limit.
            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                files_to_upload.append({
                    'file_name': self.filename.format(file_no),
                    'file_handle': tmp_file_handle,
                    'file_mime_type': file_mime_type
                })

                if self.export_format == 'csv':
                    csv_writer = self._configure_csv_file(tmp_file_handle, schema)

        return files_to_upload","['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', 'list', '(', 'map', '(', 'lambda', 'schema_tuple', ':', 'schema_tuple', '[', '0', ']', ',', 'cursor', '.', 'description', ')', ')', 'col_type_dict', '=', 'self', '.', '_get_col_type_dict', '(', ')', 'file_no', '=', '0', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'if', 'self', '.', 'export_format', '==', ""'csv'"", ':', 'file_mime_type', '=', ""'text/csv'"", 'else', ':', 'file_mime_type', '=', ""'application/json'"", 'files_to_upload', '=', '[', '{', ""'file_name'"", ':', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ',', ""'file_handle'"", ':', 'tmp_file_handle', ',', ""'file_mime_type'"", ':', 'file_mime_type', '}', ']', 'if', 'self', '.', 'export_format', '==', ""'csv'"", ':', 'csv_writer', '=', 'self', '.', '_configure_csv_file', '(', 'tmp_file_handle', ',', 'schema', ')', 'for', 'row', 'in', 'cursor', ':', '# Convert datetime objects to utc seconds, and decimals to floats.', '# Convert binary type object to string encoded with base64.', 'row', '=', 'self', '.', '_convert_types', '(', 'schema', ',', 'col_type_dict', ',', 'row', ')', 'if', 'self', '.', 'export_format', '==', ""'csv'"", ':', 'csv_writer', '.', 'writerow', '(', 'row', ')', 'else', ':', 'row_dict', '=', 'dict', '(', 'zip', '(', 'schema', ',', 'row', ')', ')', ""# TODO validate that row isn't > 2MB. BQ enforces a hard row size of 2MB."", 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ',', 'sort_keys', '=', 'True', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# Append newline to make dumps BigQuery compatible.', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', '# Stop if the file exceeds the file size limit.', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'files_to_upload', '.', 'append', '(', '{', ""'file_name'"", ':', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ',', ""'file_handle'"", ':', 'tmp_file_handle', ',', ""'file_mime_type'"", ':', 'file_mime_type', '}', ')', 'if', 'self', '.', 'export_format', '==', ""'csv'"", ':', 'csv_writer', '=', 'self', '.', '_configure_csv_file', '(', 'tmp_file_handle', ',', 'schema', ')', 'return', 'files_to_upload']","Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.","['Takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",python,W,0,True,1,test
7957,tmontaigu/pylas,pylas/lasdatas/base.py,https://github.com/tmontaigu/pylas/blob/8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06/pylas/lasdatas/base.py#L229-L283,"def write_to(self, out_stream, do_compress=False):
        """""" writes the data to a stream

        Parameters
        ----------
        out_stream: file object
            the destination stream, implementing the write method
        do_compress: bool, optional, default False
            Flag to indicate if you want the date to be compressed
        """"""

        self.update_header()

        if (
                self.vlrs.get(""ExtraBytesVlr"")
                and not self.points_data.extra_dimensions_names
        ):
            logger.error(
                ""Las contains an ExtraBytesVlr, but no extra bytes were found in the point_record, ""
                ""removing the vlr""
            )
            self.vlrs.extract(""ExtraBytesVlr"")

        if do_compress:
            laz_vrl = create_laz_vlr(self.points_data)
            self.vlrs.append(known.LasZipVlr(laz_vrl.data()))
            raw_vlrs = vlrlist.RawVLRList.from_list(self.vlrs)

            self.header.offset_to_point_data = (
                    self.header.size + raw_vlrs.total_size_in_bytes()
            )
            self.header.point_format_id = uncompressed_id_to_compressed(
                self.header.point_format_id
            )
            self.header.number_of_vlr = len(raw_vlrs)

            points_bytes = compress_buffer(
                np.frombuffer(self.points_data.array, np.uint8),
                laz_vrl.schema,
                self.header.offset_to_point_data,
            ).tobytes()

        else:
            raw_vlrs = vlrlist.RawVLRList.from_list(self.vlrs)
            self.header.number_of_vlr = len(raw_vlrs)
            self.header.offset_to_point_data = (
                    self.header.size + raw_vlrs.total_size_in_bytes()
            )
            points_bytes = self.points_data.raw_bytes()

        self.header.write_to(out_stream)
        self._raise_if_not_expected_pos(out_stream, self.header.size)
        raw_vlrs.write_to(out_stream)
        self._raise_if_not_expected_pos(out_stream, self.header.offset_to_point_data)
        out_stream.write(points_bytes)","['def', 'write_to', '(', 'self', ',', 'out_stream', ',', 'do_compress', '=', 'False', ')', ':', 'self', '.', 'update_header', '(', ')', 'if', '(', 'self', '.', 'vlrs', '.', 'get', '(', '""ExtraBytesVlr""', ')', 'and', 'not', 'self', '.', 'points_data', '.', 'extra_dimensions_names', ')', ':', 'logger', '.', 'error', '(', '""Las contains an ExtraBytesVlr, but no extra bytes were found in the point_record, ""', '""removing the vlr""', ')', 'self', '.', 'vlrs', '.', 'extract', '(', '""ExtraBytesVlr""', ')', 'if', 'do_compress', ':', 'laz_vrl', '=', 'create_laz_vlr', '(', 'self', '.', 'points_data', ')', 'self', '.', 'vlrs', '.', 'append', '(', 'known', '.', 'LasZipVlr', '(', 'laz_vrl', '.', 'data', '(', ')', ')', ')', 'raw_vlrs', '=', 'vlrlist', '.', 'RawVLRList', '.', 'from_list', '(', 'self', '.', 'vlrs', ')', 'self', '.', 'header', '.', 'offset_to_point_data', '=', '(', 'self', '.', 'header', '.', 'size', '+', 'raw_vlrs', '.', 'total_size_in_bytes', '(', ')', ')', 'self', '.', 'header', '.', 'point_format_id', '=', 'uncompressed_id_to_compressed', '(', 'self', '.', 'header', '.', 'point_format_id', ')', 'self', '.', 'header', '.', 'number_of_vlr', '=', 'len', '(', 'raw_vlrs', ')', 'points_bytes', '=', 'compress_buffer', '(', 'np', '.', 'frombuffer', '(', 'self', '.', 'points_data', '.', 'array', ',', 'np', '.', 'uint8', ')', ',', 'laz_vrl', '.', 'schema', ',', 'self', '.', 'header', '.', 'offset_to_point_data', ',', ')', '.', 'tobytes', '(', ')', 'else', ':', 'raw_vlrs', '=', 'vlrlist', '.', 'RawVLRList', '.', 'from_list', '(', 'self', '.', 'vlrs', ')', 'self', '.', 'header', '.', 'number_of_vlr', '=', 'len', '(', 'raw_vlrs', ')', 'self', '.', 'header', '.', 'offset_to_point_data', '=', '(', 'self', '.', 'header', '.', 'size', '+', 'raw_vlrs', '.', 'total_size_in_bytes', '(', ')', ')', 'points_bytes', '=', 'self', '.', 'points_data', '.', 'raw_bytes', '(', ')', 'self', '.', 'header', '.', 'write_to', '(', 'out_stream', ')', 'self', '.', '_raise_if_not_expected_pos', '(', 'out_stream', ',', 'self', '.', 'header', '.', 'size', ')', 'raw_vlrs', '.', 'write_to', '(', 'out_stream', ')', 'self', '.', '_raise_if_not_expected_pos', '(', 'out_stream', ',', 'self', '.', 'header', '.', 'offset_to_point_data', ')', 'out_stream', '.', 'write', '(', 'points_bytes', ')']","writes the data to a stream

        Parameters
        ----------
        out_stream: file object
            the destination stream, implementing the write method
        do_compress: bool, optional, default False
            Flag to indicate if you want the date to be compressed","['writes', 'the', 'data', 'to', 'a', 'stream']",python,W,0,True,1,test
10620,assemblerflow/flowcraft,flowcraft/generator/engine.py,https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/engine.py#L1412-L1447,"def write_configs(self, project_root):
        """"""Wrapper method that writes all configuration files to the pipeline
        directory
        """"""

        # Write resources config
        with open(join(project_root, ""resources.config""), ""w"") as fh:
            fh.write(self.resources)

        # Write containers config
        with open(join(project_root, ""containers.config""), ""w"") as fh:
            fh.write(self.containers)

        # Write containers config
        with open(join(project_root, ""params.config""), ""w"") as fh:
            fh.write(self.params)

        # Write manifest config
        with open(join(project_root, ""manifest.config""), ""w"") as fh:
            fh.write(self.manifest)

        # Write user config if not present in the project directory
        if not exists(join(project_root, ""user.config"")):
            with open(join(project_root, ""user.config""), ""w"") as fh:
                fh.write(self.user_config)

        lib_dir = join(project_root, ""lib"")
        if not exists(lib_dir):
            os.makedirs(lib_dir)
        with open(join(lib_dir, ""Helper.groovy""), ""w"") as fh:
            fh.write(self.help)

        # Generate the pipeline DAG
        pipeline_to_json = self.render_pipeline()
        with open(splitext(self.nf_file)[0] + "".html"", ""w"") as fh:
            fh.write(pipeline_to_json)","['def', 'write_configs', '(', 'self', ',', 'project_root', ')', ':', '# Write resources config', 'with', 'open', '(', 'join', '(', 'project_root', ',', '""resources.config""', ')', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'self', '.', 'resources', ')', '# Write containers config', 'with', 'open', '(', 'join', '(', 'project_root', ',', '""containers.config""', ')', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'self', '.', 'containers', ')', '# Write containers config', 'with', 'open', '(', 'join', '(', 'project_root', ',', '""params.config""', ')', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'self', '.', 'params', ')', '# Write manifest config', 'with', 'open', '(', 'join', '(', 'project_root', ',', '""manifest.config""', ')', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'self', '.', 'manifest', ')', '# Write user config if not present in the project directory', 'if', 'not', 'exists', '(', 'join', '(', 'project_root', ',', '""user.config""', ')', ')', ':', 'with', 'open', '(', 'join', '(', 'project_root', ',', '""user.config""', ')', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'self', '.', 'user_config', ')', 'lib_dir', '=', 'join', '(', 'project_root', ',', '""lib""', ')', 'if', 'not', 'exists', '(', 'lib_dir', ')', ':', 'os', '.', 'makedirs', '(', 'lib_dir', ')', 'with', 'open', '(', 'join', '(', 'lib_dir', ',', '""Helper.groovy""', ')', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'self', '.', 'help', ')', '# Generate the pipeline DAG', 'pipeline_to_json', '=', 'self', '.', 'render_pipeline', '(', ')', 'with', 'open', '(', 'splitext', '(', 'self', '.', 'nf_file', ')', '[', '0', ']', '+', '"".html""', ',', '""w""', ')', 'as', 'fh', ':', 'fh', '.', 'write', '(', 'pipeline_to_json', ')']","Wrapper method that writes all configuration files to the pipeline
        directory","['Wrapper', 'method', 'that', 'writes', 'all', 'configuration', 'files', 'to', 'the', 'pipeline', 'directory']",python,W,0,True,1,test
16608,marrabld/planarradpy,gui/gui_batch.py,https://github.com/marrabld/planarradpy/blob/5095d1cb98d4f67a7c3108c9282f2d59253e89a8/gui/gui_batch.py#L31-L152,"def write_batch_to_file(self, filename='batch_test_default.txt'):
        """"""
        This function creates a new file if he doesn't exist already, moves it to 'inputs/batch_file' folder
        and writes data and comments associated to them.
        Inputs: saa_values : <list> Sun Azimuth Angle (deg)
                sza_values : <list> Sun Zenith Angle (deg)
                batch_name : Name of the batch file.
                p_values : <list> Phytoplankton linear scaling factor
                x_value : <list> Scattering scaling factor
                y_value : <list> Scattering slope factor
                g_value : <list> CDOM absorption scaling factor
                s_value : <list> CDOM absorption slope factor
                s_value : <list> depth (m)
                waveL_values : Wavelength values used to test.
                verbose_value : Number concerning if the software explains a lot or not what it does.
                phytoplankton_path : The path to the file containing phytoplankton data.
                bottom_path : The path to the file containing bottom data.
                nb_cpu : The number of CPU we want to allowed to the software.
                executive_path : The path to the file where there is executive files using by PlanarRad.
                report_parameter :
        """"""

        #---------------------------------------------------------#
        # The following is the file which is passed to planarradpy.
        #---------------------------------------------------------#
        self.batch_file = open(str(filename), 'w')

        self.batch_file.write(""""""#----------------------------------------#
# Name of the batch run
#----------------------------------------#
batch_name = """""")
        self.batch_file.write(str(self.batch_name))
        self.batch_file.write(""""""

#----------------------------------------#
# Bio-optical parameters list
#----------------------------------------#
saa_list = """""")
        self.batch_file.write(str(self.saa_values))
        self.batch_file.write(""""""
sza_list = """""")
        self.batch_file.write(str(self.sza_values))
        self.batch_file.write(""""""
p_list = """""")
        self.batch_file.write(str(self.p_values))
        self.batch_file.write(""""""
x_list = """""")
        self.batch_file.write(str(self.x_value))
        self.batch_file.write(""""""
y_list = """""")
        self.batch_file.write(str(self.y_value))
        self.batch_file.write(""""""
g_list = """""")
        self.batch_file.write(str(self.g_value))
        self.batch_file.write(""""""
s_list = """""")
        self.batch_file.write(str(self.s_value))
        self.batch_file.write(""""""
z_list = """""")
        self.batch_file.write(str(self.z_value))
        self.batch_file.write(""""""

#----------------------------------------#
# Wavelengths
# All IOPs are interpolated to these 
# Wavelengths
#----------------------------------------#
wavelengths = """""")
        self.batch_file.write(str(self.wavelength_values))
        self.batch_file.write(""""""

#----------------------------------------#
# Number of CPUs
# -1 means query the number of CPUs
#----------------------------------------#
num_cpus = """""")
        self.batch_file.write(str(self.nb_cpu))
        self.batch_file.write(""""""

#----------------------------------------#
# Path of Planarrad
#----------------------------------------#
exec_path = """""")
        self.batch_file.write(self.executive_path)
        self.batch_file.write(""""""

#----------------------------------------#
# Logging level
#----------------------------------------#
verbose = """""")
        self.batch_file.write(str(self.verbose_value))
        self.batch_file.write(""""""

#----------------------------------------#
# File paths
# Using absolute paths
#----------------------------------------#
phytoplankton_absorption_file ="""""")
        self.batch_file.write(self.phytoplankton_path)
        self.batch_file.write(""""""
bottom_reflectance_file = """""")
        self.batch_file.write(self.bottom_path)
        self.batch_file.write(""""""

#----------------------------------------#
# Set the parameter to report
#----------------------------------------#
report_parameter = """""")
        self.batch_file.write(str(self.report_parameter_value))

        self.batch_file.write(""""""

"""""")

        self.batch_file.close()

        #-------------------------------------------------------------------#
        # The following is the action to move the file to the good directory.
        #-------------------------------------------------------------------#
        src = './' + filename
        dst = './inputs/batch_files'
        os.system(""mv"" + "" "" + src + "" "" + dst)","['def', 'write_batch_to_file', '(', 'self', ',', 'filename', '=', ""'batch_test_default.txt'"", ')', ':', '#---------------------------------------------------------#', '# The following is the file which is passed to planarradpy.', '#---------------------------------------------------------#', 'self', '.', 'batch_file', '=', 'open', '(', 'str', '(', 'filename', ')', ',', ""'w'"", ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""#----------------------------------------#\n# Name of the batch run\n#----------------------------------------#\nbatch_name = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'batch_name', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# Bio-optical parameters list\n#----------------------------------------#\nsaa_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'saa_values', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\nsza_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'sza_values', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\np_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'p_values', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\nx_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'x_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\ny_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'y_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\ng_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'g_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\ns_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 's_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\nz_list = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'z_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# Wavelengths\n# All IOPs are interpolated to these \n# Wavelengths\n#----------------------------------------#\nwavelengths = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'wavelength_values', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# Number of CPUs\n# -1 means query the number of CPUs\n#----------------------------------------#\nnum_cpus = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'nb_cpu', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# Path of Planarrad\n#----------------------------------------#\nexec_path = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'self', '.', 'executive_path', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# Logging level\n#----------------------------------------#\nverbose = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'verbose_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# File paths\n# Using absolute paths\n#----------------------------------------#\nphytoplankton_absorption_file =""""""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'self', '.', 'phytoplankton_path', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\nbottom_reflectance_file = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'self', '.', 'bottom_path', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n#----------------------------------------#\n# Set the parameter to report\n#----------------------------------------#\nreport_parameter = """"""', ')', 'self', '.', 'batch_file', '.', 'write', '(', 'str', '(', 'self', '.', 'report_parameter_value', ')', ')', 'self', '.', 'batch_file', '.', 'write', '(', '""""""\n\n""""""', ')', 'self', '.', 'batch_file', '.', 'close', '(', ')', '#-------------------------------------------------------------------#', '# The following is the action to move the file to the good directory.', '#-------------------------------------------------------------------#', 'src', '=', ""'./'"", '+', 'filename', 'dst', '=', ""'./inputs/batch_files'"", 'os', '.', 'system', '(', '""mv""', '+', '"" ""', '+', 'src', '+', '"" ""', '+', 'dst', ')']","This function creates a new file if he doesn't exist already, moves it to 'inputs/batch_file' folder
        and writes data and comments associated to them.
        Inputs: saa_values : <list> Sun Azimuth Angle (deg)
                sza_values : <list> Sun Zenith Angle (deg)
                batch_name : Name of the batch file.
                p_values : <list> Phytoplankton linear scaling factor
                x_value : <list> Scattering scaling factor
                y_value : <list> Scattering slope factor
                g_value : <list> CDOM absorption scaling factor
                s_value : <list> CDOM absorption slope factor
                s_value : <list> depth (m)
                waveL_values : Wavelength values used to test.
                verbose_value : Number concerning if the software explains a lot or not what it does.
                phytoplankton_path : The path to the file containing phytoplankton data.
                bottom_path : The path to the file containing bottom data.
                nb_cpu : The number of CPU we want to allowed to the software.
                executive_path : The path to the file where there is executive files using by PlanarRad.
                report_parameter :","['This', 'function', 'creates', 'a', 'new', 'file', 'if', 'he', 'doesn', 't', 'exist', 'already', 'moves', 'it', 'to', 'inputs', '/', 'batch_file', 'folder', 'and', 'writes', 'data', 'and', 'comments', 'associated', 'to', 'them', '.', 'Inputs', ':', 'saa_values', ':', '<list', '>', 'Sun', 'Azimuth', 'Angle', '(', 'deg', ')', 'sza_values', ':', '<list', '>', 'Sun', 'Zenith', 'Angle', '(', 'deg', ')', 'batch_name', ':', 'Name', 'of', 'the', 'batch', 'file', '.', 'p_values', ':', '<list', '>', 'Phytoplankton', 'linear', 'scaling', 'factor', 'x_value', ':', '<list', '>', 'Scattering', 'scaling', 'factor', 'y_value', ':', '<list', '>', 'Scattering', 'slope', 'factor', 'g_value', ':', '<list', '>', 'CDOM', 'absorption', 'scaling', 'factor', 's_value', ':', '<list', '>', 'CDOM', 'absorption', 'slope', 'factor', 's_value', ':', '<list', '>', 'depth', '(', 'm', ')', 'waveL_values', ':', 'Wavelength', 'values', 'used', 'to', 'test', '.', 'verbose_value', ':', 'Number', 'concerning', 'if', 'the', 'software', 'explains', 'a', 'lot', 'or', 'not', 'what', 'it', 'does', '.', 'phytoplankton_path', ':', 'The', 'path', 'to', 'the', 'file', 'containing', 'phytoplankton', 'data', '.', 'bottom_path', ':', 'The', 'path', 'to', 'the', 'file', 'containing', 'bottom', 'data', '.', 'nb_cpu', ':', 'The', 'number', 'of', 'CPU', 'we', 'want', 'to', 'allowed', 'to', 'the', 'software', '.', 'executive_path', ':', 'The', 'path', 'to', 'the', 'file', 'where', 'there', 'is', 'executive', 'files', 'using', 'by', 'PlanarRad', '.', 'report_parameter', ':']",python,W,0,True,1,test
16721,capitalone/giraffez,giraffez/config.py,https://github.com/capitalone/giraffez/blob/6b4d27eb1a1eaf188c6885c7364ef27e92b1b957/giraffez/config.py#L242-L264,"def lock_connection(cls, conf, dsn, key=None):
        """"""
        A class method to lock a connection (given by :code:`dsn`) in the specified
        configuration file. Automatically opens the file and writes to it before
        closing.

        :param str conf: The configuration file to modify
        :param str dsn: The name of the connection to lock
        :raises `giraffez.errors.ConfigurationError`: if the connection does not exist
        """"""
        with Config(conf, ""w"", key) as c:
            connection = c.get_connection(dsn)
            if not connection:
                raise ConfigurationError(""Unable to lock connection"")
            if dsn is None:
                dsn = c.settings[""connections""][""default""]
            value = ""connections.{}.lock"".format(dsn)
            lock = c.get_value(""connections.{}.lock"".format(dsn), default=0)
            if lock >= 2:
                raise ConnectionLock(dsn)
            lock += 1
            c.set_value(""connections.{}.lock"".format(dsn), lock)
            c.write()","['def', 'lock_connection', '(', 'cls', ',', 'conf', ',', 'dsn', ',', 'key', '=', 'None', ')', ':', 'with', 'Config', '(', 'conf', ',', '""w""', ',', 'key', ')', 'as', 'c', ':', 'connection', '=', 'c', '.', 'get_connection', '(', 'dsn', ')', 'if', 'not', 'connection', ':', 'raise', 'ConfigurationError', '(', '""Unable to lock connection""', ')', 'if', 'dsn', 'is', 'None', ':', 'dsn', '=', 'c', '.', 'settings', '[', '""connections""', ']', '[', '""default""', ']', 'value', '=', '""connections.{}.lock""', '.', 'format', '(', 'dsn', ')', 'lock', '=', 'c', '.', 'get_value', '(', '""connections.{}.lock""', '.', 'format', '(', 'dsn', ')', ',', 'default', '=', '0', ')', 'if', 'lock', '>=', '2', ':', 'raise', 'ConnectionLock', '(', 'dsn', ')', 'lock', '+=', '1', 'c', '.', 'set_value', '(', '""connections.{}.lock""', '.', 'format', '(', 'dsn', ')', ',', 'lock', ')', 'c', '.', 'write', '(', ')']","A class method to lock a connection (given by :code:`dsn`) in the specified
        configuration file. Automatically opens the file and writes to it before
        closing.

        :param str conf: The configuration file to modify
        :param str dsn: The name of the connection to lock
        :raises `giraffez.errors.ConfigurationError`: if the connection does not exist","['A', 'class', 'method', 'to', 'lock', 'a', 'connection', '(', 'given', 'by', ':', 'code', ':', 'dsn', ')', 'in', 'the', 'specified', 'configuration', 'file', '.', 'Automatically', 'opens', 'the', 'file', 'and', 'writes', 'to', 'it', 'before', 'closing', '.']",python,W,0,True,1,test
16723,capitalone/giraffez,giraffez/config.py,https://github.com/capitalone/giraffez/blob/6b4d27eb1a1eaf188c6885c7364ef27e92b1b957/giraffez/config.py#L306-L325,"def unlock_connection(cls, conf, dsn, key=None):
        """"""
        A class method to unlock a connection (given by :code:`dsn`) in the specified
        configuration file. Automatically opens the file and writes to it before
        closing.

        :param str conf: The configuration file to modify
        :param str dsn: The name of the connection to unlock
        :raises `giraffez.errors.ConfigurationError`: if the connection does not exist
        """"""
        with Config(conf, ""w"", key) as c:
            connection = c.connections.get(dsn, None)
            if not connection:
                raise ConfigurationError(""Unable to unlock connection"")
            if dsn is None:
                dsn = c.settings[""connections""][""default""]
            if connection.get(""lock"", None) is None:
                raise GiraffeError(""Connection '{}' is not locked."".format(dsn))
            c.unset_value(""connections.{}.lock"".format(dsn))
            c.write()","['def', 'unlock_connection', '(', 'cls', ',', 'conf', ',', 'dsn', ',', 'key', '=', 'None', ')', ':', 'with', 'Config', '(', 'conf', ',', '""w""', ',', 'key', ')', 'as', 'c', ':', 'connection', '=', 'c', '.', 'connections', '.', 'get', '(', 'dsn', ',', 'None', ')', 'if', 'not', 'connection', ':', 'raise', 'ConfigurationError', '(', '""Unable to unlock connection""', ')', 'if', 'dsn', 'is', 'None', ':', 'dsn', '=', 'c', '.', 'settings', '[', '""connections""', ']', '[', '""default""', ']', 'if', 'connection', '.', 'get', '(', '""lock""', ',', 'None', ')', 'is', 'None', ':', 'raise', 'GiraffeError', '(', '""Connection \'{}\' is not locked.""', '.', 'format', '(', 'dsn', ')', ')', 'c', '.', 'unset_value', '(', '""connections.{}.lock""', '.', 'format', '(', 'dsn', ')', ')', 'c', '.', 'write', '(', ')']","A class method to unlock a connection (given by :code:`dsn`) in the specified
        configuration file. Automatically opens the file and writes to it before
        closing.

        :param str conf: The configuration file to modify
        :param str dsn: The name of the connection to unlock
        :raises `giraffez.errors.ConfigurationError`: if the connection does not exist","['A', 'class', 'method', 'to', 'unlock', 'a', 'connection', '(', 'given', 'by', ':', 'code', ':', 'dsn', ')', 'in', 'the', 'specified', 'configuration', 'file', '.', 'Automatically', 'opens', 'the', 'file', 'and', 'writes', 'to', 'it', 'before', 'closing', '.']",python,W,0,True,1,test
16733,capitalone/giraffez,giraffez/export.py,https://github.com/capitalone/giraffez/blob/6b4d27eb1a1eaf188c6885c7364ef27e92b1b957/giraffez/export.py#L151-L175,"def to_archive(self, writer):
        """"""
        Writes export archive files in the Giraffez archive format.
        This takes a `giraffez.io.Writer` and writes archive chunks to
        file until all rows for a given statement have been exhausted.

        .. code-block:: python

            with giraffez.BulkExport(""database.table_name"") as export:
                with giraffez.Writer(""database.table_name.tar.gz"", 'wb', use_gzip=True) as out:
                    for n in export.to_archive(out):
                        print(""Rows: {}"".format(n))

        :param `giraffez.io.Writer` writer: A writer handling the archive output

        :rtype: iterator (yields ``int``)
        """"""
        if 'b' not in writer.mode:
            raise GiraffeError(""Archive writer must be in binary mode"")
        writer.write(GIRAFFE_MAGIC)
        writer.write(self.columns.serialize())
        i = 0
        for n, chunk in enumerate(self._fetchall(ROW_ENCODING_RAW), 1):
            writer.write(chunk)
            yield TeradataEncoder.count(chunk)","['def', 'to_archive', '(', 'self', ',', 'writer', ')', ':', 'if', ""'b'"", 'not', 'in', 'writer', '.', 'mode', ':', 'raise', 'GiraffeError', '(', '""Archive writer must be in binary mode""', ')', 'writer', '.', 'write', '(', 'GIRAFFE_MAGIC', ')', 'writer', '.', 'write', '(', 'self', '.', 'columns', '.', 'serialize', '(', ')', ')', 'i', '=', '0', 'for', 'n', ',', 'chunk', 'in', 'enumerate', '(', 'self', '.', '_fetchall', '(', 'ROW_ENCODING_RAW', ')', ',', '1', ')', ':', 'writer', '.', 'write', '(', 'chunk', ')', 'yield', 'TeradataEncoder', '.', 'count', '(', 'chunk', ')']","Writes export archive files in the Giraffez archive format.
        This takes a `giraffez.io.Writer` and writes archive chunks to
        file until all rows for a given statement have been exhausted.

        .. code-block:: python

            with giraffez.BulkExport(""database.table_name"") as export:
                with giraffez.Writer(""database.table_name.tar.gz"", 'wb', use_gzip=True) as out:
                    for n in export.to_archive(out):
                        print(""Rows: {}"".format(n))

        :param `giraffez.io.Writer` writer: A writer handling the archive output

        :rtype: iterator (yields ``int``)","['Writes', 'export', 'archive', 'files', 'in', 'the', 'Giraffez', 'archive', 'format', '.', 'This', 'takes', 'a', 'giraffez', '.', 'io', '.', 'Writer', 'and', 'writes', 'archive', 'chunks', 'to', 'file', 'until', 'all', 'rows', 'for', 'a', 'given', 'statement', 'have', 'been', 'exhausted', '.']",python,W,0,True,1,test
19731,lvh/txampext,txampext/jsondialect.py,https://github.com/lvh/txampext/blob/a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9/txampext/jsondialect.py#L111-L116,"def _writeResponse(self, response):
        """"""
        Serializes the response to JSON, and writes it to the transport.
        """"""
        encoded = dumps(response, default=_default)
        self.transport.write(encoded)","['def', '_writeResponse', '(', 'self', ',', 'response', ')', ':', 'encoded', '=', 'dumps', '(', 'response', ',', 'default', '=', '_default', ')', 'self', '.', 'transport', '.', 'write', '(', 'encoded', ')']","Serializes the response to JSON, and writes it to the transport.","['Serializes', 'the', 'response', 'to', 'JSON', 'and', 'writes', 'it', 'to', 'the', 'transport', '.']",python,W,0,True,1,test
2710,fastai/fastai,fastai/callbacks/tensorboard.py,https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/callbacks/tensorboard.py#L85-L90,"def on_batch_end(self, last_loss:Tensor, iteration:int, **kwargs)->None:
        ""Callback function that writes batch end appropriate data to Tensorboard.""
        if iteration == 0: return
        self._update_batches_if_needed()
        if iteration % self.loss_iters == 0: self._write_training_loss(iteration=iteration, last_loss=last_loss)
        if iteration % self.hist_iters == 0: self._write_weight_histograms(iteration=iteration)","['def', 'on_batch_end', '(', 'self', ',', 'last_loss', ':', 'Tensor', ',', 'iteration', ':', 'int', ',', '*', '*', 'kwargs', ')', '->', 'None', ':', 'if', 'iteration', '==', '0', ':', 'return', 'self', '.', '_update_batches_if_needed', '(', ')', 'if', 'iteration', '%', 'self', '.', 'loss_iters', '==', '0', ':', 'self', '.', '_write_training_loss', '(', 'iteration', '=', 'iteration', ',', 'last_loss', '=', 'last_loss', ')', 'if', 'iteration', '%', 'self', '.', 'hist_iters', '==', '0', ':', 'self', '.', '_write_weight_histograms', '(', 'iteration', '=', 'iteration', ')']",Callback function that writes batch end appropriate data to Tensorboard.,"['Callback', 'function', 'that', 'writes', 'batch', 'end', 'appropriate', 'data', 'to', 'Tensorboard', '.']",python,W,0,True,1,train
2711,fastai/fastai,fastai/callbacks/tensorboard.py,https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/callbacks/tensorboard.py#L93-L97,"def on_backward_end(self, iteration:int, **kwargs)->None:
        ""Callback function that writes backward end appropriate data to Tensorboard.""
        if iteration == 0: return
        self._update_batches_if_needed()
        if iteration % self.stats_iters == 0: self._write_model_stats(iteration=iteration)","['def', 'on_backward_end', '(', 'self', ',', 'iteration', ':', 'int', ',', '*', '*', 'kwargs', ')', '->', 'None', ':', 'if', 'iteration', '==', '0', ':', 'return', 'self', '.', '_update_batches_if_needed', '(', ')', 'if', 'iteration', '%', 'self', '.', 'stats_iters', '==', '0', ':', 'self', '.', '_write_model_stats', '(', 'iteration', '=', 'iteration', ')']",Callback function that writes backward end appropriate data to Tensorboard.,"['Callback', 'function', 'that', 'writes', 'backward', 'end', 'appropriate', 'data', 'to', 'Tensorboard', '.']",python,W,0,True,1,train
2712,fastai/fastai,fastai/callbacks/tensorboard.py,https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/callbacks/tensorboard.py#L99-L101,"def on_epoch_end(self, last_metrics:MetricsList, iteration:int, **kwargs)->None:
        ""Callback function that writes epoch end appropriate data to Tensorboard.""
        self._write_metrics(iteration=iteration, last_metrics=last_metrics)","['def', 'on_epoch_end', '(', 'self', ',', 'last_metrics', ':', 'MetricsList', ',', 'iteration', ':', 'int', ',', '*', '*', 'kwargs', ')', '->', 'None', ':', 'self', '.', '_write_metrics', '(', 'iteration', '=', 'iteration', ',', 'last_metrics', '=', 'last_metrics', ')']",Callback function that writes epoch end appropriate data to Tensorboard.,"['Callback', 'function', 'that', 'writes', 'epoch', 'end', 'appropriate', 'data', 'to', 'Tensorboard', '.']",python,W,0,True,1,train
2719,fastai/fastai,fastai/callbacks/tensorboard.py,https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/callbacks/tensorboard.py#L158-L162,"def on_batch_end(self, iteration:int, **kwargs)->None:
        ""Callback function that writes batch end appropriate data to Tensorboard.""
        super().on_batch_end(iteration=iteration, **kwargs)
        if iteration == 0: return
        if iteration % self.visual_iters == 0: self._write_images(iteration=iteration)","['def', 'on_batch_end', '(', 'self', ',', 'iteration', ':', 'int', ',', '*', '*', 'kwargs', ')', '->', 'None', ':', 'super', '(', ')', '.', 'on_batch_end', '(', 'iteration', '=', 'iteration', ',', '*', '*', 'kwargs', ')', 'if', 'iteration', '==', '0', ':', 'return', 'if', 'iteration', '%', 'self', '.', 'visual_iters', '==', '0', ':', 'self', '.', '_write_images', '(', 'iteration', '=', 'iteration', ')']",Callback function that writes batch end appropriate data to Tensorboard.,"['Callback', 'function', 'that', 'writes', 'batch', 'end', 'appropriate', 'data', 'to', 'Tensorboard', '.']",python,W,0,True,1,train
2720,fastai/fastai,fastai/callbacks/tensorboard.py,https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/callbacks/tensorboard.py#L164-L171,"def on_backward_end(self, iteration:int, **kwargs)->None:
        ""Callback function that writes backward end appropriate data to Tensorboard.""
        if iteration == 0: return
        self._update_batches_if_needed()
        #TODO:  This could perhaps be implemented as queues of requests instead but that seemed like overkill. 
        # But I'm not the biggest fan of maintaining these boolean flags either... Review pls.
        if iteration % self.stats_iters == 0: self.gen_stats_updated, self.crit_stats_updated = False, False
        if not (self.gen_stats_updated and self.crit_stats_updated): self._write_model_stats(iteration=iteration)","['def', 'on_backward_end', '(', 'self', ',', 'iteration', ':', 'int', ',', '*', '*', 'kwargs', ')', '->', 'None', ':', 'if', 'iteration', '==', '0', ':', 'return', 'self', '.', '_update_batches_if_needed', '(', ')', '#TODO:  This could perhaps be implemented as queues of requests instead but that seemed like overkill. ', ""# But I'm not the biggest fan of maintaining these boolean flags either... Review pls."", 'if', 'iteration', '%', 'self', '.', 'stats_iters', '==', '0', ':', 'self', '.', 'gen_stats_updated', ',', 'self', '.', 'crit_stats_updated', '=', 'False', ',', 'False', 'if', 'not', '(', 'self', '.', 'gen_stats_updated', 'and', 'self', '.', 'crit_stats_updated', ')', ':', 'self', '.', '_write_model_stats', '(', 'iteration', '=', 'iteration', ')']",Callback function that writes backward end appropriate data to Tensorboard.,"['Callback', 'function', 'that', 'writes', 'backward', 'end', 'appropriate', 'data', 'to', 'Tensorboard', '.']",python,W,0,True,1,train
8677,google/flatbuffers,python/flatbuffers/builder.py,https://github.com/google/flatbuffers/blob/6cc30b3272d79c85db7d4871ac0aa69541dc89de/python/flatbuffers/builder.py#L387-L396,"def EndVector(self, vectorNumElems):
        """"""EndVector writes data necessary to finish vector construction.""""""

        self.assertNested()
        ## @cond FLATBUFFERS_INTERNAL
        self.nested = False
        ## @endcond
        # we already made space for this, so write without PrependUint32
        self.PlaceUOffsetT(vectorNumElems)
        return self.Offset()","['def', 'EndVector', '(', 'self', ',', 'vectorNumElems', ')', ':', 'self', '.', 'assertNested', '(', ')', '## @cond FLATBUFFERS_INTERNAL', 'self', '.', 'nested', '=', 'False', '## @endcond', '# we already made space for this, so write without PrependUint32', 'self', '.', 'PlaceUOffsetT', '(', 'vectorNumElems', ')', 'return', 'self', '.', 'Offset', '(', ')']",EndVector writes data necessary to finish vector construction.,"['EndVector', 'writes', 'data', 'necessary', 'to', 'finish', 'vector', 'construction', '.']",python,W,0,True,1,train
14531,googleapis/google-cloud-python,bigquery/google/cloud/bigquery/client.py,https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/bigquery/google/cloud/bigquery/client.py#L1945-L1949,"def _schema_to_json_file_object(self, schema_list, file_obj):
        """"""Helper function for schema_to_json that takes a schema list and file
        object and writes the schema list to the file object with json.dump
        """"""
        json.dump(schema_list, file_obj, indent=2, sort_keys=True)","['def', '_schema_to_json_file_object', '(', 'self', ',', 'schema_list', ',', 'file_obj', ')', ':', 'json', '.', 'dump', '(', 'schema_list', ',', 'file_obj', ',', 'indent', '=', '2', ',', 'sort_keys', '=', 'True', ')']","Helper function for schema_to_json that takes a schema list and file
        object and writes the schema list to the file object with json.dump","['Helper', 'function', 'for', 'schema_to_json', 'that', 'takes', 'a', 'schema', 'list', 'and', 'file', 'object', 'and', 'writes', 'the', 'schema', 'list', 'to', 'the', 'file', 'object', 'with', 'json', '.', 'dump']",python,W,0,True,1,train
15384,tensorflow/cleverhans,examples/nips17_adversarial_competition/eval_infra/code/eval_lib/classification_results.py,https://github.com/tensorflow/cleverhans/blob/97488e215760547b81afc53f5e5de8ba7da5bd98/examples/nips17_adversarial_competition/eval_infra/code/eval_lib/classification_results.py#L256-L284,"def init_from_adversarial_batches_write_to_datastore(self, submissions,
                                                       adv_batches):
    """"""Populates data from adversarial batches and writes to datastore.

    Args:
      submissions: instance of CompetitionSubmissions
      adv_batches: instance of AversarialBatches
    """"""
    # prepare classification batches
    idx = 0
    for s_id in iterkeys(submissions.defenses):
      for adv_id in iterkeys(adv_batches.data):
        class_batch_id = CLASSIFICATION_BATCH_ID_PATTERN.format(idx)
        idx += 1
        self.data[class_batch_id] = {
            'adversarial_batch_id': adv_id,
            'submission_id': s_id,
            'result_path': os.path.join(
                self._round_name,
                CLASSIFICATION_BATCHES_SUBDIR,
                s_id + '_' + adv_id + '.csv')
        }
    # save them to datastore
    client = self._datastore_client
    with client.no_transact_batch() as batch:
      for key, value in iteritems(self.data):
        entity = client.entity(client.key(KIND_CLASSIFICATION_BATCH, key))
        entity.update(value)
        batch.put(entity)","['def', 'init_from_adversarial_batches_write_to_datastore', '(', 'self', ',', 'submissions', ',', 'adv_batches', ')', ':', '# prepare classification batches', 'idx', '=', '0', 'for', 's_id', 'in', 'iterkeys', '(', 'submissions', '.', 'defenses', ')', ':', 'for', 'adv_id', 'in', 'iterkeys', '(', 'adv_batches', '.', 'data', ')', ':', 'class_batch_id', '=', 'CLASSIFICATION_BATCH_ID_PATTERN', '.', 'format', '(', 'idx', ')', 'idx', '+=', '1', 'self', '.', 'data', '[', 'class_batch_id', ']', '=', '{', ""'adversarial_batch_id'"", ':', 'adv_id', ',', ""'submission_id'"", ':', 's_id', ',', ""'result_path'"", ':', 'os', '.', 'path', '.', 'join', '(', 'self', '.', '_round_name', ',', 'CLASSIFICATION_BATCHES_SUBDIR', ',', 's_id', '+', ""'_'"", '+', 'adv_id', '+', ""'.csv'"", ')', '}', '# save them to datastore', 'client', '=', 'self', '.', '_datastore_client', 'with', 'client', '.', 'no_transact_batch', '(', ')', 'as', 'batch', ':', 'for', 'key', ',', 'value', 'in', 'iteritems', '(', 'self', '.', 'data', ')', ':', 'entity', '=', 'client', '.', 'entity', '(', 'client', '.', 'key', '(', 'KIND_CLASSIFICATION_BATCH', ',', 'key', ')', ')', 'entity', '.', 'update', '(', 'value', ')', 'batch', '.', 'put', '(', 'entity', ')']","Populates data from adversarial batches and writes to datastore.

    Args:
      submissions: instance of CompetitionSubmissions
      adv_batches: instance of AversarialBatches","['Populates', 'data', 'from', 'adversarial', 'batches', 'and', 'writes', 'to', 'datastore', '.']",python,W,0,True,1,train
18094,tensorflow/hub,tensorflow_hub/resolver.py,https://github.com/tensorflow/hub/blob/09f45963f6787322967b6fec61459f3ac56fbb27/tensorflow_hub/resolver.py#L146-L157,"def _extract_file(self, tgz, tarinfo, dst_path, buffer_size=10<<20):
    """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'.""""""
    src = tgz.extractfile(tarinfo)
    dst = tf_v1.gfile.GFile(dst_path, ""wb"")
    while 1:
      buf = src.read(buffer_size)
      if not buf:
        break
      dst.write(buf)
      self._log_progress(len(buf))
    dst.close()
    src.close()","['def', '_extract_file', '(', 'self', ',', 'tgz', ',', 'tarinfo', ',', 'dst_path', ',', 'buffer_size', '=', '10', '<<', '20', ')', ':', 'src', '=', 'tgz', '.', 'extractfile', '(', 'tarinfo', ')', 'dst', '=', 'tf_v1', '.', 'gfile', '.', 'GFile', '(', 'dst_path', ',', '""wb""', ')', 'while', '1', ':', 'buf', '=', 'src', '.', 'read', '(', 'buffer_size', ')', 'if', 'not', 'buf', ':', 'break', 'dst', '.', 'write', '(', 'buf', ')', 'self', '.', '_log_progress', '(', 'len', '(', 'buf', ')', ')', 'dst', '.', 'close', '(', ')', 'src', '.', 'close', '(', ')']",Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'.,"['Extracts', 'tarinfo', 'from', 'tgz', 'and', 'writes', 'to', 'dst_path', '.']",python,W,0,True,1,train
21175,tensorflow/tensorboard,tensorboard/backend/event_processing/sqlite_writer.py,https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/backend/event_processing/sqlite_writer.py#L176-L214,"def write_summaries(self, tagged_data, experiment_name, run_name):
    """"""Transactionally writes the given tagged summary data to the DB.

    Args:
      tagged_data: map from tag to TagData instances.
      experiment_name: name of experiment.
      run_name: name of run.
    """"""
    logger.debug('Writing summaries for %s tags', len(tagged_data))
    # Connection used as context manager for auto commit/rollback on exit.
    # We still need an explicit BEGIN, because it doesn't do one on enter,
    # it waits until the first DML command - which is totally broken.
    # See: https://stackoverflow.com/a/44448465/1179226
    with self._db:
      self._db.execute('BEGIN TRANSACTION')
      run_id = self._maybe_init_run(experiment_name, run_name)
      tag_to_metadata = {
          tag: tagdata.metadata for tag, tagdata in six.iteritems(tagged_data)
      }
      tag_to_id = self._maybe_init_tags(run_id, tag_to_metadata)
      tensor_values = []
      for tag, tagdata in six.iteritems(tagged_data):
        tag_id = tag_to_id[tag]
        for step, wall_time, tensor_proto in tagdata.values:
          dtype = tensor_proto.dtype
          shape = ','.join(str(d.size) for d in tensor_proto.tensor_shape.dim)
          # Use tensor_proto.tensor_content if it's set, to skip relatively
          # expensive extraction into intermediate ndarray.
          data = self._make_blob(
              tensor_proto.tensor_content or
              tensor_util.make_ndarray(tensor_proto).tobytes())
          tensor_values.append((tag_id, step, wall_time, dtype, shape, data))
      self._db.executemany(
          """"""
          INSERT OR REPLACE INTO Tensors (
            series, step, computed_time, dtype, shape, data
          ) VALUES (?, ?, ?, ?, ?, ?)
          """""",
          tensor_values)","['def', 'write_summaries', '(', 'self', ',', 'tagged_data', ',', 'experiment_name', ',', 'run_name', ')', ':', 'logger', '.', 'debug', '(', ""'Writing summaries for %s tags'"", ',', 'len', '(', 'tagged_data', ')', ')', '# Connection used as context manager for auto commit/rollback on exit.', ""# We still need an explicit BEGIN, because it doesn't do one on enter,"", '# it waits until the first DML command - which is totally broken.', '# See: https://stackoverflow.com/a/44448465/1179226', 'with', 'self', '.', '_db', ':', 'self', '.', '_db', '.', 'execute', '(', ""'BEGIN TRANSACTION'"", ')', 'run_id', '=', 'self', '.', '_maybe_init_run', '(', 'experiment_name', ',', 'run_name', ')', 'tag_to_metadata', '=', '{', 'tag', ':', 'tagdata', '.', 'metadata', 'for', 'tag', ',', 'tagdata', 'in', 'six', '.', 'iteritems', '(', 'tagged_data', ')', '}', 'tag_to_id', '=', 'self', '.', '_maybe_init_tags', '(', 'run_id', ',', 'tag_to_metadata', ')', 'tensor_values', '=', '[', ']', 'for', 'tag', ',', 'tagdata', 'in', 'six', '.', 'iteritems', '(', 'tagged_data', ')', ':', 'tag_id', '=', 'tag_to_id', '[', 'tag', ']', 'for', 'step', ',', 'wall_time', ',', 'tensor_proto', 'in', 'tagdata', '.', 'values', ':', 'dtype', '=', 'tensor_proto', '.', 'dtype', 'shape', '=', ""','"", '.', 'join', '(', 'str', '(', 'd', '.', 'size', ')', 'for', 'd', 'in', 'tensor_proto', '.', 'tensor_shape', '.', 'dim', ')', ""# Use tensor_proto.tensor_content if it's set, to skip relatively"", '# expensive extraction into intermediate ndarray.', 'data', '=', 'self', '.', '_make_blob', '(', 'tensor_proto', '.', 'tensor_content', 'or', 'tensor_util', '.', 'make_ndarray', '(', 'tensor_proto', ')', '.', 'tobytes', '(', ')', ')', 'tensor_values', '.', 'append', '(', '(', 'tag_id', ',', 'step', ',', 'wall_time', ',', 'dtype', ',', 'shape', ',', 'data', ')', ')', 'self', '.', '_db', '.', 'executemany', '(', '""""""\n          INSERT OR REPLACE INTO Tensors (\n            series, step, computed_time, dtype, shape, data\n          ) VALUES (?, ?, ?, ?, ?, ?)\n          """"""', ',', 'tensor_values', ')']","Transactionally writes the given tagged summary data to the DB.

    Args:
      tagged_data: map from tag to TagData instances.
      experiment_name: name of experiment.
      run_name: name of run.","['Transactionally', 'writes', 'the', 'given', 'tagged', 'summary', 'data', 'to', 'the', 'DB', '.']",python,W,0,True,1,train
21281,tensorflow/tensorboard,tensorboard/plugins/beholder/beholder.py,https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/plugins/beholder/beholder.py#L158-L175,"def update(self, session, arrays=None, frame=None):
    '''Creates a frame and writes it to disk.

    Args:
      arrays: a list of np arrays. Use the ""custom"" option in the client.
      frame: a 2D np array. This way the plugin can be used for video of any
             kind, not just the visualization that comes with the plugin.

             frame can also be a function, which only is evaluated when the
             ""frame"" option is selected by the client.
    '''
    new_config = self._get_config()

    if self._enough_time_has_passed(self.previous_config['FPS']):
      self.visualizer.update(new_config)
      self.last_update_time = time.time()
      final_image = self._update_frame(session, arrays, frame, new_config)
      self._update_recording(final_image, new_config)","['def', 'update', '(', 'self', ',', 'session', ',', 'arrays', '=', 'None', ',', 'frame', '=', 'None', ')', ':', 'new_config', '=', 'self', '.', '_get_config', '(', ')', 'if', 'self', '.', '_enough_time_has_passed', '(', 'self', '.', 'previous_config', '[', ""'FPS'"", ']', ')', ':', 'self', '.', 'visualizer', '.', 'update', '(', 'new_config', ')', 'self', '.', 'last_update_time', '=', 'time', '.', 'time', '(', ')', 'final_image', '=', 'self', '.', '_update_frame', '(', 'session', ',', 'arrays', ',', 'frame', ',', 'new_config', ')', 'self', '.', '_update_recording', '(', 'final_image', ',', 'new_config', ')']","Creates a frame and writes it to disk.

    Args:
      arrays: a list of np arrays. Use the ""custom"" option in the client.
      frame: a 2D np array. This way the plugin can be used for video of any
             kind, not just the visualization that comes with the plugin.

             frame can also be a function, which only is evaluated when the
             ""frame"" option is selected by the client.","['Creates', 'a', 'frame', 'and', 'writes', 'it', 'to', 'disk', '.']",python,W,0,True,1,train
3230,saltstack/salt,salt/modules/system.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/system.py#L626-L651,"def set_reboot_required_witnessed():
    '''
    This function is used to remember that an event indicating that a reboot is
    required was witnessed. This function writes to a temporary filesystem so
    the event gets cleared upon reboot.

    Returns:
        bool: ``True`` if successful, otherwise ``False``

    .. code-block:: bash

        salt '*' system.set_reboot_required_witnessed
    '''
    errcode = -1
    dir_path = os.path.dirname(NILRT_REBOOT_WITNESS_PATH)
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path)
        except OSError as ex:
            raise SaltInvocationError('Error creating {0} (-{1}): {2}'
                                      .format(dir_path, ex.errno, ex.strerror))

        rdict = __salt__['cmd.run_all']('touch {0}'.format(NILRT_REBOOT_WITNESS_PATH))
        errcode = rdict['retcode']

    return errcode == 0","['def', 'set_reboot_required_witnessed', '(', ')', ':', 'errcode', '=', '-', '1', 'dir_path', '=', 'os', '.', 'path', '.', 'dirname', '(', 'NILRT_REBOOT_WITNESS_PATH', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'dir_path', ')', ':', 'try', ':', 'os', '.', 'makedirs', '(', 'dir_path', ')', 'except', 'OSError', 'as', 'ex', ':', 'raise', 'SaltInvocationError', '(', ""'Error creating {0} (-{1}): {2}'"", '.', 'format', '(', 'dir_path', ',', 'ex', '.', 'errno', ',', 'ex', '.', 'strerror', ')', ')', 'rdict', '=', '__salt__', '[', ""'cmd.run_all'"", ']', '(', ""'touch {0}'"", '.', 'format', '(', 'NILRT_REBOOT_WITNESS_PATH', ')', ')', 'errcode', '=', 'rdict', '[', ""'retcode'"", ']', 'return', 'errcode', '==', '0']","This function is used to remember that an event indicating that a reboot is
    required was witnessed. This function writes to a temporary filesystem so
    the event gets cleared upon reboot.

    Returns:
        bool: ``True`` if successful, otherwise ``False``

    .. code-block:: bash

        salt '*' system.set_reboot_required_witnessed","['This', 'function', 'is', 'used', 'to', 'remember', 'that', 'an', 'event', 'indicating', 'that', 'a', 'reboot', 'is', 'required', 'was', 'witnessed', '.', 'This', 'function', 'writes', 'to', 'a', 'temporary', 'filesystem', 'so', 'the', 'event', 'gets', 'cleared', 'upon', 'reboot', '.']",python,W,0,True,1,train
3545,saltstack/salt,salt/modules/incron.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/incron.py#L98-L114,"def _write_incron_lines(user, lines):
    '''
    Takes a list of lines to be committed to a user's incrontab and writes it
    '''
    if user == 'system':
        ret = {}
        ret['retcode'] = _write_file(_INCRON_SYSTEM_TAB, 'salt', ''.join(lines))
        return ret
    else:
        path = salt.utils.files.mkstemp()
        with salt.utils.files.fopen(path, 'wb') as fp_:
            fp_.writelines(salt.utils.data.encode(lines))
        if __grains__['os_family'] == 'Solaris' and user != ""root"":
            __salt__['cmd.run']('chown {0} {1}'.format(user, path), python_shell=False)
        ret = __salt__['cmd.run_all'](_get_incron_cmdstr(path), runas=user, python_shell=False)
        os.remove(path)
        return ret","['def', '_write_incron_lines', '(', 'user', ',', 'lines', ')', ':', 'if', 'user', '==', ""'system'"", ':', 'ret', '=', '{', '}', 'ret', '[', ""'retcode'"", ']', '=', '_write_file', '(', '_INCRON_SYSTEM_TAB', ',', ""'salt'"", ',', ""''"", '.', 'join', '(', 'lines', ')', ')', 'return', 'ret', 'else', ':', 'path', '=', 'salt', '.', 'utils', '.', 'files', '.', 'mkstemp', '(', ')', 'with', 'salt', '.', 'utils', '.', 'files', '.', 'fopen', '(', 'path', ',', ""'wb'"", ')', 'as', 'fp_', ':', 'fp_', '.', 'writelines', '(', 'salt', '.', 'utils', '.', 'data', '.', 'encode', '(', 'lines', ')', ')', 'if', '__grains__', '[', ""'os_family'"", ']', '==', ""'Solaris'"", 'and', 'user', '!=', '""root""', ':', '__salt__', '[', ""'cmd.run'"", ']', '(', ""'chown {0} {1}'"", '.', 'format', '(', 'user', ',', 'path', ')', ',', 'python_shell', '=', 'False', ')', 'ret', '=', '__salt__', '[', ""'cmd.run_all'"", ']', '(', '_get_incron_cmdstr', '(', 'path', ')', ',', 'runas', '=', 'user', ',', 'python_shell', '=', 'False', ')', 'os', '.', 'remove', '(', 'path', ')', 'return', 'ret']",Takes a list of lines to be committed to a user's incrontab and writes it,"['Takes', 'a', 'list', 'of', 'lines', 'to', 'be', 'committed', 'to', 'a', 'user', 's', 'incrontab', 'and', 'writes', 'it']",python,W,0,True,1,train
6577,ReFirmLabs/binwalk,src/binwalk/plugins/hilink.py,https://github.com/ReFirmLabs/binwalk/blob/a0c5315fd2bae167e5c3d8469ce95d5defc743c2/src/binwalk/plugins/hilink.py#L38-L48,"def _decrypt_and_extract(self, fname):
        '''
        This does the extraction (e.g., it decrypts the image and writes it to a new file on disk).
        '''
        with open(fname, ""r"") as fp_in:
            encrypted_data = fp_in.read()

            decrypted_data = self._hilink_decrypt(encrypted_data)

            with open(binwalk.core.common.unique_file_name(fname[:-4], ""dec""), ""w"") as fp_out:
                fp_out.write(decrypted_data)","['def', '_decrypt_and_extract', '(', 'self', ',', 'fname', ')', ':', 'with', 'open', '(', 'fname', ',', '""r""', ')', 'as', 'fp_in', ':', 'encrypted_data', '=', 'fp_in', '.', 'read', '(', ')', 'decrypted_data', '=', 'self', '.', '_hilink_decrypt', '(', 'encrypted_data', ')', 'with', 'open', '(', 'binwalk', '.', 'core', '.', 'common', '.', 'unique_file_name', '(', 'fname', '[', ':', '-', '4', ']', ',', '""dec""', ')', ',', '""w""', ')', 'as', 'fp_out', ':', 'fp_out', '.', 'write', '(', 'decrypted_data', ')']","This does the extraction (e.g., it decrypts the image and writes it to a new file on disk).","['This', 'does', 'the', 'extraction', '(', 'e', '.', 'g', '.', 'it', 'decrypts', 'the', 'image', 'and', 'writes', 'it', 'to', 'a', 'new', 'file', 'on', 'disk', ')', '.']",python,W,0,True,1,train
13054,DataDog/integrations-core,tokumx/datadog_checks/tokumx/vendor/gridfs/__init__.py,https://github.com/DataDog/integrations-core/blob/ebd41c873cf9f97a8c51bf9459bc6a7536af8acd/tokumx/datadog_checks/tokumx/vendor/gridfs/__init__.py#L601-L625,"def download_to_stream(self, file_id, destination):
        """"""Downloads the contents of the stored file specified by file_id and
        writes the contents to `destination`.

        For example::

          my_db = MongoClient().test
          fs = GridFSBucket(my_db)
          # Get _id of file to read
          file_id = fs.upload_from_stream(""test_file"", ""data I want to store!"")
          # Get file to write to
          file = open('myfile','wb+')
          fs.download_to_stream(file_id, file)
          file.seek(0)
          contents = file.read()

        Raises :exc:`~gridfs.errors.NoFile` if no file with file_id exists.

        :Parameters:
          - `file_id`: The _id of the file to be downloaded.
          - `destination`: a file-like object implementing :meth:`write`.
        """"""
        gout = self.open_download_stream(file_id)
        for chunk in gout:
            destination.write(chunk)","['def', 'download_to_stream', '(', 'self', ',', 'file_id', ',', 'destination', ')', ':', 'gout', '=', 'self', '.', 'open_download_stream', '(', 'file_id', ')', 'for', 'chunk', 'in', 'gout', ':', 'destination', '.', 'write', '(', 'chunk', ')']","Downloads the contents of the stored file specified by file_id and
        writes the contents to `destination`.

        For example::

          my_db = MongoClient().test
          fs = GridFSBucket(my_db)
          # Get _id of file to read
          file_id = fs.upload_from_stream(""test_file"", ""data I want to store!"")
          # Get file to write to
          file = open('myfile','wb+')
          fs.download_to_stream(file_id, file)
          file.seek(0)
          contents = file.read()

        Raises :exc:`~gridfs.errors.NoFile` if no file with file_id exists.

        :Parameters:
          - `file_id`: The _id of the file to be downloaded.
          - `destination`: a file-like object implementing :meth:`write`.","['Downloads', 'the', 'contents', 'of', 'the', 'stored', 'file', 'specified', 'by', 'file_id', 'and', 'writes', 'the', 'contents', 'to', 'destination', '.']",python,W,0,True,1,train
13375,DataDog/integrations-core,tokumx/datadog_checks/tokumx/vendor/pymongo/mongo_client.py,https://github.com/DataDog/integrations-core/blob/ebd41c873cf9f97a8c51bf9459bc6a7536af8acd/tokumx/datadog_checks/tokumx/vendor/pymongo/mongo_client.py#L1348-L1365,"def fsync(self, **kwargs):
        """"""Flush all pending writes to datafiles.

        :Parameters:

            Optional parameters can be passed as keyword arguments:

            - `lock`: If True lock the server to disallow writes.
            - `async`: If True don't block while synchronizing.

            .. warning:: `async` and `lock` can not be used together.

            .. warning:: MongoDB does not support the `async` option
                         on Windows and will raise an exception on that
                         platform.
        """"""
        self.admin.command(""fsync"",
                           read_preference=ReadPreference.PRIMARY, **kwargs)","['def', 'fsync', '(', 'self', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'admin', '.', 'command', '(', '""fsync""', ',', 'read_preference', '=', 'ReadPreference', '.', 'PRIMARY', ',', '*', '*', 'kwargs', ')']","Flush all pending writes to datafiles.

        :Parameters:

            Optional parameters can be passed as keyword arguments:

            - `lock`: If True lock the server to disallow writes.
            - `async`: If True don't block while synchronizing.

            .. warning:: `async` and `lock` can not be used together.

            .. warning:: MongoDB does not support the `async` option
                         on Windows and will raise an exception on that
                         platform.","['Flush', 'all', 'pending', 'writes', 'to', 'datafiles', '.']",python,W,0,True,1,train
15108,GPflow/GPflow,gpflow/training/monitor.py,https://github.com/GPflow/GPflow/blob/549394f0b1b0696c7b521a065e49bdae6e7acf27/gpflow/training/monitor.py#L836-L854,"def _eval_summary(self, context: MonitorContext, feed_dict: Optional[Dict]=None) -> None:
        """"""
        Evaluates the summary tensor and writes the result to the event file.
        :param context: Monitor context
        :param feed_dict: Input values dictionary to be provided to the `session.run`
        when evaluating the summary tensor.
        """"""

        if self._summary is None:
            raise RuntimeError('TensorBoard monitor task should set the Tensorflow.Summary object')

        if context.session is None:
            raise RuntimeError('To run a TensorBoard monitor task the TF session object'
                               ' must be provided when creating an instance of the Monitor')

        summary = context.session.run(self._summary, feed_dict=feed_dict)
        self._file_writer.add_summary(summary, context.global_step)
        if self._flush_immediately:
            self.flush()","['def', '_eval_summary', '(', 'self', ',', 'context', ':', 'MonitorContext', ',', 'feed_dict', ':', 'Optional', '[', 'Dict', ']', '=', 'None', ')', '->', 'None', ':', 'if', 'self', '.', '_summary', 'is', 'None', ':', 'raise', 'RuntimeError', '(', ""'TensorBoard monitor task should set the Tensorflow.Summary object'"", ')', 'if', 'context', '.', 'session', 'is', 'None', ':', 'raise', 'RuntimeError', '(', ""'To run a TensorBoard monitor task the TF session object'"", ""' must be provided when creating an instance of the Monitor'"", ')', 'summary', '=', 'context', '.', 'session', '.', 'run', '(', 'self', '.', '_summary', ',', 'feed_dict', '=', 'feed_dict', ')', 'self', '.', '_file_writer', '.', 'add_summary', '(', 'summary', ',', 'context', '.', 'global_step', ')', 'if', 'self', '.', '_flush_immediately', ':', 'self', '.', 'flush', '(', ')']","Evaluates the summary tensor and writes the result to the event file.
        :param context: Monitor context
        :param feed_dict: Input values dictionary to be provided to the `session.run`
        when evaluating the summary tensor.","['Evaluates', 'the', 'summary', 'tensor', 'and', 'writes', 'the', 'result', 'to', 'the', 'event', 'file', '.', ':', 'param', 'context', ':', 'Monitor', 'context', ':', 'param', 'feed_dict', ':', 'Input', 'values', 'dictionary', 'to', 'be', 'provided', 'to', 'the', 'session', '.', 'run', 'when', 'evaluating', 'the', 'summary', 'tensor', '.']",python,W,0,True,1,train
16537,google/grr,grr/server/grr_response_server/databases/mysql_flows.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/databases/mysql_flows.py#L662-L706,"def _WriteResponses(self, responses, cursor):
    """"""Builds the writes to store the given responses in the db.""""""

    query = (""INSERT IGNORE INTO flow_responses ""
             ""(client_id, flow_id, request_id, response_id, ""
             ""response, status, iterator, timestamp) VALUES "")

    templates = []
    args = []
    for r in responses:
      templates.append(""(%s, %s, %s, %s, %s, %s, %s, NOW(6))"")
      client_id_int = db_utils.ClientIDToInt(r.client_id)
      flow_id_int = db_utils.FlowIDToInt(r.flow_id)

      args.append(client_id_int)
      args.append(flow_id_int)
      args.append(r.request_id)
      args.append(r.response_id)
      if isinstance(r, rdf_flow_objects.FlowResponse):
        args.append(r.SerializeToString())
        args.append("""")
        args.append("""")
      elif isinstance(r, rdf_flow_objects.FlowStatus):
        args.append("""")
        args.append(r.SerializeToString())
        args.append("""")
      elif isinstance(r, rdf_flow_objects.FlowIterator):
        args.append("""")
        args.append("""")
        args.append(r.SerializeToString())
      else:
        # This can't really happen due to db api type checking.
        raise ValueError(""Got unexpected response type: %s %s"" % (type(r), r))

    query += "","".join(templates)
    try:
      cursor.execute(query, args)
    except MySQLdb.IntegrityError:
      # If we have multiple responses and one of them fails to insert, we try
      # them one by one so we don't lose any valid replies.
      if len(responses) > 1:
        for r in responses:
          self._WriteResponses([r], cursor)
      else:
        logging.warn(""Response for unknown request: %s"", responses[0])","['def', '_WriteResponses', '(', 'self', ',', 'responses', ',', 'cursor', ')', ':', 'query', '=', '(', '""INSERT IGNORE INTO flow_responses ""', '""(client_id, flow_id, request_id, response_id, ""', '""response, status, iterator, timestamp) VALUES ""', ')', 'templates', '=', '[', ']', 'args', '=', '[', ']', 'for', 'r', 'in', 'responses', ':', 'templates', '.', 'append', '(', '""(%s, %s, %s, %s, %s, %s, %s, NOW(6))""', ')', 'client_id_int', '=', 'db_utils', '.', 'ClientIDToInt', '(', 'r', '.', 'client_id', ')', 'flow_id_int', '=', 'db_utils', '.', 'FlowIDToInt', '(', 'r', '.', 'flow_id', ')', 'args', '.', 'append', '(', 'client_id_int', ')', 'args', '.', 'append', '(', 'flow_id_int', ')', 'args', '.', 'append', '(', 'r', '.', 'request_id', ')', 'args', '.', 'append', '(', 'r', '.', 'response_id', ')', 'if', 'isinstance', '(', 'r', ',', 'rdf_flow_objects', '.', 'FlowResponse', ')', ':', 'args', '.', 'append', '(', 'r', '.', 'SerializeToString', '(', ')', ')', 'args', '.', 'append', '(', '""""', ')', 'args', '.', 'append', '(', '""""', ')', 'elif', 'isinstance', '(', 'r', ',', 'rdf_flow_objects', '.', 'FlowStatus', ')', ':', 'args', '.', 'append', '(', '""""', ')', 'args', '.', 'append', '(', 'r', '.', 'SerializeToString', '(', ')', ')', 'args', '.', 'append', '(', '""""', ')', 'elif', 'isinstance', '(', 'r', ',', 'rdf_flow_objects', '.', 'FlowIterator', ')', ':', 'args', '.', 'append', '(', '""""', ')', 'args', '.', 'append', '(', '""""', ')', 'args', '.', 'append', '(', 'r', '.', 'SerializeToString', '(', ')', ')', 'else', ':', ""# This can't really happen due to db api type checking."", 'raise', 'ValueError', '(', '""Got unexpected response type: %s %s""', '%', '(', 'type', '(', 'r', ')', ',', 'r', ')', ')', 'query', '+=', '"",""', '.', 'join', '(', 'templates', ')', 'try', ':', 'cursor', '.', 'execute', '(', 'query', ',', 'args', ')', 'except', 'MySQLdb', '.', 'IntegrityError', ':', '# If we have multiple responses and one of them fails to insert, we try', ""# them one by one so we don't lose any valid replies."", 'if', 'len', '(', 'responses', ')', '>', '1', ':', 'for', 'r', 'in', 'responses', ':', 'self', '.', '_WriteResponses', '(', '[', 'r', ']', ',', 'cursor', ')', 'else', ':', 'logging', '.', 'warn', '(', '""Response for unknown request: %s""', ',', 'responses', '[', '0', ']', ')']",Builds the writes to store the given responses in the db.,"['Builds', 'the', 'writes', 'to', 'store', 'the', 'given', 'responses', 'in', 'the', 'db', '.']",python,W,0,True,1,train
16595,google/grr,grr/core/grr_response_core/lib/util/compat/yaml.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/core/grr_response_core/lib/util/compat/yaml.py#L166-L174,"def WriteToPath(obj, filepath):
  """"""Serializes and writes given Python object to the specified YAML file.

  Args:
    obj: A Python object to serialize.
    filepath: A path to the file into which the object is to be written.
  """"""
  with io.open(filepath, mode=""w"", encoding=""utf-8"") as filedesc:
    WriteToFile(obj, filedesc)","['def', 'WriteToPath', '(', 'obj', ',', 'filepath', ')', ':', 'with', 'io', '.', 'open', '(', 'filepath', ',', 'mode', '=', '""w""', ',', 'encoding', '=', '""utf-8""', ')', 'as', 'filedesc', ':', 'WriteToFile', '(', 'obj', ',', 'filedesc', ')']","Serializes and writes given Python object to the specified YAML file.

  Args:
    obj: A Python object to serialize.
    filepath: A path to the file into which the object is to be written.","['Serializes', 'and', 'writes', 'given', 'Python', 'object', 'to', 'the', 'specified', 'YAML', 'file', '.']",python,W,0,True,1,train
16596,google/grr,grr/core/grr_response_core/lib/util/compat/yaml.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/core/grr_response_core/lib/util/compat/yaml.py#L177-L185,"def WriteManyToPath(objs, filepath):
  """"""Serializes and writes given Python objects to a multi-document YAML file.

  Args:
    objs: An iterable of Python objects to serialize.
    filepath: A path to the file into which the object is to be written.
  """"""
  with io.open(filepath, mode=""w"", encoding=""utf-8"") as filedesc:
    WriteManyToFile(objs, filedesc)","['def', 'WriteManyToPath', '(', 'objs', ',', 'filepath', ')', ':', 'with', 'io', '.', 'open', '(', 'filepath', ',', 'mode', '=', '""w""', ',', 'encoding', '=', '""utf-8""', ')', 'as', 'filedesc', ':', 'WriteManyToFile', '(', 'objs', ',', 'filedesc', ')']","Serializes and writes given Python objects to a multi-document YAML file.

  Args:
    objs: An iterable of Python objects to serialize.
    filepath: A path to the file into which the object is to be written.","['Serializes', 'and', 'writes', 'given', 'Python', 'objects', 'to', 'a', 'multi', '-', 'document', 'YAML', 'file', '.']",python,W,0,True,1,train
22052,hardbyte/python-can,can/io/sqlite.py,https://github.com/hardbyte/python-can/blob/cdc5254d96072df7739263623f3e920628a7d214/can/io/sqlite.py#L229-L236,"def stop(self):
        """"""Stops the reader an writes all remaining messages to the database. Thus, this
        might take a while and block.
        """"""
        BufferedReader.stop(self)
        self._stop_running_event.set()
        self._writer_thread.join()
        BaseIOHandler.stop(self)","['def', 'stop', '(', 'self', ')', ':', 'BufferedReader', '.', 'stop', '(', 'self', ')', 'self', '.', '_stop_running_event', '.', 'set', '(', ')', 'self', '.', '_writer_thread', '.', 'join', '(', ')', 'BaseIOHandler', '.', 'stop', '(', 'self', ')']","Stops the reader an writes all remaining messages to the database. Thus, this
        might take a while and block.","['Stops', 'the', 'reader', 'an', 'writes', 'all', 'remaining', 'messages', 'to', 'the', 'database', '.', 'Thus', 'this', 'might', 'take', 'a', 'while', 'and', 'block', '.']",python,W,0,True,1,train
22095,hardbyte/python-can,can/io/blf.py,https://github.com/hardbyte/python-can/blob/cdc5254d96072df7739263623f3e920628a7d214/can/io/blf.py#L371-L398,"def _flush(self):
        """"""Compresses and writes data in the cache to file.""""""
        if self.file.closed:
            return
        cache = b"""".join(self.cache)
        if not cache:
            # Nothing to write
            return
        uncompressed_data = cache[:self.MAX_CACHE_SIZE]
        # Save data that comes after max size to next round
        tail = cache[self.MAX_CACHE_SIZE:]
        self.cache = [tail]
        self.cache_size = len(tail)
        compressed_data = zlib.compress(uncompressed_data,
                                        self.COMPRESSION_LEVEL)
        obj_size = (OBJ_HEADER_V1_STRUCT.size + LOG_CONTAINER_STRUCT.size +
                    len(compressed_data))
        base_header = OBJ_HEADER_BASE_STRUCT.pack(
            b""LOBJ"", OBJ_HEADER_BASE_STRUCT.size, 1, obj_size, LOG_CONTAINER)
        container_header = LOG_CONTAINER_STRUCT.pack(
            ZLIB_DEFLATE, len(uncompressed_data))
        self.file.write(base_header)
        self.file.write(container_header)
        self.file.write(compressed_data)
        # Write padding bytes
        self.file.write(b""\x00"" * (obj_size % 4))
        self.uncompressed_size += OBJ_HEADER_V1_STRUCT.size + LOG_CONTAINER_STRUCT.size
        self.uncompressed_size += len(uncompressed_data)","['def', '_flush', '(', 'self', ')', ':', 'if', 'self', '.', 'file', '.', 'closed', ':', 'return', 'cache', '=', 'b""""', '.', 'join', '(', 'self', '.', 'cache', ')', 'if', 'not', 'cache', ':', '# Nothing to write', 'return', 'uncompressed_data', '=', 'cache', '[', ':', 'self', '.', 'MAX_CACHE_SIZE', ']', '# Save data that comes after max size to next round', 'tail', '=', 'cache', '[', 'self', '.', 'MAX_CACHE_SIZE', ':', ']', 'self', '.', 'cache', '=', '[', 'tail', ']', 'self', '.', 'cache_size', '=', 'len', '(', 'tail', ')', 'compressed_data', '=', 'zlib', '.', 'compress', '(', 'uncompressed_data', ',', 'self', '.', 'COMPRESSION_LEVEL', ')', 'obj_size', '=', '(', 'OBJ_HEADER_V1_STRUCT', '.', 'size', '+', 'LOG_CONTAINER_STRUCT', '.', 'size', '+', 'len', '(', 'compressed_data', ')', ')', 'base_header', '=', 'OBJ_HEADER_BASE_STRUCT', '.', 'pack', '(', 'b""LOBJ""', ',', 'OBJ_HEADER_BASE_STRUCT', '.', 'size', ',', '1', ',', 'obj_size', ',', 'LOG_CONTAINER', ')', 'container_header', '=', 'LOG_CONTAINER_STRUCT', '.', 'pack', '(', 'ZLIB_DEFLATE', ',', 'len', '(', 'uncompressed_data', ')', ')', 'self', '.', 'file', '.', 'write', '(', 'base_header', ')', 'self', '.', 'file', '.', 'write', '(', 'container_header', ')', 'self', '.', 'file', '.', 'write', '(', 'compressed_data', ')', '# Write padding bytes', 'self', '.', 'file', '.', 'write', '(', 'b""\\x00""', '*', '(', 'obj_size', '%', '4', ')', ')', 'self', '.', 'uncompressed_size', '+=', 'OBJ_HEADER_V1_STRUCT', '.', 'size', '+', 'LOG_CONTAINER_STRUCT', '.', 'size', 'self', '.', 'uncompressed_size', '+=', 'len', '(', 'uncompressed_data', ')']",Compresses and writes data in the cache to file.,"['Compresses', 'and', 'writes', 'data', 'in', 'the', 'cache', 'to', 'file', '.']",python,W,0,True,1,train
24338,amoffat/sh,sh.py,https://github.com/amoffat/sh/blob/858adf0c682af4c40e41f34d6926696b7a5d3b12/sh.py#L2393-L2424,"def input_thread(log, stdin, is_alive, quit, close_before_term):
    """""" this is run in a separate thread.  it writes into our process's
    stdin (a streamwriter) and waits the process to end AND everything that
    can be written to be written """"""

    done = False
    closed = False
    alive = True
    poller = Poller()
    poller.register_write(stdin)

    while poller and alive:
        changed = poller.poll(1)
        for fd, events in changed:
            if events & (POLLER_EVENT_WRITE | POLLER_EVENT_HUP):
                log.debug(""%r ready for more input"", stdin)
                done = stdin.write()

                if done:
                    poller.unregister(stdin)
                    if close_before_term:
                        stdin.close()
                        closed = True

        alive, _ = is_alive()

    while alive:
        quit.wait(1)
        alive, _ = is_alive()

    if not closed:
        stdin.close()","['def', 'input_thread', '(', 'log', ',', 'stdin', ',', 'is_alive', ',', 'quit', ',', 'close_before_term', ')', ':', 'done', '=', 'False', 'closed', '=', 'False', 'alive', '=', 'True', 'poller', '=', 'Poller', '(', ')', 'poller', '.', 'register_write', '(', 'stdin', ')', 'while', 'poller', 'and', 'alive', ':', 'changed', '=', 'poller', '.', 'poll', '(', '1', ')', 'for', 'fd', ',', 'events', 'in', 'changed', ':', 'if', 'events', '&', '(', 'POLLER_EVENT_WRITE', '|', 'POLLER_EVENT_HUP', ')', ':', 'log', '.', 'debug', '(', '""%r ready for more input""', ',', 'stdin', ')', 'done', '=', 'stdin', '.', 'write', '(', ')', 'if', 'done', ':', 'poller', '.', 'unregister', '(', 'stdin', ')', 'if', 'close_before_term', ':', 'stdin', '.', 'close', '(', ')', 'closed', '=', 'True', 'alive', ',', '_', '=', 'is_alive', '(', ')', 'while', 'alive', ':', 'quit', '.', 'wait', '(', '1', ')', 'alive', ',', '_', '=', 'is_alive', '(', ')', 'if', 'not', 'closed', ':', 'stdin', '.', 'close', '(', ')']","this is run in a separate thread.  it writes into our process's
    stdin (a streamwriter) and waits the process to end AND everything that
    can be written to be written","['this', 'is', 'run', 'in', 'a', 'separate', 'thread', '.', 'it', 'writes', 'into', 'our', 'process', 's', 'stdin', '(', 'a', 'streamwriter', ')', 'and', 'waits', 'the', 'process', 'to', 'end', 'AND', 'everything', 'that', 'can', 'be', 'written', 'to', 'be', 'written']",python,W,0,True,1,train
25593,googleads/googleads-python-lib,googleads/ad_manager.py,https://github.com/googleads/googleads-python-lib/blob/aa3b1b474b0f9789ca55ca46f4b2b57aeae38874/googleads/ad_manager.py#L828-L868,"def DownloadReportToFile(self, report_job_id, export_format, outfile,
                           include_report_properties=False,
                           include_totals_row=None, use_gzip_compression=True):
    """"""Downloads report data and writes it to a file.

    The report job must be completed before calling this function.

    Args:
      report_job_id: The ID of the report job to wait for, as a string.
      export_format: The export format for the report file, as a string.
      outfile: A writeable, file-like object to write to.
      include_report_properties: Whether or not to include the report
        properties (e.g. network, user, date generated...)
        in the generated report.
      include_totals_row: Whether or not to include the totals row.
      use_gzip_compression: Whether or not to use gzip compression.
    """"""
    service = self._GetReportService()

    if include_totals_row is None:  # True unless CSV export if not specified
      include_totals_row = True if export_format != 'CSV_DUMP' else False
    opts = {
        'exportFormat': export_format,
        'includeReportProperties': include_report_properties,
        'includeTotalsRow': include_totals_row,
        'useGzipCompression': use_gzip_compression
    }
    report_url = service.getReportDownloadUrlWithOptions(report_job_id, opts)
    _data_downloader_logger.info('Request Summary: Report job ID: %s, %s',
                                 report_job_id, opts)

    response = self.url_opener.open(report_url)

    _data_downloader_logger.debug(
        'Incoming response: %s %s REDACTED REPORT DATA', response.code,
        response.msg)

    while True:
      chunk = response.read(_CHUNK_SIZE)
      if not chunk: break
      outfile.write(chunk)","['def', 'DownloadReportToFile', '(', 'self', ',', 'report_job_id', ',', 'export_format', ',', 'outfile', ',', 'include_report_properties', '=', 'False', ',', 'include_totals_row', '=', 'None', ',', 'use_gzip_compression', '=', 'True', ')', ':', 'service', '=', 'self', '.', '_GetReportService', '(', ')', 'if', 'include_totals_row', 'is', 'None', ':', '# True unless CSV export if not specified', 'include_totals_row', '=', 'True', 'if', 'export_format', '!=', ""'CSV_DUMP'"", 'else', 'False', 'opts', '=', '{', ""'exportFormat'"", ':', 'export_format', ',', ""'includeReportProperties'"", ':', 'include_report_properties', ',', ""'includeTotalsRow'"", ':', 'include_totals_row', ',', ""'useGzipCompression'"", ':', 'use_gzip_compression', '}', 'report_url', '=', 'service', '.', 'getReportDownloadUrlWithOptions', '(', 'report_job_id', ',', 'opts', ')', '_data_downloader_logger', '.', 'info', '(', ""'Request Summary: Report job ID: %s, %s'"", ',', 'report_job_id', ',', 'opts', ')', 'response', '=', 'self', '.', 'url_opener', '.', 'open', '(', 'report_url', ')', '_data_downloader_logger', '.', 'debug', '(', ""'Incoming response: %s %s REDACTED REPORT DATA'"", ',', 'response', '.', 'code', ',', 'response', '.', 'msg', ')', 'while', 'True', ':', 'chunk', '=', 'response', '.', 'read', '(', '_CHUNK_SIZE', ')', 'if', 'not', 'chunk', ':', 'break', 'outfile', '.', 'write', '(', 'chunk', ')']","Downloads report data and writes it to a file.

    The report job must be completed before calling this function.

    Args:
      report_job_id: The ID of the report job to wait for, as a string.
      export_format: The export format for the report file, as a string.
      outfile: A writeable, file-like object to write to.
      include_report_properties: Whether or not to include the report
        properties (e.g. network, user, date generated...)
        in the generated report.
      include_totals_row: Whether or not to include the totals row.
      use_gzip_compression: Whether or not to use gzip compression.","['Downloads', 'report', 'data', 'and', 'writes', 'it', 'to', 'a', 'file', '.']",python,W,0,True,1,train
725,OpenTreeOfLife/peyotl,peyotl/utility/input_output.py,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/utility/input_output.py#L87-L95,"def write_pretty_dict_str(out, obj, indent=2):
    """"""writes JSON indented representation of `obj` to `out`""""""
    json.dump(obj,
              out,
              indent=indent,
              sort_keys=True,
              separators=(',', ': '),
              ensure_ascii=False,
              encoding=""utf-8"")","['def', 'write_pretty_dict_str', '(', 'out', ',', 'obj', ',', 'indent', '=', '2', ')', ':', 'json', '.', 'dump', '(', 'obj', ',', 'out', ',', 'indent', '=', 'indent', ',', 'sort_keys', '=', 'True', ',', 'separators', '=', '(', ""','"", ',', ""': '"", ')', ',', 'ensure_ascii', '=', 'False', ',', 'encoding', '=', '""utf-8""', ')']",writes JSON indented representation of `obj` to `out`,"['writes', 'JSON', 'indented', 'representation', 'of', 'obj', 'to', 'out']",python,W,0,True,1,train
1943,SylvanasSun/python-common-cache,common_cache/__init__.py,https://github.com/SylvanasSun/python-common-cache/blob/f113eb3cd751eed5ab5373e8610a31a444220cf8/common_cache/__init__.py#L612-L720,"def access_cache(self, key=None, key_location_on_param=0, expire=None, auto_update=True,
                     cache_loader=None, cache_writer=None, timeout=1):
        """"""
        The decorator for simplifying of use cache, it supports auto-update
        cache(if parameter auto_update is True), load cache from other level cache
        system or data source and writes back the update result to the
        other level cache system or data source if cache miss.

        The parameter key assigns a key for access cache or update cache and
        if it is None so select a parameter as a key from the decorated function
        by key_location_on_param, notice: key and key_location_on_param cannot all is None.

        For function cache_loader() must is a one-parameter function and the parameter
        represent a key of the cache, if this parameter is None so use self.cache_loader(),
        if they all are None so not load cache from other caches system.

        For function cache_writer() must is a two-parameter function and the first parameter
        representing a key of the cache and the second parameter representing a value of the cache,
        notice: if the parameter auto_update is False so it will not execute.

        >>> import time
        >>> cache = Cache(log_level=logging.WARNING)
        >>> @cache.access_cache(key='a')
        ... def a():
        ...     return 'a from data source'
        >>> a()
        'a from data source'
        >>> cache.get('a')
        'a from data source'
        >>> cache.put(key='b', value='b from cache')
        >>> @cache.access_cache(key='b')
        ... def b():
        ...     return 'b from data source'
        >>> b()
        'b from cache'
        >>> c_key = 'c'
        >>> @cache.access_cache(key_location_on_param=0)
        ... def c(key):
        ...     return 'c from data source'
        >>> c(c_key)
        'c from data source'
        >>> cache.get(c_key)
        'c from data source'
        >>> @cache.access_cache(key='d', auto_update=False)
        ... def d():
        ...     return 'd from data source'
        >>> d()
        'd from data source'
        >>> cache.get('d') == None
        True
        >>> @cache.access_cache(key='e', cache_loader=lambda k: '%s from cache loader' % k)
        ... def e():
        ...     return 'e from data source'
        >>> e()
        'e from cache loader'
        >>> out_dict = {}
        >>> def writer(k, v):
        ...     out_dict[k] = v
        >>> @cache.access_cache(key='f', cache_writer=writer)
        ... def f():
        ...     return 'f from data source'
        >>> f()
        'f from data source'
        >>> time.sleep(1) # wait to execute complete because it in the other thread
        >>> out_dict
        {'f': 'f from data source'}
        >>> cache.with_cache_loader(lambda k: '%s from cache loader(global)' % k)
        True
        >>> @cache.access_cache(key='g')
        ... def g():
        ...     return 'g from data source'
        >>> g()
        'g from cache loader(global)'
        """"""

        def decorate(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                k = None
                if len(args) - 1 >= key_location_on_param:
                    k = args[key_location_on_param]
                if key is not None:
                    k = key
                cache_result = self.get(key=k, timeout=timeout)
                # if the cache is miss and cache loader is the existent
                # then query cache from cache loader
                if cache_result is None:
                    if cache_loader is not None:
                        cache_result = cache_loader(k)
                    elif self.cache_loader is not None:
                        cache_result = self.cache_loader(k)
                # if still miss then execute a function that is decorated
                # then update cache on the basis of parameter auto_update
                if cache_result is not None:
                    return cache_result
                else:
                    result = func(*args, **kwargs)

                if auto_update:
                    self.put(key=k, value=result, expire=expire, timeout=timeout)
                    if cache_writer is not None:
                        self.thread_pool.submit(cache_writer, k, result)
                    elif self.cache_writer is not None:
                        self.thread_pool.submit(self.cache_writer, k, result)
                return result

            return wrapper

        return decorate","['def', 'access_cache', '(', 'self', ',', 'key', '=', 'None', ',', 'key_location_on_param', '=', '0', ',', 'expire', '=', 'None', ',', 'auto_update', '=', 'True', ',', 'cache_loader', '=', 'None', ',', 'cache_writer', '=', 'None', ',', 'timeout', '=', '1', ')', ':', 'def', 'decorate', '(', 'func', ')', ':', '@', 'functools', '.', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'k', '=', 'None', 'if', 'len', '(', 'args', ')', '-', '1', '>=', 'key_location_on_param', ':', 'k', '=', 'args', '[', 'key_location_on_param', ']', 'if', 'key', 'is', 'not', 'None', ':', 'k', '=', 'key', 'cache_result', '=', 'self', '.', 'get', '(', 'key', '=', 'k', ',', 'timeout', '=', 'timeout', ')', '# if the cache is miss and cache loader is the existent', '# then query cache from cache loader', 'if', 'cache_result', 'is', 'None', ':', 'if', 'cache_loader', 'is', 'not', 'None', ':', 'cache_result', '=', 'cache_loader', '(', 'k', ')', 'elif', 'self', '.', 'cache_loader', 'is', 'not', 'None', ':', 'cache_result', '=', 'self', '.', 'cache_loader', '(', 'k', ')', '# if still miss then execute a function that is decorated', '# then update cache on the basis of parameter auto_update', 'if', 'cache_result', 'is', 'not', 'None', ':', 'return', 'cache_result', 'else', ':', 'result', '=', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'if', 'auto_update', ':', 'self', '.', 'put', '(', 'key', '=', 'k', ',', 'value', '=', 'result', ',', 'expire', '=', 'expire', ',', 'timeout', '=', 'timeout', ')', 'if', 'cache_writer', 'is', 'not', 'None', ':', 'self', '.', 'thread_pool', '.', 'submit', '(', 'cache_writer', ',', 'k', ',', 'result', ')', 'elif', 'self', '.', 'cache_writer', 'is', 'not', 'None', ':', 'self', '.', 'thread_pool', '.', 'submit', '(', 'self', '.', 'cache_writer', ',', 'k', ',', 'result', ')', 'return', 'result', 'return', 'wrapper', 'return', 'decorate']","The decorator for simplifying of use cache, it supports auto-update
        cache(if parameter auto_update is True), load cache from other level cache
        system or data source and writes back the update result to the
        other level cache system or data source if cache miss.

        The parameter key assigns a key for access cache or update cache and
        if it is None so select a parameter as a key from the decorated function
        by key_location_on_param, notice: key and key_location_on_param cannot all is None.

        For function cache_loader() must is a one-parameter function and the parameter
        represent a key of the cache, if this parameter is None so use self.cache_loader(),
        if they all are None so not load cache from other caches system.

        For function cache_writer() must is a two-parameter function and the first parameter
        representing a key of the cache and the second parameter representing a value of the cache,
        notice: if the parameter auto_update is False so it will not execute.

        >>> import time
        >>> cache = Cache(log_level=logging.WARNING)
        >>> @cache.access_cache(key='a')
        ... def a():
        ...     return 'a from data source'
        >>> a()
        'a from data source'
        >>> cache.get('a')
        'a from data source'
        >>> cache.put(key='b', value='b from cache')
        >>> @cache.access_cache(key='b')
        ... def b():
        ...     return 'b from data source'
        >>> b()
        'b from cache'
        >>> c_key = 'c'
        >>> @cache.access_cache(key_location_on_param=0)
        ... def c(key):
        ...     return 'c from data source'
        >>> c(c_key)
        'c from data source'
        >>> cache.get(c_key)
        'c from data source'
        >>> @cache.access_cache(key='d', auto_update=False)
        ... def d():
        ...     return 'd from data source'
        >>> d()
        'd from data source'
        >>> cache.get('d') == None
        True
        >>> @cache.access_cache(key='e', cache_loader=lambda k: '%s from cache loader' % k)
        ... def e():
        ...     return 'e from data source'
        >>> e()
        'e from cache loader'
        >>> out_dict = {}
        >>> def writer(k, v):
        ...     out_dict[k] = v
        >>> @cache.access_cache(key='f', cache_writer=writer)
        ... def f():
        ...     return 'f from data source'
        >>> f()
        'f from data source'
        >>> time.sleep(1) # wait to execute complete because it in the other thread
        >>> out_dict
        {'f': 'f from data source'}
        >>> cache.with_cache_loader(lambda k: '%s from cache loader(global)' % k)
        True
        >>> @cache.access_cache(key='g')
        ... def g():
        ...     return 'g from data source'
        >>> g()
        'g from cache loader(global)'","['The', 'decorator', 'for', 'simplifying', 'of', 'use', 'cache', 'it', 'supports', 'auto', '-', 'update', 'cache', '(', 'if', 'parameter', 'auto_update', 'is', 'True', ')', 'load', 'cache', 'from', 'other', 'level', 'cache', 'system', 'or', 'data', 'source', 'and', 'writes', 'back', 'the', 'update', 'result', 'to', 'the', 'other', 'level', 'cache', 'system', 'or', 'data', 'source', 'if', 'cache', 'miss', '.']",python,W,0,True,1,train
8951,CI-WATER/gsshapy,gsshapy/grid/grid_to_gssha.py,https://github.com/CI-WATER/gsshapy/blob/00fd4af0fd65f1614d75a52fe950a04fb0867f4c/gsshapy/grid/grid_to_gssha.py#L1061-L1069,"def _write_hmet_card_file(self, hmet_card_file_path, main_output_folder):
        """"""
        This function writes the HMET_ASCII card file
        with ASCII file list for input to GSSHA
        """"""
        with io_open(hmet_card_file_path, 'w') as out_hmet_list_file:
            for hour_time in self.data.lsm.datetime:
                date_str = self._time_to_string(hour_time, ""%Y%m%d%H"")
                out_hmet_list_file.write(u""{0}\n"".format(path.join(main_output_folder, date_str)))","['def', '_write_hmet_card_file', '(', 'self', ',', 'hmet_card_file_path', ',', 'main_output_folder', ')', ':', 'with', 'io_open', '(', 'hmet_card_file_path', ',', ""'w'"", ')', 'as', 'out_hmet_list_file', ':', 'for', 'hour_time', 'in', 'self', '.', 'data', '.', 'lsm', '.', 'datetime', ':', 'date_str', '=', 'self', '.', '_time_to_string', '(', 'hour_time', ',', '""%Y%m%d%H""', ')', 'out_hmet_list_file', '.', 'write', '(', 'u""{0}\\n""', '.', 'format', '(', 'path', '.', 'join', '(', 'main_output_folder', ',', 'date_str', ')', ')', ')']","This function writes the HMET_ASCII card file
        with ASCII file list for input to GSSHA","['This', 'function', 'writes', 'the', 'HMET_ASCII', 'card', 'file', 'with', 'ASCII', 'file', 'list', 'for', 'input', 'to', 'GSSHA']",python,W,0,True,1,train
9956,accraze/python-markov-novel,src/markov_novel/novel.py,https://github.com/accraze/python-markov-novel/blob/ff451639e93a3ac11fb0268b92bc0cffc00bfdbe/src/markov_novel/novel.py#L15-L21,"def write(self, novel_title='novel', filetype='txt'):
        """"""
        Composes chapters
        and writes the novel to a text file
        """"""
        self._compose_chapters()
        self._write_to_file(novel_title, filetype)","['def', 'write', '(', 'self', ',', 'novel_title', '=', ""'novel'"", ',', 'filetype', '=', ""'txt'"", ')', ':', 'self', '.', '_compose_chapters', '(', ')', 'self', '.', '_write_to_file', '(', 'novel_title', ',', 'filetype', ')']","Composes chapters
        and writes the novel to a text file","['Composes', 'chapters', 'and', 'writes', 'the', 'novel', 'to', 'a', 'text', 'file']",python,W,0,True,1,train
10857,LISE-B26/pylabcontrol,build/lib/pylabcontrol/src/core/scripts.py,https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/src/core/scripts.py#L674-L744,"def save_image_to_disk(self, filename_1 = None, filename_2 = None):
        """"""
        creates an image using the scripts plot function and writes it to the disk
        for single plots (plot_type: 'main', 'aux')
            - if no filname provided take default name
        for double plots (plot_type: 'main', 'aux')
            - if no filnames provided take default name
            - if only one filname provided save only the plot for which name is provided
        Args:
            filename_1: filname for figure 1
            filename_2: filname for figure 1

        Returns: None

        """"""

        def axes_empty(ax):
            """"""
            takes an axes object and checks if it is empty
            the axes object is considered empty it doesn't contain any of the following:
                - lines
                - images
                - patches
            Returns:

            """"""

            is_empty = True

            if ax is not None and len(ax)>0:
                for a in ax:
                    if len(a.lines)+len(a.images)+len(a.patches) != 0:
                        is_empty = False


            return is_empty

        # create and save images
        if (filename_1 is None):
            filename_1 = self.filename('-plt1.png')

        if (filename_2 is None):
            filename_2 = self.filename('-plt2.png')


        # windows can't deal with long filenames so we have to use the prefix '\\\\?\\'
        # if len(filename_1.split('\\\\?\\')) == 1:
        #     filename_1 = '\\\\?\\' + filename_1
        # if len(filename_2.split('\\\\?\\')) == 1:
        #     filename_2 = '\\\\?\\' + filename_2

        if os.path.exists(os.path.dirname(filename_1)) is False:
            os.makedirs(os.path.dirname(filename_1))
        if os.path.exists(os.path.dirname(filename_2)) is False:
            os.makedirs(os.path.dirname(filename_2))


        fig_1 = Figure()
        canvas_1 = FigureCanvas(fig_1)

        fig_2 = Figure()
        canvas_2 = FigureCanvas(fig_2)

        self.force_update()

        self.plot([fig_1, fig_2])

        if filename_1 is not None and not axes_empty(fig_1.axes):
            fig_1.savefig(filename_1)
        if filename_2 is not None and not axes_empty(fig_2.axes):
            fig_2.savefig(filename_2)","['def', 'save_image_to_disk', '(', 'self', ',', 'filename_1', '=', 'None', ',', 'filename_2', '=', 'None', ')', ':', 'def', 'axes_empty', '(', 'ax', ')', ':', '""""""\n            takes an axes object and checks if it is empty\n            the axes object is considered empty it doesn\'t contain any of the following:\n                - lines\n                - images\n                - patches\n            Returns:\n\n            """"""', 'is_empty', '=', 'True', 'if', 'ax', 'is', 'not', 'None', 'and', 'len', '(', 'ax', ')', '>', '0', ':', 'for', 'a', 'in', 'ax', ':', 'if', 'len', '(', 'a', '.', 'lines', ')', '+', 'len', '(', 'a', '.', 'images', ')', '+', 'len', '(', 'a', '.', 'patches', ')', '!=', '0', ':', 'is_empty', '=', 'False', 'return', 'is_empty', '# create and save images', 'if', '(', 'filename_1', 'is', 'None', ')', ':', 'filename_1', '=', 'self', '.', 'filename', '(', ""'-plt1.png'"", ')', 'if', '(', 'filename_2', 'is', 'None', ')', ':', 'filename_2', '=', 'self', '.', 'filename', '(', ""'-plt2.png'"", ')', ""# windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'"", ""# if len(filename_1.split('\\\\\\\\?\\\\')) == 1:"", ""#     filename_1 = '\\\\\\\\?\\\\' + filename_1"", ""# if len(filename_2.split('\\\\\\\\?\\\\')) == 1:"", ""#     filename_2 = '\\\\\\\\?\\\\' + filename_2"", 'if', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'dirname', '(', 'filename_1', ')', ')', 'is', 'False', ':', 'os', '.', 'makedirs', '(', 'os', '.', 'path', '.', 'dirname', '(', 'filename_1', ')', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'dirname', '(', 'filename_2', ')', ')', 'is', 'False', ':', 'os', '.', 'makedirs', '(', 'os', '.', 'path', '.', 'dirname', '(', 'filename_2', ')', ')', 'fig_1', '=', 'Figure', '(', ')', 'canvas_1', '=', 'FigureCanvas', '(', 'fig_1', ')', 'fig_2', '=', 'Figure', '(', ')', 'canvas_2', '=', 'FigureCanvas', '(', 'fig_2', ')', 'self', '.', 'force_update', '(', ')', 'self', '.', 'plot', '(', '[', 'fig_1', ',', 'fig_2', ']', ')', 'if', 'filename_1', 'is', 'not', 'None', 'and', 'not', 'axes_empty', '(', 'fig_1', '.', 'axes', ')', ':', 'fig_1', '.', 'savefig', '(', 'filename_1', ')', 'if', 'filename_2', 'is', 'not', 'None', 'and', 'not', 'axes_empty', '(', 'fig_2', '.', 'axes', ')', ':', 'fig_2', '.', 'savefig', '(', 'filename_2', ')']","creates an image using the scripts plot function and writes it to the disk
        for single plots (plot_type: 'main', 'aux')
            - if no filname provided take default name
        for double plots (plot_type: 'main', 'aux')
            - if no filnames provided take default name
            - if only one filname provided save only the plot for which name is provided
        Args:
            filename_1: filname for figure 1
            filename_2: filname for figure 1

        Returns: None","['creates', 'an', 'image', 'using', 'the', 'scripts', 'plot', 'function', 'and', 'writes', 'it', 'to', 'the', 'disk', 'for', 'single', 'plots', '(', 'plot_type', ':', 'main', 'aux', ')', '-', 'if', 'no', 'filname', 'provided', 'take', 'default', 'name', 'for', 'double', 'plots', '(', 'plot_type', ':', 'main', 'aux', ')', '-', 'if', 'no', 'filnames', 'provided', 'take', 'default', 'name', '-', 'if', 'only', 'one', 'filname', 'provided', 'save', 'only', 'the', 'plot', 'for', 'which', 'name', 'is', 'provided', 'Args', ':', 'filename_1', ':', 'filname', 'for', 'figure', '1', 'filename_2', ':', 'filname', 'for', 'figure', '1']",python,W,0,True,1,train
15302,geophysics-ubonn/reda,lib/reda/exporters/syscal.py,https://github.com/geophysics-ubonn/reda/blob/46a939729e40c7c4723315c03679c40761152e9e/lib/reda/exporters/syscal.py#L5-L19,"def _syscal_write_electrode_coords(fid, spacing, N):
    """"""helper function that writes out electrode positions to a file descriptor

    Parameters
    ----------
    fid: file descriptor
        data is written here
    spacing: float
        spacing of electrodes
    N: int
        number of electrodes
    """"""
    fid.write('# X Y Z\n')
    for i in range(0, N):
        fid.write('{0} {1} {2} {3}\n'.format(i + 1, i * spacing, 0, 0))","['def', '_syscal_write_electrode_coords', '(', 'fid', ',', 'spacing', ',', 'N', ')', ':', 'fid', '.', 'write', '(', ""'# X Y Z\\n'"", ')', 'for', 'i', 'in', 'range', '(', '0', ',', 'N', ')', ':', 'fid', '.', 'write', '(', ""'{0} {1} {2} {3}\\n'"", '.', 'format', '(', 'i', '+', '1', ',', 'i', '*', 'spacing', ',', '0', ',', '0', ')', ')']","helper function that writes out electrode positions to a file descriptor

    Parameters
    ----------
    fid: file descriptor
        data is written here
    spacing: float
        spacing of electrodes
    N: int
        number of electrodes","['helper', 'function', 'that', 'writes', 'out', 'electrode', 'positions', 'to', 'a', 'file', 'descriptor']",python,W,0,True,1,train
15303,geophysics-ubonn/reda,lib/reda/exporters/syscal.py,https://github.com/geophysics-ubonn/reda/blob/46a939729e40c7c4723315c03679c40761152e9e/lib/reda/exporters/syscal.py#L22-L38,"def _syscal_write_quadpoles(fid, quadpoles):
    """"""helper function that writes the actual measurement configurations to a
    file descriptor.

    Parameters
    ----------
    fid: file descriptor
        data is written here
    quadpoles: numpy.ndarray
        measurement configurations

    """"""
    fid.write('# A B M N\n')
    for nr, quadpole in enumerate(quadpoles):
        fid.write(
            '{0} {1} {2} {3} {4}\n'.format(
                nr, quadpole[0], quadpole[1], quadpole[2], quadpole[3]))","['def', '_syscal_write_quadpoles', '(', 'fid', ',', 'quadpoles', ')', ':', 'fid', '.', 'write', '(', ""'# A B M N\\n'"", ')', 'for', 'nr', ',', 'quadpole', 'in', 'enumerate', '(', 'quadpoles', ')', ':', 'fid', '.', 'write', '(', ""'{0} {1} {2} {3} {4}\\n'"", '.', 'format', '(', 'nr', ',', 'quadpole', '[', '0', ']', ',', 'quadpole', '[', '1', ']', ',', 'quadpole', '[', '2', ']', ',', 'quadpole', '[', '3', ']', ')', ')']","helper function that writes the actual measurement configurations to a
    file descriptor.

    Parameters
    ----------
    fid: file descriptor
        data is written here
    quadpoles: numpy.ndarray
        measurement configurations","['helper', 'function', 'that', 'writes', 'the', 'actual', 'measurement', 'configurations', 'to', 'a', 'file', 'descriptor', '.']",python,W,0,True,1,train
17613,lexibank/pylexibank,src/pylexibank/lingpy_util.py,https://github.com/lexibank/pylexibank/blob/c28e7f122f20de1232623dd7003cb5b01535e581/src/pylexibank/lingpy_util.py#L80-L110,"def iter_alignments(dataset, cognate_sets, column='Segments', method='library'):
    """"""
    Function computes automatic alignments and writes them to file.
    """"""
    if not isinstance(dataset, lingpy.basic.parser.QLCParser):
        wordlist = _cldf2wordlist(dataset)
        cognates = {r['Form_ID']: r for r in cognate_sets}
        wordlist.add_entries(
            'cogid',
            'lid',
            lambda x: cognates[x]['Cognateset_ID'] if x in cognates else 0)
        alm = lingpy.Alignments(
            wordlist,
            ref='cogid',
            row='parameter_id',
            col='language_id',
            segments=column.lower())
        alm.align(method=method)
        for k in alm:
            if alm[k, 'lid'] in cognates:
                cognate = cognates[alm[k, 'lid']]
                cognate['Alignment'] = alm[k, 'alignment']
                cognate['Alignment_Method'] = method
    else:
        alm = lingpy.Alignments(dataset, ref='cogid')
        alm.align(method=method)

        for cognate in cognate_sets:
            idx = cognate['ID'] or cognate['Form_ID']
            cognate['Alignment'] = alm[int(idx), 'alignment']
            cognate['Alignment_Method'] = 'SCA-' + method","['def', 'iter_alignments', '(', 'dataset', ',', 'cognate_sets', ',', 'column', '=', ""'Segments'"", ',', 'method', '=', ""'library'"", ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'lingpy', '.', 'basic', '.', 'parser', '.', 'QLCParser', ')', ':', 'wordlist', '=', '_cldf2wordlist', '(', 'dataset', ')', 'cognates', '=', '{', 'r', '[', ""'Form_ID'"", ']', ':', 'r', 'for', 'r', 'in', 'cognate_sets', '}', 'wordlist', '.', 'add_entries', '(', ""'cogid'"", ',', ""'lid'"", ',', 'lambda', 'x', ':', 'cognates', '[', 'x', ']', '[', ""'Cognateset_ID'"", ']', 'if', 'x', 'in', 'cognates', 'else', '0', ')', 'alm', '=', 'lingpy', '.', 'Alignments', '(', 'wordlist', ',', 'ref', '=', ""'cogid'"", ',', 'row', '=', ""'parameter_id'"", ',', 'col', '=', ""'language_id'"", ',', 'segments', '=', 'column', '.', 'lower', '(', ')', ')', 'alm', '.', 'align', '(', 'method', '=', 'method', ')', 'for', 'k', 'in', 'alm', ':', 'if', 'alm', '[', 'k', ',', ""'lid'"", ']', 'in', 'cognates', ':', 'cognate', '=', 'cognates', '[', 'alm', '[', 'k', ',', ""'lid'"", ']', ']', 'cognate', '[', ""'Alignment'"", ']', '=', 'alm', '[', 'k', ',', ""'alignment'"", ']', 'cognate', '[', ""'Alignment_Method'"", ']', '=', 'method', 'else', ':', 'alm', '=', 'lingpy', '.', 'Alignments', '(', 'dataset', ',', 'ref', '=', ""'cogid'"", ')', 'alm', '.', 'align', '(', 'method', '=', 'method', ')', 'for', 'cognate', 'in', 'cognate_sets', ':', 'idx', '=', 'cognate', '[', ""'ID'"", ']', 'or', 'cognate', '[', ""'Form_ID'"", ']', 'cognate', '[', ""'Alignment'"", ']', '=', 'alm', '[', 'int', '(', 'idx', ')', ',', ""'alignment'"", ']', 'cognate', '[', ""'Alignment_Method'"", ']', '=', ""'SCA-'"", '+', 'method']",Function computes automatic alignments and writes them to file.,"['Function', 'computes', 'automatic', 'alignments', 'and', 'writes', 'them', 'to', 'file', '.']",python,W,0,True,1,train
19168,starling-lab/rnlp,rnlp/parse.py,https://github.com/starling-lab/rnlp/blob/72054cc2c0cbaea1d281bf3d56b271d4da29fc4a/rnlp/parse.py#L38-L46,"def _writeBlock(block, blockID):
    '''writes the block to a file with the id'''
    with open(""blockIDs.txt"", ""a"") as fp:
        fp.write(""blockID: "" + str(blockID) + ""\n"")
        sentences = """"
        for sentence in block:
            sentences += sentence+"",""
        fp.write(""block sentences: ""+sentences[:-1]+""\n"")
        fp.write(""\n"")","['def', '_writeBlock', '(', 'block', ',', 'blockID', ')', ':', 'with', 'open', '(', '""blockIDs.txt""', ',', '""a""', ')', 'as', 'fp', ':', 'fp', '.', 'write', '(', '""blockID: ""', '+', 'str', '(', 'blockID', ')', '+', '""\\n""', ')', 'sentences', '=', '""""', 'for', 'sentence', 'in', 'block', ':', 'sentences', '+=', 'sentence', '+', '"",""', 'fp', '.', 'write', '(', '""block sentences: ""', '+', 'sentences', '[', ':', '-', '1', ']', '+', '""\\n""', ')', 'fp', '.', 'write', '(', '""\\n""', ')']",writes the block to a file with the id,"['writes', 'the', 'block', 'to', 'a', 'file', 'with', 'the', 'id']",python,W,0,True,1,train
19169,starling-lab/rnlp,rnlp/parse.py,https://github.com/starling-lab/rnlp/blob/72054cc2c0cbaea1d281bf3d56b271d4da29fc4a/rnlp/parse.py#L49-L54,"def _writeSentenceInBlock(sentence, blockID, sentenceID):
    '''writes the sentence in a block to a file with the id'''
    with open(""sentenceIDs.txt"", ""a"") as fp:
        fp.write(""sentenceID: ""+str(blockID)+""_""+str(sentenceID)+""\n"")
        fp.write(""sentence string: ""+sentence+""\n"")
        fp.write(""\n"")","['def', '_writeSentenceInBlock', '(', 'sentence', ',', 'blockID', ',', 'sentenceID', ')', ':', 'with', 'open', '(', '""sentenceIDs.txt""', ',', '""a""', ')', 'as', 'fp', ':', 'fp', '.', 'write', '(', '""sentenceID: ""', '+', 'str', '(', 'blockID', ')', '+', '""_""', '+', 'str', '(', 'sentenceID', ')', '+', '""\\n""', ')', 'fp', '.', 'write', '(', '""sentence string: ""', '+', 'sentence', '+', '""\\n""', ')', 'fp', '.', 'write', '(', '""\\n""', ')']",writes the sentence in a block to a file with the id,"['writes', 'the', 'sentence', 'in', 'a', 'block', 'to', 'a', 'file', 'with', 'the', 'id']",python,W,0,True,1,train
19170,starling-lab/rnlp,rnlp/parse.py,https://github.com/starling-lab/rnlp/blob/72054cc2c0cbaea1d281bf3d56b271d4da29fc4a/rnlp/parse.py#L57-L63,"def _writeWordFromSentenceInBlock(word, blockID, sentenceID, wordID):
    '''writes the word from a sentence in a block to a file with the id'''
    with open(""wordIDs.txt"", ""a"") as fp:
        fp.write(""wordID: "" + str(blockID) + ""_"" +
                 str(sentenceID) + ""_"" + str(wordID) + ""\n"")
        fp.write(""wordString: "" + word + ""\n"")
        fp.write(""\n"")","['def', '_writeWordFromSentenceInBlock', '(', 'word', ',', 'blockID', ',', 'sentenceID', ',', 'wordID', ')', ':', 'with', 'open', '(', '""wordIDs.txt""', ',', '""a""', ')', 'as', 'fp', ':', 'fp', '.', 'write', '(', '""wordID: ""', '+', 'str', '(', 'blockID', ')', '+', '""_""', '+', 'str', '(', 'sentenceID', ')', '+', '""_""', '+', 'str', '(', 'wordID', ')', '+', '""\\n""', ')', 'fp', '.', 'write', '(', '""wordString: ""', '+', 'word', '+', '""\\n""', ')', 'fp', '.', 'write', '(', '""\\n""', ')']",writes the word from a sentence in a block to a file with the id,"['writes', 'the', 'word', 'from', 'a', 'sentence', 'in', 'a', 'block', 'to', 'a', 'file', 'with', 'the', 'id']",python,W,0,True,1,train
22007,lsst-sqre/documenteer,documenteer/bin/refreshlsstbib.py,https://github.com/lsst-sqre/documenteer/blob/75f02901a80042b28d074df1cc1dca32eb8e38c8/documenteer/bin/refreshlsstbib.py#L73-L115,"def process_bib_files(local_dir):
    """"""Run the refresh-lsst-bib program's logic: iterates through bib URLs,
    downloads the file from GitHub, and writes it to a local directory.

    Parameters
    ----------
    local_dir : `str`
        Directory to write bib files into.

    Returns
    -------
    error_count : `int`
        Number of download errors.
    """"""
    logger = logging.getLogger(__name__)

    # check the output directory exists
    if not os.path.isdir(local_dir):
        logger.error('Output directory ""{}"" does not exist'.format(local_dir))
        sys.exit(1)

    root_blob_url = ('https://raw.githubusercontent.com/lsst/lsst-texmf/'
                     'master/texmf/bibtex/bib/')
    bib_filenames = ['books.bib', 'lsst-dm.bib', 'lsst.bib', 'refs.bib',
                     'refs_ads.bib']

    error_count = 0
    for bib_filename in bib_filenames:
        url = urllib.parse.urljoin(root_blob_url, bib_filename)
        logger.info('Downloading {}'.format(url))
        try:
            content = _get_content(url)
        except requests.HTTPError as e:
            logger.exception(str(e))
            logger.warning('Could not download {}'.format(url))
            error_count += 1
            continue

        local_filename = os.path.join(local_dir, bib_filename)
        with open(local_filename, 'w') as f:
            f.write(content)

    return error_count","['def', 'process_bib_files', '(', 'local_dir', ')', ':', 'logger', '=', 'logging', '.', 'getLogger', '(', '__name__', ')', '# check the output directory exists', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'local_dir', ')', ':', 'logger', '.', 'error', '(', '\'Output directory ""{}"" does not exist\'', '.', 'format', '(', 'local_dir', ')', ')', 'sys', '.', 'exit', '(', '1', ')', 'root_blob_url', '=', '(', ""'https://raw.githubusercontent.com/lsst/lsst-texmf/'"", ""'master/texmf/bibtex/bib/'"", ')', 'bib_filenames', '=', '[', ""'books.bib'"", ',', ""'lsst-dm.bib'"", ',', ""'lsst.bib'"", ',', ""'refs.bib'"", ',', ""'refs_ads.bib'"", ']', 'error_count', '=', '0', 'for', 'bib_filename', 'in', 'bib_filenames', ':', 'url', '=', 'urllib', '.', 'parse', '.', 'urljoin', '(', 'root_blob_url', ',', 'bib_filename', ')', 'logger', '.', 'info', '(', ""'Downloading {}'"", '.', 'format', '(', 'url', ')', ')', 'try', ':', 'content', '=', '_get_content', '(', 'url', ')', 'except', 'requests', '.', 'HTTPError', 'as', 'e', ':', 'logger', '.', 'exception', '(', 'str', '(', 'e', ')', ')', 'logger', '.', 'warning', '(', ""'Could not download {}'"", '.', 'format', '(', 'url', ')', ')', 'error_count', '+=', '1', 'continue', 'local_filename', '=', 'os', '.', 'path', '.', 'join', '(', 'local_dir', ',', 'bib_filename', ')', 'with', 'open', '(', 'local_filename', ',', ""'w'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'content', ')', 'return', 'error_count']","Run the refresh-lsst-bib program's logic: iterates through bib URLs,
    downloads the file from GitHub, and writes it to a local directory.

    Parameters
    ----------
    local_dir : `str`
        Directory to write bib files into.

    Returns
    -------
    error_count : `int`
        Number of download errors.","['Run', 'the', 'refresh', '-', 'lsst', '-', 'bib', 'program', 's', 'logic', ':', 'iterates', 'through', 'bib', 'URLs', 'downloads', 'the', 'file', 'from', 'GitHub', 'and', 'writes', 'it', 'to', 'a', 'local', 'directory', '.']",python,W,0,True,1,train
24187,Kortemme-Lab/klab,klab/benchmarking/analysis/ddg_monomeric_stability_analysis.py,https://github.com/Kortemme-Lab/klab/blob/6d410ad08f1bd9f7cbbb28d7d946e94fbaaa2b6b/klab/benchmarking/analysis/ddg_monomeric_stability_analysis.py#L1188-L1459,"def calculate_metrics(self, analysis_set = '', analysis_directory = None, drop_missing = True, case_n_cutoff = 5, verbose = True):
        '''Calculates the main metrics for the benchmark run and writes them to file and LaTeX object.'''

        dataframe = self.dataframe
        if drop_missing:
            dataframe = dataframe.dropna(subset=['Predicted'])

        if self.calculate_scalar_adjustments:
            scalar_adjustment = self.scalar_adjustments[analysis_set]
        experimental_field = BenchmarkRun.get_analysis_set_fieldname('Experimental', analysis_set)

        self.metric_latex_objects.append( lr.LatexPageSection('Data tables', None, True) )
        intro_text = lr.LatexText( text = self.ddg_analysis_type_description )
        header_row = ['Statistic name', '{Value}', 'p-value']
        stats_column_format = ['l', 'S[table-format=3.2]', 'l']

        if self.include_derived_mutations:
            running_analysis_str = '\nDerived mutations in analysis are included):'
        else:
            running_analysis_str = '\nDerived mutations in analysis are omitted):'
        intro_text.add_text(running_analysis_str)
        if verbose:
            self.report(running_analysis_str, fn = colortext.message)

        classification_cutoffs_str = 'The stability classification cutoffs are: Experimental=%0.2f kcal/mol, Predicted=%0.2f energy units.' % (self.stability_classication_x_cutoff, self.stability_classication_y_cutoff)
        intro_text.add_text( classification_cutoffs_str )
        if verbose:
            self.report(classification_cutoffs_str, fn = colortext.warning)

        self.metric_latex_objects.append( intro_text )

        amino_acid_details, CAA, PAA, HAA = self.amino_acid_details, self.CAA, self.PAA, self.HAA

        # This dict is used for the print-statement below
        volume_groups = {}
        for aa_code, aa_details in amino_acid_details.iteritems():
            v = int(aa_details['van der Waals volume']) # Note: I only convert to int here to match the old script behavior and because all volumes are integer values so it does not do any harm
            volume_groups[v] = volume_groups.get(v, [])
            volume_groups[v].append(aa_code)

        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Breakdown by volume',
            'A case is considered a small-to-large (resp. large-to-small) mutation if all of the wildtype residues have a smaller (resp. larger) van der Waals volume than the corresponding mutant residue. The order is defined as %s so some cases are considered to have no change in volume e.g. MET -> LEU.' % (' < '.join([''.join(sorted(v)) for k, v in sorted(volume_groups.iteritems())]))
        ) )
        for subcase in ('XX', 'SL', 'LS'):
            subcase_dataframe = dataframe[dataframe['VolumeChange'] == subcase]
            table_header = 'Statistics - %s (%d cases)' % (BenchmarkRun.by_volume_descriptions[subcase], len(subcase_dataframe))
            if len(subcase_dataframe) >= 8:
                list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
                section_latex_objs.append( lr.LatexTable(
                    header_row,
                    list_stats,
                    column_format = stats_column_format,
                    header_text = table_header
                ))
                self.add_stored_metric_to_df(BenchmarkRun.by_volume_descriptions[subcase], len(subcase_dataframe), list_stats)
            else:
                section_latex_objs.append( lr.LatexText(
                    'Not enough data for analysis of mutations ''%s'' (at least 8 cases are required).' % BenchmarkRun.by_volume_descriptions[subcase]
                ))
        if verbose:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )


        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Mutations to alanine',
            'And mutations not to alanine'
        ))
        subcase_dataframe = dataframe[dataframe['MutantAA'] == 'A']
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - all mutations to alanine (including multiple mutations, if they are all to alanine) (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('all mutations to alanine', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[(dataframe['MutantAA'] == 'A') & (dataframe['NumberOfMutations'] == 1)]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - single mutations to alanine (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('single mutations to alanine', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[(dataframe['MutantAA'] == 'A') & (dataframe['NumberOfMutations'] != 1)]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - multiple mutations to alanine (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('multiple mutations to alanine', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[dataframe['MutantAA'] != 'A']
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - mutations to anything other than alanine (including multiple mutations that include a non-alanine mutation) (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('mutations not to alanine', len(subcase_dataframe), list_stats)
        if verbose and len(section_latex_objs) > 0:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )


        section_latex_objs = []
        subcase_dataframe = dataframe[dataframe['HasGPMutation'] == 1]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - cases with G or P (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('cases with G or P', len(subcase_dataframe), list_stats)

        subcase_dataframe = dataframe[dataframe['HasGPMutation'] == 0]
        if len(subcase_dataframe) > 0:
            table_header = 'Statistics - cases without G or P (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_y_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('cases without G or P', len(subcase_dataframe), list_stats)

        if len(section_latex_objs) > 0:
            section_latex_objs.insert( 0, lr.LatexSubSection(
                'Separating out mutations involving glycine or proline.',
                'This cases may involve changes to secondary structure so we separate them out here.'
            ))

        if verbose and len(section_latex_objs) > 0:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )

        #### Single mutations
        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Number of mutations',
        ))
        subcase_dataframe = dataframe[dataframe['NumberOfMutations'] == 1]
        if len(subcase_dataframe) >= case_n_cutoff:
            table_header = 'Statistics - single mutations (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('single mutations', len(subcase_dataframe), list_stats)
        subcase_dataframe = dataframe[dataframe['NumberOfMutations'] > 1]
        if len(subcase_dataframe) >= case_n_cutoff:
            table_header = 'Statistics - multiple mutations (%d cases)' % len(subcase_dataframe)
            list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('multiple mutations', len(subcase_dataframe), list_stats)
        # subcase_dataframe = dataframe[(dataframe.NumberOfMutations >= 2) & (dataframe.NumberOfMutations <= 5)]
        # if len(subcase_dataframe) >= case_n_cutoff:
        #     table_header = 'Statistics - 2-4 mutations (%d cases)' % len(subcase_dataframe)
        #     list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
        #     section_latex_objs.append( LatexTable(
        #         header_row,
        #         list_stats,
        #         column_format = stats_column_format,
        #         header_text = table_header
        #     ))
        #     self.add_stored_metric_to_df('2-4 mutations', len(subcase_dataframe), list_stats)
        # mutation_cutoffs = [5, 10, 20, 50, 100, 200]
        # for i, mutation_cutoff in enumerate(mutation_cutoffs):
        #     if len(mutation_cutoffs) - 1 == i:
        #         break
        #     next_cutoff = mutation_cutoffs[i+1]
        #     subcase_dataframe = dataframe[(dataframe.NumberOfMutations >= mutation_cutoff) & (dataframe.NumberOfMutations <= next_cutoff)]
        #     if len(subcase_dataframe) >= case_n_cutoff:
        #         table_header = 'Statistics - %d $<=$ number of mutations $<=$ %d (%d cases)' % (mutation_cutoff, next_cutoff, len(subcase_dataframe))
        #         list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
        #         section_latex_objs.append( LatexTable(
        #             header_row,
        #             list_stats,
        #             column_format = stats_column_format,
        #             header_text = table_header
        #         ))
        #         self.add_stored_metric_to_df('%d <= mutations<= %d' % (mutation_cutoff, next_cutoff), len(subcase_dataframe), list_stats)
        if verbose:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )
        ####

        #### Complete dataset (scaled)
        if self.calculate_scalar_adjustments:
            section_latex_objs = []
            section_latex_objs.append( lr.LatexSubSection(
                'Entire dataset using a scaling factor of 1/%.03f to improve the fraction correct metric.' % scalar_adjustment,
                'Warning: Results in this section use an averaged scaling factor to improve the value for the fraction correct metric. This scalar will vary over benchmark runs so these results should not be interpreted as performance results; they should be considered as what could be obtained if the predicted values were scaled by a ""magic"" value.'
            ))
            table_header = 'Statistics - complete dataset (scaled) (%d cases)' % len(dataframe)
            # For these statistics, we assume that we have reduced any scaling issues and use the same cutoff for the Y-axis as the user specified for the X-axis
            list_stats = format_stats(get_xy_dataset_statistics_pandas(dataframe, experimental_field, BenchmarkRun.get_analysis_set_fieldname('Predicted_adj', analysis_set), fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
            section_latex_objs.append( lr.LatexTable(
                header_row,
                list_stats,
                column_format = stats_column_format,
                header_text = table_header
            ))
            self.add_stored_metric_to_df('complete dataset (scaled)', len(dataframe), list_stats)
            if verbose:
                self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
            self.metric_latex_objects.extend( section_latex_objs )
        ####

        section_latex_objs = []
        section_latex_objs.append( lr.LatexSubSection(
            'Entire dataset',
            'Overall statistics'
        ))
        table_header = 'Statistics - complete dataset (%d cases)' % len(dataframe)
        # For these statistics, we assume that we have reduced any scaling issues and use the same cutoff for the Y-axis as the user specified for the X-axis
        list_stats = format_stats(get_xy_dataset_statistics_pandas(dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)
        section_latex_objs.append( lr.LatexTable(
            header_row,
            list_stats,
            column_format = stats_column_format,
            header_text = table_header
        ))
        self.add_stored_metric_to_df('complete dataset', len(dataframe), list_stats)
        if verbose:
            self.report('\n'.join([x.generate_plaintext() for x in section_latex_objs]), fn = colortext.sprint)
        self.metric_latex_objects.extend( section_latex_objs )

        # There is probably a better way of writing the pandas code here
        record_with_most_errors = (dataframe[['PDBFileID', 'NumberOfDerivativeErrors', 'Mutations']].sort_values(by = 'NumberOfDerivativeErrors')).tail(1)
        record_index = record_with_most_errors.index.tolist()[0]
        pdb_id, num_errors, mutation_str = dataframe.loc[record_index, 'PDBFileID'], dataframe.loc[record_index, 'NumberOfDerivativeErrors'], dataframe.loc[record_index, 'Mutations']
        if num_errors > 0:
            error_detection_text = '\n\nDerivative errors were found in the run. Record #{0} - {1}, {2} - has the most amount ({3}) of derivative errors.'.format(record_index, pdb_id, mutation_str, num_errors)
            self.metric_latex_objects.append( lr.LatexText(error_detection_text, color = 'red') )
            if verbose:
                self.report(error_detection_text, fn = colortext.warning)

        # Write the analysis to file
        self.create_analysis_directory(analysis_directory)
        self.metrics_filepath = os.path.join(self.analysis_directory, '{0}_metrics.txt'.format(self.benchmark_run_name))
        write_file(self.metrics_filepath, '\n'.join([x.generate_plaintext() for x in self.metric_latex_objects]))","['def', 'calculate_metrics', '(', 'self', ',', 'analysis_set', '=', ""''"", ',', 'analysis_directory', '=', 'None', ',', 'drop_missing', '=', 'True', ',', 'case_n_cutoff', '=', '5', ',', 'verbose', '=', 'True', ')', ':', 'dataframe', '=', 'self', '.', 'dataframe', 'if', 'drop_missing', ':', 'dataframe', '=', 'dataframe', '.', 'dropna', '(', 'subset', '=', '[', ""'Predicted'"", ']', ')', 'if', 'self', '.', 'calculate_scalar_adjustments', ':', 'scalar_adjustment', '=', 'self', '.', 'scalar_adjustments', '[', 'analysis_set', ']', 'experimental_field', '=', 'BenchmarkRun', '.', 'get_analysis_set_fieldname', '(', ""'Experimental'"", ',', 'analysis_set', ')', 'self', '.', 'metric_latex_objects', '.', 'append', '(', 'lr', '.', 'LatexPageSection', '(', ""'Data tables'"", ',', 'None', ',', 'True', ')', ')', 'intro_text', '=', 'lr', '.', 'LatexText', '(', 'text', '=', 'self', '.', 'ddg_analysis_type_description', ')', 'header_row', '=', '[', ""'Statistic name'"", ',', ""'{Value}'"", ',', ""'p-value'"", ']', 'stats_column_format', '=', '[', ""'l'"", ',', ""'S[table-format=3.2]'"", ',', ""'l'"", ']', 'if', 'self', '.', 'include_derived_mutations', ':', 'running_analysis_str', '=', ""'\\nDerived mutations in analysis are included):'"", 'else', ':', 'running_analysis_str', '=', ""'\\nDerived mutations in analysis are omitted):'"", 'intro_text', '.', 'add_text', '(', 'running_analysis_str', ')', 'if', 'verbose', ':', 'self', '.', 'report', '(', 'running_analysis_str', ',', 'fn', '=', 'colortext', '.', 'message', ')', 'classification_cutoffs_str', '=', ""'The stability classification cutoffs are: Experimental=%0.2f kcal/mol, Predicted=%0.2f energy units.'"", '%', '(', 'self', '.', 'stability_classication_x_cutoff', ',', 'self', '.', 'stability_classication_y_cutoff', ')', 'intro_text', '.', 'add_text', '(', 'classification_cutoffs_str', ')', 'if', 'verbose', ':', 'self', '.', 'report', '(', 'classification_cutoffs_str', ',', 'fn', '=', 'colortext', '.', 'warning', ')', 'self', '.', 'metric_latex_objects', '.', 'append', '(', 'intro_text', ')', 'amino_acid_details', ',', 'CAA', ',', 'PAA', ',', 'HAA', '=', 'self', '.', 'amino_acid_details', ',', 'self', '.', 'CAA', ',', 'self', '.', 'PAA', ',', 'self', '.', 'HAA', '# This dict is used for the print-statement below', 'volume_groups', '=', '{', '}', 'for', 'aa_code', ',', 'aa_details', 'in', 'amino_acid_details', '.', 'iteritems', '(', ')', ':', 'v', '=', 'int', '(', 'aa_details', '[', ""'van der Waals volume'"", ']', ')', '# Note: I only convert to int here to match the old script behavior and because all volumes are integer values so it does not do any harm', 'volume_groups', '[', 'v', ']', '=', 'volume_groups', '.', 'get', '(', 'v', ',', '[', ']', ')', 'volume_groups', '[', 'v', ']', '.', 'append', '(', 'aa_code', ')', 'section_latex_objs', '=', '[', ']', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexSubSection', '(', ""'Breakdown by volume'"", ',', ""'A case is considered a small-to-large (resp. large-to-small) mutation if all of the wildtype residues have a smaller (resp. larger) van der Waals volume than the corresponding mutant residue. The order is defined as %s so some cases are considered to have no change in volume e.g. MET -> LEU.'"", '%', '(', ""' < '"", '.', 'join', '(', '[', ""''"", '.', 'join', '(', 'sorted', '(', 'v', ')', ')', 'for', 'k', ',', 'v', 'in', 'sorted', '(', 'volume_groups', '.', 'iteritems', '(', ')', ')', ']', ')', ')', ')', ')', 'for', 'subcase', 'in', '(', ""'XX'"", ',', ""'SL'"", ',', ""'LS'"", ')', ':', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'VolumeChange'"", ']', '==', 'subcase', ']', 'table_header', '=', ""'Statistics - %s (%d cases)'"", '%', '(', 'BenchmarkRun', '.', 'by_volume_descriptions', '[', 'subcase', ']', ',', 'len', '(', 'subcase_dataframe', ')', ')', 'if', 'len', '(', 'subcase_dataframe', ')', '>=', '8', ':', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', 'BenchmarkRun', '.', 'by_volume_descriptions', '[', 'subcase', ']', ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'else', ':', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexText', '(', ""'Not enough data for analysis of mutations '"", ""'%s'"", ""' (at least 8 cases are required).'"", '%', 'BenchmarkRun', '.', 'by_volume_descriptions', '[', 'subcase', ']', ')', ')', 'if', 'verbose', ':', 'self', '.', 'report', '(', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'section_latex_objs', ']', ')', ',', 'fn', '=', 'colortext', '.', 'sprint', ')', 'self', '.', 'metric_latex_objects', '.', 'extend', '(', 'section_latex_objs', ')', 'section_latex_objs', '=', '[', ']', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexSubSection', '(', ""'Mutations to alanine'"", ',', ""'And mutations not to alanine'"", ')', ')', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'MutantAA'"", ']', '==', ""'A'"", ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>', '0', ':', 'table_header', '=', ""'Statistics - all mutations to alanine (including multiple mutations, if they are all to alanine) (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'all mutations to alanine'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'subcase_dataframe', '=', 'dataframe', '[', '(', 'dataframe', '[', ""'MutantAA'"", ']', '==', ""'A'"", ')', '&', '(', 'dataframe', '[', ""'NumberOfMutations'"", ']', '==', '1', ')', ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>', '0', ':', 'table_header', '=', ""'Statistics - single mutations to alanine (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'single mutations to alanine'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'subcase_dataframe', '=', 'dataframe', '[', '(', 'dataframe', '[', ""'MutantAA'"", ']', '==', ""'A'"", ')', '&', '(', 'dataframe', '[', ""'NumberOfMutations'"", ']', '!=', '1', ')', ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>', '0', ':', 'table_header', '=', ""'Statistics - multiple mutations to alanine (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'multiple mutations to alanine'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'MutantAA'"", ']', '!=', ""'A'"", ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>', '0', ':', 'table_header', '=', ""'Statistics - mutations to anything other than alanine (including multiple mutations that include a non-alanine mutation) (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'mutations not to alanine'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'if', 'verbose', 'and', 'len', '(', 'section_latex_objs', ')', '>', '0', ':', 'self', '.', 'report', '(', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'section_latex_objs', ']', ')', ',', 'fn', '=', 'colortext', '.', 'sprint', ')', 'self', '.', 'metric_latex_objects', '.', 'extend', '(', 'section_latex_objs', ')', 'section_latex_objs', '=', '[', ']', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'HasGPMutation'"", ']', '==', '1', ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>', '0', ':', 'table_header', '=', ""'Statistics - cases with G or P (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'cases with G or P'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'HasGPMutation'"", ']', '==', '0', ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>', '0', ':', 'table_header', '=', ""'Statistics - cases without G or P (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_y_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'cases without G or P'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'if', 'len', '(', 'section_latex_objs', ')', '>', '0', ':', 'section_latex_objs', '.', 'insert', '(', '0', ',', 'lr', '.', 'LatexSubSection', '(', ""'Separating out mutations involving glycine or proline.'"", ',', ""'This cases may involve changes to secondary structure so we separate them out here.'"", ')', ')', 'if', 'verbose', 'and', 'len', '(', 'section_latex_objs', ')', '>', '0', ':', 'self', '.', 'report', '(', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'section_latex_objs', ']', ')', ',', 'fn', '=', 'colortext', '.', 'sprint', ')', 'self', '.', 'metric_latex_objects', '.', 'extend', '(', 'section_latex_objs', ')', '#### Single mutations', 'section_latex_objs', '=', '[', ']', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexSubSection', '(', ""'Number of mutations'"", ',', ')', ')', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'NumberOfMutations'"", ']', '==', '1', ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>=', 'case_n_cutoff', ':', 'table_header', '=', ""'Statistics - single mutations (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'single mutations'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', 'subcase_dataframe', '=', 'dataframe', '[', 'dataframe', '[', ""'NumberOfMutations'"", ']', '>', '1', ']', 'if', 'len', '(', 'subcase_dataframe', ')', '>=', 'case_n_cutoff', ':', 'table_header', '=', ""'Statistics - multiple mutations (%d cases)'"", '%', 'len', '(', 'subcase_dataframe', ')', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'subcase_dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'multiple mutations'"", ',', 'len', '(', 'subcase_dataframe', ')', ',', 'list_stats', ')', '# subcase_dataframe = dataframe[(dataframe.NumberOfMutations >= 2) & (dataframe.NumberOfMutations <= 5)]', '# if len(subcase_dataframe) >= case_n_cutoff:', ""#     table_header = 'Statistics - 2-4 mutations (%d cases)' % len(subcase_dataframe)"", ""#     list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)"", '#     section_latex_objs.append( LatexTable(', '#         header_row,', '#         list_stats,', '#         column_format = stats_column_format,', '#         header_text = table_header', '#     ))', ""#     self.add_stored_metric_to_df('2-4 mutations', len(subcase_dataframe), list_stats)"", '# mutation_cutoffs = [5, 10, 20, 50, 100, 200]', '# for i, mutation_cutoff in enumerate(mutation_cutoffs):', '#     if len(mutation_cutoffs) - 1 == i:', '#         break', '#     next_cutoff = mutation_cutoffs[i+1]', '#     subcase_dataframe = dataframe[(dataframe.NumberOfMutations >= mutation_cutoff) & (dataframe.NumberOfMutations <= next_cutoff)]', '#     if len(subcase_dataframe) >= case_n_cutoff:', ""#         table_header = 'Statistics - %d $<=$ number of mutations $<=$ %d (%d cases)' % (mutation_cutoff, next_cutoff, len(subcase_dataframe))"", ""#         list_stats = format_stats(get_xy_dataset_statistics_pandas(subcase_dataframe, experimental_field, 'Predicted', fcorrect_x_cutoff = self.stability_classication_x_cutoff, fcorrect_y_cutoff = self.stability_classication_x_cutoff, ignore_null_values = True, run_standardized_analysis = False), return_string = False)"", '#         section_latex_objs.append( LatexTable(', '#             header_row,', '#             list_stats,', '#             column_format = stats_column_format,', '#             header_text = table_header', '#         ))', ""#         self.add_stored_metric_to_df('%d <= mutations<= %d' % (mutation_cutoff, next_cutoff), len(subcase_dataframe), list_stats)"", 'if', 'verbose', ':', 'self', '.', 'report', '(', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'section_latex_objs', ']', ')', ',', 'fn', '=', 'colortext', '.', 'sprint', ')', 'self', '.', 'metric_latex_objects', '.', 'extend', '(', 'section_latex_objs', ')', '####', '#### Complete dataset (scaled)', 'if', 'self', '.', 'calculate_scalar_adjustments', ':', 'section_latex_objs', '=', '[', ']', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexSubSection', '(', ""'Entire dataset using a scaling factor of 1/%.03f to improve the fraction correct metric.'"", '%', 'scalar_adjustment', ',', '\'Warning: Results in this section use an averaged scaling factor to improve the value for the fraction correct metric. This scalar will vary over benchmark runs so these results should not be interpreted as performance results; they should be considered as what could be obtained if the predicted values were scaled by a ""magic"" value.\'', ')', ')', 'table_header', '=', ""'Statistics - complete dataset (scaled) (%d cases)'"", '%', 'len', '(', 'dataframe', ')', '# For these statistics, we assume that we have reduced any scaling issues and use the same cutoff for the Y-axis as the user specified for the X-axis', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'dataframe', ',', 'experimental_field', ',', 'BenchmarkRun', '.', 'get_analysis_set_fieldname', '(', ""'Predicted_adj'"", ',', 'analysis_set', ')', ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'complete dataset (scaled)'"", ',', 'len', '(', 'dataframe', ')', ',', 'list_stats', ')', 'if', 'verbose', ':', 'self', '.', 'report', '(', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'section_latex_objs', ']', ')', ',', 'fn', '=', 'colortext', '.', 'sprint', ')', 'self', '.', 'metric_latex_objects', '.', 'extend', '(', 'section_latex_objs', ')', '####', 'section_latex_objs', '=', '[', ']', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexSubSection', '(', ""'Entire dataset'"", ',', ""'Overall statistics'"", ')', ')', 'table_header', '=', ""'Statistics - complete dataset (%d cases)'"", '%', 'len', '(', 'dataframe', ')', '# For these statistics, we assume that we have reduced any scaling issues and use the same cutoff for the Y-axis as the user specified for the X-axis', 'list_stats', '=', 'format_stats', '(', 'get_xy_dataset_statistics_pandas', '(', 'dataframe', ',', 'experimental_field', ',', ""'Predicted'"", ',', 'fcorrect_x_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'fcorrect_y_cutoff', '=', 'self', '.', 'stability_classication_x_cutoff', ',', 'ignore_null_values', '=', 'True', ',', 'run_standardized_analysis', '=', 'False', ')', ',', 'return_string', '=', 'False', ')', 'section_latex_objs', '.', 'append', '(', 'lr', '.', 'LatexTable', '(', 'header_row', ',', 'list_stats', ',', 'column_format', '=', 'stats_column_format', ',', 'header_text', '=', 'table_header', ')', ')', 'self', '.', 'add_stored_metric_to_df', '(', ""'complete dataset'"", ',', 'len', '(', 'dataframe', ')', ',', 'list_stats', ')', 'if', 'verbose', ':', 'self', '.', 'report', '(', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'section_latex_objs', ']', ')', ',', 'fn', '=', 'colortext', '.', 'sprint', ')', 'self', '.', 'metric_latex_objects', '.', 'extend', '(', 'section_latex_objs', ')', '# There is probably a better way of writing the pandas code here', 'record_with_most_errors', '=', '(', 'dataframe', '[', '[', ""'PDBFileID'"", ',', ""'NumberOfDerivativeErrors'"", ',', ""'Mutations'"", ']', ']', '.', 'sort_values', '(', 'by', '=', ""'NumberOfDerivativeErrors'"", ')', ')', '.', 'tail', '(', '1', ')', 'record_index', '=', 'record_with_most_errors', '.', 'index', '.', 'tolist', '(', ')', '[', '0', ']', 'pdb_id', ',', 'num_errors', ',', 'mutation_str', '=', 'dataframe', '.', 'loc', '[', 'record_index', ',', ""'PDBFileID'"", ']', ',', 'dataframe', '.', 'loc', '[', 'record_index', ',', ""'NumberOfDerivativeErrors'"", ']', ',', 'dataframe', '.', 'loc', '[', 'record_index', ',', ""'Mutations'"", ']', 'if', 'num_errors', '>', '0', ':', 'error_detection_text', '=', ""'\\n\\nDerivative errors were found in the run. Record #{0} - {1}, {2} - has the most amount ({3}) of derivative errors.'"", '.', 'format', '(', 'record_index', ',', 'pdb_id', ',', 'mutation_str', ',', 'num_errors', ')', 'self', '.', 'metric_latex_objects', '.', 'append', '(', 'lr', '.', 'LatexText', '(', 'error_detection_text', ',', 'color', '=', ""'red'"", ')', ')', 'if', 'verbose', ':', 'self', '.', 'report', '(', 'error_detection_text', ',', 'fn', '=', 'colortext', '.', 'warning', ')', '# Write the analysis to file', 'self', '.', 'create_analysis_directory', '(', 'analysis_directory', ')', 'self', '.', 'metrics_filepath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'analysis_directory', ',', ""'{0}_metrics.txt'"", '.', 'format', '(', 'self', '.', 'benchmark_run_name', ')', ')', 'write_file', '(', 'self', '.', 'metrics_filepath', ',', ""'\\n'"", '.', 'join', '(', '[', 'x', '.', 'generate_plaintext', '(', ')', 'for', 'x', 'in', 'self', '.', 'metric_latex_objects', ']', ')', ')']",Calculates the main metrics for the benchmark run and writes them to file and LaTeX object.,"['Calculates', 'the', 'main', 'metrics', 'for', 'the', 'benchmark', 'run', 'and', 'writes', 'them', 'to', 'file', 'and', 'LaTeX', 'object', '.']",python,W,0,True,1,train
26084,mgoral/subconvert,src/subconvert/utils/SubFile.py,https://github.com/mgoral/subconvert/blob/59701e5e69ef1ca26ce7d1d766c936664aa2cb32/src/subconvert/utils/SubFile.py#L117-L159,"def _writeFile(cls, filePath, content, encoding = None):
        """"""Safe file writing. Most common mistakes are checked against and reported before write
        operation. After that, if anything unexpected happens, user won't be left without data or
        with corrupted one as this method writes to a temporary file and then simply renames it
        (which should be atomic operation according to POSIX but who knows how Ext4 really works.
        @see: http://lwn.net/Articles/322823/).""""""

        filePath = os.path.realpath(filePath)
        log.debug(_(""Real file path to write: %s"" % filePath))

        if encoding is None:
            encoding = File.DEFAULT_ENCODING

        try:
            encodedContent = ''.join(content).encode(encoding)
        except LookupError as msg:
            raise SubFileError(_(""Unknown encoding name: '%s'."") % encoding)
        except UnicodeEncodeError:
            raise SubFileError(
                _(""There are some characters in '%(file)s' that cannot be encoded to '%(enc)s'."")
                % {""file"": filePath, ""enc"": encoding})

        tmpFilePath = ""%s.tmp"" % filePath
        bakFilePath = ""%s.bak"" % filePath
        with open(tmpFilePath, 'wb') as f:
            f.write(encodedContent)
            # ensure that all data is on disk.
            # for performance reasons, we skip os.fsync(f.fileno())
            f.flush()

        try:
            os.rename(filePath, bakFilePath)
        except FileNotFoundError:
            # there's nothing to move when filePath doesn't exist
            # note the Python bug: http://bugs.python.org/issue16074
            pass

        os.rename(tmpFilePath, filePath)

        try:
            os.unlink(bakFilePath)
        except FileNotFoundError:
            pass","['def', '_writeFile', '(', 'cls', ',', 'filePath', ',', 'content', ',', 'encoding', '=', 'None', ')', ':', 'filePath', '=', 'os', '.', 'path', '.', 'realpath', '(', 'filePath', ')', 'log', '.', 'debug', '(', '_', '(', '""Real file path to write: %s""', '%', 'filePath', ')', ')', 'if', 'encoding', 'is', 'None', ':', 'encoding', '=', 'File', '.', 'DEFAULT_ENCODING', 'try', ':', 'encodedContent', '=', ""''"", '.', 'join', '(', 'content', ')', '.', 'encode', '(', 'encoding', ')', 'except', 'LookupError', 'as', 'msg', ':', 'raise', 'SubFileError', '(', '_', '(', '""Unknown encoding name: \'%s\'.""', ')', '%', 'encoding', ')', 'except', 'UnicodeEncodeError', ':', 'raise', 'SubFileError', '(', '_', '(', '""There are some characters in \'%(file)s\' that cannot be encoded to \'%(enc)s\'.""', ')', '%', '{', '""file""', ':', 'filePath', ',', '""enc""', ':', 'encoding', '}', ')', 'tmpFilePath', '=', '""%s.tmp""', '%', 'filePath', 'bakFilePath', '=', '""%s.bak""', '%', 'filePath', 'with', 'open', '(', 'tmpFilePath', ',', ""'wb'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'encodedContent', ')', '# ensure that all data is on disk.', '# for performance reasons, we skip os.fsync(f.fileno())', 'f', '.', 'flush', '(', ')', 'try', ':', 'os', '.', 'rename', '(', 'filePath', ',', 'bakFilePath', ')', 'except', 'FileNotFoundError', ':', ""# there's nothing to move when filePath doesn't exist"", '# note the Python bug: http://bugs.python.org/issue16074', 'pass', 'os', '.', 'rename', '(', 'tmpFilePath', ',', 'filePath', ')', 'try', ':', 'os', '.', 'unlink', '(', 'bakFilePath', ')', 'except', 'FileNotFoundError', ':', 'pass']","Safe file writing. Most common mistakes are checked against and reported before write
        operation. After that, if anything unexpected happens, user won't be left without data or
        with corrupted one as this method writes to a temporary file and then simply renames it
        (which should be atomic operation according to POSIX but who knows how Ext4 really works.
        @see: http://lwn.net/Articles/322823/).","['Safe', 'file', 'writing', '.', 'Most', 'common', 'mistakes', 'are', 'checked', 'against', 'and', 'reported', 'before', 'write', 'operation', '.', 'After', 'that', 'if', 'anything', 'unexpected', 'happens', 'user', 'won', 't', 'be', 'left', 'without', 'data', 'or', 'with', 'corrupted', 'one', 'as', 'this', 'method', 'writes', 'to', 'a', 'temporary', 'file', 'and', 'then', 'simply', 'renames', 'it', '(', 'which', 'should', 'be', 'atomic', 'operation', 'according', 'to', 'POSIX', 'but', 'who', 'knows', 'how', 'Ext4', 'really', 'works', '.']",python,W,0,True,1,train
29649,lowandrew/OLCTools,databasesetup/database_setup.py,https://github.com/lowandrew/OLCTools/blob/88aa90ac85f84d0bbeb03e43c29b0a9d36e4ce2a/databasesetup/database_setup.py#L438-L455,"def database_clone(targetcall, databasepath, complete=False):
        """"""
        Checks to see if the database has already been downloaded. If not, runs the system call to
        download the database, and writes stdout and stderr to the logfile
        :param targetcall: system call to download, and possibly set-up the database
        :param databasepath: absolute path of the database
        :param complete: boolean variable to determine whether the complete file should be created
        """"""
        # Create a file to store the logs; it will be used to determine if the database was downloaded and set-up
        completefile = os.path.join(databasepath, 'complete')
        # Run the system call if the database is not already downloaded
        if not os.path.isfile(completefile):
            out, err = run_subprocess(targetcall)
            if complete:
                # Create the database completeness assessment file and populate it with the out and err streams
                with open(completefile, 'w') as complete:
                    complete.write(out)
                    complete.write(err)","['def', 'database_clone', '(', 'targetcall', ',', 'databasepath', ',', 'complete', '=', 'False', ')', ':', '# Create a file to store the logs; it will be used to determine if the database was downloaded and set-up', 'completefile', '=', 'os', '.', 'path', '.', 'join', '(', 'databasepath', ',', ""'complete'"", ')', '# Run the system call if the database is not already downloaded', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'completefile', ')', ':', 'out', ',', 'err', '=', 'run_subprocess', '(', 'targetcall', ')', 'if', 'complete', ':', '# Create the database completeness assessment file and populate it with the out and err streams', 'with', 'open', '(', 'completefile', ',', ""'w'"", ')', 'as', 'complete', ':', 'complete', '.', 'write', '(', 'out', ')', 'complete', '.', 'write', '(', 'err', ')']","Checks to see if the database has already been downloaded. If not, runs the system call to
        download the database, and writes stdout and stderr to the logfile
        :param targetcall: system call to download, and possibly set-up the database
        :param databasepath: absolute path of the database
        :param complete: boolean variable to determine whether the complete file should be created","['Checks', 'to', 'see', 'if', 'the', 'database', 'has', 'already', 'been', 'downloaded', '.', 'If', 'not', 'runs', 'the', 'system', 'call', 'to', 'download', 'the', 'database', 'and', 'writes', 'stdout', 'and', 'stderr', 'to', 'the', 'logfile', ':', 'param', 'targetcall', ':', 'system', 'call', 'to', 'download', 'and', 'possibly', 'set', '-', 'up', 'the', 'database', ':', 'param', 'databasepath', ':', 'absolute', 'path', 'of', 'the', 'database', ':', 'param', 'complete', ':', 'boolean', 'variable', 'to', 'determine', 'whether', 'the', 'complete', 'file', 'should', 'be', 'created']",python,W,0,True,1,train
708,VikParuchuri/percept,percept/datahandlers/formatters.py,https://github.com/VikParuchuri/percept/blob/90304ba82053e2a9ad2bacaab3479403d3923bcf/percept/datahandlers/formatters.py#L105-L120,"def to_dataframe(self):
        """"""
        Reads the common format self.data and writes out to a dataframe.
        """"""
        keys = self.data[0].keys()
        column_list =[]
        for k in keys:
            key_list = []
            for i in xrange(0,len(self.data)):
                key_list.append(self.data[i][k])
            column_list.append(key_list)
        df = DataFrame(np.asarray(column_list).transpose(), columns=keys)
        for i in xrange(0,df.shape[1]):
            if is_number(df.iloc[:,i]):
                df.iloc[:,i] = df.iloc[:,i].astype(float)
        return df","['def', 'to_dataframe', '(', 'self', ')', ':', 'keys', '=', 'self', '.', 'data', '[', '0', ']', '.', 'keys', '(', ')', 'column_list', '=', '[', ']', 'for', 'k', 'in', 'keys', ':', 'key_list', '=', '[', ']', 'for', 'i', 'in', 'xrange', '(', '0', ',', 'len', '(', 'self', '.', 'data', ')', ')', ':', 'key_list', '.', 'append', '(', 'self', '.', 'data', '[', 'i', ']', '[', 'k', ']', ')', 'column_list', '.', 'append', '(', 'key_list', ')', 'df', '=', 'DataFrame', '(', 'np', '.', 'asarray', '(', 'column_list', ')', '.', 'transpose', '(', ')', ',', 'columns', '=', 'keys', ')', 'for', 'i', 'in', 'xrange', '(', '0', ',', 'df', '.', 'shape', '[', '1', ']', ')', ':', 'if', 'is_number', '(', 'df', '.', 'iloc', '[', ':', ',', 'i', ']', ')', ':', 'df', '.', 'iloc', '[', ':', ',', 'i', ']', '=', 'df', '.', 'iloc', '[', ':', ',', 'i', ']', '.', 'astype', '(', 'float', ')', 'return', 'df']",Reads the common format self.data and writes out to a dataframe.,"['Reads', 'the', 'common', 'format', 'self', '.', 'data', 'and', 'writes', 'out', 'to', 'a', 'dataframe', '.']",python,W,0,True,1,train
780,bioidiap/bob.ip.facedetect,bob/ip/facedetect/train/TrainingSet.py,https://github.com/bioidiap/bob.ip.facedetect/blob/601da5141ca7302ad36424d1421b33190ba46779/bob/ip/facedetect/train/TrainingSet.py#L157-L292,"def extract(self, sampler, feature_extractor, number_of_examples_per_scale = (100, 100), similarity_thresholds = (0.5, 0.8), parallel = None, mirror = False, use_every_nth_negative_scale = 1):
    """"""Extracts features from **all** images in **all** scales and writes them to file.

    This function iterates over all images that are present in the internally stored list, and extracts features using the given ``feature_extractor`` for every image patch that the given ``sampler`` returns.
    The final features will be stored in the ``feature_directory`` that is set in the constructor.

    For each image, the ``sampler`` samples patch locations, which cover the whole image in different scales.
    For each patch locations is tested, how similar they are to the face bounding boxes that belong to that image, using the Jaccard :py:meth:`BoundingBox.similarity`.
    The similarity is compared to the ``similarity_thresholds``.
    If it is smaller than the first threshold, the patch is considered as background, when it is greater the the second threshold, it is considered as a face, otherwise it is rejected.
    Depending on the image resolution and the number of bounding boxes, this will usually result in some positive and thousands of negative patches per image.
    To limit the total amount of training data, for all scales, only up to a given number of positive and negative patches are kept.
    Also, to further limit the number of negative samples, only every ``use_every_nth_negative_scale`` scale is considered (for the positives, always all scales are processed).

    To increase the number (especially of positive) examples, features can also be extracted for horizontally mirrored images.
    Simply set the ``mirror`` parameter to ``True``.
    Furthermore, this function is designed to be run using several parallel processes, e.g., using the `GridTK <https://pypi.python.org/pypi/gridtk>`_.
    Each of the processes will run on a particular subset of the images, which is defined by the ``SGE_TASK_ID`` environment variable.
    The ``parallel`` parameter defines the total number of parallel processes that are used.

    **Parameters:**

    ``sampler`` : :py:class:`Sampler`
      The sampler to use to sample patches of the images. Please assure that the sampler is set up such that it samples patch locations which can overlap with the face locations.

    ``feature_extractor`` : :py:class:`FeatureExtractor`
      The feature extractor to be used to extract features from image patches

    ``number_of_examples_per_scale`` : (int, int)
      The maximum number of positive and negative examples to extract for each scale of the image

    ``similarity_thresholds`` : (float, float)
      The Jaccard similarity threshold, below which patch locations are considered to be negative, and above which patch locations are considered to be positive examples.

    ``parallel`` : int or ``None``
      If given, the total number of parallel processes, which are used to extract features (the current process index is read from the ``SGE_TASK_ID`` environment variable)

    ``mirror`` : bool
      Extract positive and negative samples also from horizontally mirrored images?

    ``use_every_nth_negative_scale`` : int
      Skip some negative scales to decrease the number of negative examples, i.e., only extract and store negative features, when ``scale_counter % use_every_nth_negative_scale == 0``

      .. note::
         The ``scale_counter`` is not reset between images, so that we might get features from different scales in subsequent images.
    """"""

    feature_file = self._feature_file(parallel)
    bob.io.base.create_directories_safe(self.feature_directory)

    if parallel is None or ""SGE_TASK_ID"" not in os.environ or os.environ[""SGE_TASK_ID""] == '1':
      extractor_file = os.path.join(self.feature_directory, ""Extractor.hdf5"")
      hdf5 = bob.io.base.HDF5File(extractor_file, ""w"")
      feature_extractor.save(hdf5)
      del hdf5

    total_positives, total_negatives = 0, 0

    indices = parallel_part(range(len(self)), parallel)
    if not indices:
      logger.warning(""The index range for the current parallel thread is empty."")
    else:
      logger.info(""Extracting features for images in range %d - %d of %d"", indices[0], indices[-1], len(self))

    hdf5 = bob.io.base.HDF5File(feature_file, ""w"")
    for index in indices:
      hdf5.create_group(""Image-%d"" % index)
      hdf5.cd(""Image-%d"" % index)

      logger.debug(""Processing file %d of %d: %s"", index+1, indices[-1]+1, self.image_paths[index])

      # load image
      image = bob.io.base.load(self.image_paths[index])
      if image.ndim == 3:
        image = bob.ip.color.rgb_to_gray(image)
      # get ground_truth bounding boxes
      ground_truth = self.bounding_boxes[index]

      # collect image and GT for originally and mirrored image
      images = [image] if not mirror else [image, bob.ip.base.flop(image)]
      ground_truths = [ground_truth] if not mirror else [ground_truth, [gt.mirror_x(image.shape[1]) for gt in ground_truth]]
      parts = ""om""

      # now, sample
      scale_counter = -1
      for image, ground_truth, part in zip(images, ground_truths, parts):
        for scale, scaled_image_shape in sampler.scales(image):
          scale_counter += 1
          scaled_gt = [gt.scale(scale) for gt in ground_truth]
          positives = []
          negatives = []
          # iterate over all possible positions in the image
          for bb in sampler.sample_scaled(scaled_image_shape):
            # check if the patch is a positive example
            positive = False
            negative = True
            for gt in scaled_gt:
              similarity = bb.similarity(gt)
              if similarity > similarity_thresholds[1]:
                positive = True
                break
              if similarity > similarity_thresholds[0]:
                negative = False
                break

            if positive:
              positives.append(bb)
            elif negative and scale_counter % use_every_nth_negative_scale == 0:
              negatives.append(bb)

          # per scale, limit the number of positive and negative samples
          positives = [positives[i] for i in quasi_random_indices(len(positives), number_of_examples_per_scale[0])]
          negatives = [negatives[i] for i in quasi_random_indices(len(negatives), number_of_examples_per_scale[1])]

          # extract features
          feature_extractor.prepare(image, scale)
          # .. negative features
          if negatives:
            negative_features = numpy.zeros((len(negatives), feature_extractor.number_of_features), numpy.uint16)
            for i, bb in enumerate(negatives):
              feature_extractor.extract_all(bb, negative_features, i)
            hdf5.set(""Negatives-%s-%.5f"" % (part,scale), negative_features)
            total_negatives += len(negatives)

          # positive features
          if positives:
            positive_features = numpy.zeros((len(positives), feature_extractor.number_of_features), numpy.uint16)
            for i, bb in enumerate(positives):
              feature_extractor.extract_all(bb, positive_features, i)
            hdf5.set(""Positives-%s-%.5f"" % (part,scale), positive_features)
            total_positives += len(positives)
      # cd backwards after each image
      hdf5.cd("".."")

    hdf5.set(""TotalPositives"", total_positives)
    hdf5.set(""TotalNegatives"", total_negatives)","['def', 'extract', '(', 'self', ',', 'sampler', ',', 'feature_extractor', ',', 'number_of_examples_per_scale', '=', '(', '100', ',', '100', ')', ',', 'similarity_thresholds', '=', '(', '0.5', ',', '0.8', ')', ',', 'parallel', '=', 'None', ',', 'mirror', '=', 'False', ',', 'use_every_nth_negative_scale', '=', '1', ')', ':', 'feature_file', '=', 'self', '.', '_feature_file', '(', 'parallel', ')', 'bob', '.', 'io', '.', 'base', '.', 'create_directories_safe', '(', 'self', '.', 'feature_directory', ')', 'if', 'parallel', 'is', 'None', 'or', '""SGE_TASK_ID""', 'not', 'in', 'os', '.', 'environ', 'or', 'os', '.', 'environ', '[', '""SGE_TASK_ID""', ']', '==', ""'1'"", ':', 'extractor_file', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'feature_directory', ',', '""Extractor.hdf5""', ')', 'hdf5', '=', 'bob', '.', 'io', '.', 'base', '.', 'HDF5File', '(', 'extractor_file', ',', '""w""', ')', 'feature_extractor', '.', 'save', '(', 'hdf5', ')', 'del', 'hdf5', 'total_positives', ',', 'total_negatives', '=', '0', ',', '0', 'indices', '=', 'parallel_part', '(', 'range', '(', 'len', '(', 'self', ')', ')', ',', 'parallel', ')', 'if', 'not', 'indices', ':', 'logger', '.', 'warning', '(', '""The index range for the current parallel thread is empty.""', ')', 'else', ':', 'logger', '.', 'info', '(', '""Extracting features for images in range %d - %d of %d""', ',', 'indices', '[', '0', ']', ',', 'indices', '[', '-', '1', ']', ',', 'len', '(', 'self', ')', ')', 'hdf5', '=', 'bob', '.', 'io', '.', 'base', '.', 'HDF5File', '(', 'feature_file', ',', '""w""', ')', 'for', 'index', 'in', 'indices', ':', 'hdf5', '.', 'create_group', '(', '""Image-%d""', '%', 'index', ')', 'hdf5', '.', 'cd', '(', '""Image-%d""', '%', 'index', ')', 'logger', '.', 'debug', '(', '""Processing file %d of %d: %s""', ',', 'index', '+', '1', ',', 'indices', '[', '-', '1', ']', '+', '1', ',', 'self', '.', 'image_paths', '[', 'index', ']', ')', '# load image', 'image', '=', 'bob', '.', 'io', '.', 'base', '.', 'load', '(', 'self', '.', 'image_paths', '[', 'index', ']', ')', 'if', 'image', '.', 'ndim', '==', '3', ':', 'image', '=', 'bob', '.', 'ip', '.', 'color', '.', 'rgb_to_gray', '(', 'image', ')', '# get ground_truth bounding boxes', 'ground_truth', '=', 'self', '.', 'bounding_boxes', '[', 'index', ']', '# collect image and GT for originally and mirrored image', 'images', '=', '[', 'image', ']', 'if', 'not', 'mirror', 'else', '[', 'image', ',', 'bob', '.', 'ip', '.', 'base', '.', 'flop', '(', 'image', ')', ']', 'ground_truths', '=', '[', 'ground_truth', ']', 'if', 'not', 'mirror', 'else', '[', 'ground_truth', ',', '[', 'gt', '.', 'mirror_x', '(', 'image', '.', 'shape', '[', '1', ']', ')', 'for', 'gt', 'in', 'ground_truth', ']', ']', 'parts', '=', '""om""', '# now, sample', 'scale_counter', '=', '-', '1', 'for', 'image', ',', 'ground_truth', ',', 'part', 'in', 'zip', '(', 'images', ',', 'ground_truths', ',', 'parts', ')', ':', 'for', 'scale', ',', 'scaled_image_shape', 'in', 'sampler', '.', 'scales', '(', 'image', ')', ':', 'scale_counter', '+=', '1', 'scaled_gt', '=', '[', 'gt', '.', 'scale', '(', 'scale', ')', 'for', 'gt', 'in', 'ground_truth', ']', 'positives', '=', '[', ']', 'negatives', '=', '[', ']', '# iterate over all possible positions in the image', 'for', 'bb', 'in', 'sampler', '.', 'sample_scaled', '(', 'scaled_image_shape', ')', ':', '# check if the patch is a positive example', 'positive', '=', 'False', 'negative', '=', 'True', 'for', 'gt', 'in', 'scaled_gt', ':', 'similarity', '=', 'bb', '.', 'similarity', '(', 'gt', ')', 'if', 'similarity', '>', 'similarity_thresholds', '[', '1', ']', ':', 'positive', '=', 'True', 'break', 'if', 'similarity', '>', 'similarity_thresholds', '[', '0', ']', ':', 'negative', '=', 'False', 'break', 'if', 'positive', ':', 'positives', '.', 'append', '(', 'bb', ')', 'elif', 'negative', 'and', 'scale_counter', '%', 'use_every_nth_negative_scale', '==', '0', ':', 'negatives', '.', 'append', '(', 'bb', ')', '# per scale, limit the number of positive and negative samples', 'positives', '=', '[', 'positives', '[', 'i', ']', 'for', 'i', 'in', 'quasi_random_indices', '(', 'len', '(', 'positives', ')', ',', 'number_of_examples_per_scale', '[', '0', ']', ')', ']', 'negatives', '=', '[', 'negatives', '[', 'i', ']', 'for', 'i', 'in', 'quasi_random_indices', '(', 'len', '(', 'negatives', ')', ',', 'number_of_examples_per_scale', '[', '1', ']', ')', ']', '# extract features', 'feature_extractor', '.', 'prepare', '(', 'image', ',', 'scale', ')', '# .. negative features', 'if', 'negatives', ':', 'negative_features', '=', 'numpy', '.', 'zeros', '(', '(', 'len', '(', 'negatives', ')', ',', 'feature_extractor', '.', 'number_of_features', ')', ',', 'numpy', '.', 'uint16', ')', 'for', 'i', ',', 'bb', 'in', 'enumerate', '(', 'negatives', ')', ':', 'feature_extractor', '.', 'extract_all', '(', 'bb', ',', 'negative_features', ',', 'i', ')', 'hdf5', '.', 'set', '(', '""Negatives-%s-%.5f""', '%', '(', 'part', ',', 'scale', ')', ',', 'negative_features', ')', 'total_negatives', '+=', 'len', '(', 'negatives', ')', '# positive features', 'if', 'positives', ':', 'positive_features', '=', 'numpy', '.', 'zeros', '(', '(', 'len', '(', 'positives', ')', ',', 'feature_extractor', '.', 'number_of_features', ')', ',', 'numpy', '.', 'uint16', ')', 'for', 'i', ',', 'bb', 'in', 'enumerate', '(', 'positives', ')', ':', 'feature_extractor', '.', 'extract_all', '(', 'bb', ',', 'positive_features', ',', 'i', ')', 'hdf5', '.', 'set', '(', '""Positives-%s-%.5f""', '%', '(', 'part', ',', 'scale', ')', ',', 'positive_features', ')', 'total_positives', '+=', 'len', '(', 'positives', ')', '# cd backwards after each image', 'hdf5', '.', 'cd', '(', '""..""', ')', 'hdf5', '.', 'set', '(', '""TotalPositives""', ',', 'total_positives', ')', 'hdf5', '.', 'set', '(', '""TotalNegatives""', ',', 'total_negatives', ')']","Extracts features from **all** images in **all** scales and writes them to file.

    This function iterates over all images that are present in the internally stored list, and extracts features using the given ``feature_extractor`` for every image patch that the given ``sampler`` returns.
    The final features will be stored in the ``feature_directory`` that is set in the constructor.

    For each image, the ``sampler`` samples patch locations, which cover the whole image in different scales.
    For each patch locations is tested, how similar they are to the face bounding boxes that belong to that image, using the Jaccard :py:meth:`BoundingBox.similarity`.
    The similarity is compared to the ``similarity_thresholds``.
    If it is smaller than the first threshold, the patch is considered as background, when it is greater the the second threshold, it is considered as a face, otherwise it is rejected.
    Depending on the image resolution and the number of bounding boxes, this will usually result in some positive and thousands of negative patches per image.
    To limit the total amount of training data, for all scales, only up to a given number of positive and negative patches are kept.
    Also, to further limit the number of negative samples, only every ``use_every_nth_negative_scale`` scale is considered (for the positives, always all scales are processed).

    To increase the number (especially of positive) examples, features can also be extracted for horizontally mirrored images.
    Simply set the ``mirror`` parameter to ``True``.
    Furthermore, this function is designed to be run using several parallel processes, e.g., using the `GridTK <https://pypi.python.org/pypi/gridtk>`_.
    Each of the processes will run on a particular subset of the images, which is defined by the ``SGE_TASK_ID`` environment variable.
    The ``parallel`` parameter defines the total number of parallel processes that are used.

    **Parameters:**

    ``sampler`` : :py:class:`Sampler`
      The sampler to use to sample patches of the images. Please assure that the sampler is set up such that it samples patch locations which can overlap with the face locations.

    ``feature_extractor`` : :py:class:`FeatureExtractor`
      The feature extractor to be used to extract features from image patches

    ``number_of_examples_per_scale`` : (int, int)
      The maximum number of positive and negative examples to extract for each scale of the image

    ``similarity_thresholds`` : (float, float)
      The Jaccard similarity threshold, below which patch locations are considered to be negative, and above which patch locations are considered to be positive examples.

    ``parallel`` : int or ``None``
      If given, the total number of parallel processes, which are used to extract features (the current process index is read from the ``SGE_TASK_ID`` environment variable)

    ``mirror`` : bool
      Extract positive and negative samples also from horizontally mirrored images?

    ``use_every_nth_negative_scale`` : int
      Skip some negative scales to decrease the number of negative examples, i.e., only extract and store negative features, when ``scale_counter % use_every_nth_negative_scale == 0``

      .. note::
         The ``scale_counter`` is not reset between images, so that we might get features from different scales in subsequent images.","['Extracts', 'features', 'from', '**', 'all', '**', 'images', 'in', '**', 'all', '**', 'scales', 'and', 'writes', 'them', 'to', 'file', '.']",python,W,0,True,1,train
3037,hollenstein/maspy,maspy/auxiliary.py,https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L175-L193,"def writeJsonZipfile(filelike, data, compress=True, mode='w', name='data'):
    """"""Serializes the objects contained in data to a JSON formated string and
    writes it to a zipfile.

    :param filelike: path to a file (str) or a file-like object
    :param data: object that should be converted to a JSON formated string.
        Objects and types in data must be supported by the json.JSONEncoder or
        have the method ``._reprJSON()`` defined.
    :param compress: bool, True to use zip file compression
    :param mode: 'w' to truncate and write a new file, or 'a' to append to an
        existing file
    :param name: the file name that will be given to the JSON output in the
        archive
    """"""
    zipcomp = zipfile.ZIP_DEFLATED if compress else zipfile.ZIP_STORED
    with zipfile.ZipFile(filelike, mode, allowZip64=True) as containerFile:
        containerFile.writestr(name, json.dumps(data, cls=MaspyJsonEncoder),
                               zipcomp
                               )","['def', 'writeJsonZipfile', '(', 'filelike', ',', 'data', ',', 'compress', '=', 'True', ',', 'mode', '=', ""'w'"", ',', 'name', '=', ""'data'"", ')', ':', 'zipcomp', '=', 'zipfile', '.', 'ZIP_DEFLATED', 'if', 'compress', 'else', 'zipfile', '.', 'ZIP_STORED', 'with', 'zipfile', '.', 'ZipFile', '(', 'filelike', ',', 'mode', ',', 'allowZip64', '=', 'True', ')', 'as', 'containerFile', ':', 'containerFile', '.', 'writestr', '(', 'name', ',', 'json', '.', 'dumps', '(', 'data', ',', 'cls', '=', 'MaspyJsonEncoder', ')', ',', 'zipcomp', ')']","Serializes the objects contained in data to a JSON formated string and
    writes it to a zipfile.

    :param filelike: path to a file (str) or a file-like object
    :param data: object that should be converted to a JSON formated string.
        Objects and types in data must be supported by the json.JSONEncoder or
        have the method ``._reprJSON()`` defined.
    :param compress: bool, True to use zip file compression
    :param mode: 'w' to truncate and write a new file, or 'a' to append to an
        existing file
    :param name: the file name that will be given to the JSON output in the
        archive","['Serializes', 'the', 'objects', 'contained', 'in', 'data', 'to', 'a', 'JSON', 'formated', 'string', 'and', 'writes', 'it', 'to', 'a', 'zipfile', '.']",python,W,0,True,1,train
3040,hollenstein/maspy,maspy/auxiliary.py,https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L266-L287,"def _dumpArrayToFile(filelike, array):
    """"""Serializes a 1-dimensional ``numpy.array`` to bytes, writes the bytes to
    the filelike object and returns a dictionary with metadata, necessary to
    restore the ``numpy.array`` from the file.

    :param filelike: can be a file or a file-like object that provides the
        methods ``.write()`` and ``.tell()``.
    :param array: a 1-dimensional ``numpy.array``

    :returns: a metadata dictionary ::
        {'start': start position in the file, 'end': end position in the file,
         'size': size of the array, 'dtype': numpy data type of the array
         }
    """"""
    bytedata = array.tobytes('C')
    start = filelike.tell()
    end = start + len(bytedata)
    metadata = {'start': start, 'end': end, 'size': array.size,
                'dtype': array.dtype.name
                }
    filelike.write(bytedata)
    return metadata","['def', '_dumpArrayToFile', '(', 'filelike', ',', 'array', ')', ':', 'bytedata', '=', 'array', '.', 'tobytes', '(', ""'C'"", ')', 'start', '=', 'filelike', '.', 'tell', '(', ')', 'end', '=', 'start', '+', 'len', '(', 'bytedata', ')', 'metadata', '=', '{', ""'start'"", ':', 'start', ',', ""'end'"", ':', 'end', ',', ""'size'"", ':', 'array', '.', 'size', ',', ""'dtype'"", ':', 'array', '.', 'dtype', '.', 'name', '}', 'filelike', '.', 'write', '(', 'bytedata', ')', 'return', 'metadata']","Serializes a 1-dimensional ``numpy.array`` to bytes, writes the bytes to
    the filelike object and returns a dictionary with metadata, necessary to
    restore the ``numpy.array`` from the file.

    :param filelike: can be a file or a file-like object that provides the
        methods ``.write()`` and ``.tell()``.
    :param array: a 1-dimensional ``numpy.array``

    :returns: a metadata dictionary ::
        {'start': start position in the file, 'end': end position in the file,
         'size': size of the array, 'dtype': numpy data type of the array
         }","['Serializes', 'a', '1', '-', 'dimensional', 'numpy', '.', 'array', 'to', 'bytes', 'writes', 'the', 'bytes', 'to', 'the', 'filelike', 'object', 'and', 'returns', 'a', 'dictionary', 'with', 'metadata', 'necessary', 'to', 'restore', 'the', 'numpy', '.', 'array', 'from', 'the', 'file', '.']",python,W,0,True,1,train
3041,hollenstein/maspy,maspy/auxiliary.py,https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L290-L312,"def _dumpNdarrayToFile(filelike, ndarray):
    """"""Serializes an N-dimensional ``numpy.array`` to bytes, writes the bytes to
    the filelike object and returns a dictionary with metadata, necessary to
    restore the ``numpy.array`` from the file.

    :param filelike: can be a file or a file-like object that provides the
        methods ``.write()`` and ``.tell()``.
    :param ndarray: a N-dimensional ``numpy.array``

    :returns: a metadata dictionary ::
        {'start': start position in the file, 'end': end position in the file,
         'size': size of the array, 'dtype': numpy data type of the array,
         'shape': description of the array shape
         }
    """"""
    bytedata = ndarray.tobytes('C')
    start = filelike.tell()
    end = start + len(bytedata)
    metadata = {'start': start, 'end': end, 'size': ndarray.size,
                'dtype': ndarray.dtype.name, 'shape': ndarray.shape
                }
    filelike.write(bytedata)
    return metadata","['def', '_dumpNdarrayToFile', '(', 'filelike', ',', 'ndarray', ')', ':', 'bytedata', '=', 'ndarray', '.', 'tobytes', '(', ""'C'"", ')', 'start', '=', 'filelike', '.', 'tell', '(', ')', 'end', '=', 'start', '+', 'len', '(', 'bytedata', ')', 'metadata', '=', '{', ""'start'"", ':', 'start', ',', ""'end'"", ':', 'end', ',', ""'size'"", ':', 'ndarray', '.', 'size', ',', ""'dtype'"", ':', 'ndarray', '.', 'dtype', '.', 'name', ',', ""'shape'"", ':', 'ndarray', '.', 'shape', '}', 'filelike', '.', 'write', '(', 'bytedata', ')', 'return', 'metadata']","Serializes an N-dimensional ``numpy.array`` to bytes, writes the bytes to
    the filelike object and returns a dictionary with metadata, necessary to
    restore the ``numpy.array`` from the file.

    :param filelike: can be a file or a file-like object that provides the
        methods ``.write()`` and ``.tell()``.
    :param ndarray: a N-dimensional ``numpy.array``

    :returns: a metadata dictionary ::
        {'start': start position in the file, 'end': end position in the file,
         'size': size of the array, 'dtype': numpy data type of the array,
         'shape': description of the array shape
         }","['Serializes', 'an', 'N', '-', 'dimensional', 'numpy', '.', 'array', 'to', 'bytes', 'writes', 'the', 'bytes', 'to', 'the', 'filelike', 'object', 'and', 'returns', 'a', 'dictionary', 'with', 'metadata', 'necessary', 'to', 'restore', 'the', 'numpy', '.', 'array', 'from', 'the', 'file', '.']",python,W,0,True,1,train
7992,agrc/agrc.python,agrc/logging.py,https://github.com/agrc/agrc.python/blob/be427e919bd4cdd6f19524b7f7fe18882429c25b/agrc/logging.py#L51-L59,"def writeLogToFile(self):
        """"""
        writes the log to a
        """"""
        if not os.path.exists(self.logFolder):
            os.mkdir(self.logFolder)

        with open(self.logFile, mode='a') as f:
            f.write('\n\n' + self.log)","['def', 'writeLogToFile', '(', 'self', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'self', '.', 'logFolder', ')', ':', 'os', '.', 'mkdir', '(', 'self', '.', 'logFolder', ')', 'with', 'open', '(', 'self', '.', 'logFile', ',', 'mode', '=', ""'a'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', ""'\\n\\n'"", '+', 'self', '.', 'log', ')']",writes the log to a,"['writes', 'the', 'log', 'to', 'a']",python,W,0,True,1,train
13671,pkgw/pwkit,pwkit/cli/latexdriver.py,https://github.com/pkgw/pwkit/blob/d40957a1c3d2ea34e7ceac2267ee9635135f2793/pwkit/cli/latexdriver.py#L105-L123,"def write_bibtex_dict(stream, entries):
    """"""bibtexparser.write converts the entire database to one big string and
    writes it out in one go. I'm sure it will always all fit in RAM but some
    things just will not stand.

    """"""
    from bibtexparser.bwriter import BibTexWriter

    writer = BibTexWriter()
    writer.indent = '  '
    writer.entry_separator = ''
    first = True

    for rec in entries:
        if first:
            first = False
        else:
            stream.write(b'\n')
        stream.write(writer._entry_to_bibtex(rec).encode('utf8'))","['def', 'write_bibtex_dict', '(', 'stream', ',', 'entries', ')', ':', 'from', 'bibtexparser', '.', 'bwriter', 'import', 'BibTexWriter', 'writer', '=', 'BibTexWriter', '(', ')', 'writer', '.', 'indent', '=', ""'  '"", 'writer', '.', 'entry_separator', '=', ""''"", 'first', '=', 'True', 'for', 'rec', 'in', 'entries', ':', 'if', 'first', ':', 'first', '=', 'False', 'else', ':', 'stream', '.', 'write', '(', ""b'\\n'"", ')', 'stream', '.', 'write', '(', 'writer', '.', '_entry_to_bibtex', '(', 'rec', ')', '.', 'encode', '(', ""'utf8'"", ')', ')']","bibtexparser.write converts the entire database to one big string and
    writes it out in one go. I'm sure it will always all fit in RAM but some
    things just will not stand.","['bibtexparser', '.', 'write', 'converts', 'the', 'entire', 'database', 'to', 'one', 'big', 'string', 'and', 'writes', 'it', 'out', 'in', 'one', 'go', '.', 'I', 'm', 'sure', 'it', 'will', 'always', 'all', 'fit', 'in', 'RAM', 'but', 'some', 'things', 'just', 'will', 'not', 'stand', '.']",python,W,0,True,1,train
19909,arne-cl/discoursegraphs,src/discoursegraphs/readwrite/neo4j.py,https://github.com/arne-cl/discoursegraphs/blob/842f0068a3190be2c75905754521b176b25a54fb/src/discoursegraphs/readwrite/neo4j.py#L51-L60,"def write_geoff(discoursegraph, output_file):
    """"""
    converts a DiscourseDocumentGraph into a Geoff file and
    writes it to the given file (or file path).
    """"""
    if isinstance(output_file, str):
        with open(output_file, 'w') as outfile:
            outfile.write(convert_to_geoff(discoursegraph))
    else:  # output_file is a file object
        output_file.write(convert_to_geoff(discoursegraph))","['def', 'write_geoff', '(', 'discoursegraph', ',', 'output_file', ')', ':', 'if', 'isinstance', '(', 'output_file', ',', 'str', ')', ':', 'with', 'open', '(', 'output_file', ',', ""'w'"", ')', 'as', 'outfile', ':', 'outfile', '.', 'write', '(', 'convert_to_geoff', '(', 'discoursegraph', ')', ')', 'else', ':', '# output_file is a file object', 'output_file', '.', 'write', '(', 'convert_to_geoff', '(', 'discoursegraph', ')', ')']","converts a DiscourseDocumentGraph into a Geoff file and
    writes it to the given file (or file path).","['converts', 'a', 'DiscourseDocumentGraph', 'into', 'a', 'Geoff', 'file', 'and', 'writes', 'it', 'to', 'the', 'given', 'file', '(', 'or', 'file', 'path', ')', '.']",python,W,0,True,1,train
19915,arne-cl/discoursegraphs,src/discoursegraphs/readwrite/graphml.py,https://github.com/arne-cl/discoursegraphs/blob/842f0068a3190be2c75905754521b176b25a54fb/src/discoursegraphs/readwrite/graphml.py#L16-L25,"def write_graphml(docgraph, output_file):
    """"""
    takes a document graph, converts it into GraphML format and writes it to
    a file.
    """"""
    dg_copy = deepcopy(docgraph)
    layerset2str(dg_copy)
    attriblist2str(dg_copy)
    remove_root_metadata(dg_copy)
    nx_write_graphml(dg_copy, output_file)","['def', 'write_graphml', '(', 'docgraph', ',', 'output_file', ')', ':', 'dg_copy', '=', 'deepcopy', '(', 'docgraph', ')', 'layerset2str', '(', 'dg_copy', ')', 'attriblist2str', '(', 'dg_copy', ')', 'remove_root_metadata', '(', 'dg_copy', ')', 'nx_write_graphml', '(', 'dg_copy', ',', 'output_file', ')']","takes a document graph, converts it into GraphML format and writes it to
    a file.","['takes', 'a', 'document', 'graph', 'converts', 'it', 'into', 'GraphML', 'format', 'and', 'writes', 'it', 'to', 'a', 'file', '.']",python,W,0,True,1,train
19974,arne-cl/discoursegraphs,src/discoursegraphs/readwrite/exmaralda.py,https://github.com/arne-cl/discoursegraphs/blob/842f0068a3190be2c75905754521b176b25a54fb/src/discoursegraphs/readwrite/exmaralda.py#L483-L496,"def write_exb(docgraph, output_file):
    """"""
    converts a DiscourseDocumentGraph into an Exmaralda ``*.exb`` file and
    writes it to the given file (or file path).
    """"""
    exmaralda_file = ExmaraldaFile(docgraph)
    assert isinstance(output_file, (str, file))
    if isinstance(output_file, str):
        path_to_file = os.path.dirname(output_file)
        if not os.path.isdir(path_to_file):
            create_dir(path_to_file)
        exmaralda_file.write(output_file)
    else:  # output_file is a file object
        output_file.write(exmaralda_file.__str__())","['def', 'write_exb', '(', 'docgraph', ',', 'output_file', ')', ':', 'exmaralda_file', '=', 'ExmaraldaFile', '(', 'docgraph', ')', 'assert', 'isinstance', '(', 'output_file', ',', '(', 'str', ',', 'file', ')', ')', 'if', 'isinstance', '(', 'output_file', ',', 'str', ')', ':', 'path_to_file', '=', 'os', '.', 'path', '.', 'dirname', '(', 'output_file', ')', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'path_to_file', ')', ':', 'create_dir', '(', 'path_to_file', ')', 'exmaralda_file', '.', 'write', '(', 'output_file', ')', 'else', ':', '# output_file is a file object', 'output_file', '.', 'write', '(', 'exmaralda_file', '.', '__str__', '(', ')', ')']","converts a DiscourseDocumentGraph into an Exmaralda ``*.exb`` file and
    writes it to the given file (or file path).","['converts', 'a', 'DiscourseDocumentGraph', 'into', 'an', 'Exmaralda', '*', '.', 'exb', 'file', 'and', 'writes', 'it', 'to', 'the', 'given', 'file', '(', 'or', 'file', 'path', ')', '.']",python,W,0,True,1,train
20211,arne-cl/discoursegraphs,src/discoursegraphs/readwrite/conll.py,https://github.com/arne-cl/discoursegraphs/blob/842f0068a3190be2c75905754521b176b25a54fb/src/discoursegraphs/readwrite/conll.py#L504-L522,"def write_conll(docgraph, output_file, coreference_layer=None,
                markable_layer=None):
    """"""
    converts a DiscourseDocumentGraph into a tab-separated CoNLL 2009 file and
    writes it to the given file (or file path).
    """"""
    if markable_layer is None:
        markable_layer = docgraph.ns+':markable'
    conll_file = Conll2009File(docgraph,
                               coreference_layer=coreference_layer,
                               markable_layer=markable_layer)
    assert isinstance(output_file, (str, file))
    if isinstance(output_file, str):
        path_to_file = os.path.dirname(output_file)
        if not os.path.isdir(path_to_file):
            create_dir(path_to_file)
        conll_file.write(output_file)
    else:  # output_file is a file object
        output_file.write(conll_file.__str__())","['def', 'write_conll', '(', 'docgraph', ',', 'output_file', ',', 'coreference_layer', '=', 'None', ',', 'markable_layer', '=', 'None', ')', ':', 'if', 'markable_layer', 'is', 'None', ':', 'markable_layer', '=', 'docgraph', '.', 'ns', '+', ""':markable'"", 'conll_file', '=', 'Conll2009File', '(', 'docgraph', ',', 'coreference_layer', '=', 'coreference_layer', ',', 'markable_layer', '=', 'markable_layer', ')', 'assert', 'isinstance', '(', 'output_file', ',', '(', 'str', ',', 'file', ')', ')', 'if', 'isinstance', '(', 'output_file', ',', 'str', ')', ':', 'path_to_file', '=', 'os', '.', 'path', '.', 'dirname', '(', 'output_file', ')', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'path_to_file', ')', ':', 'create_dir', '(', 'path_to_file', ')', 'conll_file', '.', 'write', '(', 'output_file', ')', 'else', ':', '# output_file is a file object', 'output_file', '.', 'write', '(', 'conll_file', '.', '__str__', '(', ')', ')']","converts a DiscourseDocumentGraph into a tab-separated CoNLL 2009 file and
    writes it to the given file (or file path).","['converts', 'a', 'DiscourseDocumentGraph', 'into', 'a', 'tab', '-', 'separated', 'CoNLL', '2009', 'file', 'and', 'writes', 'it', 'to', 'the', 'given', 'file', '(', 'or', 'file', 'path', ')', '.']",python,W,0,True,1,train
20223,arne-cl/discoursegraphs,src/discoursegraphs/readwrite/gexf.py,https://github.com/arne-cl/discoursegraphs/blob/842f0068a3190be2c75905754521b176b25a54fb/src/discoursegraphs/readwrite/gexf.py#L16-L25,"def write_gexf(docgraph, output_file):
    """"""
    takes a document graph, converts it into GEXF format and writes it to
    a file.
    """"""
    dg_copy = deepcopy(docgraph)
    remove_root_metadata(dg_copy)
    layerset2str(dg_copy)
    attriblist2str(dg_copy)
    nx_write_gexf(dg_copy, output_file)","['def', 'write_gexf', '(', 'docgraph', ',', 'output_file', ')', ':', 'dg_copy', '=', 'deepcopy', '(', 'docgraph', ')', 'remove_root_metadata', '(', 'dg_copy', ')', 'layerset2str', '(', 'dg_copy', ')', 'attriblist2str', '(', 'dg_copy', ')', 'nx_write_gexf', '(', 'dg_copy', ',', 'output_file', ')']","takes a document graph, converts it into GEXF format and writes it to
    a file.","['takes', 'a', 'document', 'graph', 'converts', 'it', 'into', 'GEXF', 'format', 'and', 'writes', 'it', 'to', 'a', 'file', '.']",python,W,0,True,1,train
26809,wummel/dosage,dosagelib/util.py,https://github.com/wummel/dosage/blob/a0109c3a46219f280e6e5e77183674e40da0f304/dosagelib/util.py#L564-L589,"def writeFile(filename, content, encoding=None):
    """"""Write content to given filename. Checks for zero-sized files.
    If encoding is given writes to a codec.open() file.""""""
    if not content:
        raise OSError(""empty content for file %s"" % filename)

    def getfp(filename, encoding):
        """"""Get open file object.""""""
        if encoding:
            return codecs.open(filename, 'w', encoding)
        return open(filename, 'wb')

    try:
        with getfp(filename, encoding) as fp:
            fp.write(content)
            fp.flush()
            os.fsync(fp.fileno())
            size = os.path.getsize(filename)
            if size == 0:
                raise OSError(""empty file %s"" % filename)
    except Exception:
        if os.path.isfile(filename):
            os.remove(filename)
        raise
    else:
        out.info(u""Saved %s (%s)."" % (filename, strsize(size)))","['def', 'writeFile', '(', 'filename', ',', 'content', ',', 'encoding', '=', 'None', ')', ':', 'if', 'not', 'content', ':', 'raise', 'OSError', '(', '""empty content for file %s""', '%', 'filename', ')', 'def', 'getfp', '(', 'filename', ',', 'encoding', ')', ':', '""""""Get open file object.""""""', 'if', 'encoding', ':', 'return', 'codecs', '.', 'open', '(', 'filename', ',', ""'w'"", ',', 'encoding', ')', 'return', 'open', '(', 'filename', ',', ""'wb'"", ')', 'try', ':', 'with', 'getfp', '(', 'filename', ',', 'encoding', ')', 'as', 'fp', ':', 'fp', '.', 'write', '(', 'content', ')', 'fp', '.', 'flush', '(', ')', 'os', '.', 'fsync', '(', 'fp', '.', 'fileno', '(', ')', ')', 'size', '=', 'os', '.', 'path', '.', 'getsize', '(', 'filename', ')', 'if', 'size', '==', '0', ':', 'raise', 'OSError', '(', '""empty file %s""', '%', 'filename', ')', 'except', 'Exception', ':', 'if', 'os', '.', 'path', '.', 'isfile', '(', 'filename', ')', ':', 'os', '.', 'remove', '(', 'filename', ')', 'raise', 'else', ':', 'out', '.', 'info', '(', 'u""Saved %s (%s).""', '%', '(', 'filename', ',', 'strsize', '(', 'size', ')', ')', ')']","Write content to given filename. Checks for zero-sized files.
    If encoding is given writes to a codec.open() file.","['Write', 'content', 'to', 'given', 'filename', '.', 'Checks', 'for', 'zero', '-', 'sized', 'files', '.', 'If', 'encoding', 'is', 'given', 'writes', 'to', 'a', 'codec', '.', 'open', '()', 'file', '.']",python,W,0,True,1,train
28009,mushkevych/scheduler,synergy/db/manager/db_manager.py,https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/db/manager/db_manager.py#L44-L55,"def update_db():
    """""" writes to managed_process table records from the context.process_context """"""
    logger = get_logger(PROCESS_SCHEDULER)
    managed_process_dao = ManagedProcessDao(logger)
    managed_process_dao.clear()

    for process_name, process_entry in context.process_context.items():
        if not isinstance(process_entry, ManagedProcessEntry):
            continue

        managed_process_dao.update(process_entry)
        logger.info('Updated DB with process entry {0} from the context.'.format(process_entry.key))","['def', 'update_db', '(', ')', ':', 'logger', '=', 'get_logger', '(', 'PROCESS_SCHEDULER', ')', 'managed_process_dao', '=', 'ManagedProcessDao', '(', 'logger', ')', 'managed_process_dao', '.', 'clear', '(', ')', 'for', 'process_name', ',', 'process_entry', 'in', 'context', '.', 'process_context', '.', 'items', '(', ')', ':', 'if', 'not', 'isinstance', '(', 'process_entry', ',', 'ManagedProcessEntry', ')', ':', 'continue', 'managed_process_dao', '.', 'update', '(', 'process_entry', ')', 'logger', '.', 'info', '(', ""'Updated DB with process entry {0} from the context.'"", '.', 'format', '(', 'process_entry', '.', 'key', ')', ')']",writes to managed_process table records from the context.process_context,"['writes', 'to', 'managed_process', 'table', 'records', 'from', 'the', 'context', '.', 'process_context']",python,W,0,True,1,train
1518,flo-compbio/genometools,genometools/ncbi/extract_entrez2gene.py,https://github.com/flo-compbio/genometools/blob/dd962bb26d60a0f14ca14d8c9a4dd75768962c7d/genometools/ncbi/extract_entrez2gene.py#L186-L231,"def main(args=None):
    """"""Extracts Entrez ID -> gene symbol mapping and writes it to a text file.

    Parameters
    ----------
    args: argparse.Namespace object, optional
        The argument values. If not specified, the values will be obtained by
        parsing the command line arguments using the `argparse` module.

    Returns
    -------
    int
        Exit code (0 if no error occurred).

    Raises
    ------
    SystemError
        If the version of the Python interpreter is not >= 2.7.
    """"""
    vinfo = sys.version_info
    if not (vinfo >= (2, 7)):
        raise SystemError('Python interpreter version >= 2.7 required, '
                          'found %d.%d instead.' % (vinfo.major, vinfo.minor))

    if args is None:
        parser = get_argument_parser()
        args = parser.parse_args()

    gene2acc_file = args.gene2acc_file
    output_file = args.output_file
    log_file = args.log_file
    quiet = args.quiet
    verbose = args.verbose

    # configure logger
    log_stream = sys.stdout
    if output_file == '-':
        log_stream = sys.stderr

    logger = misc.get_logger(log_stream=log_stream, log_file=log_file,
                             quiet=quiet, verbose=verbose)

    entrez2gene = read_gene2acc(gene2acc_file, logger)
    write_entrez2gene(output_file, entrez2gene, logger)

    return 0","['def', 'main', '(', 'args', '=', 'None', ')', ':', 'vinfo', '=', 'sys', '.', 'version_info', 'if', 'not', '(', 'vinfo', '>=', '(', '2', ',', '7', ')', ')', ':', 'raise', 'SystemError', '(', ""'Python interpreter version >= 2.7 required, '"", ""'found %d.%d instead.'"", '%', '(', 'vinfo', '.', 'major', ',', 'vinfo', '.', 'minor', ')', ')', 'if', 'args', 'is', 'None', ':', 'parser', '=', 'get_argument_parser', '(', ')', 'args', '=', 'parser', '.', 'parse_args', '(', ')', 'gene2acc_file', '=', 'args', '.', 'gene2acc_file', 'output_file', '=', 'args', '.', 'output_file', 'log_file', '=', 'args', '.', 'log_file', 'quiet', '=', 'args', '.', 'quiet', 'verbose', '=', 'args', '.', 'verbose', '# configure logger', 'log_stream', '=', 'sys', '.', 'stdout', 'if', 'output_file', '==', ""'-'"", ':', 'log_stream', '=', 'sys', '.', 'stderr', 'logger', '=', 'misc', '.', 'get_logger', '(', 'log_stream', '=', 'log_stream', ',', 'log_file', '=', 'log_file', ',', 'quiet', '=', 'quiet', ',', 'verbose', '=', 'verbose', ')', 'entrez2gene', '=', 'read_gene2acc', '(', 'gene2acc_file', ',', 'logger', ')', 'write_entrez2gene', '(', 'output_file', ',', 'entrez2gene', ',', 'logger', ')', 'return', '0']","Extracts Entrez ID -> gene symbol mapping and writes it to a text file.

    Parameters
    ----------
    args: argparse.Namespace object, optional
        The argument values. If not specified, the values will be obtained by
        parsing the command line arguments using the `argparse` module.

    Returns
    -------
    int
        Exit code (0 if no error occurred).

    Raises
    ------
    SystemError
        If the version of the Python interpreter is not >= 2.7.","['Extracts', 'Entrez', 'ID', '-', '>', 'gene', 'symbol', 'mapping', 'and', 'writes', 'it', 'to', 'a', 'text', 'file', '.']",python,W,0,True,1,train
4097,biocore/burrito-fillings,bfillings/usearch.py,https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/usearch.py#L934-L951,"def concatenate_fastas(output_fna_clustered,
                       output_fna_failures,
                       output_concat_filepath):
    """""" Concatenates two input fastas, writes to output_concat_filepath

    output_fna_clustered: fasta of successful ref clusters
    output_fna_failures: de novo fasta of cluster failures
    output_concat_filepath: path to write combined fastas to
    """"""

    output_fp = open(output_concat_filepath, ""w"")

    for label, seq in parse_fasta(open(output_fna_clustered, ""U"")):
        output_fp.write("">%s\n%s\n"" % (label, seq))
    for label, seq in parse_fasta(open(output_fna_failures, ""U"")):
        output_fp.write("">%s\n%s\n"" % (label, seq))

    return output_concat_filepath","['def', 'concatenate_fastas', '(', 'output_fna_clustered', ',', 'output_fna_failures', ',', 'output_concat_filepath', ')', ':', 'output_fp', '=', 'open', '(', 'output_concat_filepath', ',', '""w""', ')', 'for', 'label', ',', 'seq', 'in', 'parse_fasta', '(', 'open', '(', 'output_fna_clustered', ',', '""U""', ')', ')', ':', 'output_fp', '.', 'write', '(', '"">%s\\n%s\\n""', '%', '(', 'label', ',', 'seq', ')', ')', 'for', 'label', ',', 'seq', 'in', 'parse_fasta', '(', 'open', '(', 'output_fna_failures', ',', '""U""', ')', ')', ':', 'output_fp', '.', 'write', '(', '"">%s\\n%s\\n""', '%', '(', 'label', ',', 'seq', ')', ')', 'return', 'output_concat_filepath']","Concatenates two input fastas, writes to output_concat_filepath

    output_fna_clustered: fasta of successful ref clusters
    output_fna_failures: de novo fasta of cluster failures
    output_concat_filepath: path to write combined fastas to","['Concatenates', 'two', 'input', 'fastas', 'writes', 'to', 'output_concat_filepath']",python,W,0,True,1,train
4116,biocore/burrito-fillings,bfillings/usearch.py,https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/usearch.py#L2528-L2545,"def parse_usearch61_failures(seq_path,
                             failures,
                             output_fasta_fp):
    """""" Parses seq IDs from failures list, writes to output_fasta_fp

    seq_path: filepath of original input fasta file.
    failures: list/set of failure seq IDs
    output_fasta_fp: path to write parsed sequences
    """"""

    parsed_out = open(output_fasta_fp, ""w"")

    for label, seq in parse_fasta(open(seq_path), ""U""):
        curr_label = label.split()[0]
        if curr_label in failures:
            parsed_out.write("">%s\n%s\n"" % (label, seq))
    parsed_out.close()
    return output_fasta_fp","['def', 'parse_usearch61_failures', '(', 'seq_path', ',', 'failures', ',', 'output_fasta_fp', ')', ':', 'parsed_out', '=', 'open', '(', 'output_fasta_fp', ',', '""w""', ')', 'for', 'label', ',', 'seq', 'in', 'parse_fasta', '(', 'open', '(', 'seq_path', ')', ',', '""U""', ')', ':', 'curr_label', '=', 'label', '.', 'split', '(', ')', '[', '0', ']', 'if', 'curr_label', 'in', 'failures', ':', 'parsed_out', '.', 'write', '(', '"">%s\\n%s\\n""', '%', '(', 'label', ',', 'seq', ')', ')', 'parsed_out', '.', 'close', '(', ')', 'return', 'output_fasta_fp']","Parses seq IDs from failures list, writes to output_fasta_fp

    seq_path: filepath of original input fasta file.
    failures: list/set of failure seq IDs
    output_fasta_fp: path to write parsed sequences","['Parses', 'seq', 'IDs', 'from', 'failures', 'list', 'writes', 'to', 'output_fasta_fp']",python,W,0,True,1,train
7265,davebridges/mousedb,mousedb/data/views.py,https://github.com/davebridges/mousedb/blob/2a33f6d15d88b1540b05f7232b154fdbf8568580/mousedb/data/views.py#L225-L248,"def experiment_details_csv(request, pk):
    """"""This view generates a csv output file of an experiment.
	
	The view writes to a csv table the animal, genotype, age (in days), assay and values.""""""
    experiment = get_object_or_404(Experiment, pk=pk)
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=experiment.csv'
    writer = csv.writer(response)
    writer.writerow([""Animal"",""Cage"", ""Strain"", ""Genotype"", ""Gender"",""Age"", ""Assay"", ""Values"", ""Feeding"", ""Experiment Date"", ""Treatment""])
    for measurement in experiment.measurement_set.iterator():
        writer.writerow([
			measurement.animal,
            measurement.animal.Cage,
            measurement.animal.Strain,
			measurement.animal.Genotype, 
			measurement.animal.Gender,
			measurement.age(), 
			measurement.assay, 
			measurement.values, 
            measurement.experiment.feeding_state,
            measurement.experiment.date,
			measurement.animal.treatment_set.all()
			])
    return response","['def', 'experiment_details_csv', '(', 'request', ',', 'pk', ')', ':', 'experiment', '=', 'get_object_or_404', '(', 'Experiment', ',', 'pk', '=', 'pk', ')', 'response', '=', 'HttpResponse', '(', 'content_type', '=', ""'text/csv'"", ')', 'response', '[', ""'Content-Disposition'"", ']', '=', ""'attachment; filename=experiment.csv'"", 'writer', '=', 'csv', '.', 'writer', '(', 'response', ')', 'writer', '.', 'writerow', '(', '[', '""Animal""', ',', '""Cage""', ',', '""Strain""', ',', '""Genotype""', ',', '""Gender""', ',', '""Age""', ',', '""Assay""', ',', '""Values""', ',', '""Feeding""', ',', '""Experiment Date""', ',', '""Treatment""', ']', ')', 'for', 'measurement', 'in', 'experiment', '.', 'measurement_set', '.', 'iterator', '(', ')', ':', 'writer', '.', 'writerow', '(', '[', 'measurement', '.', 'animal', ',', 'measurement', '.', 'animal', '.', 'Cage', ',', 'measurement', '.', 'animal', '.', 'Strain', ',', 'measurement', '.', 'animal', '.', 'Genotype', ',', 'measurement', '.', 'animal', '.', 'Gender', ',', 'measurement', '.', 'age', '(', ')', ',', 'measurement', '.', 'assay', ',', 'measurement', '.', 'values', ',', 'measurement', '.', 'experiment', '.', 'feeding_state', ',', 'measurement', '.', 'experiment', '.', 'date', ',', 'measurement', '.', 'animal', '.', 'treatment_set', '.', 'all', '(', ')', ']', ')', 'return', 'response']","This view generates a csv output file of an experiment.
	
	The view writes to a csv table the animal, genotype, age (in days), assay and values.","['This', 'view', 'generates', 'a', 'csv', 'output', 'file', 'of', 'an', 'experiment', '.', 'The', 'view', 'writes', 'to', 'a', 'csv', 'table', 'the', 'animal', 'genotype', 'age', '(', 'in', 'days', ')', 'assay', 'and', 'values', '.']",python,W,0,True,1,train
7266,davebridges/mousedb,mousedb/data/views.py,https://github.com/davebridges/mousedb/blob/2a33f6d15d88b1540b05f7232b154fdbf8568580/mousedb/data/views.py#L251-L270,"def aging_csv(request):
    """"""This view generates a csv output file of all animal data for use in aging analysis.
	
	The view writes to a csv table the animal, strain, genotype, age (in days), and cause of death.""""""
    animal_list = Animal.objects.all()
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=aging.csv'
    writer = csv.writer(response)
    writer.writerow([""Animal"", ""Strain"", ""Genotype"", ""Gender"", ""Age"", ""Death"", ""Alive""])
    for animal in animal_list.iterator():
        writer.writerow([
            animal.MouseID, 
            animal.Strain, 
            animal.Genotype, 
            animal.Gender,
            animal.age(),
            animal.Cause_of_Death,
            animal.Alive            
            ])
    return response","['def', 'aging_csv', '(', 'request', ')', ':', 'animal_list', '=', 'Animal', '.', 'objects', '.', 'all', '(', ')', 'response', '=', 'HttpResponse', '(', 'content_type', '=', ""'text/csv'"", ')', 'response', '[', ""'Content-Disposition'"", ']', '=', ""'attachment; filename=aging.csv'"", 'writer', '=', 'csv', '.', 'writer', '(', 'response', ')', 'writer', '.', 'writerow', '(', '[', '""Animal""', ',', '""Strain""', ',', '""Genotype""', ',', '""Gender""', ',', '""Age""', ',', '""Death""', ',', '""Alive""', ']', ')', 'for', 'animal', 'in', 'animal_list', '.', 'iterator', '(', ')', ':', 'writer', '.', 'writerow', '(', '[', 'animal', '.', 'MouseID', ',', 'animal', '.', 'Strain', ',', 'animal', '.', 'Genotype', ',', 'animal', '.', 'Gender', ',', 'animal', '.', 'age', '(', ')', ',', 'animal', '.', 'Cause_of_Death', ',', 'animal', '.', 'Alive', ']', ')', 'return', 'response']","This view generates a csv output file of all animal data for use in aging analysis.
	
	The view writes to a csv table the animal, strain, genotype, age (in days), and cause of death.","['This', 'view', 'generates', 'a', 'csv', 'output', 'file', 'of', 'all', 'animal', 'data', 'for', 'use', 'in', 'aging', 'analysis', '.', 'The', 'view', 'writes', 'to', 'a', 'csv', 'table', 'the', 'animal', 'strain', 'genotype', 'age', '(', 'in', 'days', ')', 'and', 'cause', 'of', 'death', '.']",python,W,0,True,1,train
7267,davebridges/mousedb,mousedb/data/views.py,https://github.com/davebridges/mousedb/blob/2a33f6d15d88b1540b05f7232b154fdbf8568580/mousedb/data/views.py#L272-L287,"def litters_csv(request):
    """"""This view generates a csv output file of all animal data for use in litter analysis.
	
	The view writes to a csv table the birthdate, breeding cage and strain.""""""
    animal_list = Animal.objects.all()
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=litters.csv'
    writer = csv.writer(response)
    writer.writerow([""Born"", ""Breeding"", ""Strain""])
    for animal in animal_list:
        writer.writerow([
            animal.Born,
            animal.Breeding,
            animal.Strain
            ])
    return response","['def', 'litters_csv', '(', 'request', ')', ':', 'animal_list', '=', 'Animal', '.', 'objects', '.', 'all', '(', ')', 'response', '=', 'HttpResponse', '(', 'content_type', '=', ""'text/csv'"", ')', 'response', '[', ""'Content-Disposition'"", ']', '=', ""'attachment; filename=litters.csv'"", 'writer', '=', 'csv', '.', 'writer', '(', 'response', ')', 'writer', '.', 'writerow', '(', '[', '""Born""', ',', '""Breeding""', ',', '""Strain""', ']', ')', 'for', 'animal', 'in', 'animal_list', ':', 'writer', '.', 'writerow', '(', '[', 'animal', '.', 'Born', ',', 'animal', '.', 'Breeding', ',', 'animal', '.', 'Strain', ']', ')', 'return', 'response']","This view generates a csv output file of all animal data for use in litter analysis.
	
	The view writes to a csv table the birthdate, breeding cage and strain.","['This', 'view', 'generates', 'a', 'csv', 'output', 'file', 'of', 'all', 'animal', 'data', 'for', 'use', 'in', 'litter', 'analysis', '.', 'The', 'view', 'writes', 'to', 'a', 'csv', 'table', 'the', 'birthdate', 'breeding', 'cage', 'and', 'strain', '.']",python,W,0,True,1,train
7343,flying-sheep/bcode,bcoding.py,https://github.com/flying-sheep/bcode/blob/a50996aa1741685c2daba6a9b4893692f377695a/bcoding.py#L181-L195,"def bencode(data, f=None):
	""""""
	Writes a serializable data piece to f
	The order of tests is nonarbitrary,
	as strings and mappings are iterable.
	
	If f is None, it writes to a byte buffer
	and returns a bytestring
	""""""
	if f is None:
		f = BytesIO()
		_bencode_to_file(data, f)
		return f.getvalue()
	else:
		_bencode_to_file(data, f)","['def', 'bencode', '(', 'data', ',', 'f', '=', 'None', ')', ':', 'if', 'f', 'is', 'None', ':', 'f', '=', 'BytesIO', '(', ')', '_bencode_to_file', '(', 'data', ',', 'f', ')', 'return', 'f', '.', 'getvalue', '(', ')', 'else', ':', '_bencode_to_file', '(', 'data', ',', 'f', ')']","Writes a serializable data piece to f
	The order of tests is nonarbitrary,
	as strings and mappings are iterable.
	
	If f is None, it writes to a byte buffer
	and returns a bytestring","['Writes', 'a', 'serializable', 'data', 'piece', 'to', 'f', 'The', 'order', 'of', 'tests', 'is', 'nonarbitrary', 'as', 'strings', 'and', 'mappings', 'are', 'iterable', '.', 'If', 'f', 'is', 'None', 'it', 'writes', 'to', 'a', 'byte', 'buffer', 'and', 'returns', 'a', 'bytestring']",python,W,0,True,1,train
7534,wglass/lighthouse,lighthouse/haproxy/balancer.py,https://github.com/wglass/lighthouse/blob/f4ce6550895acc31e433ede0c05d366718a3ffe5/lighthouse/haproxy/balancer.py#L151-L171,"def sync_file(self, clusters):
        """"""
        Generates new HAProxy config file content and writes it to the
        file at `haproxy_config_path`.

        If a restart is not necessary the nodes configured in HAProxy will
        be synced on the fly.  If a restart *is* necessary, one will be
        triggered.
        """"""
        logger.info(""Updating HAProxy config file."")
        if not self.restart_required:
            self.sync_nodes(clusters)

        version = self.control.get_version()

        with open(self.haproxy_config_path, ""w"") as f:
            f.write(self.config_file.generate(clusters, version=version))

        if self.restart_required:
            with self.restart_lock:
                self.restart()","['def', 'sync_file', '(', 'self', ',', 'clusters', ')', ':', 'logger', '.', 'info', '(', '""Updating HAProxy config file.""', ')', 'if', 'not', 'self', '.', 'restart_required', ':', 'self', '.', 'sync_nodes', '(', 'clusters', ')', 'version', '=', 'self', '.', 'control', '.', 'get_version', '(', ')', 'with', 'open', '(', 'self', '.', 'haproxy_config_path', ',', '""w""', ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'self', '.', 'config_file', '.', 'generate', '(', 'clusters', ',', 'version', '=', 'version', ')', ')', 'if', 'self', '.', 'restart_required', ':', 'with', 'self', '.', 'restart_lock', ':', 'self', '.', 'restart', '(', ')']","Generates new HAProxy config file content and writes it to the
        file at `haproxy_config_path`.

        If a restart is not necessary the nodes configured in HAProxy will
        be synced on the fly.  If a restart *is* necessary, one will be
        triggered.","['Generates', 'new', 'HAProxy', 'config', 'file', 'content', 'and', 'writes', 'it', 'to', 'the', 'file', 'at', 'haproxy_config_path', '.']",python,W,0,True,1,train
8794,LIVVkit/LIVVkit,livvkit/util/TexHelper.py,https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/util/TexHelper.py#L46-L62,"def write_tex():
    """"""
    Finds all of the output data files, and writes them out to .tex
    """"""
    datadir = livvkit.index_dir
    outdir = os.path.join(datadir, ""tex"")
    print(outdir)
    # functions.mkdir_p(outdir)

    data_files = glob.glob(datadir + ""/**/*.json"", recursive=True)

    for each in data_files:
        data = functions.read_json(each)
        tex = translate_page(data)
        outfile = os.path.join(outdir, os.path.basename(each).replace('json', 'tex'))
        with open(outfile, 'w') as f:
            f.write(tex)","['def', 'write_tex', '(', ')', ':', 'datadir', '=', 'livvkit', '.', 'index_dir', 'outdir', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '""tex""', ')', 'print', '(', 'outdir', ')', '# functions.mkdir_p(outdir)', 'data_files', '=', 'glob', '.', 'glob', '(', 'datadir', '+', '""/**/*.json""', ',', 'recursive', '=', 'True', ')', 'for', 'each', 'in', 'data_files', ':', 'data', '=', 'functions', '.', 'read_json', '(', 'each', ')', 'tex', '=', 'translate_page', '(', 'data', ')', 'outfile', '=', 'os', '.', 'path', '.', 'join', '(', 'outdir', ',', 'os', '.', 'path', '.', 'basename', '(', 'each', ')', '.', 'replace', '(', ""'json'"", ',', ""'tex'"", ')', ')', 'with', 'open', '(', 'outfile', ',', ""'w'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'tex', ')']","Finds all of the output data files, and writes them out to .tex","['Finds', 'all', 'of', 'the', 'output', 'data', 'files', 'and', 'writes', 'them', 'out', 'to', '.', 'tex']",python,W,0,True,1,train
10928,aerogear/digger-build-cli,digger/helpers/common.py,https://github.com/aerogear/digger-build-cli/blob/8b88a31063526ec7222dbea6a87309686ad21320/digger/helpers/common.py#L10-L48,"def run_cmd(cmd, log='log.log', cwd='.', stdout=sys.stdout, bufsize=1, encode='utf-8'):
  """"""
  Runs a command in the backround by creating a new process and writes the output to a specified log file.

  :param log(str) - log filename to be used
  :param cwd(str) - basedir to write/create the log file
  :param stdout(pipe) - stdout process pipe (can be default stdout, a file, etc)
  :param bufsize(int) - set the output buffering, default is 1 (per line)
  :param encode(str) - string encoding to decode the logged content, default is utf-8

  Returns:
    The process object
  """"""
  logfile = '%s/%s' % (cwd, log)
  
  if os.path.exists(logfile):
    os.remove(logfile)
  proc_args = {
    'stdout': subprocess.PIPE,
    'stderr': subprocess.PIPE,
    'cwd': cwd,
    'universal_newlines': True
  }

  proc = subprocess.Popen(cmd, **proc_args)
  
  while True:
    line = proc.stdout.readline()
    if proc.poll() is None:
      stdout.write(line)
    else:
      break
  out, err = proc.communicate()

  with open(logfile, 'w') as f:
    if out:
      f.write(out)
    else:
      f.write(err)","['def', 'run_cmd', '(', 'cmd', ',', 'log', '=', ""'log.log'"", ',', 'cwd', '=', ""'.'"", ',', 'stdout', '=', 'sys', '.', 'stdout', ',', 'bufsize', '=', '1', ',', 'encode', '=', ""'utf-8'"", ')', ':', 'logfile', '=', ""'%s/%s'"", '%', '(', 'cwd', ',', 'log', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'logfile', ')', ':', 'os', '.', 'remove', '(', 'logfile', ')', 'proc_args', '=', '{', ""'stdout'"", ':', 'subprocess', '.', 'PIPE', ',', ""'stderr'"", ':', 'subprocess', '.', 'PIPE', ',', ""'cwd'"", ':', 'cwd', ',', ""'universal_newlines'"", ':', 'True', '}', 'proc', '=', 'subprocess', '.', 'Popen', '(', 'cmd', ',', '*', '*', 'proc_args', ')', 'while', 'True', ':', 'line', '=', 'proc', '.', 'stdout', '.', 'readline', '(', ')', 'if', 'proc', '.', 'poll', '(', ')', 'is', 'None', ':', 'stdout', '.', 'write', '(', 'line', ')', 'else', ':', 'break', 'out', ',', 'err', '=', 'proc', '.', 'communicate', '(', ')', 'with', 'open', '(', 'logfile', ',', ""'w'"", ')', 'as', 'f', ':', 'if', 'out', ':', 'f', '.', 'write', '(', 'out', ')', 'else', ':', 'f', '.', 'write', '(', 'err', ')']","Runs a command in the backround by creating a new process and writes the output to a specified log file.

  :param log(str) - log filename to be used
  :param cwd(str) - basedir to write/create the log file
  :param stdout(pipe) - stdout process pipe (can be default stdout, a file, etc)
  :param bufsize(int) - set the output buffering, default is 1 (per line)
  :param encode(str) - string encoding to decode the logged content, default is utf-8

  Returns:
    The process object","['Runs', 'a', 'command', 'in', 'the', 'backround', 'by', 'creating', 'a', 'new', 'process', 'and', 'writes', 'the', 'output', 'to', 'a', 'specified', 'log', 'file', '.']",python,W,0,True,1,train
11380,dArignac/pelican-minification,minification/__init__.py,https://github.com/dArignac/pelican-minification/blob/8d9b3322fb3961f6496b2519c2303ffa6625f775/minification/__init__.py#L46-L66,"def write_to_file(path_file, callback):
        """"""
        Reads the content of the given file, puts the content into the callback and writes the result back to the file.
        :param path_file: the path to the file
        :type path_file: str
        :param callback: the callback function
        :type callback: function
        """"""
        try:
            with open(path_file, 'r+', encoding='utf-8') as f:
                content = callback(f.read())
                f.seek(0)
                f.write(content)
                f.truncate()
        except Exception as e:
            raise Exception(
                'unable to minify file %(file)s, exception was %(exception)r' % {
                    'file': path_file,
                    'exception': e,
                }
            )","['def', 'write_to_file', '(', 'path_file', ',', 'callback', ')', ':', 'try', ':', 'with', 'open', '(', 'path_file', ',', ""'r+'"", ',', 'encoding', '=', ""'utf-8'"", ')', 'as', 'f', ':', 'content', '=', 'callback', '(', 'f', '.', 'read', '(', ')', ')', 'f', '.', 'seek', '(', '0', ')', 'f', '.', 'write', '(', 'content', ')', 'f', '.', 'truncate', '(', ')', 'except', 'Exception', 'as', 'e', ':', 'raise', 'Exception', '(', ""'unable to minify file %(file)s, exception was %(exception)r'"", '%', '{', ""'file'"", ':', 'path_file', ',', ""'exception'"", ':', 'e', ',', '}', ')']","Reads the content of the given file, puts the content into the callback and writes the result back to the file.
        :param path_file: the path to the file
        :type path_file: str
        :param callback: the callback function
        :type callback: function","['Reads', 'the', 'content', 'of', 'the', 'given', 'file', 'puts', 'the', 'content', 'into', 'the', 'callback', 'and', 'writes', 'the', 'result', 'back', 'to', 'the', 'file', '.', ':', 'param', 'path_file', ':', 'the', 'path', 'to', 'the', 'file', ':', 'type', 'path_file', ':', 'str', ':', 'param', 'callback', ':', 'the', 'callback', 'function', ':', 'type', 'callback', ':', 'function']",python,W,0,True,1,train
18875,OpenAgInitiative/openag_python,openag/cli/firmware/base.py,https://github.com/OpenAgInitiative/openag_python/blob/f6202340292bbf7185e1a7d4290188c0dacbb8d0/openag/cli/firmware/base.py#L255-L319,"def write_to(self, f):
        """"""
        Generates code based on the given module configuration and writes it to
        the file object `f`.
        """"""
        f = CodeWriter(f)

        # Write all header files
        headers = set()
        for plugin in self.plugins:
            headers = headers.union(plugin.header_files())
        for header in headers:
            f.writeln(""#include <{}>"".format(header))
        f.writeln("""")

        # Write all declarations
        for plugin in self.plugins:
            plugin.write_declarations(f)
        f.writeln("""")

        # Write the setup function
        with f._function(""void"", ""setup""):
            # Setup all plugins
            f.writeln(""// Setup all plugins"")
            for plugin in self.plugins:
                plugin.setup_plugin(f)
            # Setup all modules
            f.writeln(""// Setup all modules"")
            for mod_name in self.modules.keys():
                for plugin in self.plugins:
                    plugin.setup_module(mod_name, f)
        f.writeln("""")

        # Write the loop function
        with f._function(""void"", ""loop""):
            # Update all plugins
            f.writeln(""// Update all plugins"")
            for plugin in self.plugins:
                plugin.update_plugin(f)
            # Update all modules
            f.writeln(""// Update all modules"")
            for mod_name, mod_info in self.modules.items():
                for plugin in self.plugins:
                    plugin.update_module(mod_name, f)

                # Read all module outputs
                for output_name in mod_info[""outputs""]:
                    cond = ""{mod_name}.get_{output_name}({msg_name})"".format(
                        mod_name=mod_name, output_name=output_name,
                        msg_name=self.msg_name(mod_name, output_name)
                    )
                    with f._if(cond):
                        for plugin in self.plugins:
                            plugin.on_output(mod_name, output_name, f)

            # Read statuses of all modules
            f.writeln(""// Read statuses of all modules"")
            with f._if(""should_read_statuses()""):
                for plugin in self.plugins:
                    plugin.start_read_module_status(f)
                for mod_name in self.modules:
                    for plugin in self.plugins:
                        plugin.read_module_status(mod_name, f)
                for plugin in self.plugins:
                    plugin.end_read_module_status(f)","['def', 'write_to', '(', 'self', ',', 'f', ')', ':', 'f', '=', 'CodeWriter', '(', 'f', ')', '# Write all header files', 'headers', '=', 'set', '(', ')', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'headers', '=', 'headers', '.', 'union', '(', 'plugin', '.', 'header_files', '(', ')', ')', 'for', 'header', 'in', 'headers', ':', 'f', '.', 'writeln', '(', '""#include <{}>""', '.', 'format', '(', 'header', ')', ')', 'f', '.', 'writeln', '(', '""""', ')', '# Write all declarations', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'write_declarations', '(', 'f', ')', 'f', '.', 'writeln', '(', '""""', ')', '# Write the setup function', 'with', 'f', '.', '_function', '(', '""void""', ',', '""setup""', ')', ':', '# Setup all plugins', 'f', '.', 'writeln', '(', '""// Setup all plugins""', ')', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'setup_plugin', '(', 'f', ')', '# Setup all modules', 'f', '.', 'writeln', '(', '""// Setup all modules""', ')', 'for', 'mod_name', 'in', 'self', '.', 'modules', '.', 'keys', '(', ')', ':', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'setup_module', '(', 'mod_name', ',', 'f', ')', 'f', '.', 'writeln', '(', '""""', ')', '# Write the loop function', 'with', 'f', '.', '_function', '(', '""void""', ',', '""loop""', ')', ':', '# Update all plugins', 'f', '.', 'writeln', '(', '""// Update all plugins""', ')', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'update_plugin', '(', 'f', ')', '# Update all modules', 'f', '.', 'writeln', '(', '""// Update all modules""', ')', 'for', 'mod_name', ',', 'mod_info', 'in', 'self', '.', 'modules', '.', 'items', '(', ')', ':', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'update_module', '(', 'mod_name', ',', 'f', ')', '# Read all module outputs', 'for', 'output_name', 'in', 'mod_info', '[', '""outputs""', ']', ':', 'cond', '=', '""{mod_name}.get_{output_name}({msg_name})""', '.', 'format', '(', 'mod_name', '=', 'mod_name', ',', 'output_name', '=', 'output_name', ',', 'msg_name', '=', 'self', '.', 'msg_name', '(', 'mod_name', ',', 'output_name', ')', ')', 'with', 'f', '.', '_if', '(', 'cond', ')', ':', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'on_output', '(', 'mod_name', ',', 'output_name', ',', 'f', ')', '# Read statuses of all modules', 'f', '.', 'writeln', '(', '""// Read statuses of all modules""', ')', 'with', 'f', '.', '_if', '(', '""should_read_statuses()""', ')', ':', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'start_read_module_status', '(', 'f', ')', 'for', 'mod_name', 'in', 'self', '.', 'modules', ':', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'read_module_status', '(', 'mod_name', ',', 'f', ')', 'for', 'plugin', 'in', 'self', '.', 'plugins', ':', 'plugin', '.', 'end_read_module_status', '(', 'f', ')']","Generates code based on the given module configuration and writes it to
        the file object `f`.","['Generates', 'code', 'based', 'on', 'the', 'given', 'module', 'configuration', 'and', 'writes', 'it', 'to', 'the', 'file', 'object', 'f', '.']",python,W,0,True,1,train
22123,TissueMAPS/TmClient,src/python/tmclient/api.py,https://github.com/TissueMAPS/TmClient/blob/6fb40622af19142cb5169a64b8c2965993a25ab1/src/python/tmclient/api.py#L1545-L1590,"def download_channel_image_file(self, channel_name, plate_name,
            well_name, well_pos_y, well_pos_x, cycle_index,
            tpoint, zplane, correct, align, directory):
        '''Downloads a channel image and writes it to a `PNG` file on disk.

        Parameters
        ----------
        channel_name: str
            name of the channel
        plate_name: str
            name of the plate
        well_name: str
            name of the well
        well_pos_x: int
            zero-based x cooridinate of the acquisition site within the well
        well_pos_y: int
            zero-based y cooridinate of the acquisition site within the well
        cycle_index: str
            zero-based cycle index
        tpoint: int
            zero-based time point index
        zplane: int
            zero-based z-plane index
        correct: bool
            whether image should be corrected for illumination artifacts
        align: bool
            whether image should be aligned to the other cycles
        directory: str
            absolute path to the directory on disk where the file should be saved

        Note
        ----
        Image gets automatically aligned between cycles.

        See also
        --------
        :meth:`tmclient.api.TmClient.download_channel_image`
        '''
        response = self._download_channel_image(
            channel_name, plate_name, well_name, well_pos_y, well_pos_x,
            cycle_index=cycle_index, tpoint=tpoint, zplane=zplane,
            correct=correct, align = align
        )
        data = response.content
        filename = self._extract_filename_from_headers(response.headers)
        self._write_file(directory, os.path.basename(filename), data)","['def', 'download_channel_image_file', '(', 'self', ',', 'channel_name', ',', 'plate_name', ',', 'well_name', ',', 'well_pos_y', ',', 'well_pos_x', ',', 'cycle_index', ',', 'tpoint', ',', 'zplane', ',', 'correct', ',', 'align', ',', 'directory', ')', ':', 'response', '=', 'self', '.', '_download_channel_image', '(', 'channel_name', ',', 'plate_name', ',', 'well_name', ',', 'well_pos_y', ',', 'well_pos_x', ',', 'cycle_index', '=', 'cycle_index', ',', 'tpoint', '=', 'tpoint', ',', 'zplane', '=', 'zplane', ',', 'correct', '=', 'correct', ',', 'align', '=', 'align', ')', 'data', '=', 'response', '.', 'content', 'filename', '=', 'self', '.', '_extract_filename_from_headers', '(', 'response', '.', 'headers', ')', 'self', '.', '_write_file', '(', 'directory', ',', 'os', '.', 'path', '.', 'basename', '(', 'filename', ')', ',', 'data', ')']","Downloads a channel image and writes it to a `PNG` file on disk.

        Parameters
        ----------
        channel_name: str
            name of the channel
        plate_name: str
            name of the plate
        well_name: str
            name of the well
        well_pos_x: int
            zero-based x cooridinate of the acquisition site within the well
        well_pos_y: int
            zero-based y cooridinate of the acquisition site within the well
        cycle_index: str
            zero-based cycle index
        tpoint: int
            zero-based time point index
        zplane: int
            zero-based z-plane index
        correct: bool
            whether image should be corrected for illumination artifacts
        align: bool
            whether image should be aligned to the other cycles
        directory: str
            absolute path to the directory on disk where the file should be saved

        Note
        ----
        Image gets automatically aligned between cycles.

        See also
        --------
        :meth:`tmclient.api.TmClient.download_channel_image`","['Downloads', 'a', 'channel', 'image', 'and', 'writes', 'it', 'to', 'a', 'PNG', 'file', 'on', 'disk', '.']",python,W,0,True,1,train
22125,TissueMAPS/TmClient,src/python/tmclient/api.py,https://github.com/TissueMAPS/TmClient/blob/6fb40622af19142cb5169a64b8c2965993a25ab1/src/python/tmclient/api.py#L1662-L1712,"def download_segmentation_image_file(self, mapobject_type_name,
            plate_name, well_name, well_pos_y, well_pos_x, tpoint, zplane, align,
            directory):
        '''Downloads a segmentation image and writes it to a *PNG* file on disk.

        Parameters
        ----------
        mapobject_type_name: str
            name of the segmented objects
        plate_name: str
            name of the plate
        well_name: str
            name of the well in which the image is located
        well_pos_y: int
            y-position of the site relative to the well grid
        well_pos_x: int
            x-position of the site relative to the well grid
        tpoint: int
            zero-based time point index
        zplane: int
            zero-based z-plane index
        align: bool
            option to apply alignment to download
        directory: str
            absolute path to the directory on disk where the file should be saved

        Warning
        -------
        Due to the *PNG* file format the approach is limited to images which
        contain less than 65536 objects.

        See also
        --------
        :meth:`tmclient.api.TmClient.download_segmentation_image`
        '''
        response = self._download_segmentation_image(
            mapobject_type_name, plate_name, well_name, well_pos_y, well_pos_x,
            tpoint, zplane, align
        )
        image = np.array(response, np.int32)
        if np.max(image) >= 2**16:
            raise ValueError(
                'Cannot store segmentation image as PNG file because it '
                'contains more than 65536 objects.'
            )
        filename = '{0}_{1}_{2}_y{3:03d}_x{4:03d}_z{5:03d}_t{6:03d}_{7}.png'.format(
            self.experiment_name, plate_name, well_name, well_pos_y,
            well_pos_x, zplane, tpoint, mapobject_type_name
        )
        data = cv2.imencode(filename, image.astype(np.uint16))[1]
        self._write_file(directory, filename, data)","['def', 'download_segmentation_image_file', '(', 'self', ',', 'mapobject_type_name', ',', 'plate_name', ',', 'well_name', ',', 'well_pos_y', ',', 'well_pos_x', ',', 'tpoint', ',', 'zplane', ',', 'align', ',', 'directory', ')', ':', 'response', '=', 'self', '.', '_download_segmentation_image', '(', 'mapobject_type_name', ',', 'plate_name', ',', 'well_name', ',', 'well_pos_y', ',', 'well_pos_x', ',', 'tpoint', ',', 'zplane', ',', 'align', ')', 'image', '=', 'np', '.', 'array', '(', 'response', ',', 'np', '.', 'int32', ')', 'if', 'np', '.', 'max', '(', 'image', ')', '>=', '2', '**', '16', ':', 'raise', 'ValueError', '(', ""'Cannot store segmentation image as PNG file because it '"", ""'contains more than 65536 objects.'"", ')', 'filename', '=', ""'{0}_{1}_{2}_y{3:03d}_x{4:03d}_z{5:03d}_t{6:03d}_{7}.png'"", '.', 'format', '(', 'self', '.', 'experiment_name', ',', 'plate_name', ',', 'well_name', ',', 'well_pos_y', ',', 'well_pos_x', ',', 'zplane', ',', 'tpoint', ',', 'mapobject_type_name', ')', 'data', '=', 'cv2', '.', 'imencode', '(', 'filename', ',', 'image', '.', 'astype', '(', 'np', '.', 'uint16', ')', ')', '[', '1', ']', 'self', '.', '_write_file', '(', 'directory', ',', 'filename', ',', 'data', ')']","Downloads a segmentation image and writes it to a *PNG* file on disk.

        Parameters
        ----------
        mapobject_type_name: str
            name of the segmented objects
        plate_name: str
            name of the plate
        well_name: str
            name of the well in which the image is located
        well_pos_y: int
            y-position of the site relative to the well grid
        well_pos_x: int
            x-position of the site relative to the well grid
        tpoint: int
            zero-based time point index
        zplane: int
            zero-based z-plane index
        align: bool
            option to apply alignment to download
        directory: str
            absolute path to the directory on disk where the file should be saved

        Warning
        -------
        Due to the *PNG* file format the approach is limited to images which
        contain less than 65536 objects.

        See also
        --------
        :meth:`tmclient.api.TmClient.download_segmentation_image`","['Downloads', 'a', 'segmentation', 'image', 'and', 'writes', 'it', 'to', 'a', '*', 'PNG', '*', 'file', 'on', 'disk', '.']",python,W,0,True,1,train
22137,TissueMAPS/TmClient,src/python/tmclient/api.py,https://github.com/TissueMAPS/TmClient/blob/6fb40622af19142cb5169a64b8c2965993a25ab1/src/python/tmclient/api.py#L2353-L2371,"def download_workflow_description_file(self, filename):
        '''Downloads the workflow description and writes it to a *YAML* file.

        Parameters
        ----------
        filename: str
            path to the file to which description should be written

        See also
        --------
        :meth:`tmclient.api.TmClient.download_workflow_description`
        '''
        description = self.download_workflow_description()
        logger.info('write workflow description to file: %s', filename)
        with open(filename, 'w') as f:
            content = yaml.safe_dump(
                description, default_flow_style=False, explicit_start=True
            )
            f.write(content)","['def', 'download_workflow_description_file', '(', 'self', ',', 'filename', ')', ':', 'description', '=', 'self', '.', 'download_workflow_description', '(', ')', 'logger', '.', 'info', '(', ""'write workflow description to file: %s'"", ',', 'filename', ')', 'with', 'open', '(', 'filename', ',', ""'w'"", ')', 'as', 'f', ':', 'content', '=', 'yaml', '.', 'safe_dump', '(', 'description', ',', 'default_flow_style', '=', 'False', ',', 'explicit_start', '=', 'True', ')', 'f', '.', 'write', '(', 'content', ')']","Downloads the workflow description and writes it to a *YAML* file.

        Parameters
        ----------
        filename: str
            path to the file to which description should be written

        See also
        --------
        :meth:`tmclient.api.TmClient.download_workflow_description`","['Downloads', 'the', 'workflow', 'description', 'and', 'writes', 'it', 'to', 'a', '*', 'YAML', '*', 'file', '.']",python,W,0,True,1,train
22809,20c/munge,munge/config.py,https://github.com/20c/munge/blob/e20fef8c24e48d4b0a5c387820fbb2b7bebb0af0/munge/config.py#L165-L190,"def write(self, config_dir=None, config_name=None, codec=None):
        """"""
        writes config to config_dir using config_name
        """"""
        # get name of config directory
        if not config_dir:
            config_dir = self._meta_config_dir
            if not config_dir:
                raise IOError(""config_dir not set"")

        # get name of config file
        if not config_name:
            config_name = self._defaults.get('config_name', None)
        if not config_name:
            raise KeyError('config_name not set')

        if codec:
            codec = munge.get_codec(codec)()
        else:
            codec = munge.get_codec(self._defaults['codec'])()

        config_dir = os.path.expanduser(config_dir)
        if not os.path.exists(config_dir):
            os.mkdir(config_dir)

        codec.dumpu(self.data, os.path.join(config_dir, 'config.' + codec.extension))","['def', 'write', '(', 'self', ',', 'config_dir', '=', 'None', ',', 'config_name', '=', 'None', ',', 'codec', '=', 'None', ')', ':', '# get name of config directory', 'if', 'not', 'config_dir', ':', 'config_dir', '=', 'self', '.', '_meta_config_dir', 'if', 'not', 'config_dir', ':', 'raise', 'IOError', '(', '""config_dir not set""', ')', '# get name of config file', 'if', 'not', 'config_name', ':', 'config_name', '=', 'self', '.', '_defaults', '.', 'get', '(', ""'config_name'"", ',', 'None', ')', 'if', 'not', 'config_name', ':', 'raise', 'KeyError', '(', ""'config_name not set'"", ')', 'if', 'codec', ':', 'codec', '=', 'munge', '.', 'get_codec', '(', 'codec', ')', '(', ')', 'else', ':', 'codec', '=', 'munge', '.', 'get_codec', '(', 'self', '.', '_defaults', '[', ""'codec'"", ']', ')', '(', ')', 'config_dir', '=', 'os', '.', 'path', '.', 'expanduser', '(', 'config_dir', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'config_dir', ')', ':', 'os', '.', 'mkdir', '(', 'config_dir', ')', 'codec', '.', 'dumpu', '(', 'self', '.', 'data', ',', 'os', '.', 'path', '.', 'join', '(', 'config_dir', ',', ""'config.'"", '+', 'codec', '.', 'extension', ')', ')']",writes config to config_dir using config_name,"['writes', 'config', 'to', 'config_dir', 'using', 'config_name']",python,W,0,True,1,train
25073,ska-sa/purr,Purr/Plugins/local_pychart/basecanvas.py,https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Plugins/local_pychart/basecanvas.py#L105-L113,"def close(self):
        """"""This method closes the canvas and writes
        contents to the associated file.
        Calling this procedure is optional, because
        Pychart calls this procedure for every open canvas on normal exit.""""""
        for i in range(0, len(active_canvases)):
            if active_canvases[i] == self:
                del active_canvases[i]
                return","['def', 'close', '(', 'self', ')', ':', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'active_canvases', ')', ')', ':', 'if', 'active_canvases', '[', 'i', ']', '==', 'self', ':', 'del', 'active_canvases', '[', 'i', ']', 'return']","This method closes the canvas and writes
        contents to the associated file.
        Calling this procedure is optional, because
        Pychart calls this procedure for every open canvas on normal exit.","['This', 'method', 'closes', 'the', 'canvas', 'and', 'writes', 'contents', 'to', 'the', 'associated', 'file', '.', 'Calling', 'this', 'procedure', 'is', 'optional', 'because', 'Pychart', 'calls', 'this', 'procedure', 'for', 'every', 'open', 'canvas', 'on', 'normal', 'exit', '.']",python,W,0,True,1,train
25462,ska-sa/purr,Purr/Plugins/local_pychart/chart_data.py,https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Plugins/local_pychart/chart_data.py#L258-L268,"def write_csv(path, data):
    """"""This function writes comma-separated <data> to
    <path>. Parameter <path> is either a pathname or a file-like
    object that supports the |write()| method.""""""

    fd = _try_open_file(path, 'w',
                        'The first argument must be a pathname or an object that supports write() method')
    for v in data:
        fd.write("","".join([str(x) for x in v]))
        fd.write(""\n"")
    _try_close_file(fd, path)","['def', 'write_csv', '(', 'path', ',', 'data', ')', ':', 'fd', '=', '_try_open_file', '(', 'path', ',', ""'w'"", ',', ""'The first argument must be a pathname or an object that supports write() method'"", ')', 'for', 'v', 'in', 'data', ':', 'fd', '.', 'write', '(', '"",""', '.', 'join', '(', '[', 'str', '(', 'x', ')', 'for', 'x', 'in', 'v', ']', ')', ')', 'fd', '.', 'write', '(', '""\\n""', ')', '_try_close_file', '(', 'fd', ',', 'path', ')']","This function writes comma-separated <data> to
    <path>. Parameter <path> is either a pathname or a file-like
    object that supports the |write()| method.","['This', 'function', 'writes', 'comma', '-', 'separated', '<data', '>', 'to', '<path', '>', '.', 'Parameter', '<path', '>', 'is', 'either', 'a', 'pathname', 'or', 'a', 'file', '-', 'like', 'object', 'that', 'supports', 'the', '|write', '()', '|', 'method', '.']",python,W,0,True,1,train
25543,f3at/feat,src/feat/extern/log/log.py,https://github.com/f3at/feat/blob/15da93fc9d6ec8154f52a9172824e25821195ef8/src/feat/extern/log/log.py#L422-L452,"def stderrHandler(level, object, category, file, line, message):
    """"""
    A log handler that writes to stderr.

    @type level:    string
    @type object:   string (or None)
    @type category: string
    @type message:  string
    """"""

    o = """"
    if object:
        o = '""' + object + '""'

    where = ""(%s:%d)"" % (file, line)

    # level   pid     object   cat      time
    # 5 + 1 + 7 + 1 + 32 + 1 + 17 + 1 + 15 == 80
    safeprintf(sys.stderr, '%s [%5d] %-32s %-17s %-15s ',
               getFormattedLevelName(level), os.getpid(), o, category,
               time.strftime(""%b %d %H:%M:%S""))

    try:
        safeprintf(sys.stderr, '%-4s %s %s\n', """", message, where)
    except UnicodeEncodeError:
        # this can happen if message is a unicode object, convert it back into
        # a string using the UTF-8 encoding
        message = message.encode('UTF-8')
        safeprintf(sys.stderr, '%-4s %s %s\n', """", message, where)

    sys.stderr.flush()","['def', 'stderrHandler', '(', 'level', ',', 'object', ',', 'category', ',', 'file', ',', 'line', ',', 'message', ')', ':', 'o', '=', '""""', 'if', 'object', ':', 'o', '=', '\'""\'', '+', 'object', '+', '\'""\'', 'where', '=', '""(%s:%d)""', '%', '(', 'file', ',', 'line', ')', '# level   pid     object   cat      time', '# 5 + 1 + 7 + 1 + 32 + 1 + 17 + 1 + 15 == 80', 'safeprintf', '(', 'sys', '.', 'stderr', ',', ""'%s [%5d] %-32s %-17s %-15s '"", ',', 'getFormattedLevelName', '(', 'level', ')', ',', 'os', '.', 'getpid', '(', ')', ',', 'o', ',', 'category', ',', 'time', '.', 'strftime', '(', '""%b %d %H:%M:%S""', ')', ')', 'try', ':', 'safeprintf', '(', 'sys', '.', 'stderr', ',', ""'%-4s %s %s\\n'"", ',', '""""', ',', 'message', ',', 'where', ')', 'except', 'UnicodeEncodeError', ':', '# this can happen if message is a unicode object, convert it back into', '# a string using the UTF-8 encoding', 'message', '=', 'message', '.', 'encode', '(', ""'UTF-8'"", ')', 'safeprintf', '(', 'sys', '.', 'stderr', ',', ""'%-4s %s %s\\n'"", ',', '""""', ',', 'message', ',', 'where', ')', 'sys', '.', 'stderr', '.', 'flush', '(', ')']","A log handler that writes to stderr.

    @type level:    string
    @type object:   string (or None)
    @type category: string
    @type message:  string","['A', 'log', 'handler', 'that', 'writes', 'to', 'stderr', '.']",python,W,0,True,1,train
26440,ska-sa/purr,Purr/Pipe.py,https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Pipe.py#L24-L30,"def _write(self, what):
        """"""writes something to the Purr pipe""""""
        try:
            open(self.pipefile, ""a"").write(what)
        except:
            print(""Error writing to %s:"" % self.pipefile)
            traceback.print_exc()","['def', '_write', '(', 'self', ',', 'what', ')', ':', 'try', ':', 'open', '(', 'self', '.', 'pipefile', ',', '""a""', ')', '.', 'write', '(', 'what', ')', 'except', ':', 'print', '(', '""Error writing to %s:""', '%', 'self', '.', 'pipefile', ')', 'traceback', '.', 'print_exc', '(', ')']",writes something to the Purr pipe,"['writes', 'something', 'to', 'the', 'Purr', 'pipe']",python,W,0,True,1,train
26573,Jma353/py-podcast,podcasts/series_worker.py,https://github.com/Jma353/py-podcast/blob/ad1f7fe17815edd6a92c1600e1c0f15a4dd82756/podcasts/series_worker.py#L20-L46,"def run(self):
    """"""
    Requests, parses series, writes to appropriate CSV
    """"""
    empty = False
    while not empty:
      try:
        # Grab fields
        url = self.genre_urls.get()
        namestamp = ""{}.csv"".format(str(int(round(time.time() * 1000000))))
        # GET request
        self.logger.info('Attempting to request %s', url)
        self.crawler.set_url(url)
        series = self.crawler.get_series()
        self.logger.info('Attempting to write %s', url)
        # Grab writer -> writes series
        csv_dir = './{}/{}'.format(self.directory, namestamp)
        writer = csv.writer(open(csv_dir, 'wb'))
        writer.writerow(Series.fields)
        for s in series:
          writer.writerow(s.to_line())
        self.logger.info('Wrote %s', namestamp)
      except Exception, e: # pylint: disable=W0703
        print e
      finally:
        self.genre_urls.task_done()
        empty = self.genre_urls.empty()","['def', 'run', '(', 'self', ')', ':', 'empty', '=', 'False', 'while', 'not', 'empty', ':', 'try', ':', '# Grab fields', 'url', '=', 'self', '.', 'genre_urls', '.', 'get', '(', ')', 'namestamp', '=', '""{}.csv""', '.', 'format', '(', 'str', '(', 'int', '(', 'round', '(', 'time', '.', 'time', '(', ')', '*', '1000000', ')', ')', ')', ')', '# GET request', 'self', '.', 'logger', '.', 'info', '(', ""'Attempting to request %s'"", ',', 'url', ')', 'self', '.', 'crawler', '.', 'set_url', '(', 'url', ')', 'series', '=', 'self', '.', 'crawler', '.', 'get_series', '(', ')', 'self', '.', 'logger', '.', 'info', '(', ""'Attempting to write %s'"", ',', 'url', ')', '# Grab writer -> writes series', 'csv_dir', '=', ""'./{}/{}'"", '.', 'format', '(', 'self', '.', 'directory', ',', 'namestamp', ')', 'writer', '=', 'csv', '.', 'writer', '(', 'open', '(', 'csv_dir', ',', ""'wb'"", ')', ')', 'writer', '.', 'writerow', '(', 'Series', '.', 'fields', ')', 'for', 's', 'in', 'series', ':', 'writer', '.', 'writerow', '(', 's', '.', 'to_line', '(', ')', ')', 'self', '.', 'logger', '.', 'info', '(', ""'Wrote %s'"", ',', 'namestamp', ')', 'except', 'Exception', ',', 'e', ':', '# pylint: disable=W0703', 'print', 'e', 'finally', ':', 'self', '.', 'genre_urls', '.', 'task_done', '(', ')', 'empty', '=', 'self', '.', 'genre_urls', '.', 'empty', '(', ')']","Requests, parses series, writes to appropriate CSV","['Requests', 'parses', 'series', 'writes', 'to', 'appropriate', 'CSV']",python,W,0,True,1,train
317,bwesterb/sarah,src/io.py,https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/io.py#L234-L241,"def pump(fin, fout):
    """""" Reads data from <fin> and writes it to <fout> until EOF is
        reached. """"""
    while True:
        tmp = fin.read(4096)
        if not tmp:
            break
        fout.write(tmp)","['def', 'pump', '(', 'fin', ',', 'fout', ')', ':', 'while', 'True', ':', 'tmp', '=', 'fin', '.', 'read', '(', '4096', ')', 'if', 'not', 'tmp', ':', 'break', 'fout', '.', 'write', '(', 'tmp', ')']","Reads data from <fin> and writes it to <fout> until EOF is
        reached.","['Reads', 'data', 'from', '<fin', '>', 'and', 'writes', 'it', 'to', '<fout', '>', 'until', 'EOF', 'is', 'reached', '.']",python,W,0,True,1,train
2676,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py#L434-L457,"def get_metrics_rollups_queue(self, name, queue_name, metric):
        '''
        This operation gets rollup data for Service Bus metrics queue.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        queue_name:
            Name of the service bus queue in this namespace.
        metric:
            name of a supported metric
        '''
        response = self._perform_get(
            self._get_get_metrics_rollup_queue_path(name, queue_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )","['def', 'get_metrics_rollups_queue', '(', 'self', ',', 'name', ',', 'queue_name', ',', 'metric', ')', ':', 'response', '=', 'self', '.', '_perform_get', '(', 'self', '.', '_get_get_metrics_rollup_queue_path', '(', 'name', ',', 'queue_name', ',', 'metric', ')', ',', 'None', ')', 'return', '_MinidomXmlToObject', '.', 'convert_response_to_feeds', '(', 'response', ',', 'partial', '(', '_ServiceBusManagementXmlSerializer', '.', 'xml_to_metrics', ',', 'object_type', '=', 'MetricRollups', ')', ')']","This operation gets rollup data for Service Bus metrics queue.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        queue_name:
            Name of the service bus queue in this namespace.
        metric:
            name of a supported metric","['This', 'operation', 'gets', 'rollup', 'data', 'for', 'Service', 'Bus', 'metrics', 'queue', '.', 'Rollup', 'data', 'includes', 'the', 'time', 'granularity', 'for', 'the', 'telemetry', 'aggregation', 'as', 'well', 'as', 'the', 'retention', 'settings', 'for', 'each', 'time', 'granularity', '.']",python,E,3,True,1,test
2677,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py#L459-L482,"def get_metrics_rollups_topic(self, name, topic_name, metric):
        '''
        This operation gets rollup data for Service Bus metrics topic.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        topic_name:
            Name of the service bus queue in this namespace.
        metric:
            name of a supported metric
        '''
        response = self._perform_get(
            self._get_get_metrics_rollup_topic_path(name, topic_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )","['def', 'get_metrics_rollups_topic', '(', 'self', ',', 'name', ',', 'topic_name', ',', 'metric', ')', ':', 'response', '=', 'self', '.', '_perform_get', '(', 'self', '.', '_get_get_metrics_rollup_topic_path', '(', 'name', ',', 'topic_name', ',', 'metric', ')', ',', 'None', ')', 'return', '_MinidomXmlToObject', '.', 'convert_response_to_feeds', '(', 'response', ',', 'partial', '(', '_ServiceBusManagementXmlSerializer', '.', 'xml_to_metrics', ',', 'object_type', '=', 'MetricRollups', ')', ')']","This operation gets rollup data for Service Bus metrics topic.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        topic_name:
            Name of the service bus queue in this namespace.
        metric:
            name of a supported metric","['This', 'operation', 'gets', 'rollup', 'data', 'for', 'Service', 'Bus', 'metrics', 'topic', '.', 'Rollup', 'data', 'includes', 'the', 'time', 'granularity', 'for', 'the', 'telemetry', 'aggregation', 'as', 'well', 'as', 'the', 'retention', 'settings', 'for', 'each', 'time', 'granularity', '.']",python,E,3,True,1,test
2678,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py#L484-L507,"def get_metrics_rollups_notification_hub(self, name, hub_name, metric):
        '''
        This operation gets rollup data for Service Bus metrics notification hub.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        hub_name:
            Name of the service bus notification hub in this namespace.
        metric:
            name of a supported metric
        '''
        response = self._perform_get(
            self._get_get_metrics_rollup_hub_path(name, hub_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )","['def', 'get_metrics_rollups_notification_hub', '(', 'self', ',', 'name', ',', 'hub_name', ',', 'metric', ')', ':', 'response', '=', 'self', '.', '_perform_get', '(', 'self', '.', '_get_get_metrics_rollup_hub_path', '(', 'name', ',', 'hub_name', ',', 'metric', ')', ',', 'None', ')', 'return', '_MinidomXmlToObject', '.', 'convert_response_to_feeds', '(', 'response', ',', 'partial', '(', '_ServiceBusManagementXmlSerializer', '.', 'xml_to_metrics', ',', 'object_type', '=', 'MetricRollups', ')', ')']","This operation gets rollup data for Service Bus metrics notification hub.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        hub_name:
            Name of the service bus notification hub in this namespace.
        metric:
            name of a supported metric","['This', 'operation', 'gets', 'rollup', 'data', 'for', 'Service', 'Bus', 'metrics', 'notification', 'hub', '.', 'Rollup', 'data', 'includes', 'the', 'time', 'granularity', 'for', 'the', 'telemetry', 'aggregation', 'as', 'well', 'as', 'the', 'retention', 'settings', 'for', 'each', 'time', 'granularity', '.']",python,E,3,True,1,test
2679,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicebusmanagementservice.py#L509-L532,"def get_metrics_rollups_relay(self, name, relay_name, metric):
        '''
        This operation gets rollup data for Service Bus metrics relay.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        relay_name:
            Name of the service bus relay in this namespace.
        metric:
            name of a supported metric
        '''
        response = self._perform_get(
            self._get_get_metrics_rollup_relay_path(name, relay_name, metric),
            None)

        return _MinidomXmlToObject.convert_response_to_feeds(
            response,
            partial(
                _ServiceBusManagementXmlSerializer.xml_to_metrics,
                object_type=MetricRollups
            )
        )","['def', 'get_metrics_rollups_relay', '(', 'self', ',', 'name', ',', 'relay_name', ',', 'metric', ')', ':', 'response', '=', 'self', '.', '_perform_get', '(', 'self', '.', '_get_get_metrics_rollup_relay_path', '(', 'name', ',', 'relay_name', ',', 'metric', ')', ',', 'None', ')', 'return', '_MinidomXmlToObject', '.', 'convert_response_to_feeds', '(', 'response', ',', 'partial', '(', '_ServiceBusManagementXmlSerializer', '.', 'xml_to_metrics', ',', 'object_type', '=', 'MetricRollups', ')', ')']","This operation gets rollup data for Service Bus metrics relay.
        Rollup data includes the time granularity for the telemetry aggregation as well as
        the retention settings for each time granularity.

        name:
            Name of the service bus namespace.
        relay_name:
            Name of the service bus relay in this namespace.
        metric:
            name of a supported metric","['This', 'operation', 'gets', 'rollup', 'data', 'for', 'Service', 'Bus', 'metrics', 'relay', '.', 'Rollup', 'data', 'includes', 'the', 'time', 'granularity', 'for', 'the', 'telemetry', 'aggregation', 'as', 'well', 'as', 'the', 'retention', 'settings', 'for', 'each', 'time', 'granularity', '.']",python,E,3,True,1,test
2858,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/schedulermanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/schedulermanagementservice.py#L127-L137,"def delete_cloud_service(self, cloud_service_id):
        '''
        The Get Cloud Service operation gets all the resources (job collections)
        in the cloud service.

        cloud_service_id:
            The cloud service id
        '''
        _validate_not_none('cloud_service_id', cloud_service_id)
        path = self._get_cloud_services_path(cloud_service_id)
        return self._perform_delete(path, as_async=True)","['def', 'delete_cloud_service', '(', 'self', ',', 'cloud_service_id', ')', ':', '_validate_not_none', '(', ""'cloud_service_id'"", ',', 'cloud_service_id', ')', 'path', '=', 'self', '.', '_get_cloud_services_path', '(', 'cloud_service_id', ')', 'return', 'self', '.', '_perform_delete', '(', 'path', ',', 'as_async', '=', 'True', ')']","The Get Cloud Service operation gets all the resources (job collections)
        in the cloud service.

        cloud_service_id:
            The cloud service id","['The', 'Get', 'Cloud', 'Service', 'operation', 'gets', 'all', 'the', 'resources', '(', 'job', 'collections', ')', 'in', 'the', 'cloud', 'service', '.']",python,E,3,True,1,test
2862,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/schedulermanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/schedulermanagementservice.py#L204-L219,"def get_job_collection(self, cloud_service_id, job_collection_id):
        '''
        The Get Job Collection operation gets the details of a job collection

        cloud_service_id:
            The cloud service id
        job_collection_id:
            Name of the hosted service.
        '''
        _validate_not_none('cloud_service_id', cloud_service_id)
        _validate_not_none('job_collection_id', job_collection_id)

        path = self._get_job_collection_path(
            cloud_service_id, job_collection_id)

        return self._perform_get(path, Resource)","['def', 'get_job_collection', '(', 'self', ',', 'cloud_service_id', ',', 'job_collection_id', ')', ':', '_validate_not_none', '(', ""'cloud_service_id'"", ',', 'cloud_service_id', ')', '_validate_not_none', '(', ""'job_collection_id'"", ',', 'job_collection_id', ')', 'path', '=', 'self', '.', '_get_job_collection_path', '(', 'cloud_service_id', ',', 'job_collection_id', ')', 'return', 'self', '.', '_perform_get', '(', 'path', ',', 'Resource', ')']","The Get Job Collection operation gets the details of a job collection

        cloud_service_id:
            The cloud service id
        job_collection_id:
            Name of the hosted service.","['The', 'Get', 'Job', 'Collection', 'operation', 'gets', 'the', 'details', 'of', 'a', 'job', 'collection']",python,E,3,True,1,test
2865,Azure/azure-sdk-for-python,azure-servicemanagement-legacy/azure/servicemanagement/schedulermanagementservice.py,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/schedulermanagementservice.py#L262-L285,"def get_job(self, cloud_service_id, job_collection_id, job_id):
        '''
        The Get Job operation gets the details (including the current job status)
        of the specified job from the specified job collection.

        The return type is

        cloud_service_id:
            The cloud service id
        job_collection_id:
            Name of the hosted service.
        job_id:
            The job id you wish to create.
        '''
        _validate_not_none('cloud_service_id', cloud_service_id)
        _validate_not_none('job_collection_id', job_collection_id)
        _validate_not_none('job_id', job_id)

        path = self._get_job_collection_path(
            cloud_service_id, job_collection_id, job_id)

        self.content_type = ""application/json""
        payload = self._perform_get(path).body.decode()
        return json.loads(payload)","['def', 'get_job', '(', 'self', ',', 'cloud_service_id', ',', 'job_collection_id', ',', 'job_id', ')', ':', '_validate_not_none', '(', ""'cloud_service_id'"", ',', 'cloud_service_id', ')', '_validate_not_none', '(', ""'job_collection_id'"", ',', 'job_collection_id', ')', '_validate_not_none', '(', ""'job_id'"", ',', 'job_id', ')', 'path', '=', 'self', '.', '_get_job_collection_path', '(', 'cloud_service_id', ',', 'job_collection_id', ',', 'job_id', ')', 'self', '.', 'content_type', '=', '""application/json""', 'payload', '=', 'self', '.', '_perform_get', '(', 'path', ')', '.', 'body', '.', 'decode', '(', ')', 'return', 'json', '.', 'loads', '(', 'payload', ')']","The Get Job operation gets the details (including the current job status)
        of the specified job from the specified job collection.

        The return type is

        cloud_service_id:
            The cloud service id
        job_collection_id:
            Name of the hosted service.
        job_id:
            The job id you wish to create.","['The', 'Get', 'Job', 'operation', 'gets', 'the', 'details', '(', 'including', 'the', 'current', 'job', 'status', ')', 'of', 'the', 'specified', 'job', 'from', 'the', 'specified', 'job', 'collection', '.']",python,E,3,True,1,test
4783,SectorLabs/django-postgres-extra,psqlextra/manager/manager.py,https://github.com/SectorLabs/django-postgres-extra/blob/eef2ed5504d225858d4e4f5d77a838082ca6053e/psqlextra/manager/manager.py#L197-L236,"def insert_and_get(self, **fields):
        """"""Creates a new record in the database and then gets
        the entire row.

        This allows specifying custom conflict behavior using .on_conflict().
        If no special behavior was specified, this uses the normal Django create(..)

        Arguments:
            fields:
                The fields of the row to create.

        Returns:
            The model instance representing the row that was created.
        """"""

        if not self.conflict_target and not self.conflict_action:
            # no special action required, use the standard Django create(..)
            return super().create(**fields)

        compiler = self._build_insert_compiler([fields])
        rows = compiler.execute_sql(return_id=False)

        columns = rows[0]

        # get a list of columns that are officially part of the model and preserve the fact that the attribute name
        # might be different than the database column name
        model_columns = {}
        for field in self.model._meta.local_concrete_fields:
            model_columns[field.column] = field.attname

        # strip out any columns/fields returned by the db that
        # are not present in the model
        model_init_fields = {}
        for column_name, column_value in columns.items():
            try:
                model_init_fields[model_columns[column_name]] = column_value
            except KeyError:
                pass

        return self.model(**model_init_fields)","['def', 'insert_and_get', '(', 'self', ',', '*', '*', 'fields', ')', ':', 'if', 'not', 'self', '.', 'conflict_target', 'and', 'not', 'self', '.', 'conflict_action', ':', '# no special action required, use the standard Django create(..)', 'return', 'super', '(', ')', '.', 'create', '(', '*', '*', 'fields', ')', 'compiler', '=', 'self', '.', '_build_insert_compiler', '(', '[', 'fields', ']', ')', 'rows', '=', 'compiler', '.', 'execute_sql', '(', 'return_id', '=', 'False', ')', 'columns', '=', 'rows', '[', '0', ']', '# get a list of columns that are officially part of the model and preserve the fact that the attribute name', '# might be different than the database column name', 'model_columns', '=', '{', '}', 'for', 'field', 'in', 'self', '.', 'model', '.', '_meta', '.', 'local_concrete_fields', ':', 'model_columns', '[', 'field', '.', 'column', ']', '=', 'field', '.', 'attname', '# strip out any columns/fields returned by the db that', '# are not present in the model', 'model_init_fields', '=', '{', '}', 'for', 'column_name', ',', 'column_value', 'in', 'columns', '.', 'items', '(', ')', ':', 'try', ':', 'model_init_fields', '[', 'model_columns', '[', 'column_name', ']', ']', '=', 'column_value', 'except', 'KeyError', ':', 'pass', 'return', 'self', '.', 'model', '(', '*', '*', 'model_init_fields', ')']","Creates a new record in the database and then gets
        the entire row.

        This allows specifying custom conflict behavior using .on_conflict().
        If no special behavior was specified, this uses the normal Django create(..)

        Arguments:
            fields:
                The fields of the row to create.

        Returns:
            The model instance representing the row that was created.","['Creates', 'a', 'new', 'record', 'in', 'the', 'database', 'and', 'then', 'gets', 'the', 'entire', 'row', '.']",python,E,3,True,1,test
4785,SectorLabs/django-postgres-extra,psqlextra/manager/manager.py,https://github.com/SectorLabs/django-postgres-extra/blob/eef2ed5504d225858d4e4f5d77a838082ca6053e/psqlextra/manager/manager.py#L260-L281,"def upsert_and_get(self, conflict_target: List, fields: Dict, index_predicate: str=None):
        """"""Creates a new record or updates the existing one
        with the specified data and then gets the row.

        Arguments:
            conflict_target:
                Fields to pass into the ON CONFLICT clause.

            fields:
                Fields to insert/update.

            index_predicate:
                The index predicate to satisfy an arbiter partial index (i.e. what partial index to use for checking
                conflicts)

        Returns:
            The model instance representing the row
            that was created/updated.
        """"""

        self.on_conflict(conflict_target, ConflictAction.UPDATE, index_predicate)
        return self.insert_and_get(**fields)","['def', 'upsert_and_get', '(', 'self', ',', 'conflict_target', ':', 'List', ',', 'fields', ':', 'Dict', ',', 'index_predicate', ':', 'str', '=', 'None', ')', ':', 'self', '.', 'on_conflict', '(', 'conflict_target', ',', 'ConflictAction', '.', 'UPDATE', ',', 'index_predicate', ')', 'return', 'self', '.', 'insert_and_get', '(', '*', '*', 'fields', ')']","Creates a new record or updates the existing one
        with the specified data and then gets the row.

        Arguments:
            conflict_target:
                Fields to pass into the ON CONFLICT clause.

            fields:
                Fields to insert/update.

            index_predicate:
                The index predicate to satisfy an arbiter partial index (i.e. what partial index to use for checking
                conflicts)

        Returns:
            The model instance representing the row
            that was created/updated.","['Creates', 'a', 'new', 'record', 'or', 'updates', 'the', 'existing', 'one', 'with', 'the', 'specified', 'data', 'and', 'then', 'gets', 'the', 'row', '.']",python,E,3,True,1,test
4792,SectorLabs/django-postgres-extra,psqlextra/manager/manager.py,https://github.com/SectorLabs/django-postgres-extra/blob/eef2ed5504d225858d4e4f5d77a838082ca6053e/psqlextra/manager/manager.py#L527-L546,"def upsert_and_get(self, conflict_target: List, fields: Dict, index_predicate: str=None):
        """"""Creates a new record or updates the existing one
        with the specified data and then gets the row.

        Arguments:
            conflict_target:
                Fields to pass into the ON CONFLICT clause.

            fields:
                Fields to insert/update.

            index_predicate:
                The index predicate to satisfy an arbiter partial index.

        Returns:
            The model instance representing the row
            that was created/updated.
        """"""

        return self.get_queryset().upsert_and_get(conflict_target, fields, index_predicate)","['def', 'upsert_and_get', '(', 'self', ',', 'conflict_target', ':', 'List', ',', 'fields', ':', 'Dict', ',', 'index_predicate', ':', 'str', '=', 'None', ')', ':', 'return', 'self', '.', 'get_queryset', '(', ')', '.', 'upsert_and_get', '(', 'conflict_target', ',', 'fields', ',', 'index_predicate', ')']","Creates a new record or updates the existing one
        with the specified data and then gets the row.

        Arguments:
            conflict_target:
                Fields to pass into the ON CONFLICT clause.

            fields:
                Fields to insert/update.

            index_predicate:
                The index predicate to satisfy an arbiter partial index.

        Returns:
            The model instance representing the row
            that was created/updated.","['Creates', 'a', 'new', 'record', 'or', 'updates', 'the', 'existing', 'one', 'with', 'the', 'specified', 'data', 'and', 'then', 'gets', 'the', 'row', '.']",python,E,3,True,1,test
6418,maxcountryman/flask-uploads,flask_uploads.py,https://github.com/maxcountryman/flask-uploads/blob/dc24fa0c53d605876e5b4502cadffdf1a4345b1d/flask_uploads.py#L315-L329,"def config(self):
        """"""
        This gets the current configuration. By default, it looks up the
        current application and gets the configuration from there. But if you
        don't want to go to the full effort of setting an application, or it's
        otherwise outside of a request context, set the `_config` attribute to
        an `UploadConfiguration` instance, then set it back to `None` when
        you're done.
        """"""
        if self._config is not None:
            return self._config
        try:
            return current_app.upload_set_config[self.name]
        except AttributeError:
            raise RuntimeError(""cannot access configuration outside request"")","['def', 'config', '(', 'self', ')', ':', 'if', 'self', '.', '_config', 'is', 'not', 'None', ':', 'return', 'self', '.', '_config', 'try', ':', 'return', 'current_app', '.', 'upload_set_config', '[', 'self', '.', 'name', ']', 'except', 'AttributeError', ':', 'raise', 'RuntimeError', '(', '""cannot access configuration outside request""', ')']","This gets the current configuration. By default, it looks up the
        current application and gets the configuration from there. But if you
        don't want to go to the full effort of setting an application, or it's
        otherwise outside of a request context, set the `_config` attribute to
        an `UploadConfiguration` instance, then set it back to `None` when
        you're done.","['This', 'gets', 'the', 'current', 'configuration', '.', 'By', 'default', 'it', 'looks', 'up', 'the', 'current', 'application', 'and', 'gets', 'the', 'configuration', 'from', 'there', '.', 'But', 'if', 'you', 'don', 't', 'want', 'to', 'go', 'to', 'the', 'full', 'effort', 'of', 'setting', 'an', 'application', 'or', 'it', 's', 'otherwise', 'outside', 'of', 'a', 'request', 'context', 'set', 'the', '_config', 'attribute', 'to', 'an', 'UploadConfiguration', 'instance', 'then', 'set', 'it', 'back', 'to', 'None', 'when', 'you', 're', 'done', '.']",python,E,3,True,1,test
6419,maxcountryman/flask-uploads,flask_uploads.py,https://github.com/maxcountryman/flask-uploads/blob/dc24fa0c53d605876e5b4502cadffdf1a4345b1d/flask_uploads.py#L331-L343,"def url(self, filename):
        """"""
        This function gets the URL a file uploaded to this set would be
        accessed at. It doesn't check whether said file exists.

        :param filename: The filename to return the URL for.
        """"""
        base = self.config.base_url
        if base is None:
            return url_for('_uploads.uploaded_file', setname=self.name,
                           filename=filename, _external=True)
        else:
            return base + filename","['def', 'url', '(', 'self', ',', 'filename', ')', ':', 'base', '=', 'self', '.', 'config', '.', 'base_url', 'if', 'base', 'is', 'None', ':', 'return', 'url_for', '(', ""'_uploads.uploaded_file'"", ',', 'setname', '=', 'self', '.', 'name', ',', 'filename', '=', 'filename', ',', '_external', '=', 'True', ')', 'else', ':', 'return', 'base', '+', 'filename']","This function gets the URL a file uploaded to this set would be
        accessed at. It doesn't check whether said file exists.

        :param filename: The filename to return the URL for.","['This', 'function', 'gets', 'the', 'URL', 'a', 'file', 'uploaded', 'to', 'this', 'set', 'would', 'be', 'accessed', 'at', '.', 'It', 'doesn', 't', 'check', 'whether', 'said', 'file', 'exists', '.']",python,E,3,True,1,test
7572,iotaledger/iota.lib.py,iota/api.py,https://github.com/iotaledger/iota.lib.py/blob/97cdd1e241498446b46157b79b2a1ea2ec6d387a/iota/api.py#L927-L971,"def replay_bundle(
            self,
            transaction,
            depth=3,
            min_weight_magnitude=None,
    ):
        # type: (TransactionHash, int, Optional[int]) -> dict
        """"""
        Takes a tail transaction hash as input, gets the bundle
        associated with the transaction and then replays the bundle by
        attaching it to the Tangle.

        :param transaction:
            Transaction hash.  Must be a tail.

        :param depth:
            Depth at which to attach the bundle.
            Defaults to 3.

        :param min_weight_magnitude:
            Min weight magnitude, used by the node to calibrate Proof of
            Work.

            If not provided, a default value will be used.

        :return:
            Dict with the following structure::

                {
                    'trytes': List[TransactionTrytes],
                        Raw trytes that were published to the Tangle.
                }

        References:

        - https://github.com/iotaledger/wiki/blob/master/api-proposal.md#replaytransfer
        """"""
        if min_weight_magnitude is None:
            min_weight_magnitude = self.default_min_weight_magnitude

        return extended.ReplayBundleCommand(self.adapter)(
            transaction=transaction,
            depth=depth,
            minWeightMagnitude=min_weight_magnitude,
        )","['def', 'replay_bundle', '(', 'self', ',', 'transaction', ',', 'depth', '=', '3', ',', 'min_weight_magnitude', '=', 'None', ',', ')', ':', '# type: (TransactionHash, int, Optional[int]) -> dict', 'if', 'min_weight_magnitude', 'is', 'None', ':', 'min_weight_magnitude', '=', 'self', '.', 'default_min_weight_magnitude', 'return', 'extended', '.', 'ReplayBundleCommand', '(', 'self', '.', 'adapter', ')', '(', 'transaction', '=', 'transaction', ',', 'depth', '=', 'depth', ',', 'minWeightMagnitude', '=', 'min_weight_magnitude', ',', ')']","Takes a tail transaction hash as input, gets the bundle
        associated with the transaction and then replays the bundle by
        attaching it to the Tangle.

        :param transaction:
            Transaction hash.  Must be a tail.

        :param depth:
            Depth at which to attach the bundle.
            Defaults to 3.

        :param min_weight_magnitude:
            Min weight magnitude, used by the node to calibrate Proof of
            Work.

            If not provided, a default value will be used.

        :return:
            Dict with the following structure::

                {
                    'trytes': List[TransactionTrytes],
                        Raw trytes that were published to the Tangle.
                }

        References:

        - https://github.com/iotaledger/wiki/blob/master/api-proposal.md#replaytransfer","['Takes', 'a', 'tail', 'transaction', 'hash', 'as', 'input', 'gets', 'the', 'bundle', 'associated', 'with', 'the', 'transaction', 'and', 'then', 'replays', 'the', 'bundle', 'by', 'attaching', 'it', 'to', 'the', 'Tangle', '.']",python,E,3,True,1,test
8761,josiahcarlson/rom,rom/util.py,https://github.com/josiahcarlson/rom/blob/8b5607a856341df85df33422accc30ba9294dbdb/rom/util.py#L181-L193,"def _connect(obj):
    '''
    Tries to get the _conn attribute from a model. Barring that, gets the
    global default connection using other methods.
    '''
    from .columns import MODELS
    if isinstance(obj, MODELS['Model']):
        obj = obj.__class__
    if hasattr(obj, '_conn'):
        return obj._conn
    if hasattr(obj, 'CONN'):
        return obj.CONN
    return get_connection()","['def', '_connect', '(', 'obj', ')', ':', 'from', '.', 'columns', 'import', 'MODELS', 'if', 'isinstance', '(', 'obj', ',', 'MODELS', '[', ""'Model'"", ']', ')', ':', 'obj', '=', 'obj', '.', '__class__', 'if', 'hasattr', '(', 'obj', ',', ""'_conn'"", ')', ':', 'return', 'obj', '.', '_conn', 'if', 'hasattr', '(', 'obj', ',', ""'CONN'"", ')', ':', 'return', 'obj', '.', 'CONN', 'return', 'get_connection', '(', ')']","Tries to get the _conn attribute from a model. Barring that, gets the
    global default connection using other methods.","['Tries', 'to', 'get', 'the', '_conn', 'attribute', 'from', 'a', 'model', '.', 'Barring', 'that', 'gets', 'the', 'global', 'default', 'connection', 'using', 'other', 'methods', '.']",python,E,3,True,1,test
9892,MisterY/price-database,pricedb/config.py,https://github.com/MisterY/price-database/blob/b4fd366b7763891c690fe3000b8840e656da023e/pricedb/config.py#L63-L68,"def __get_config_template_path(self) -> str:
        """""" gets the default config path from resources """"""
        filename = resource_filename(
            Requirement.parse(package_name),
            template_path + config_filename)
        return filename","['def', '__get_config_template_path', '(', 'self', ')', '->', 'str', ':', 'filename', '=', 'resource_filename', '(', 'Requirement', '.', 'parse', '(', 'package_name', ')', ',', 'template_path', '+', 'config_filename', ')', 'return', 'filename']",gets the default config path from resources,"['gets', 'the', 'default', 'config', 'path', 'from', 'resources']",python,E,3,True,1,test
10312,LLNL/scraper,scripts/get_users_emails.py,https://github.com/LLNL/scraper/blob/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea/scripts/get_users_emails.py#L87-L102,"def get_mems_of_org(self):
        """"""
        Retrieves the emails of the members of the organization. Note this Only
        gets public emails. Private emails would need authentication for each
        user.
        """"""
        print 'Getting members\' emails.'
        for member in self.org_retrieved.iter_members():
            login = member.to_json()['login']
            user_email = self.logged_in_gh.user(login).to_json()['email']
            if user_email is not None:
                self.emails[login] = user_email
            else:#user has no public email
                self.emails[login] = 'none'
            #used for sorting regardless of case
            self.logins_lower[login.lower()] = login","['def', 'get_mems_of_org', '(', 'self', ')', ':', 'print', ""'Getting members\\' emails.'"", 'for', 'member', 'in', 'self', '.', 'org_retrieved', '.', 'iter_members', '(', ')', ':', 'login', '=', 'member', '.', 'to_json', '(', ')', '[', ""'login'"", ']', 'user_email', '=', 'self', '.', 'logged_in_gh', '.', 'user', '(', 'login', ')', '.', 'to_json', '(', ')', '[', ""'email'"", ']', 'if', 'user_email', 'is', 'not', 'None', ':', 'self', '.', 'emails', '[', 'login', ']', '=', 'user_email', 'else', ':', '#user has no public email', 'self', '.', 'emails', '[', 'login', ']', '=', ""'none'"", '#used for sorting regardless of case', 'self', '.', 'logins_lower', '[', 'login', '.', 'lower', '(', ')', ']', '=', 'login']","Retrieves the emails of the members of the organization. Note this Only
        gets public emails. Private emails would need authentication for each
        user.","['Retrieves', 'the', 'emails', 'of', 'the', 'members', 'of', 'the', 'organization', '.', 'Note', 'this', 'Only', 'gets', 'public', 'emails', '.', 'Private', 'emails', 'would', 'need', 'authentication', 'for', 'each', 'user', '.']",python,E,3,True,1,test
10337,LLNL/scraper,scripts/get_year_commits.py,https://github.com/LLNL/scraper/blob/881a316e4c04dfa5a9cf491b7c7f9f997a7c56ea/scripts/get_year_commits.py#L11-L34,"def get_year_commits(self, username='', password='', organization='llnl', force=True):
        """"""
        Does setup such as login, printing API info, and waiting for GitHub to
        build the commit statistics. Then gets the last year of commits and
        prints them to file.
        """"""
        date = str(datetime.date.today())
        file_path =  ('year_commits.csv')
        if force or not os.path.isfile(file_path):
            my_github.login(username, password)
            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1
            print 'Rate Limit: ' + str(calls_beginning)
            my_github.get_org(organization)
            my_github.repos(building_stats=True)
            print ""Letting GitHub build statistics.""
            time.sleep(30)
            print ""Trying again.""
            my_github.repos(building_stats=False)
            my_github.calc_total_commits(starting_commits=35163)
            my_github.write_to_file()
            calls_remaining = self.logged_in_gh.ratelimit_remaining
            calls_used = calls_beginning - calls_remaining
            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\nUsed '
                + str(calls_used) + ' API calls.')","['def', 'get_year_commits', '(', 'self', ',', 'username', '=', ""''"", ',', 'password', '=', ""''"", ',', 'organization', '=', ""'llnl'"", ',', 'force', '=', 'True', ')', ':', 'date', '=', 'str', '(', 'datetime', '.', 'date', '.', 'today', '(', ')', ')', 'file_path', '=', '(', ""'year_commits.csv'"", ')', 'if', 'force', 'or', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'file_path', ')', ':', 'my_github', '.', 'login', '(', 'username', ',', 'password', ')', 'calls_beginning', '=', 'self', '.', 'logged_in_gh', '.', 'ratelimit_remaining', '+', '1', 'print', ""'Rate Limit: '"", '+', 'str', '(', 'calls_beginning', ')', 'my_github', '.', 'get_org', '(', 'organization', ')', 'my_github', '.', 'repos', '(', 'building_stats', '=', 'True', ')', 'print', '""Letting GitHub build statistics.""', 'time', '.', 'sleep', '(', '30', ')', 'print', '""Trying again.""', 'my_github', '.', 'repos', '(', 'building_stats', '=', 'False', ')', 'my_github', '.', 'calc_total_commits', '(', 'starting_commits', '=', '35163', ')', 'my_github', '.', 'write_to_file', '(', ')', 'calls_remaining', '=', 'self', '.', 'logged_in_gh', '.', 'ratelimit_remaining', 'calls_used', '=', 'calls_beginning', '-', 'calls_remaining', 'print', '(', ""'Rate Limit Remaining: '"", '+', 'str', '(', 'calls_remaining', ')', '+', ""'\\nUsed '"", '+', 'str', '(', 'calls_used', ')', '+', ""' API calls.'"", ')']","Does setup such as login, printing API info, and waiting for GitHub to
        build the commit statistics. Then gets the last year of commits and
        prints them to file.","['Does', 'setup', 'such', 'as', 'login', 'printing', 'API', 'info', 'and', 'waiting', 'for', 'GitHub', 'to', 'build', 'the', 'commit', 'statistics', '.', 'Then', 'gets', 'the', 'last', 'year', 'of', 'commits', 'and', 'prints', 'them', 'to', 'file', '.']",python,E,3,True,1,test
16639,marrabld/planarradpy,gui/gui_mainLayout.py,https://github.com/marrabld/planarradpy/blob/5095d1cb98d4f67a7c3108c9282f2d59253e89a8/gui/gui_mainLayout.py#L138-L157,"def data(self):
        """"""
        This function gets back data that the user typed.
        """"""
        self.batch_name_value = self.ui.batch_name_value.text()
        self.saa_values = self.ui.saa_values.text()
        self.sza_values = self.ui.sza_values.text()
        self.p_values = self.ui.p_values.text()
        self.x_value = self.ui.x_value.text()
        self.y_value = self.ui.y_value.text()
        self.g_value = self.ui.g_value.text()
        self.s_value = self.ui.s_value.text()
        self.z_value = self.ui.z_value.text()
        self.wavelength_values = self.ui.wavelength_values.text()
        self.verbose_value = self.ui.verbose_value.text()
        self.phytoplankton_path = self.ui.phyto_path.text()
        self.bottom_path = self.ui.bottom_path.text()
        self.executive_path = self.ui.exec_path.text()
        self.nb_cpu = self.ui.nb_cpu.currentText()
        self.report_parameter_value = str(self.ui.report_parameter_value.text())","['def', 'data', '(', 'self', ')', ':', 'self', '.', 'batch_name_value', '=', 'self', '.', 'ui', '.', 'batch_name_value', '.', 'text', '(', ')', 'self', '.', 'saa_values', '=', 'self', '.', 'ui', '.', 'saa_values', '.', 'text', '(', ')', 'self', '.', 'sza_values', '=', 'self', '.', 'ui', '.', 'sza_values', '.', 'text', '(', ')', 'self', '.', 'p_values', '=', 'self', '.', 'ui', '.', 'p_values', '.', 'text', '(', ')', 'self', '.', 'x_value', '=', 'self', '.', 'ui', '.', 'x_value', '.', 'text', '(', ')', 'self', '.', 'y_value', '=', 'self', '.', 'ui', '.', 'y_value', '.', 'text', '(', ')', 'self', '.', 'g_value', '=', 'self', '.', 'ui', '.', 'g_value', '.', 'text', '(', ')', 'self', '.', 's_value', '=', 'self', '.', 'ui', '.', 's_value', '.', 'text', '(', ')', 'self', '.', 'z_value', '=', 'self', '.', 'ui', '.', 'z_value', '.', 'text', '(', ')', 'self', '.', 'wavelength_values', '=', 'self', '.', 'ui', '.', 'wavelength_values', '.', 'text', '(', ')', 'self', '.', 'verbose_value', '=', 'self', '.', 'ui', '.', 'verbose_value', '.', 'text', '(', ')', 'self', '.', 'phytoplankton_path', '=', 'self', '.', 'ui', '.', 'phyto_path', '.', 'text', '(', ')', 'self', '.', 'bottom_path', '=', 'self', '.', 'ui', '.', 'bottom_path', '.', 'text', '(', ')', 'self', '.', 'executive_path', '=', 'self', '.', 'ui', '.', 'exec_path', '.', 'text', '(', ')', 'self', '.', 'nb_cpu', '=', 'self', '.', 'ui', '.', 'nb_cpu', '.', 'currentText', '(', ')', 'self', '.', 'report_parameter_value', '=', 'str', '(', 'self', '.', 'ui', '.', 'report_parameter_value', '.', 'text', '(', ')', ')']",This function gets back data that the user typed.,"['This', 'function', 'gets', 'back', 'data', 'that', 'the', 'user', 'typed', '.']",python,E,3,True,1,test
16659,marrabld/planarradpy,gui/gui_mainLayout.py,https://github.com/marrabld/planarradpy/blob/5095d1cb98d4f67a7c3108c9282f2d59253e89a8/gui/gui_mainLayout.py#L863-L871,"def mouse_move(self, event):
        """"""
        The following gets back coordinates of the mouse on the canvas.
        """"""
        if (self.ui.tabWidget.currentIndex() == TabWidget.NORMAL_MODE):
            self.posX = event.xdata
            self.posY = event.ydata

            self.graphic_target(self.posX, self.posY)","['def', 'mouse_move', '(', 'self', ',', 'event', ')', ':', 'if', '(', 'self', '.', 'ui', '.', 'tabWidget', '.', 'currentIndex', '(', ')', '==', 'TabWidget', '.', 'NORMAL_MODE', ')', ':', 'self', '.', 'posX', '=', 'event', '.', 'xdata', 'self', '.', 'posY', '=', 'event', '.', 'ydata', 'self', '.', 'graphic_target', '(', 'self', '.', 'posX', ',', 'self', '.', 'posY', ')']",The following gets back coordinates of the mouse on the canvas.,"['The', 'following', 'gets', 'back', 'coordinates', 'of', 'the', 'mouse', 'on', 'the', 'canvas', '.']",python,E,3,True,1,test
19245,thusoy/pwm,pwm/encoding.py,https://github.com/thusoy/pwm/blob/fff7d755c34f3a7235a8bf217ffa2ff5aed4926f/pwm/encoding.py#L55-L61,"def _encode_chunk(self, data, index):
        '''
        gets a chunk from the input data, converts it to a number and
        encodes that number
        '''
        chunk = self._get_chunk(data, index)
        return self._encode_long(self._chunk_to_long(chunk))","['def', '_encode_chunk', '(', 'self', ',', 'data', ',', 'index', ')', ':', 'chunk', '=', 'self', '.', '_get_chunk', '(', 'data', ',', 'index', ')', 'return', 'self', '.', '_encode_long', '(', 'self', '.', '_chunk_to_long', '(', 'chunk', ')', ')']","gets a chunk from the input data, converts it to a number and
        encodes that number","['gets', 'a', 'chunk', 'from', 'the', 'input', 'data', 'converts', 'it', 'to', 'a', 'number', 'and', 'encodes', 'that', 'number']",python,E,3,True,1,test
19689,lvh/txampext,txampext/nested.py,https://github.com/lvh/txampext/blob/a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9/txampext/nested.py#L13-L18,"def fromStringProto(self, inString, proto):
        """"""
        Defers to `amp.AmpList`, then gets the element from the list.
        """"""
        value, = amp.AmpList.fromStringProto(self, inString, proto)
        return value","['def', 'fromStringProto', '(', 'self', ',', 'inString', ',', 'proto', ')', ':', 'value', ',', '=', 'amp', '.', 'AmpList', '.', 'fromStringProto', '(', 'self', ',', 'inString', ',', 'proto', ')', 'return', 'value']","Defers to `amp.AmpList`, then gets the element from the list.","['Defers', 'to', 'amp', '.', 'AmpList', 'then', 'gets', 'the', 'element', 'from', 'the', 'list', '.']",python,E,3,True,1,test
229,apache/spark,python/pyspark/sql/dataframe.py,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/dataframe.py#L2239-L2254,"def _to_corrected_pandas_type(dt):
    """"""
    When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.
    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.
    """"""
    import numpy as np
    if type(dt) == ByteType:
        return np.int8
    elif type(dt) == ShortType:
        return np.int16
    elif type(dt) == IntegerType:
        return np.int32
    elif type(dt) == FloatType:
        return np.float32
    else:
        return None","['def', '_to_corrected_pandas_type', '(', 'dt', ')', ':', 'import', 'numpy', 'as', 'np', 'if', 'type', '(', 'dt', ')', '==', 'ByteType', ':', 'return', 'np', '.', 'int8', 'elif', 'type', '(', 'dt', ')', '==', 'ShortType', ':', 'return', 'np', '.', 'int16', 'elif', 'type', '(', 'dt', ')', '==', 'IntegerType', ':', 'return', 'np', '.', 'int32', 'elif', 'type', '(', 'dt', ')', '==', 'FloatType', ':', 'return', 'np', '.', 'float32', 'else', ':', 'return', 'None']","When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.
    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.","['When', 'converting', 'Spark', 'SQL', 'records', 'to', 'Pandas', 'DataFrame', 'the', 'inferred', 'data', 'type', 'may', 'be', 'wrong', '.', 'This', 'method', 'gets', 'the', 'corrected', 'data', 'type', 'for', 'Pandas', 'if', 'that', 'type', 'may', 'be', 'inferred', 'uncorrectly', '.']",python,E,3,True,1,train
6179,allenai/allennlp,allennlp/semparse/domain_languages/domain_language.py,https://github.com/allenai/allennlp/blob/648a36f77db7e45784c047176074f98534c76636/allennlp/semparse/domain_languages/domain_language.py#L647-L691,"def _get_function_transitions(self,
                                  expression: Union[str, List],
                                  expected_type: PredicateType) -> Tuple[List[str],
                                                                         PredicateType,
                                                                         List[PredicateType]]:
        """"""
        A helper method for ``_get_transitions``.  This gets the transitions for the predicate
        itself in a function call.  If we only had simple functions (e.g., ""(add 2 3)""), this would
        be pretty straightforward and we wouldn't need a separate method to handle it.  We split it
        out into its own method because handling higher-order functions is complicated (e.g.,
        something like ""((negate add) 2 3)"").
        """"""
        # This first block handles getting the transitions and function type (and some error
        # checking) _just for the function itself_.  If this is a simple function, this is easy; if
        # it's a higher-order function, it involves some recursion.
        if isinstance(expression, list):
            # This is a higher-order function.  TODO(mattg): we'll just ignore type checking on
            # higher-order functions, for now.
            transitions, function_type = self._get_transitions(expression, None)
        elif expression in self._functions:
            name = expression
            function_types = self._function_types[expression]
            if len(function_types) != 1:
                raise ParsingError(f""{expression} had multiple types; this is not yet supported for functions"")
            function_type = function_types[0]
            transitions = [f'{function_type} -> {name}']
        else:
            if isinstance(expression, str):
                raise ParsingError(f""Unrecognized function: {expression[0]}"")
            else:
                raise ParsingError(f""Unsupported expression type: {expression}"")
        if not isinstance(function_type, FunctionType):
            raise ParsingError(f'Zero-arg function or constant called with arguments: {name}')

        # Now that we have the transitions for the function itself, and the function's type, we can
        # get argument types and do the rest of the transitions.
        argument_types = function_type.argument_types
        return_type = function_type.return_type
        right_side = f'[{function_type}, {"", "".join(str(arg) for arg in argument_types)}]'
        first_transition = f'{return_type} -> {right_side}'
        transitions.insert(0, first_transition)
        if expected_type and expected_type != return_type:
            raise ParsingError(f'{expression} did not have expected type {expected_type} '
                               f'(found {return_type})')
        return transitions, return_type, argument_types","['def', '_get_function_transitions', '(', 'self', ',', 'expression', ':', 'Union', '[', 'str', ',', 'List', ']', ',', 'expected_type', ':', 'PredicateType', ')', '->', 'Tuple', '[', 'List', '[', 'str', ']', ',', 'PredicateType', ',', 'List', '[', 'PredicateType', ']', ']', ':', '# This first block handles getting the transitions and function type (and some error', '# checking) _just for the function itself_.  If this is a simple function, this is easy; if', ""# it's a higher-order function, it involves some recursion."", 'if', 'isinstance', '(', 'expression', ',', 'list', ')', ':', ""# This is a higher-order function.  TODO(mattg): we'll just ignore type checking on"", '# higher-order functions, for now.', 'transitions', ',', 'function_type', '=', 'self', '.', '_get_transitions', '(', 'expression', ',', 'None', ')', 'elif', 'expression', 'in', 'self', '.', '_functions', ':', 'name', '=', 'expression', 'function_types', '=', 'self', '.', '_function_types', '[', 'expression', ']', 'if', 'len', '(', 'function_types', ')', '!=', '1', ':', 'raise', 'ParsingError', '(', 'f""{expression} had multiple types; this is not yet supported for functions""', ')', 'function_type', '=', 'function_types', '[', '0', ']', 'transitions', '=', '[', ""f'{function_type} -> {name}'"", ']', 'else', ':', 'if', 'isinstance', '(', 'expression', ',', 'str', ')', ':', 'raise', 'ParsingError', '(', 'f""Unrecognized function: {expression[0]}""', ')', 'else', ':', 'raise', 'ParsingError', '(', 'f""Unsupported expression type: {expression}""', ')', 'if', 'not', 'isinstance', '(', 'function_type', ',', 'FunctionType', ')', ':', 'raise', 'ParsingError', '(', ""f'Zero-arg function or constant called with arguments: {name}'"", ')', ""# Now that we have the transitions for the function itself, and the function's type, we can"", '# get argument types and do the rest of the transitions.', 'argument_types', '=', 'function_type', '.', 'argument_types', 'return_type', '=', 'function_type', '.', 'return_type', 'right_side', '=', 'f\'[{function_type}, {"", "".join(str(arg) for arg in argument_types)}]\'', 'first_transition', '=', ""f'{return_type} -> {right_side}'"", 'transitions', '.', 'insert', '(', '0', ',', 'first_transition', ')', 'if', 'expected_type', 'and', 'expected_type', '!=', 'return_type', ':', 'raise', 'ParsingError', '(', ""f'{expression} did not have expected type {expected_type} '"", ""f'(found {return_type})'"", ')', 'return', 'transitions', ',', 'return_type', ',', 'argument_types']","A helper method for ``_get_transitions``.  This gets the transitions for the predicate
        itself in a function call.  If we only had simple functions (e.g., ""(add 2 3)""), this would
        be pretty straightforward and we wouldn't need a separate method to handle it.  We split it
        out into its own method because handling higher-order functions is complicated (e.g.,
        something like ""((negate add) 2 3)"").","['A', 'helper', 'method', 'for', '_get_transitions', '.', 'This', 'gets', 'the', 'transitions', 'for', 'the', 'predicate', 'itself', 'in', 'a', 'function', 'call', '.', 'If', 'we', 'only', 'had', 'simple', 'functions', '(', 'e', '.', 'g', '.', '(', 'add', '2', '3', ')', ')', 'this', 'would', 'be', 'pretty', 'straightforward', 'and', 'we', 'wouldn', 't', 'need', 'a', 'separate', 'method', 'to', 'handle', 'it', '.', 'We', 'split', 'it', 'out', 'into', 'its', 'own', 'method', 'because', 'handling', 'higher', '-', 'order', 'functions', 'is', 'complicated', '(', 'e', '.', 'g', '.', 'something', 'like', '((', 'negate', 'add', ')', '2', '3', ')', ')', '.']",python,E,3,True,1,train
7197,apache/incubator-mxnet,python/mxnet/symbol/symbol.py,https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/symbol/symbol.py#L604-L633,"def attr_dict(self):
        """"""Recursively gets all attributes from the symbol and its children.

        Example
        -------
        >>> a = mx.sym.Variable('a', attr={'a1':'a2'})
        >>> b = mx.sym.Variable('b', attr={'b1':'b2'})
        >>> c = a+b
        >>> c.attr_dict()
        {'a': {'a1': 'a2'}, 'b': {'b1': 'b2'}}

        Returns
        -------
        ret : Dict of str to dict
            There is a key in the returned dict for every child with non-empty attribute set.
            For each symbol, the name of the symbol is its key in the dict
            and the correspond value is that symbol's attribute list (itself a dictionary).
        """"""
        size = mx_uint()
        pairs = ctypes.POINTER(ctypes.c_char_p)()
        f_handle = _LIB.MXSymbolListAttr
        check_call(f_handle(self.handle, ctypes.byref(size), ctypes.byref(pairs)))
        ret = {}
        for i in range(size.value):
            name, key = py_str(pairs[i * 2]).split('$')
            val = py_str(pairs[i * 2 + 1])
            if name not in ret:
                ret[name] = {}
            ret[name][key] = val
        return ret","['def', 'attr_dict', '(', 'self', ')', ':', 'size', '=', 'mx_uint', '(', ')', 'pairs', '=', 'ctypes', '.', 'POINTER', '(', 'ctypes', '.', 'c_char_p', ')', '(', ')', 'f_handle', '=', '_LIB', '.', 'MXSymbolListAttr', 'check_call', '(', 'f_handle', '(', 'self', '.', 'handle', ',', 'ctypes', '.', 'byref', '(', 'size', ')', ',', 'ctypes', '.', 'byref', '(', 'pairs', ')', ')', ')', 'ret', '=', '{', '}', 'for', 'i', 'in', 'range', '(', 'size', '.', 'value', ')', ':', 'name', ',', 'key', '=', 'py_str', '(', 'pairs', '[', 'i', '*', '2', ']', ')', '.', 'split', '(', ""'$'"", ')', 'val', '=', 'py_str', '(', 'pairs', '[', 'i', '*', '2', '+', '1', ']', ')', 'if', 'name', 'not', 'in', 'ret', ':', 'ret', '[', 'name', ']', '=', '{', '}', 'ret', '[', 'name', ']', '[', 'key', ']', '=', 'val', 'return', 'ret']","Recursively gets all attributes from the symbol and its children.

        Example
        -------
        >>> a = mx.sym.Variable('a', attr={'a1':'a2'})
        >>> b = mx.sym.Variable('b', attr={'b1':'b2'})
        >>> c = a+b
        >>> c.attr_dict()
        {'a': {'a1': 'a2'}, 'b': {'b1': 'b2'}}

        Returns
        -------
        ret : Dict of str to dict
            There is a key in the returned dict for every child with non-empty attribute set.
            For each symbol, the name of the symbol is its key in the dict
            and the correspond value is that symbol's attribute list (itself a dictionary).","['Recursively', 'gets', 'all', 'attributes', 'from', 'the', 'symbol', 'and', 'its', 'children', '.']",python,E,3,True,1,train
8039,slundberg/shap,shap/explainers/tree.py,https://github.com/slundberg/shap/blob/b280cb81d498b9d98565cad8dd16fc88ae52649f/shap/explainers/tree.py#L907-L919,"def get_xgboost_json(model):
    """""" This gets a JSON dump of an XGBoost model while ensuring the features names are their indexes.
    """"""
    fnames = model.feature_names
    model.feature_names = None
    json_trees = model.get_dump(with_stats=True, dump_format=""json"")
    model.feature_names = fnames

    # this fixes a bug where XGBoost can return invalid JSON
    json_trees = [t.replace("": inf,"", "": 1000000000000.0,"") for t in json_trees]
    json_trees = [t.replace("": -inf,"", "": -1000000000000.0,"") for t in json_trees]

    return json_trees","['def', 'get_xgboost_json', '(', 'model', ')', ':', 'fnames', '=', 'model', '.', 'feature_names', 'model', '.', 'feature_names', '=', 'None', 'json_trees', '=', 'model', '.', 'get_dump', '(', 'with_stats', '=', 'True', ',', 'dump_format', '=', '""json""', ')', 'model', '.', 'feature_names', '=', 'fnames', '# this fixes a bug where XGBoost can return invalid JSON', 'json_trees', '=', '[', 't', '.', 'replace', '(', '"": inf,""', ',', '"": 1000000000000.0,""', ')', 'for', 't', 'in', 'json_trees', ']', 'json_trees', '=', '[', 't', '.', 'replace', '(', '"": -inf,""', ',', '"": -1000000000000.0,""', ')', 'for', 't', 'in', 'json_trees', ']', 'return', 'json_trees']",This gets a JSON dump of an XGBoost model while ensuring the features names are their indexes.,"['This', 'gets', 'a', 'JSON', 'dump', 'of', 'an', 'XGBoost', 'model', 'while', 'ensuring', 'the', 'features', 'names', 'are', 'their', 'indexes', '.']",python,E,3,True,1,train
8657,google/flatbuffers,python/flatbuffers/table.py,https://github.com/google/flatbuffers/blob/6cc30b3272d79c85db7d4871ac0aa69541dc89de/python/flatbuffers/table.py#L48-L54,"def String(self, off):
        """"""String gets a string from data stored inside the flatbuffer.""""""
        N.enforce_number(off, N.UOffsetTFlags)
        off += encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)
        start = off + N.UOffsetTFlags.bytewidth
        length = encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)
        return bytes(self.Bytes[start:start+length])","['def', 'String', '(', 'self', ',', 'off', ')', ':', 'N', '.', 'enforce_number', '(', 'off', ',', 'N', '.', 'UOffsetTFlags', ')', 'off', '+=', 'encode', '.', 'Get', '(', 'N', '.', 'UOffsetTFlags', '.', 'packer_type', ',', 'self', '.', 'Bytes', ',', 'off', ')', 'start', '=', 'off', '+', 'N', '.', 'UOffsetTFlags', '.', 'bytewidth', 'length', '=', 'encode', '.', 'Get', '(', 'N', '.', 'UOffsetTFlags', '.', 'packer_type', ',', 'self', '.', 'Bytes', ',', 'off', ')', 'return', 'bytes', '(', 'self', '.', 'Bytes', '[', 'start', ':', 'start', '+', 'length', ']', ')']",String gets a string from data stored inside the flatbuffer.,"['String', 'gets', 'a', 'string', 'from', 'data', 'stored', 'inside', 'the', 'flatbuffer', '.']",python,E,3,True,1,train
10714,quantopian/zipline,zipline/data/data_portal.py,https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/data/data_portal.py#L1065-L1122,"def _get_daily_window_data(self,
                               assets,
                               field,
                               days_in_window,
                               extra_slot=True):
        """"""
        Internal method that gets a window of adjusted daily data for a sid
        and specified date range.  Used to support the history API method for
        daily bars.

        Parameters
        ----------
        asset : Asset
            The asset whose data is desired.

        start_dt: pandas.Timestamp
            The start of the desired window of data.

        bar_count: int
            The number of days of data to return.

        field: string
            The specific field to return.  ""open"", ""high"", ""close_price"", etc.

        extra_slot: boolean
            Whether to allocate an extra slot in the returned numpy array.
            This extra slot will hold the data for the last partial day.  It's
            much better to create it here than to create a copy of the array
            later just to add a slot.

        Returns
        -------
        A numpy array with requested values.  Any missing slots filled with
        nan.

        """"""
        bar_count = len(days_in_window)
        # create an np.array of size bar_count
        dtype = float64 if field != 'sid' else int64
        if extra_slot:
            return_array = np.zeros((bar_count + 1, len(assets)), dtype=dtype)
        else:
            return_array = np.zeros((bar_count, len(assets)), dtype=dtype)

        if field != ""volume"":
            # volumes default to 0, so we don't need to put NaNs in the array
            return_array[:] = np.NAN

        if bar_count != 0:
            data = self._history_loader.history(assets,
                                                days_in_window,
                                                field,
                                                extra_slot)
            if extra_slot:
                return_array[:len(return_array) - 1, :] = data
            else:
                return_array[:len(data)] = data
        return return_array","['def', '_get_daily_window_data', '(', 'self', ',', 'assets', ',', 'field', ',', 'days_in_window', ',', 'extra_slot', '=', 'True', ')', ':', 'bar_count', '=', 'len', '(', 'days_in_window', ')', '# create an np.array of size bar_count', 'dtype', '=', 'float64', 'if', 'field', '!=', ""'sid'"", 'else', 'int64', 'if', 'extra_slot', ':', 'return_array', '=', 'np', '.', 'zeros', '(', '(', 'bar_count', '+', '1', ',', 'len', '(', 'assets', ')', ')', ',', 'dtype', '=', 'dtype', ')', 'else', ':', 'return_array', '=', 'np', '.', 'zeros', '(', '(', 'bar_count', ',', 'len', '(', 'assets', ')', ')', ',', 'dtype', '=', 'dtype', ')', 'if', 'field', '!=', '""volume""', ':', ""# volumes default to 0, so we don't need to put NaNs in the array"", 'return_array', '[', ':', ']', '=', 'np', '.', 'NAN', 'if', 'bar_count', '!=', '0', ':', 'data', '=', 'self', '.', '_history_loader', '.', 'history', '(', 'assets', ',', 'days_in_window', ',', 'field', ',', 'extra_slot', ')', 'if', 'extra_slot', ':', 'return_array', '[', ':', 'len', '(', 'return_array', ')', '-', '1', ',', ':', ']', '=', 'data', 'else', ':', 'return_array', '[', ':', 'len', '(', 'data', ')', ']', '=', 'data', 'return', 'return_array']","Internal method that gets a window of adjusted daily data for a sid
        and specified date range.  Used to support the history API method for
        daily bars.

        Parameters
        ----------
        asset : Asset
            The asset whose data is desired.

        start_dt: pandas.Timestamp
            The start of the desired window of data.

        bar_count: int
            The number of days of data to return.

        field: string
            The specific field to return.  ""open"", ""high"", ""close_price"", etc.

        extra_slot: boolean
            Whether to allocate an extra slot in the returned numpy array.
            This extra slot will hold the data for the last partial day.  It's
            much better to create it here than to create a copy of the array
            later just to add a slot.

        Returns
        -------
        A numpy array with requested values.  Any missing slots filled with
        nan.","['Internal', 'method', 'that', 'gets', 'a', 'window', 'of', 'adjusted', 'daily', 'data', 'for', 'a', 'sid', 'and', 'specified', 'date', 'range', '.', 'Used', 'to', 'support', 'the', 'history', 'API', 'method', 'for', 'daily', 'bars', '.']",python,E,3,True,1,train
17794,awslabs/aws-sam-cli,samcli/commands/local/generate_event/event_generation.py,https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/commands/local/generate_event/event_generation.py#L45-L63,"def get_command(self, ctx, cmd_name):
        """"""
        gets the subcommands under the service name

        Parameters
        ----------
        ctx : Context
            the context object passed into the method
        cmd_name : str
            the service name
        Returns
        -------
        EventTypeSubCommand:
            returns subcommand if successful, None if not.
        """"""

        if cmd_name not in self.all_cmds:
            return None
        return EventTypeSubCommand(self.events_lib, cmd_name, self.all_cmds[cmd_name])","['def', 'get_command', '(', 'self', ',', 'ctx', ',', 'cmd_name', ')', ':', 'if', 'cmd_name', 'not', 'in', 'self', '.', 'all_cmds', ':', 'return', 'None', 'return', 'EventTypeSubCommand', '(', 'self', '.', 'events_lib', ',', 'cmd_name', ',', 'self', '.', 'all_cmds', '[', 'cmd_name', ']', ')']","gets the subcommands under the service name

        Parameters
        ----------
        ctx : Context
            the context object passed into the method
        cmd_name : str
            the service name
        Returns
        -------
        EventTypeSubCommand:
            returns subcommand if successful, None if not.","['gets', 'the', 'subcommands', 'under', 'the', 'service', 'name']",python,E,3,True,1,train
17795,awslabs/aws-sam-cli,samcli/commands/local/generate_event/event_generation.py,https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/commands/local/generate_event/event_generation.py#L119-L157,"def get_command(self, ctx, cmd_name):

        """"""
        gets the Click Commands underneath a service name

        Parameters
        ----------
        ctx: Context
            context object passed in
        cmd_name: string
            the service name
        Returns
        -------
        cmd: Click.Command
            the Click Commands that can be called from the CLI
        """"""

        if cmd_name not in self.subcmd_definition:
            return None
        parameters = []
        for param_name in self.subcmd_definition[cmd_name][self.TAGS].keys():
            default = self.subcmd_definition[cmd_name][self.TAGS][param_name][""default""]
            parameters.append(click.Option(
                [""--{}"".format(param_name)],
                default=default,
                help=""Specify the {} name you'd like, otherwise the default = {}"".format(param_name, default)
            ))

        command_callback = functools.partial(self.cmd_implementation,
                                             self.events_lib,
                                             self.top_level_cmd_name,
                                             cmd_name)
        cmd = click.Command(name=cmd_name,
                            short_help=self.subcmd_definition[cmd_name][""help""],
                            params=parameters,
                            callback=command_callback)

        cmd = debug_option(cmd)
        return cmd","['def', 'get_command', '(', 'self', ',', 'ctx', ',', 'cmd_name', ')', ':', 'if', 'cmd_name', 'not', 'in', 'self', '.', 'subcmd_definition', ':', 'return', 'None', 'parameters', '=', '[', ']', 'for', 'param_name', 'in', 'self', '.', 'subcmd_definition', '[', 'cmd_name', ']', '[', 'self', '.', 'TAGS', ']', '.', 'keys', '(', ')', ':', 'default', '=', 'self', '.', 'subcmd_definition', '[', 'cmd_name', ']', '[', 'self', '.', 'TAGS', ']', '[', 'param_name', ']', '[', '""default""', ']', 'parameters', '.', 'append', '(', 'click', '.', 'Option', '(', '[', '""--{}""', '.', 'format', '(', 'param_name', ')', ']', ',', 'default', '=', 'default', ',', 'help', '=', '""Specify the {} name you\'d like, otherwise the default = {}""', '.', 'format', '(', 'param_name', ',', 'default', ')', ')', ')', 'command_callback', '=', 'functools', '.', 'partial', '(', 'self', '.', 'cmd_implementation', ',', 'self', '.', 'events_lib', ',', 'self', '.', 'top_level_cmd_name', ',', 'cmd_name', ')', 'cmd', '=', 'click', '.', 'Command', '(', 'name', '=', 'cmd_name', ',', 'short_help', '=', 'self', '.', 'subcmd_definition', '[', 'cmd_name', ']', '[', '""help""', ']', ',', 'params', '=', 'parameters', ',', 'callback', '=', 'command_callback', ')', 'cmd', '=', 'debug_option', '(', 'cmd', ')', 'return', 'cmd']","gets the Click Commands underneath a service name

        Parameters
        ----------
        ctx: Context
            context object passed in
        cmd_name: string
            the service name
        Returns
        -------
        cmd: Click.Command
            the Click Commands that can be called from the CLI","['gets', 'the', 'Click', 'Commands', 'underneath', 'a', 'service', 'name']",python,E,3,True,1,train
742,saltstack/salt,salt/modules/nix.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/nix.py#L89-L96,"def _output_format(out,
                   operation):
    '''
    gets a list of all the packages that were affected by ``operation``, splits it up (there can be multiple packages on a line), and then
    flattens that list. We make it to a list for easier parsing.
    '''
    return [s.split()[1:] for s in out
            if s.startswith(operation)]","['def', '_output_format', '(', 'out', ',', 'operation', ')', ':', 'return', '[', 's', '.', 'split', '(', ')', '[', '1', ':', ']', 'for', 's', 'in', 'out', 'if', 's', '.', 'startswith', '(', 'operation', ')', ']']","gets a list of all the packages that were affected by ``operation``, splits it up (there can be multiple packages on a line), and then
    flattens that list. We make it to a list for easier parsing.","['gets', 'a', 'list', 'of', 'all', 'the', 'packages', 'that', 'were', 'affected', 'by', 'operation', 'splits', 'it', 'up', '(', 'there', 'can', 'be', 'multiple', 'packages', 'on', 'a', 'line', ')', 'and', 'then', 'flattens', 'that', 'list', '.', 'We', 'make', 'it', 'to', 'a', 'list', 'for', 'easier', 'parsing', '.']",python,E,3,True,1,train
3167,saltstack/salt,salt/modules/win_groupadd.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/win_groupadd.py#L70-L82,"def _get_all_groups():
    '''
    A helper function that gets a list of group objects for all groups on the
    machine

    Returns:
        iter: A list of objects for all groups on the machine
    '''
    with salt.utils.winapi.Com():
        nt = win32com.client.Dispatch('AdsNameSpaces')
    results = nt.GetObject('', 'WinNT://.')
    results.Filter = ['group']
    return results","['def', '_get_all_groups', '(', ')', ':', 'with', 'salt', '.', 'utils', '.', 'winapi', '.', 'Com', '(', ')', ':', 'nt', '=', 'win32com', '.', 'client', '.', 'Dispatch', '(', ""'AdsNameSpaces'"", ')', 'results', '=', 'nt', '.', 'GetObject', '(', ""''"", ',', ""'WinNT://.'"", ')', 'results', '.', 'Filter', '=', '[', ""'group'"", ']', 'return', 'results']","A helper function that gets a list of group objects for all groups on the
    machine

    Returns:
        iter: A list of objects for all groups on the machine","['A', 'helper', 'function', 'that', 'gets', 'a', 'list', 'of', 'group', 'objects', 'for', 'all', 'groups', 'on', 'the', 'machine']",python,E,3,True,1,train
4128,angr/angr,angr/state_plugins/solver.py,https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/state_plugins/solver.py#L261-L301,"def _solver(self):
        """"""
        Creates or gets a Claripy solver, based on the state options.
        """"""
        if self._stored_solver is not None:
            return self._stored_solver

        track = o.CONSTRAINT_TRACKING_IN_SOLVER in self.state.options
        approximate_first = o.APPROXIMATE_FIRST in self.state.options

        if o.STRINGS_ANALYSIS in self.state.options:
            if 'smtlib_cvc4' in backend_manager.backends._backends_by_name:
                our_backend = backend_manager.backends.smtlib_cvc4
            elif 'smtlib_z3' in backend_manager.backends._backends_by_name:
                our_backend = backend_manager.backends.smtlib_z3
            elif 'smtlib_abc' in backend_manager.backends._backends_by_name:
                our_backend = backend_manager.backends.smtlib_abc
            else:
                raise ValueError(""Could not find suitable string solver!"")
            if o.COMPOSITE_SOLVER in self.state.options:
                self._stored_solver = claripy.SolverComposite(
                    template_solver_string=claripy.SolverCompositeChild(backend=our_backend, track=track)
                )
        elif o.ABSTRACT_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverVSA()
        elif o.SYMBOLIC in self.state.options and o.REPLACEMENT_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverReplacement(auto_replace=False)
        elif o.SYMBOLIC in self.state.options and o.CACHELESS_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverCacheless(track=track)
        elif o.SYMBOLIC in self.state.options and o.COMPOSITE_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverComposite(track=track)
        elif o.SYMBOLIC in self.state.options and any(opt in self.state.options for opt in o.approximation):
            self._stored_solver = claripy.SolverHybrid(track=track, approximate_first=approximate_first)
        elif o.HYBRID_SOLVER in self.state.options:
            self._stored_solver = claripy.SolverHybrid(track=track, approximate_first=approximate_first)
        elif o.SYMBOLIC in self.state.options:
            self._stored_solver = claripy.Solver(track=track)
        else:
            self._stored_solver = claripy.SolverConcrete()

        return self._stored_solver","['def', '_solver', '(', 'self', ')', ':', 'if', 'self', '.', '_stored_solver', 'is', 'not', 'None', ':', 'return', 'self', '.', '_stored_solver', 'track', '=', 'o', '.', 'CONSTRAINT_TRACKING_IN_SOLVER', 'in', 'self', '.', 'state', '.', 'options', 'approximate_first', '=', 'o', '.', 'APPROXIMATE_FIRST', 'in', 'self', '.', 'state', '.', 'options', 'if', 'o', '.', 'STRINGS_ANALYSIS', 'in', 'self', '.', 'state', '.', 'options', ':', 'if', ""'smtlib_cvc4'"", 'in', 'backend_manager', '.', 'backends', '.', '_backends_by_name', ':', 'our_backend', '=', 'backend_manager', '.', 'backends', '.', 'smtlib_cvc4', 'elif', ""'smtlib_z3'"", 'in', 'backend_manager', '.', 'backends', '.', '_backends_by_name', ':', 'our_backend', '=', 'backend_manager', '.', 'backends', '.', 'smtlib_z3', 'elif', ""'smtlib_abc'"", 'in', 'backend_manager', '.', 'backends', '.', '_backends_by_name', ':', 'our_backend', '=', 'backend_manager', '.', 'backends', '.', 'smtlib_abc', 'else', ':', 'raise', 'ValueError', '(', '""Could not find suitable string solver!""', ')', 'if', 'o', '.', 'COMPOSITE_SOLVER', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverComposite', '(', 'template_solver_string', '=', 'claripy', '.', 'SolverCompositeChild', '(', 'backend', '=', 'our_backend', ',', 'track', '=', 'track', ')', ')', 'elif', 'o', '.', 'ABSTRACT_SOLVER', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverVSA', '(', ')', 'elif', 'o', '.', 'SYMBOLIC', 'in', 'self', '.', 'state', '.', 'options', 'and', 'o', '.', 'REPLACEMENT_SOLVER', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverReplacement', '(', 'auto_replace', '=', 'False', ')', 'elif', 'o', '.', 'SYMBOLIC', 'in', 'self', '.', 'state', '.', 'options', 'and', 'o', '.', 'CACHELESS_SOLVER', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverCacheless', '(', 'track', '=', 'track', ')', 'elif', 'o', '.', 'SYMBOLIC', 'in', 'self', '.', 'state', '.', 'options', 'and', 'o', '.', 'COMPOSITE_SOLVER', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverComposite', '(', 'track', '=', 'track', ')', 'elif', 'o', '.', 'SYMBOLIC', 'in', 'self', '.', 'state', '.', 'options', 'and', 'any', '(', 'opt', 'in', 'self', '.', 'state', '.', 'options', 'for', 'opt', 'in', 'o', '.', 'approximation', ')', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverHybrid', '(', 'track', '=', 'track', ',', 'approximate_first', '=', 'approximate_first', ')', 'elif', 'o', '.', 'HYBRID_SOLVER', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverHybrid', '(', 'track', '=', 'track', ',', 'approximate_first', '=', 'approximate_first', ')', 'elif', 'o', '.', 'SYMBOLIC', 'in', 'self', '.', 'state', '.', 'options', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'Solver', '(', 'track', '=', 'track', ')', 'else', ':', 'self', '.', '_stored_solver', '=', 'claripy', '.', 'SolverConcrete', '(', ')', 'return', 'self', '.', '_stored_solver']","Creates or gets a Claripy solver, based on the state options.","['Creates', 'or', 'gets', 'a', 'Claripy', 'solver', 'based', 'on', 'the', 'state', 'options', '.']",python,E,3,True,1,train
5065,buildbot/buildbot,master/buildbot/steps/source/base.py,https://github.com/buildbot/buildbot/blob/5df3cfae6d760557d99156633c32b1822a1e130c/master/buildbot/steps/source/base.py#L143-L149,"def _getAttrGroupMember(self, attrGroup, attr):
        """"""
        The getattr equivalent for attribute groups: gets and returns the
        attribute group member.
        """"""
        method_name = '%s_%s' % (attrGroup, attr)
        return getattr(self, method_name)","['def', '_getAttrGroupMember', '(', 'self', ',', 'attrGroup', ',', 'attr', ')', ':', 'method_name', '=', ""'%s_%s'"", '%', '(', 'attrGroup', ',', 'attr', ')', 'return', 'getattr', '(', 'self', ',', 'method_name', ')']","The getattr equivalent for attribute groups: gets and returns the
        attribute group member.","['The', 'getattr', 'equivalent', 'for', 'attribute', 'groups', ':', 'gets', 'and', 'returns', 'the', 'attribute', 'group', 'member', '.']",python,E,3,True,1,train
7166,deepmind/sonnet,sonnet/python/modules/util.py,https://github.com/deepmind/sonnet/blob/00612ca3178964d86b556e062694d808ff81fcca/sonnet/python/modules/util.py#L975-L981,"def _recursive_getattr(module, path):
  """"""Recursively gets attributes inside `module` as specified by `path`.""""""
  if ""."" not in path:
    return getattr(module, path)
  else:
    first, rest = path.split(""."", 1)
    return _recursive_getattr(getattr(module, first), rest)","['def', '_recursive_getattr', '(', 'module', ',', 'path', ')', ':', 'if', '"".""', 'not', 'in', 'path', ':', 'return', 'getattr', '(', 'module', ',', 'path', ')', 'else', ':', 'first', ',', 'rest', '=', 'path', '.', 'split', '(', '"".""', ',', '1', ')', 'return', '_recursive_getattr', '(', 'getattr', '(', 'module', ',', 'first', ')', ',', 'rest', ')']",Recursively gets attributes inside `module` as specified by `path`.,"['Recursively', 'gets', 'attributes', 'inside', 'module', 'as', 'specified', 'by', 'path', '.']",python,E,3,True,1,train
9062,deepmind/pysc2,pysc2/lib/renderer_human.py,https://github.com/deepmind/pysc2/blob/df4cc4b00f07a2242be9ba153d4a7f4ad2017897/pysc2/lib/renderer_human.py#L1442-L1510,"def run(self, run_config, controller, max_game_steps=0, max_episodes=0,
          game_steps_per_episode=0, save_replay=False):
    """"""Run loop that gets observations, renders them, and sends back actions.""""""
    is_replay = (controller.status == remote_controller.Status.in_replay)
    total_game_steps = 0
    start_time = time.time()
    num_episodes = 0

    try:
      while True:
        self.init(controller.game_info(), controller.data())
        episode_steps = 0
        num_episodes += 1

        controller.step()

        while True:
          total_game_steps += self._step_mul
          episode_steps += self._step_mul
          frame_start_time = time.time()

          obs = controller.observe()
          self.render(obs)

          if obs.player_result:
            break

          cmd = self.get_actions(run_config, controller)
          if cmd == ActionCmd.STEP:
            pass
          elif cmd == ActionCmd.QUIT:
            if not is_replay and save_replay:
              self.save_replay(run_config, controller)
            return
          elif cmd == ActionCmd.RESTART:
            break
          else:
            raise Exception(""Unexpected command: %s"" % cmd)

          controller.step(self._step_mul)

          if max_game_steps and total_game_steps >= max_game_steps:
            return

          if game_steps_per_episode and episode_steps >= game_steps_per_episode:
            break

          with sw(""sleep""):
            elapsed_time = time.time() - frame_start_time
            time.sleep(max(0, 1 / self._fps - elapsed_time))

        if is_replay:
          break

        if save_replay:
          self.save_replay(run_config, controller)

        if max_episodes and num_episodes >= max_episodes:
          break

        print(""Restarting"")
        controller.restart()
    except KeyboardInterrupt:
      pass
    finally:
      self.close()
      elapsed_time = time.time() - start_time
      print(""took %.3f seconds for %s steps: %.3f fps"" %
            (elapsed_time, total_game_steps, total_game_steps / elapsed_time))","['def', 'run', '(', 'self', ',', 'run_config', ',', 'controller', ',', 'max_game_steps', '=', '0', ',', 'max_episodes', '=', '0', ',', 'game_steps_per_episode', '=', '0', ',', 'save_replay', '=', 'False', ')', ':', 'is_replay', '=', '(', 'controller', '.', 'status', '==', 'remote_controller', '.', 'Status', '.', 'in_replay', ')', 'total_game_steps', '=', '0', 'start_time', '=', 'time', '.', 'time', '(', ')', 'num_episodes', '=', '0', 'try', ':', 'while', 'True', ':', 'self', '.', 'init', '(', 'controller', '.', 'game_info', '(', ')', ',', 'controller', '.', 'data', '(', ')', ')', 'episode_steps', '=', '0', 'num_episodes', '+=', '1', 'controller', '.', 'step', '(', ')', 'while', 'True', ':', 'total_game_steps', '+=', 'self', '.', '_step_mul', 'episode_steps', '+=', 'self', '.', '_step_mul', 'frame_start_time', '=', 'time', '.', 'time', '(', ')', 'obs', '=', 'controller', '.', 'observe', '(', ')', 'self', '.', 'render', '(', 'obs', ')', 'if', 'obs', '.', 'player_result', ':', 'break', 'cmd', '=', 'self', '.', 'get_actions', '(', 'run_config', ',', 'controller', ')', 'if', 'cmd', '==', 'ActionCmd', '.', 'STEP', ':', 'pass', 'elif', 'cmd', '==', 'ActionCmd', '.', 'QUIT', ':', 'if', 'not', 'is_replay', 'and', 'save_replay', ':', 'self', '.', 'save_replay', '(', 'run_config', ',', 'controller', ')', 'return', 'elif', 'cmd', '==', 'ActionCmd', '.', 'RESTART', ':', 'break', 'else', ':', 'raise', 'Exception', '(', '""Unexpected command: %s""', '%', 'cmd', ')', 'controller', '.', 'step', '(', 'self', '.', '_step_mul', ')', 'if', 'max_game_steps', 'and', 'total_game_steps', '>=', 'max_game_steps', ':', 'return', 'if', 'game_steps_per_episode', 'and', 'episode_steps', '>=', 'game_steps_per_episode', ':', 'break', 'with', 'sw', '(', '""sleep""', ')', ':', 'elapsed_time', '=', 'time', '.', 'time', '(', ')', '-', 'frame_start_time', 'time', '.', 'sleep', '(', 'max', '(', '0', ',', '1', '/', 'self', '.', '_fps', '-', 'elapsed_time', ')', ')', 'if', 'is_replay', ':', 'break', 'if', 'save_replay', ':', 'self', '.', 'save_replay', '(', 'run_config', ',', 'controller', ')', 'if', 'max_episodes', 'and', 'num_episodes', '>=', 'max_episodes', ':', 'break', 'print', '(', '""Restarting""', ')', 'controller', '.', 'restart', '(', ')', 'except', 'KeyboardInterrupt', ':', 'pass', 'finally', ':', 'self', '.', 'close', '(', ')', 'elapsed_time', '=', 'time', '.', 'time', '(', ')', '-', 'start_time', 'print', '(', '""took %.3f seconds for %s steps: %.3f fps""', '%', '(', 'elapsed_time', ',', 'total_game_steps', ',', 'total_game_steps', '/', 'elapsed_time', ')', ')']","Run loop that gets observations, renders them, and sends back actions.","['Run', 'loop', 'that', 'gets', 'observations', 'renders', 'them', 'and', 'sends', 'back', 'actions', '.']",python,E,3,True,1,train
9221,dpgaspar/Flask-AppBuilder,flask_appbuilder/api/__init__.py,https://github.com/dpgaspar/Flask-AppBuilder/blob/c293734c1b86e176a3ba57ee2deab6676d125576/flask_appbuilder/api/__init__.py#L88-L141,"def rison(schema=None):
    """"""
        Use this decorator to parse URI *Rison* arguments to
        a python data structure, your method gets the data
        structure on kwargs['rison']. Response is HTTP 400
        if *Rison* is not correct::

            class ExampleApi(BaseApi):
                    @expose('/risonjson')
                    @rison()
                    def rison_json(self, **kwargs):
                        return self.response(200, result=kwargs['rison'])

        You can additionally pass a JSON schema to
        validate Rison arguments::

            schema = {
                ""type"": ""object"",
                ""properties"": {
                    ""arg1"": {
                        ""type"": ""integer""
                    }
                }
            }

            class ExampleApi(BaseApi):
                    @expose('/risonjson')
                    @rison(schema)
                    def rison_json(self, **kwargs):
                        return self.response(200, result=kwargs['rison'])

    """"""

    def _rison(f):
        def wraps(self, *args, **kwargs):
            value = request.args.get(API_URI_RIS_KEY, None)
            kwargs[""rison""] = dict()
            if value:
                try:
                    kwargs[""rison""] = prison.loads(value)
                except prison.decoder.ParserException:
                    return self.response_400(message=""Not a valid rison argument"")
            if schema:
                try:
                    jsonschema.validate(instance=kwargs[""rison""], schema=schema)
                except jsonschema.ValidationError as e:
                    return self.response_400(
                        message=""Not a valid rison schema {}"".format(e)
                    )
            return f(self, *args, **kwargs)

        return functools.update_wrapper(wraps, f)

    return _rison","['def', 'rison', '(', 'schema', '=', 'None', ')', ':', 'def', '_rison', '(', 'f', ')', ':', 'def', 'wraps', '(', 'self', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'value', '=', 'request', '.', 'args', '.', 'get', '(', 'API_URI_RIS_KEY', ',', 'None', ')', 'kwargs', '[', '""rison""', ']', '=', 'dict', '(', ')', 'if', 'value', ':', 'try', ':', 'kwargs', '[', '""rison""', ']', '=', 'prison', '.', 'loads', '(', 'value', ')', 'except', 'prison', '.', 'decoder', '.', 'ParserException', ':', 'return', 'self', '.', 'response_400', '(', 'message', '=', '""Not a valid rison argument""', ')', 'if', 'schema', ':', 'try', ':', 'jsonschema', '.', 'validate', '(', 'instance', '=', 'kwargs', '[', '""rison""', ']', ',', 'schema', '=', 'schema', ')', 'except', 'jsonschema', '.', 'ValidationError', 'as', 'e', ':', 'return', 'self', '.', 'response_400', '(', 'message', '=', '""Not a valid rison schema {}""', '.', 'format', '(', 'e', ')', ')', 'return', 'f', '(', 'self', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', 'return', 'functools', '.', 'update_wrapper', '(', 'wraps', ',', 'f', ')', 'return', '_rison']","Use this decorator to parse URI *Rison* arguments to
        a python data structure, your method gets the data
        structure on kwargs['rison']. Response is HTTP 400
        if *Rison* is not correct::

            class ExampleApi(BaseApi):
                    @expose('/risonjson')
                    @rison()
                    def rison_json(self, **kwargs):
                        return self.response(200, result=kwargs['rison'])

        You can additionally pass a JSON schema to
        validate Rison arguments::

            schema = {
                ""type"": ""object"",
                ""properties"": {
                    ""arg1"": {
                        ""type"": ""integer""
                    }
                }
            }

            class ExampleApi(BaseApi):
                    @expose('/risonjson')
                    @rison(schema)
                    def rison_json(self, **kwargs):
                        return self.response(200, result=kwargs['rison'])","['Use', 'this', 'decorator', 'to', 'parse', 'URI', '*', 'Rison', '*', 'arguments', 'to', 'a', 'python', 'data', 'structure', 'your', 'method', 'gets', 'the', 'data', 'structure', 'on', 'kwargs', '[', 'rison', ']', '.', 'Response', 'is', 'HTTP', '400', 'if', '*', 'Rison', '*', 'is', 'not', 'correct', '::']",python,E,3,True,1,train
12885,DataDog/integrations-core,kubelet/datadog_checks/kubelet/prometheus.py,https://github.com/DataDog/integrations-core/blob/ebd41c873cf9f97a8c51bf9459bc6a7536af8acd/kubelet/datadog_checks/kubelet/prometheus.py#L152-L163,"def _get_container_id(self, labels):
        """"""
        Should only be called on a container-scoped metric
        It gets the container id from the podlist using the metrics labels

        :param labels
        :return str or None
        """"""
        namespace = CadvisorPrometheusScraperMixin._get_container_label(labels, ""namespace"")
        pod_name = CadvisorPrometheusScraperMixin._get_container_label(labels, ""pod_name"")
        container_name = CadvisorPrometheusScraperMixin._get_container_label(labels, ""container_name"")
        return self.pod_list_utils.get_cid_by_name_tuple((namespace, pod_name, container_name))","['def', '_get_container_id', '(', 'self', ',', 'labels', ')', ':', 'namespace', '=', 'CadvisorPrometheusScraperMixin', '.', '_get_container_label', '(', 'labels', ',', '""namespace""', ')', 'pod_name', '=', 'CadvisorPrometheusScraperMixin', '.', '_get_container_label', '(', 'labels', ',', '""pod_name""', ')', 'container_name', '=', 'CadvisorPrometheusScraperMixin', '.', '_get_container_label', '(', 'labels', ',', '""container_name""', ')', 'return', 'self', '.', 'pod_list_utils', '.', 'get_cid_by_name_tuple', '(', '(', 'namespace', ',', 'pod_name', ',', 'container_name', ')', ')']","Should only be called on a container-scoped metric
        It gets the container id from the podlist using the metrics labels

        :param labels
        :return str or None","['Should', 'only', 'be', 'called', 'on', 'a', 'container', '-', 'scoped', 'metric', 'It', 'gets', 'the', 'container', 'id', 'from', 'the', 'podlist', 'using', 'the', 'metrics', 'labels']",python,E,3,True,1,train
13959,DistrictDataLabs/yellowbrick,yellowbrick/text/freqdist.py,https://github.com/DistrictDataLabs/yellowbrick/blob/59b67236a3862c73363e8edad7cd86da5b69e3b2/yellowbrick/text/freqdist.py#L132-L155,"def count(self, X):
        """"""
        Called from the fit method, this method gets all the
        words from the corpus and their corresponding frequency
        counts.

        Parameters
        ----------

        X : ndarray or masked ndarray
            Pass in the matrix of vectorized documents, can be masked in
            order to sum the word frequencies for only a subset of documents.

        Returns
        -------

        counts : array
            A vector containing the counts of all words in X (columns)

        """"""
        # Sum on axis 0 (by columns), each column is a word
        # Convert the matrix to an array
        # Squeeze to remove the 1 dimension objects (like ravel)
        return np.squeeze(np.asarray(X.sum(axis=0)))","['def', 'count', '(', 'self', ',', 'X', ')', ':', '# Sum on axis 0 (by columns), each column is a word', '# Convert the matrix to an array', '# Squeeze to remove the 1 dimension objects (like ravel)', 'return', 'np', '.', 'squeeze', '(', 'np', '.', 'asarray', '(', 'X', '.', 'sum', '(', 'axis', '=', '0', ')', ')', ')']","Called from the fit method, this method gets all the
        words from the corpus and their corresponding frequency
        counts.

        Parameters
        ----------

        X : ndarray or masked ndarray
            Pass in the matrix of vectorized documents, can be masked in
            order to sum the word frequencies for only a subset of documents.

        Returns
        -------

        counts : array
            A vector containing the counts of all words in X (columns)","['Called', 'from', 'the', 'fit', 'method', 'this', 'method', 'gets', 'all', 'the', 'words', 'from', 'the', 'corpus', 'and', 'their', 'corresponding', 'frequency', 'counts', '.']",python,E,3,True,1,train
15700,openthread/openthread,tools/harness-thci/OpenThread.py,https://github.com/openthread/openthread/blob/0208d10563aa21c518092985c78ecf9cd223ab74/tools/harness-thci/OpenThread.py#L1788-L1817,"def getNeighbouringDevices(self):
        """"""gets the neighboring devices' extended address to compute the DUT
           extended address automatically

        Returns:
            A list including extended address of neighboring routers, parent
            as well as children
        """"""
        print '%s call getNeighbouringDevices' % self.port
        neighbourList = []

        # get parent info
        parentAddr = self.getParentAddress()
        if parentAddr != 0:
            neighbourList.append(parentAddr)

        # get ED/SED children info
        childNeighbours = self.getChildrenInfo()
        if childNeighbours != None and len(childNeighbours) > 0:
            for entry in childNeighbours:
                neighbourList.append(entry)

        # get neighboring routers info
        routerNeighbours = self.getNeighbouringRouters()
        if routerNeighbours != None and len(routerNeighbours) > 0:
            for entry in routerNeighbours:
                neighbourList.append(entry)

        print neighbourList
        return neighbourList","['def', 'getNeighbouringDevices', '(', 'self', ')', ':', 'print', ""'%s call getNeighbouringDevices'"", '%', 'self', '.', 'port', 'neighbourList', '=', '[', ']', '# get parent info', 'parentAddr', '=', 'self', '.', 'getParentAddress', '(', ')', 'if', 'parentAddr', '!=', '0', ':', 'neighbourList', '.', 'append', '(', 'parentAddr', ')', '# get ED/SED children info', 'childNeighbours', '=', 'self', '.', 'getChildrenInfo', '(', ')', 'if', 'childNeighbours', '!=', 'None', 'and', 'len', '(', 'childNeighbours', ')', '>', '0', ':', 'for', 'entry', 'in', 'childNeighbours', ':', 'neighbourList', '.', 'append', '(', 'entry', ')', '# get neighboring routers info', 'routerNeighbours', '=', 'self', '.', 'getNeighbouringRouters', '(', ')', 'if', 'routerNeighbours', '!=', 'None', 'and', 'len', '(', 'routerNeighbours', ')', '>', '0', ':', 'for', 'entry', 'in', 'routerNeighbours', ':', 'neighbourList', '.', 'append', '(', 'entry', ')', 'print', 'neighbourList', 'return', 'neighbourList']","gets the neighboring devices' extended address to compute the DUT
           extended address automatically

        Returns:
            A list including extended address of neighboring routers, parent
            as well as children","['gets', 'the', 'neighboring', 'devices', 'extended', 'address', 'to', 'compute', 'the', 'DUT', 'extended', 'address', 'automatically']",python,E,3,True,1,train
15861,ranaroussi/qtpylib,qtpylib/algo.py,https://github.com/ranaroussi/qtpylib/blob/0dbbc465fafd9cb9b0f4d10e1e07fae4e15032dd/qtpylib/algo.py#L509-L533,"def get_history(self, symbols, start, end=None, resolution=""1T"", tz=""UTC""):
        """"""Get historical market data.
        Connects to Blotter and gets historical data from storage

        :Parameters:
            symbols : list
                List of symbols to fetch history for
            start : datetime / string
                History time period start date
                datetime or YYYY-MM-DD[ HH:MM[:SS]] string)

        :Optional:
            end : datetime / string
                History time period end date
                (datetime or YYYY-MM-DD[ HH:MM[:SS]] string)
            resolution : string
                History resoluton (Pandas resample, defaults to 1T/1min)
            tz : string
                History timezone (defaults to UTC)

        :Returns:
            history : pd.DataFrame
                Pandas DataFrame object with historical data for all symbols
        """"""
        return self.blotter.history(symbols, start, end, resolution, tz)","['def', 'get_history', '(', 'self', ',', 'symbols', ',', 'start', ',', 'end', '=', 'None', ',', 'resolution', '=', '""1T""', ',', 'tz', '=', '""UTC""', ')', ':', 'return', 'self', '.', 'blotter', '.', 'history', '(', 'symbols', ',', 'start', ',', 'end', ',', 'resolution', ',', 'tz', ')']","Get historical market data.
        Connects to Blotter and gets historical data from storage

        :Parameters:
            symbols : list
                List of symbols to fetch history for
            start : datetime / string
                History time period start date
                datetime or YYYY-MM-DD[ HH:MM[:SS]] string)

        :Optional:
            end : datetime / string
                History time period end date
                (datetime or YYYY-MM-DD[ HH:MM[:SS]] string)
            resolution : string
                History resoluton (Pandas resample, defaults to 1T/1min)
            tz : string
                History timezone (defaults to UTC)

        :Returns:
            history : pd.DataFrame
                Pandas DataFrame object with historical data for all symbols","['Get', 'historical', 'market', 'data', '.', 'Connects', 'to', 'Blotter', 'and', 'gets', 'historical', 'data', 'from', 'storage']",python,E,3,True,1,train
16920,google/grr,grr/client/grr_response_client/client_utils_osx.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/client/grr_response_client/client_utils_osx.py#L34-L72,"def FindProxies():
  """"""This reads the OSX system configuration and gets the proxies.""""""

  sc = objc.SystemConfiguration()

  # Get the dictionary of network proxy settings
  settings = sc.dll.SCDynamicStoreCopyProxies(None)
  if not settings:
    return []

  try:
    cf_http_enabled = sc.CFDictRetrieve(settings, ""kSCPropNetProxiesHTTPEnable"")
    if cf_http_enabled and bool(sc.CFNumToInt32(cf_http_enabled)):
      # Proxy settings for HTTP are enabled
      cfproxy = sc.CFDictRetrieve(settings, ""kSCPropNetProxiesHTTPProxy"")
      cfport = sc.CFDictRetrieve(settings, ""kSCPropNetProxiesHTTPPort"")
      if cfproxy and cfport:
        proxy = sc.CFStringToPystring(cfproxy)
        port = sc.CFNumToInt32(cfport)
        return [""http://%s:%d/"" % (proxy, port)]

    cf_auto_enabled = sc.CFDictRetrieve(
        settings, ""kSCPropNetProxiesProxyAutoConfigEnable"")

    if cf_auto_enabled and bool(sc.CFNumToInt32(cf_auto_enabled)):
      cfurl = sc.CFDictRetrieve(settings,
                                ""kSCPropNetProxiesProxyAutoConfigURLString"")
      if cfurl:
        unused_url = sc.CFStringToPystring(cfurl)
        # TODO(amoser): Auto config is enabled, what is the plan here?
        # Basically, all we get is the URL of a javascript file. To get the
        # correct proxy for a given URL, browsers call a Javascript function
        # that returns the correct proxy URL. The question is now, do we really
        # want to start running downloaded js on the client?
        return []

  finally:
    sc.dll.CFRelease(settings)
  return []","['def', 'FindProxies', '(', ')', ':', 'sc', '=', 'objc', '.', 'SystemConfiguration', '(', ')', '# Get the dictionary of network proxy settings', 'settings', '=', 'sc', '.', 'dll', '.', 'SCDynamicStoreCopyProxies', '(', 'None', ')', 'if', 'not', 'settings', ':', 'return', '[', ']', 'try', ':', 'cf_http_enabled', '=', 'sc', '.', 'CFDictRetrieve', '(', 'settings', ',', '""kSCPropNetProxiesHTTPEnable""', ')', 'if', 'cf_http_enabled', 'and', 'bool', '(', 'sc', '.', 'CFNumToInt32', '(', 'cf_http_enabled', ')', ')', ':', '# Proxy settings for HTTP are enabled', 'cfproxy', '=', 'sc', '.', 'CFDictRetrieve', '(', 'settings', ',', '""kSCPropNetProxiesHTTPProxy""', ')', 'cfport', '=', 'sc', '.', 'CFDictRetrieve', '(', 'settings', ',', '""kSCPropNetProxiesHTTPPort""', ')', 'if', 'cfproxy', 'and', 'cfport', ':', 'proxy', '=', 'sc', '.', 'CFStringToPystring', '(', 'cfproxy', ')', 'port', '=', 'sc', '.', 'CFNumToInt32', '(', 'cfport', ')', 'return', '[', '""http://%s:%d/""', '%', '(', 'proxy', ',', 'port', ')', ']', 'cf_auto_enabled', '=', 'sc', '.', 'CFDictRetrieve', '(', 'settings', ',', '""kSCPropNetProxiesProxyAutoConfigEnable""', ')', 'if', 'cf_auto_enabled', 'and', 'bool', '(', 'sc', '.', 'CFNumToInt32', '(', 'cf_auto_enabled', ')', ')', ':', 'cfurl', '=', 'sc', '.', 'CFDictRetrieve', '(', 'settings', ',', '""kSCPropNetProxiesProxyAutoConfigURLString""', ')', 'if', 'cfurl', ':', 'unused_url', '=', 'sc', '.', 'CFStringToPystring', '(', 'cfurl', ')', '# TODO(amoser): Auto config is enabled, what is the plan here?', '# Basically, all we get is the URL of a javascript file. To get the', '# correct proxy for a given URL, browsers call a Javascript function', '# that returns the correct proxy URL. The question is now, do we really', '# want to start running downloaded js on the client?', 'return', '[', ']', 'finally', ':', 'sc', '.', 'dll', '.', 'CFRelease', '(', 'settings', ')', 'return', '[', ']']",This reads the OSX system configuration and gets the proxies.,"['This', 'reads', 'the', 'OSX', 'system', 'configuration', 'and', 'gets', 'the', 'proxies', '.']",python,E,3,True,1,train
21108,materialsproject/pymatgen,pymatgen/io/lmto.py,https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/lmto.py#L103-L152,"def as_dict(self):
        """"""
        Returns the CTRL as a dictionary. ""SITE"" and ""CLASS"" are of
        the form {'CATEGORY': {'TOKEN': value}}, the rest is of the
        form 'TOKEN'/'CATEGORY': value. It gets the conventional standard
        structure because primitive cells use the conventional
        a-lattice parameter as the scaling factor and not the a-lattice
        parameter of the primitive cell.
        """"""
        ctrl_dict = {""@module"": self.__class__.__module__,
                     ""@class"": self.__class__.__name__}
        if self.header is not None:
            ctrl_dict[""HEADER""] = self.header
        if self.version is not None:
            ctrl_dict[""VERS""] = self.version
        sga = SpacegroupAnalyzer(self.structure)
        alat = sga.get_conventional_standard_structure().lattice.a
        plat = self.structure.lattice.matrix/alat

        """"""
        The following is to find the classes (atoms that are not symmetry
        equivalent, and create labels. Note that LMTO only attaches
        numbers with the second atom of the same species, e.g. ""Bi"", ""Bi1"",
        ""Bi2"", etc.
        """"""

        eq_atoms = sga.get_symmetry_dataset()['equivalent_atoms']
        ineq_sites_index = list(set(eq_atoms))
        sites = []
        classes = []
        num_atoms = {}
        for s, site in enumerate(self.structure.sites):
            atom = site.specie
            label_index = ineq_sites_index.index(eq_atoms[s])
            if atom.symbol in num_atoms:
                if label_index + 1 > sum(num_atoms.values()):
                    num_atoms[atom.symbol] += 1
                    atom_label = atom.symbol + str(num_atoms[atom.symbol] - 1)
                    classes.append({""ATOM"": atom_label, ""Z"": atom.Z})
            else:
                num_atoms[atom.symbol] = 1
                classes.append({""ATOM"": atom.symbol, ""Z"": atom.Z})
            sites.append({""ATOM"": classes[label_index][""ATOM""],
                          ""POS"": site.coords/alat})

        ctrl_dict.update({""ALAT"": alat/bohr_to_angstrom,
                          ""PLAT"": plat,
                          ""CLASS"": classes,
                          ""SITE"": sites})
        return ctrl_dict","['def', 'as_dict', '(', 'self', ')', ':', 'ctrl_dict', '=', '{', '""@module""', ':', 'self', '.', '__class__', '.', '__module__', ',', '""@class""', ':', 'self', '.', '__class__', '.', '__name__', '}', 'if', 'self', '.', 'header', 'is', 'not', 'None', ':', 'ctrl_dict', '[', '""HEADER""', ']', '=', 'self', '.', 'header', 'if', 'self', '.', 'version', 'is', 'not', 'None', ':', 'ctrl_dict', '[', '""VERS""', ']', '=', 'self', '.', 'version', 'sga', '=', 'SpacegroupAnalyzer', '(', 'self', '.', 'structure', ')', 'alat', '=', 'sga', '.', 'get_conventional_standard_structure', '(', ')', '.', 'lattice', '.', 'a', 'plat', '=', 'self', '.', 'structure', '.', 'lattice', '.', 'matrix', '/', 'alat', '""""""\n        The following is to find the classes (atoms that are not symmetry\n        equivalent, and create labels. Note that LMTO only attaches\n        numbers with the second atom of the same species, e.g. ""Bi"", ""Bi1"",\n        ""Bi2"", etc.\n        """"""', 'eq_atoms', '=', 'sga', '.', 'get_symmetry_dataset', '(', ')', '[', ""'equivalent_atoms'"", ']', 'ineq_sites_index', '=', 'list', '(', 'set', '(', 'eq_atoms', ')', ')', 'sites', '=', '[', ']', 'classes', '=', '[', ']', 'num_atoms', '=', '{', '}', 'for', 's', ',', 'site', 'in', 'enumerate', '(', 'self', '.', 'structure', '.', 'sites', ')', ':', 'atom', '=', 'site', '.', 'specie', 'label_index', '=', 'ineq_sites_index', '.', 'index', '(', 'eq_atoms', '[', 's', ']', ')', 'if', 'atom', '.', 'symbol', 'in', 'num_atoms', ':', 'if', 'label_index', '+', '1', '>', 'sum', '(', 'num_atoms', '.', 'values', '(', ')', ')', ':', 'num_atoms', '[', 'atom', '.', 'symbol', ']', '+=', '1', 'atom_label', '=', 'atom', '.', 'symbol', '+', 'str', '(', 'num_atoms', '[', 'atom', '.', 'symbol', ']', '-', '1', ')', 'classes', '.', 'append', '(', '{', '""ATOM""', ':', 'atom_label', ',', '""Z""', ':', 'atom', '.', 'Z', '}', ')', 'else', ':', 'num_atoms', '[', 'atom', '.', 'symbol', ']', '=', '1', 'classes', '.', 'append', '(', '{', '""ATOM""', ':', 'atom', '.', 'symbol', ',', '""Z""', ':', 'atom', '.', 'Z', '}', ')', 'sites', '.', 'append', '(', '{', '""ATOM""', ':', 'classes', '[', 'label_index', ']', '[', '""ATOM""', ']', ',', '""POS""', ':', 'site', '.', 'coords', '/', 'alat', '}', ')', 'ctrl_dict', '.', 'update', '(', '{', '""ALAT""', ':', 'alat', '/', 'bohr_to_angstrom', ',', '""PLAT""', ':', 'plat', ',', '""CLASS""', ':', 'classes', ',', '""SITE""', ':', 'sites', '}', ')', 'return', 'ctrl_dict']","Returns the CTRL as a dictionary. ""SITE"" and ""CLASS"" are of
        the form {'CATEGORY': {'TOKEN': value}}, the rest is of the
        form 'TOKEN'/'CATEGORY': value. It gets the conventional standard
        structure because primitive cells use the conventional
        a-lattice parameter as the scaling factor and not the a-lattice
        parameter of the primitive cell.","['Returns', 'the', 'CTRL', 'as', 'a', 'dictionary', '.', 'SITE', 'and', 'CLASS', 'are', 'of', 'the', 'form', '{', 'CATEGORY', ':', '{', 'TOKEN', ':', 'value', '}}', 'the', 'rest', 'is', 'of', 'the', 'form', 'TOKEN', '/', 'CATEGORY', ':', 'value', '.', 'It', 'gets', 'the', 'conventional', 'standard', 'structure', 'because', 'primitive', 'cells', 'use', 'the', 'conventional', 'a', '-', 'lattice', 'parameter', 'as', 'the', 'scaling', 'factor', 'and', 'not', 'the', 'a', '-', 'lattice', 'parameter', 'of', 'the', 'primitive', 'cell', '.']",python,E,3,True,1,train
22806,seleniumbase/SeleniumBase,seleniumbase/fixtures/email_manager.py,https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/fixtures/email_manager.py#L341-L359,"def get_content_type(self, msg, content_type=""HTML""):
        """"""
        Given an Email.Message object, gets the content-type payload
        as specified by @content_type. This is the actual body of the
        email.
        @Params
        msg - Email.Message object to get message content for
        content_type - Type of content to get from the email
        @Return
        String content of the email in the given type
        """"""
        if ""HTML"" in content_type.upper():
            content_type = self.HTML
        elif ""PLAIN"" in content_type.upper():
            content_type = self.PLAIN

        for part in msg.walk():
            if str(part.get_content_type()) == content_type:
                return str(part.get_payload(decode=True))","['def', 'get_content_type', '(', 'self', ',', 'msg', ',', 'content_type', '=', '""HTML""', ')', ':', 'if', '""HTML""', 'in', 'content_type', '.', 'upper', '(', ')', ':', 'content_type', '=', 'self', '.', 'HTML', 'elif', '""PLAIN""', 'in', 'content_type', '.', 'upper', '(', ')', ':', 'content_type', '=', 'self', '.', 'PLAIN', 'for', 'part', 'in', 'msg', '.', 'walk', '(', ')', ':', 'if', 'str', '(', 'part', '.', 'get_content_type', '(', ')', ')', '==', 'content_type', ':', 'return', 'str', '(', 'part', '.', 'get_payload', '(', 'decode', '=', 'True', ')', ')']","Given an Email.Message object, gets the content-type payload
        as specified by @content_type. This is the actual body of the
        email.
        @Params
        msg - Email.Message object to get message content for
        content_type - Type of content to get from the email
        @Return
        String content of the email in the given type","['Given', 'an', 'Email', '.', 'Message', 'object', 'gets', 'the', 'content', '-', 'type', 'payload', 'as', 'specified', 'by']",python,E,3,True,1,train
22819,seleniumbase/SeleniumBase,seleniumbase/core/mysql.py,https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/core/mysql.py#L39-L46,"def query_fetch_all(self, query, values):
        """"""
        Executes a db query, gets all the values, and closes the connection.
        """"""
        self.cursor.execute(query, values)
        retval = self.cursor.fetchall()
        self.__close_db()
        return retval","['def', 'query_fetch_all', '(', 'self', ',', 'query', ',', 'values', ')', ':', 'self', '.', 'cursor', '.', 'execute', '(', 'query', ',', 'values', ')', 'retval', '=', 'self', '.', 'cursor', '.', 'fetchall', '(', ')', 'self', '.', '__close_db', '(', ')', 'return', 'retval']","Executes a db query, gets all the values, and closes the connection.","['Executes', 'a', 'db', 'query', 'gets', 'all', 'the', 'values', 'and', 'closes', 'the', 'connection', '.']",python,E,3,True,1,train
22820,seleniumbase/SeleniumBase,seleniumbase/core/mysql.py,https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/core/mysql.py#L48-L55,"def query_fetch_one(self, query, values):
        """"""
        Executes a db query, gets the first value, and closes the connection.
        """"""
        self.cursor.execute(query, values)
        retval = self.cursor.fetchone()
        self.__close_db()
        return retval","['def', 'query_fetch_one', '(', 'self', ',', 'query', ',', 'values', ')', ':', 'self', '.', 'cursor', '.', 'execute', '(', 'query', ',', 'values', ')', 'retval', '=', 'self', '.', 'cursor', '.', 'fetchone', '(', ')', 'self', '.', '__close_db', '(', ')', 'return', 'retval']","Executes a db query, gets the first value, and closes the connection.","['Executes', 'a', 'db', 'query', 'gets', 'the', 'first', 'value', 'and', 'closes', 'the', 'connection', '.']",python,E,3,True,1,train
25830,O365/python-o365,O365/connection.py,https://github.com/O365/python-o365/blob/02a71cf3775cc6a3c042e003365d6a07c8c75a73/O365/connection.py#L428-L471,"def request_token(self, authorization_url, store_token=True,
                      token_path=None, **kwargs):
        """""" Authenticates for the specified url and gets the token, save the
        token for future based if requested

        :param str authorization_url: url given by the authorization flow
        :param bool store_token: whether or not to store the token,
         so u don't have to keep opening the auth link and
         authenticating every time
        :param Path token_path: full path to where the token should be saved to
        :param kwargs: allow to pass unused params in conjunction with Connection
        :return: Success/Failure
        :rtype: bool
        """"""

        if self.session is None:
            raise RuntimeError(""Fist call 'get_authorization_url' to ""
                               ""generate a valid oauth object"")

        # TODO: remove token_path in future versions
        if token_path is not None:
            warnings.warn('""token_path"" param will be removed in future versions.'
                          ' Use a TokenBackend instead', DeprecationWarning)
        _, client_secret = self.auth

        # Allow token scope to not match requested scope.
        # (Other auth libraries allow this, but Requests-OAuthlib
        # raises exception on scope mismatch by default.)
        os.environ['OAUTHLIB_RELAX_TOKEN_SCOPE'] = '1'
        os.environ['OAUTHLIB_IGNORE_SCOPE_CHANGE'] = '1'

        try:
            self.token_backend.token = Token(self.session.fetch_token(
                token_url=self._oauth2_token_url,
                authorization_response=authorization_url,
                include_client_id=True,
                client_secret=client_secret))
        except Exception as e:
            log.error('Unable to fetch auth token. Error: {}'.format(str(e)))
            return False

        if store_token:
            self.token_backend.save_token()
        return True","['def', 'request_token', '(', 'self', ',', 'authorization_url', ',', 'store_token', '=', 'True', ',', 'token_path', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'if', 'self', '.', 'session', 'is', 'None', ':', 'raise', 'RuntimeError', '(', '""Fist call \'get_authorization_url\' to ""', '""generate a valid oauth object""', ')', '# TODO: remove token_path in future versions', 'if', 'token_path', 'is', 'not', 'None', ':', 'warnings', '.', 'warn', '(', '\'""token_path"" param will be removed in future versions.\'', ""' Use a TokenBackend instead'"", ',', 'DeprecationWarning', ')', '_', ',', 'client_secret', '=', 'self', '.', 'auth', '# Allow token scope to not match requested scope.', '# (Other auth libraries allow this, but Requests-OAuthlib', '# raises exception on scope mismatch by default.)', 'os', '.', 'environ', '[', ""'OAUTHLIB_RELAX_TOKEN_SCOPE'"", ']', '=', ""'1'"", 'os', '.', 'environ', '[', ""'OAUTHLIB_IGNORE_SCOPE_CHANGE'"", ']', '=', ""'1'"", 'try', ':', 'self', '.', 'token_backend', '.', 'token', '=', 'Token', '(', 'self', '.', 'session', '.', 'fetch_token', '(', 'token_url', '=', 'self', '.', '_oauth2_token_url', ',', 'authorization_response', '=', 'authorization_url', ',', 'include_client_id', '=', 'True', ',', 'client_secret', '=', 'client_secret', ')', ')', 'except', 'Exception', 'as', 'e', ':', 'log', '.', 'error', '(', ""'Unable to fetch auth token. Error: {}'"", '.', 'format', '(', 'str', '(', 'e', ')', ')', ')', 'return', 'False', 'if', 'store_token', ':', 'self', '.', 'token_backend', '.', 'save_token', '(', ')', 'return', 'True']","Authenticates for the specified url and gets the token, save the
        token for future based if requested

        :param str authorization_url: url given by the authorization flow
        :param bool store_token: whether or not to store the token,
         so u don't have to keep opening the auth link and
         authenticating every time
        :param Path token_path: full path to where the token should be saved to
        :param kwargs: allow to pass unused params in conjunction with Connection
        :return: Success/Failure
        :rtype: bool","['Authenticates', 'for', 'the', 'specified', 'url', 'and', 'gets', 'the', 'token', 'save', 'the', 'token', 'for', 'future', 'based', 'if', 'requested']",python,E,3,True,1,train
26428,osrg/ryu,ryu/services/protocols/bgp/bgpspeaker.py,https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/services/protocols/bgp/bgpspeaker.py#L1349-L1377,"def attribute_map_get(self, address, route_dist=None,
                          route_family=RF_VPN_V4):
        """"""This method gets in-bound filters of the specified neighbor.

        ``address`` specifies the IP address of the neighbor.

        ``route_dist`` specifies route distinguisher that has attribute_maps.

        ``route_family`` specifies route family of the VRF.
        This parameter must be one of the following.

        - RF_VPN_V4 (default) = 'ipv4'
        - RF_VPN_V6           = 'ipv6'

        Returns a list object containing an instance of AttributeMap
        """"""

        if route_family not in SUPPORTED_VRF_RF:
            raise ValueError('Unsupported route_family: %s' % route_family)

        func_name = 'neighbor.attribute_map.get'
        param = {
            neighbors.IP_ADDRESS: address,
        }
        if route_dist is not None:
            param[vrfs.ROUTE_DISTINGUISHER] = route_dist
            param[vrfs.VRF_RF] = route_family

        return call(func_name, **param)","['def', 'attribute_map_get', '(', 'self', ',', 'address', ',', 'route_dist', '=', 'None', ',', 'route_family', '=', 'RF_VPN_V4', ')', ':', 'if', 'route_family', 'not', 'in', 'SUPPORTED_VRF_RF', ':', 'raise', 'ValueError', '(', ""'Unsupported route_family: %s'"", '%', 'route_family', ')', 'func_name', '=', ""'neighbor.attribute_map.get'"", 'param', '=', '{', 'neighbors', '.', 'IP_ADDRESS', ':', 'address', ',', '}', 'if', 'route_dist', 'is', 'not', 'None', ':', 'param', '[', 'vrfs', '.', 'ROUTE_DISTINGUISHER', ']', '=', 'route_dist', 'param', '[', 'vrfs', '.', 'VRF_RF', ']', '=', 'route_family', 'return', 'call', '(', 'func_name', ',', '*', '*', 'param', ')']","This method gets in-bound filters of the specified neighbor.

        ``address`` specifies the IP address of the neighbor.

        ``route_dist`` specifies route distinguisher that has attribute_maps.

        ``route_family`` specifies route family of the VRF.
        This parameter must be one of the following.

        - RF_VPN_V4 (default) = 'ipv4'
        - RF_VPN_V6           = 'ipv6'

        Returns a list object containing an instance of AttributeMap","['This', 'method', 'gets', 'in', '-', 'bound', 'filters', 'of', 'the', 'specified', 'neighbor', '.']",python,E,3,True,1,train
493,seperman/s3utils,s3utils/s3utils.py,https://github.com/seperman/s3utils/blob/aea41388a023dcf1e95588402077e31097514cf1/s3utils/s3utils.py#L613-L667,"def ls(self, folder="""", begin_from_file="""", num=-1, get_grants=False, all_grant_data=False):
        """"""
        gets the list of file names (keys) in a s3 folder

        Parameters
        ----------

        folder : string
            Path to file on S3

        num: integer, optional
            number of results to return, by default it returns all results.

        begin_from_file: string, optional
            which file to start from on S3.
            This is usedful in case you are iterating over lists of files and you need to page the result by
            starting listing from a certain file and fetching certain num (number) of files.


        Examples
        --------

            >>> from s3utils import S3utils
            >>> s3utils = S3utils(
            ... AWS_ACCESS_KEY_ID = 'your access key',
            ... AWS_SECRET_ACCESS_KEY = 'your secret key',
            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',
            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose
            ... )
            >>> print(s3utils.ls(""test/""))
            {u'test/myfolder/', u'test/myfolder/em/', u'test/myfolder/hoho/', u'test/myfolder/hoho/.DS_Store', u'test/myfolder/hoho/haha/', u'test/myfolder/hoho/haha/ff', u'test/myfolder/hoho/haha/photo.JPG'}

        """"""
        # S3 object key can't start with /
        folder = re.sub(r""^/"", """", folder)

        bucket_files = self.bucket.list(prefix=folder, marker=begin_from_file)

        # in case listing grants
        if get_grants:
            list_of_files = OrderedDict()
            for (i, v) in enumerate(bucket_files):
                file_info = {v.name: self.__get_grants(v.name, all_grant_data)}
                list_of_files.update(file_info)
                if i == num:
                    break

        else:
            list_of_files = set([])
            for (i, v) in enumerate(bucket_files):
                list_of_files.add(v.name)
                if i == num:
                    break

        return list_of_files","['def', 'ls', '(', 'self', ',', 'folder', '=', '""""', ',', 'begin_from_file', '=', '""""', ',', 'num', '=', '-', '1', ',', 'get_grants', '=', 'False', ',', 'all_grant_data', '=', 'False', ')', ':', ""# S3 object key can't start with /"", 'folder', '=', 're', '.', 'sub', '(', 'r""^/""', ',', '""""', ',', 'folder', ')', 'bucket_files', '=', 'self', '.', 'bucket', '.', 'list', '(', 'prefix', '=', 'folder', ',', 'marker', '=', 'begin_from_file', ')', '# in case listing grants', 'if', 'get_grants', ':', 'list_of_files', '=', 'OrderedDict', '(', ')', 'for', '(', 'i', ',', 'v', ')', 'in', 'enumerate', '(', 'bucket_files', ')', ':', 'file_info', '=', '{', 'v', '.', 'name', ':', 'self', '.', '__get_grants', '(', 'v', '.', 'name', ',', 'all_grant_data', ')', '}', 'list_of_files', '.', 'update', '(', 'file_info', ')', 'if', 'i', '==', 'num', ':', 'break', 'else', ':', 'list_of_files', '=', 'set', '(', '[', ']', ')', 'for', '(', 'i', ',', 'v', ')', 'in', 'enumerate', '(', 'bucket_files', ')', ':', 'list_of_files', '.', 'add', '(', 'v', '.', 'name', ')', 'if', 'i', '==', 'num', ':', 'break', 'return', 'list_of_files']","gets the list of file names (keys) in a s3 folder

        Parameters
        ----------

        folder : string
            Path to file on S3

        num: integer, optional
            number of results to return, by default it returns all results.

        begin_from_file: string, optional
            which file to start from on S3.
            This is usedful in case you are iterating over lists of files and you need to page the result by
            starting listing from a certain file and fetching certain num (number) of files.


        Examples
        --------

            >>> from s3utils import S3utils
            >>> s3utils = S3utils(
            ... AWS_ACCESS_KEY_ID = 'your access key',
            ... AWS_SECRET_ACCESS_KEY = 'your secret key',
            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',
            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose
            ... )
            >>> print(s3utils.ls(""test/""))
            {u'test/myfolder/', u'test/myfolder/em/', u'test/myfolder/hoho/', u'test/myfolder/hoho/.DS_Store', u'test/myfolder/hoho/haha/', u'test/myfolder/hoho/haha/ff', u'test/myfolder/hoho/haha/photo.JPG'}","['gets', 'the', 'list', 'of', 'file', 'names', '(', 'keys', ')', 'in', 'a', 's3', 'folder']",python,E,3,True,1,train
2151,crossbario/txaio-etcd,txaioetcd/_client_tx.py,https://github.com/crossbario/txaio-etcd/blob/c9aebff7f288a0b219bffc9d2579d22cf543baa5/txaioetcd/_client_tx.py#L294-L381,"def get(self,
            key,
            range_end=None,
            count_only=None,
            keys_only=None,
            limit=None,
            max_create_revision=None,
            min_create_revision=None,
            min_mod_revision=None,
            revision=None,
            serializable=None,
            sort_order=None,
            sort_target=None,
            timeout=None):
        """"""
        Range gets the keys in the range from the key-value store.

        :param key: key is the first key for the range. If range_end is not given,
            the request only looks up key.
        :type key: bytes

        :param range_end: range_end is the upper bound on the requested range
            [key, range_end). If range_end is ``\\0``, the range is all keys ``\u003e=`` key.
            If the range_end is one bit larger than the given key, then the range requests
            get the all keys with the prefix (the given key). If both key and range_end
            are ``\\0``, then range requests returns all keys.
        :type range_end: bytes

        :param prefix: If set, and no range_end is given, compute range_end from key prefix.
        :type prefix: bool

        :param count_only: count_only when set returns only the count of the keys in the range.
        :type count_only: bool

        :param keys_only: keys_only when set returns only the keys and not the values.
        :type keys_only: bool

        :param limit: limit is a limit on the number of keys returned for the request.
        :type limit: int

        :param max_create_revision: max_create_revision is the upper bound for returned
            key create revisions; all keys with greater create revisions will be filtered away.
        :type max_create_revision: int

        :param max_mod_revision: max_mod_revision is the upper bound for returned key
            mod revisions; all keys with greater mod revisions will be filtered away.
        :type max_mod_revision: int

        :param min_create_revision: min_create_revision is the lower bound for returned
            key create revisions; all keys with lesser create trevisions will be filtered away.
        :type min_create_revision: int

        :param min_mod_revision: min_mod_revision is the lower bound for returned key
            mod revisions; all keys with lesser mod revisions will be filtered away.
        :type min_min_revision: int

        :param revision: revision is the point-in-time of the key-value store to use for the
            range. If revision is less or equal to zero, the range is over the newest
            key-value store. If the revision has been compacted, ErrCompacted is returned as
            a response.
        :type revision: int

        :param serializable: serializable sets the range request to use serializable
            member-local reads. Range requests are linearizable by default; linearizable
            requests have higher latency and lower throughput than serializable requests
            but reflect the current consensus of the cluster. For better performance, in
            exchange for possible stale reads, a serializable range request is served
            locally without needing to reach consensus with other nodes in the cluster.
        :type serializable: bool

        :param sort_order: Sort order for returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_ORDERS`.
        :type sort_order: str

        :param sort_target: Sort target for sorting returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_TARGETS`.
        :type sort_taget: str or None

        :param timeout: Request timeout in seconds.
        :type timeout: int or None
        """"""
        assembler = commons.GetRequestAssembler(self._url, key, range_end)

        obj = yield self._post(assembler.url, assembler.data, timeout)

        result = Range._parse(obj)

        returnValue(result)","['def', 'get', '(', 'self', ',', 'key', ',', 'range_end', '=', 'None', ',', 'count_only', '=', 'None', ',', 'keys_only', '=', 'None', ',', 'limit', '=', 'None', ',', 'max_create_revision', '=', 'None', ',', 'min_create_revision', '=', 'None', ',', 'min_mod_revision', '=', 'None', ',', 'revision', '=', 'None', ',', 'serializable', '=', 'None', ',', 'sort_order', '=', 'None', ',', 'sort_target', '=', 'None', ',', 'timeout', '=', 'None', ')', ':', 'assembler', '=', 'commons', '.', 'GetRequestAssembler', '(', 'self', '.', '_url', ',', 'key', ',', 'range_end', ')', 'obj', '=', 'yield', 'self', '.', '_post', '(', 'assembler', '.', 'url', ',', 'assembler', '.', 'data', ',', 'timeout', ')', 'result', '=', 'Range', '.', '_parse', '(', 'obj', ')', 'returnValue', '(', 'result', ')']","Range gets the keys in the range from the key-value store.

        :param key: key is the first key for the range. If range_end is not given,
            the request only looks up key.
        :type key: bytes

        :param range_end: range_end is the upper bound on the requested range
            [key, range_end). If range_end is ``\\0``, the range is all keys ``\u003e=`` key.
            If the range_end is one bit larger than the given key, then the range requests
            get the all keys with the prefix (the given key). If both key and range_end
            are ``\\0``, then range requests returns all keys.
        :type range_end: bytes

        :param prefix: If set, and no range_end is given, compute range_end from key prefix.
        :type prefix: bool

        :param count_only: count_only when set returns only the count of the keys in the range.
        :type count_only: bool

        :param keys_only: keys_only when set returns only the keys and not the values.
        :type keys_only: bool

        :param limit: limit is a limit on the number of keys returned for the request.
        :type limit: int

        :param max_create_revision: max_create_revision is the upper bound for returned
            key create revisions; all keys with greater create revisions will be filtered away.
        :type max_create_revision: int

        :param max_mod_revision: max_mod_revision is the upper bound for returned key
            mod revisions; all keys with greater mod revisions will be filtered away.
        :type max_mod_revision: int

        :param min_create_revision: min_create_revision is the lower bound for returned
            key create revisions; all keys with lesser create trevisions will be filtered away.
        :type min_create_revision: int

        :param min_mod_revision: min_mod_revision is the lower bound for returned key
            mod revisions; all keys with lesser mod revisions will be filtered away.
        :type min_min_revision: int

        :param revision: revision is the point-in-time of the key-value store to use for the
            range. If revision is less or equal to zero, the range is over the newest
            key-value store. If the revision has been compacted, ErrCompacted is returned as
            a response.
        :type revision: int

        :param serializable: serializable sets the range request to use serializable
            member-local reads. Range requests are linearizable by default; linearizable
            requests have higher latency and lower throughput than serializable requests
            but reflect the current consensus of the cluster. For better performance, in
            exchange for possible stale reads, a serializable range request is served
            locally without needing to reach consensus with other nodes in the cluster.
        :type serializable: bool

        :param sort_order: Sort order for returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_ORDERS`.
        :type sort_order: str

        :param sort_target: Sort target for sorting returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_TARGETS`.
        :type sort_taget: str or None

        :param timeout: Request timeout in seconds.
        :type timeout: int or None","['Range', 'gets', 'the', 'keys', 'in', 'the', 'range', 'from', 'the', 'key', '-', 'value', 'store', '.']",python,E,3,True,1,train
2272,crossbario/txaio-etcd,txaioetcd/_client_pg.py,https://github.com/crossbario/txaio-etcd/blob/c9aebff7f288a0b219bffc9d2579d22cf543baa5/txaioetcd/_client_pg.py#L173-L261,"def get(self,
            key,
            range_end=None,
            count_only=None,
            keys_only=None,
            limit=None,
            max_create_revision=None,
            min_create_revision=None,
            min_mod_revision=None,
            revision=None,
            serializable=None,
            sort_order=None,
            sort_target=None,
            timeout=None):
        """"""
        Range gets the keys in the range from the key-value store.

        :param key: key is the first key for the range. If range_end is not given,
            the request only looks up key.
        :type key: bytes

        :param range_end: range_end is the upper bound on the requested range
            [key, range_end). If range_end is ``\\0``, the range is all keys ``\u003e=`` key.
            If the range_end is one bit larger than the given key, then the range requests
            get the all keys with the prefix (the given key). If both key and range_end
            are ``\\0``, then range requests returns all keys.
        :type range_end: bytes

        :param prefix: If set, and no range_end is given, compute range_end from key prefix.
        :type prefix: bool

        :param count_only: count_only when set returns only the count of the keys in the range.
        :type count_only: bool

        :param keys_only: keys_only when set returns only the keys and not the values.
        :type keys_only: bool

        :param limit: limit is a limit on the number of keys returned for the request.
        :type limit: int

        :param max_create_revision: max_create_revision is the upper bound for returned
            key create revisions; all keys with greater create revisions will be filtered away.
        :type max_create_revision: int

        :param max_mod_revision: max_mod_revision is the upper bound for returned key
            mod revisions; all keys with greater mod revisions will be filtered away.
        :type max_mod_revision: int

        :param min_create_revision: min_create_revision is the lower bound for returned
            key create revisions; all keys with lesser create trevisions will be filtered away.
        :type min_create_revision: int

        :param min_mod_revision: min_mod_revision is the lower bound for returned key
            mod revisions; all keys with lesser mod revisions will be filtered away.
        :type min_min_revision: int

        :param revision: revision is the point-in-time of the key-value store to use for the
            range. If revision is less or equal to zero, the range is over the newest
            key-value store. If the revision has been compacted, ErrCompacted is returned as
            a response.
        :type revision: int

        :param serializable: serializable sets the range request to use serializable
            member-local reads. Range requests are linearizable by default; linearizable
            requests have higher latency and lower throughput than serializable requests
            but reflect the current consensus of the cluster. For better performance, in
            exchange for possible stale reads, a serializable range request is served
            locally without needing to reach consensus with other nodes in the cluster.
        :type serializable: bool

        :param sort_order: Sort order for returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_ORDERS`.
        :type sort_order: str

        :param sort_target: Sort target for sorting returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_TARGETS`.
        :type sort_taget: str or None

        :param timeout: Request timeout in seconds.
        :type timeout: int or None
        """"""

        def run(pg_txn):
            pg_txn.execute(""SELECT pgetcd.get(%s,%s)"", (Binary(key), 10))
            rows = pg_txn.fetchall()
            res = ""{0}"".format(rows[0][0])
            return res

        return self._pool.runInteraction(run)","['def', 'get', '(', 'self', ',', 'key', ',', 'range_end', '=', 'None', ',', 'count_only', '=', 'None', ',', 'keys_only', '=', 'None', ',', 'limit', '=', 'None', ',', 'max_create_revision', '=', 'None', ',', 'min_create_revision', '=', 'None', ',', 'min_mod_revision', '=', 'None', ',', 'revision', '=', 'None', ',', 'serializable', '=', 'None', ',', 'sort_order', '=', 'None', ',', 'sort_target', '=', 'None', ',', 'timeout', '=', 'None', ')', ':', 'def', 'run', '(', 'pg_txn', ')', ':', 'pg_txn', '.', 'execute', '(', '""SELECT pgetcd.get(%s,%s)""', ',', '(', 'Binary', '(', 'key', ')', ',', '10', ')', ')', 'rows', '=', 'pg_txn', '.', 'fetchall', '(', ')', 'res', '=', '""{0}""', '.', 'format', '(', 'rows', '[', '0', ']', '[', '0', ']', ')', 'return', 'res', 'return', 'self', '.', '_pool', '.', 'runInteraction', '(', 'run', ')']","Range gets the keys in the range from the key-value store.

        :param key: key is the first key for the range. If range_end is not given,
            the request only looks up key.
        :type key: bytes

        :param range_end: range_end is the upper bound on the requested range
            [key, range_end). If range_end is ``\\0``, the range is all keys ``\u003e=`` key.
            If the range_end is one bit larger than the given key, then the range requests
            get the all keys with the prefix (the given key). If both key and range_end
            are ``\\0``, then range requests returns all keys.
        :type range_end: bytes

        :param prefix: If set, and no range_end is given, compute range_end from key prefix.
        :type prefix: bool

        :param count_only: count_only when set returns only the count of the keys in the range.
        :type count_only: bool

        :param keys_only: keys_only when set returns only the keys and not the values.
        :type keys_only: bool

        :param limit: limit is a limit on the number of keys returned for the request.
        :type limit: int

        :param max_create_revision: max_create_revision is the upper bound for returned
            key create revisions; all keys with greater create revisions will be filtered away.
        :type max_create_revision: int

        :param max_mod_revision: max_mod_revision is the upper bound for returned key
            mod revisions; all keys with greater mod revisions will be filtered away.
        :type max_mod_revision: int

        :param min_create_revision: min_create_revision is the lower bound for returned
            key create revisions; all keys with lesser create trevisions will be filtered away.
        :type min_create_revision: int

        :param min_mod_revision: min_mod_revision is the lower bound for returned key
            mod revisions; all keys with lesser mod revisions will be filtered away.
        :type min_min_revision: int

        :param revision: revision is the point-in-time of the key-value store to use for the
            range. If revision is less or equal to zero, the range is over the newest
            key-value store. If the revision has been compacted, ErrCompacted is returned as
            a response.
        :type revision: int

        :param serializable: serializable sets the range request to use serializable
            member-local reads. Range requests are linearizable by default; linearizable
            requests have higher latency and lower throughput than serializable requests
            but reflect the current consensus of the cluster. For better performance, in
            exchange for possible stale reads, a serializable range request is served
            locally without needing to reach consensus with other nodes in the cluster.
        :type serializable: bool

        :param sort_order: Sort order for returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_ORDERS`.
        :type sort_order: str

        :param sort_target: Sort target for sorting returned KVs,
            one of :class:`txaioetcd.OpGet.SORT_TARGETS`.
        :type sort_taget: str or None

        :param timeout: Request timeout in seconds.
        :type timeout: int or None","['Range', 'gets', 'the', 'keys', 'in', 'the', 'range', 'from', 'the', 'key', '-', 'value', 'store', '.']",python,E,3,True,1,train
3986,acutesoftware/AIKIF,aikif/programs.py,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/programs.py#L140-L154,"def collect_program_info(self, fname):
        """"""
        gets details on the program, size, date, list of functions
        and produces a Markdown file for documentation
        """"""
        md = '#AIKIF Technical details\n'
        md += 'Autogenerated list of programs with comments and progress\n'
        md += '\nFilename | Comment | Date | Size\n'
        md += '--- | --- | --- | ---\n'
        for i in self.lstPrograms:
            md += self.get_file_info_line(i, ' | ')
        
        # save the details an Markdown file 
        with open(fname, 'w') as f:
            f.write(md)","['def', 'collect_program_info', '(', 'self', ',', 'fname', ')', ':', 'md', '=', ""'#AIKIF Technical details\\n'"", 'md', '+=', ""'Autogenerated list of programs with comments and progress\\n'"", 'md', '+=', ""'\\nFilename | Comment | Date | Size\\n'"", 'md', '+=', ""'--- | --- | --- | ---\\n'"", 'for', 'i', 'in', 'self', '.', 'lstPrograms', ':', 'md', '+=', 'self', '.', 'get_file_info_line', '(', 'i', ',', ""' | '"", ')', '# save the details an Markdown file ', 'with', 'open', '(', 'fname', ',', ""'w'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'md', ')']","gets details on the program, size, date, list of functions
        and produces a Markdown file for documentation","['gets', 'details', 'on', 'the', 'program', 'size', 'date', 'list', 'of', 'functions', 'and', 'produces', 'a', 'Markdown', 'file', 'for', 'documentation']",python,E,3,True,1,train
4259,acutesoftware/AIKIF,scripts/examples/gui_view_world.py,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/scripts/examples/gui_view_world.py#L167-L194,"def agent_color(self, val):
        """"""
        gets a colour for agent 0 - 9
        """"""
        if val == '0': 
            colour = 'blue'
        elif val == '1':
            colour = 'navy'
        elif val == '2':
            colour = 'firebrick'
        elif val == '3':
            colour = 'blue'
        elif val == '4':
            colour = 'blue2'
        elif val == '5':
            colour = 'blue4'
        elif val == '6':
            colour = 'gray22'
        elif val == '7':
            colour = 'gray57'
        elif val == '8':
            colour = 'red4'
        elif val == '9':
            colour = 'red3'

    
        
        return colour","['def', 'agent_color', '(', 'self', ',', 'val', ')', ':', 'if', 'val', '==', ""'0'"", ':', 'colour', '=', ""'blue'"", 'elif', 'val', '==', ""'1'"", ':', 'colour', '=', ""'navy'"", 'elif', 'val', '==', ""'2'"", ':', 'colour', '=', ""'firebrick'"", 'elif', 'val', '==', ""'3'"", ':', 'colour', '=', ""'blue'"", 'elif', 'val', '==', ""'4'"", ':', 'colour', '=', ""'blue2'"", 'elif', 'val', '==', ""'5'"", ':', 'colour', '=', ""'blue4'"", 'elif', 'val', '==', ""'6'"", ':', 'colour', '=', ""'gray22'"", 'elif', 'val', '==', ""'7'"", ':', 'colour', '=', ""'gray57'"", 'elif', 'val', '==', ""'8'"", ':', 'colour', '=', ""'red4'"", 'elif', 'val', '==', ""'9'"", ':', 'colour', '=', ""'red3'"", 'return', 'colour']",gets a colour for agent 0 - 9,"['gets', 'a', 'colour', 'for', 'agent', '0', '-', '9']",python,E,3,True,1,train
4491,acutesoftware/AIKIF,aikif/toolbox/file_tools.py,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/toolbox/file_tools.py#L82-L98,"def copy_all_files_and_subfolders(src, dest, base_path_ignore, xtn_list):
	""""""
	file_tools.copy_all_files_and_subfolders(src, dest, backup_path, ['*.*'])
	gets list of all subfolders and copies each file to 
	its own folder in 'dest' folder
	paths, xtn, excluded, output_file_name = 'my_files.csv')
	""""""
	ensure_dir(dest)
	fl = mod_fl.FileList([src], xtn_list, exclude_folders,  '')
	all_paths = fl.get_list_of_paths()
	fl.save_filelist(os.path.join(dest,'files_backed_up.csv'),  [""name"", ""path"", ""size"", ""date""])
	
	for p in all_paths:
		dest_folder = os.path.join(dest, p[len(base_path_ignore):])
		ensure_dir(dest_folder)
		#print('copying ' + p)
		copy_files_to_folder(p, dest_folder, xtn='*')","['def', 'copy_all_files_and_subfolders', '(', 'src', ',', 'dest', ',', 'base_path_ignore', ',', 'xtn_list', ')', ':', 'ensure_dir', '(', 'dest', ')', 'fl', '=', 'mod_fl', '.', 'FileList', '(', '[', 'src', ']', ',', 'xtn_list', ',', 'exclude_folders', ',', ""''"", ')', 'all_paths', '=', 'fl', '.', 'get_list_of_paths', '(', ')', 'fl', '.', 'save_filelist', '(', 'os', '.', 'path', '.', 'join', '(', 'dest', ',', ""'files_backed_up.csv'"", ')', ',', '[', '""name""', ',', '""path""', ',', '""size""', ',', '""date""', ']', ')', 'for', 'p', 'in', 'all_paths', ':', 'dest_folder', '=', 'os', '.', 'path', '.', 'join', '(', 'dest', ',', 'p', '[', 'len', '(', 'base_path_ignore', ')', ':', ']', ')', 'ensure_dir', '(', 'dest_folder', ')', ""#print('copying ' + p)"", 'copy_files_to_folder', '(', 'p', ',', 'dest_folder', ',', 'xtn', '=', ""'*'"", ')']","file_tools.copy_all_files_and_subfolders(src, dest, backup_path, ['*.*'])
	gets list of all subfolders and copies each file to 
	its own folder in 'dest' folder
	paths, xtn, excluded, output_file_name = 'my_files.csv')","['file_tools', '.', 'copy_all_files_and_subfolders', '(', 'src', 'dest', 'backup_path', '[', '*', '.', '*', ']', ')', 'gets', 'list', 'of', 'all', 'subfolders', 'and', 'copies', 'each', 'file', 'to', 'its', 'own', 'folder', 'in', 'dest', 'folder', 'paths', 'xtn', 'excluded', 'output_file_name', '=', 'my_files', '.', 'csv', ')']",python,E,3,True,1,train
7214,CiscoUcs/UcsPythonSDK,src/UcsSdk/UcsBase.py,https://github.com/CiscoUcs/UcsPythonSDK/blob/bf6b07d6abeacb922c92b198352eda4eb9e4629b/src/UcsSdk/UcsBase.py#L230-L256,"def getattr(self, key):
		"""""" This method gets attribute value of a Managed Object. """"""
		if ((key == ""classId"") and (self.__dict__.has_key(key))):
			return self.__dict__[key]

		if UcsUtils.FindClassIdInMoMetaIgnoreCase(self.classId):
			if self.__dict__.has_key(key):
				if key in _ManagedObjectMeta[self.classId]:
					"""""" property exists """"""
					return self.__dict__[key]
			else:
				if self.__dict__.has_key('XtraProperty'):
					if self.__dict__['XtraProperty'].has_key(key):
						return self.__dict__['XtraProperty'][UcsUtils.WordU(key)]
					else:
						raise AttributeError(key)
				else:
					# TODO: Add Warning/Error messages in Logger.
					print ""No XtraProperty in mo:"", self.classId, "" key:"", key
		else:
			"""""" property does not exist """"""
			if self.__dict__['XtraProperty'].has_key(key):
				return self.__dict__['XtraProperty'][UcsUtils.WordU(key)]
			elif key == ""Dn"" or key == ""Rn"":
				return None
			else:
				raise AttributeError(key)","['def', 'getattr', '(', 'self', ',', 'key', ')', ':', 'if', '(', '(', 'key', '==', '""classId""', ')', 'and', '(', 'self', '.', '__dict__', '.', 'has_key', '(', 'key', ')', ')', ')', ':', 'return', 'self', '.', '__dict__', '[', 'key', ']', 'if', 'UcsUtils', '.', 'FindClassIdInMoMetaIgnoreCase', '(', 'self', '.', 'classId', ')', ':', 'if', 'self', '.', '__dict__', '.', 'has_key', '(', 'key', ')', ':', 'if', 'key', 'in', '_ManagedObjectMeta', '[', 'self', '.', 'classId', ']', ':', '"""""" property exists """"""', 'return', 'self', '.', '__dict__', '[', 'key', ']', 'else', ':', 'if', 'self', '.', '__dict__', '.', 'has_key', '(', ""'XtraProperty'"", ')', ':', 'if', 'self', '.', '__dict__', '[', ""'XtraProperty'"", ']', '.', 'has_key', '(', 'key', ')', ':', 'return', 'self', '.', '__dict__', '[', ""'XtraProperty'"", ']', '[', 'UcsUtils', '.', 'WordU', '(', 'key', ')', ']', 'else', ':', 'raise', 'AttributeError', '(', 'key', ')', 'else', ':', '# TODO: Add Warning/Error messages in Logger.', 'print', '""No XtraProperty in mo:""', ',', 'self', '.', 'classId', ',', '"" key:""', ',', 'key', 'else', ':', '"""""" property does not exist """"""', 'if', 'self', '.', '__dict__', '[', ""'XtraProperty'"", ']', '.', 'has_key', '(', 'key', ')', ':', 'return', 'self', '.', '__dict__', '[', ""'XtraProperty'"", ']', '[', 'UcsUtils', '.', 'WordU', '(', 'key', ')', ']', 'elif', 'key', '==', '""Dn""', 'or', 'key', '==', '""Rn""', ':', 'return', 'None', 'else', ':', 'raise', 'AttributeError', '(', 'key', ')']",This method gets attribute value of a Managed Object.,"['This', 'method', 'gets', 'attribute', 'value', 'of', 'a', 'Managed', 'Object', '.']",python,E,3,True,1,train
7220,CiscoUcs/UcsPythonSDK,src/UcsSdk/UcsBase.py,https://github.com/CiscoUcs/UcsPythonSDK/blob/bf6b07d6abeacb922c92b198352eda4eb9e4629b/src/UcsSdk/UcsBase.py#L423-L430,"def getattr(self, key):
		"""""" This method gets the attribute value of external method object. """"""
		if key in _MethodFactoryMeta[self.classId]:
			"""""" property exists """"""
			return self.__dict__[key]
		else:
			"""""" property does not exist """"""
			return None","['def', 'getattr', '(', 'self', ',', 'key', ')', ':', 'if', 'key', 'in', '_MethodFactoryMeta', '[', 'self', '.', 'classId', ']', ':', '"""""" property exists """"""', 'return', 'self', '.', '__dict__', '[', 'key', ']', 'else', ':', '"""""" property does not exist """"""', 'return', 'None']",This method gets the attribute value of external method object.,"['This', 'method', 'gets', 'the', 'attribute', 'value', 'of', 'external', 'method', 'object', '.']",python,E,3,True,1,train
10783,LISE-B26/pylabcontrol,build/lib/pylabcontrol/src/core/read_write_functions.py,https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/src/core/read_write_functions.py#L41-L74,"def get_config_value(name, path_to_file='config.txt'):
    """"""
    gets the value for ""name"" from ""path_to_file"" config file
    Args:
        name: name of varibale in config file
        path_to_file: path to config file

    Returns: path to dll if name exists in the file; otherwise, returns None

    """"""

    # if the function is called from gui then the file has to be located with respect to the gui folder
    if not os.path.isfile(path_to_file):
        path_to_file = os.path.join('../instruments/', path_to_file)

    path_to_file = os.path.abspath(path_to_file)

    if not os.path.isfile(path_to_file):
        print(('path_to_file', path_to_file))
        #raise IOError('{:s}: config file is not valid'.format(path_to_file))
        return None

    f = open(path_to_file, 'r')
    string_of_file_contents = f.read()

    if name[-1] is not ':':
        name += ':'

    if name not in string_of_file_contents:
        return None
    else:
        config_value = [line.split(name)[1] for line in string_of_file_contents.split('\n')
                        if len(line.split(name)) > 1][0].strip()
        return config_value","['def', 'get_config_value', '(', 'name', ',', 'path_to_file', '=', ""'config.txt'"", ')', ':', '# if the function is called from gui then the file has to be located with respect to the gui folder', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'path_to_file', ')', ':', 'path_to_file', '=', 'os', '.', 'path', '.', 'join', '(', ""'../instruments/'"", ',', 'path_to_file', ')', 'path_to_file', '=', 'os', '.', 'path', '.', 'abspath', '(', 'path_to_file', ')', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'path_to_file', ')', ':', 'print', '(', '(', ""'path_to_file'"", ',', 'path_to_file', ')', ')', ""#raise IOError('{:s}: config file is not valid'.format(path_to_file))"", 'return', 'None', 'f', '=', 'open', '(', 'path_to_file', ',', ""'r'"", ')', 'string_of_file_contents', '=', 'f', '.', 'read', '(', ')', 'if', 'name', '[', '-', '1', ']', 'is', 'not', ""':'"", ':', 'name', '+=', ""':'"", 'if', 'name', 'not', 'in', 'string_of_file_contents', ':', 'return', 'None', 'else', ':', 'config_value', '=', '[', 'line', '.', 'split', '(', 'name', ')', '[', '1', ']', 'for', 'line', 'in', 'string_of_file_contents', '.', 'split', '(', ""'\\n'"", ')', 'if', 'len', '(', 'line', '.', 'split', '(', 'name', ')', ')', '>', '1', ']', '[', '0', ']', '.', 'strip', '(', ')', 'return', 'config_value']","gets the value for ""name"" from ""path_to_file"" config file
    Args:
        name: name of varibale in config file
        path_to_file: path to config file

    Returns: path to dll if name exists in the file; otherwise, returns None","['gets', 'the', 'value', 'for', 'name', 'from', 'path_to_file', 'config', 'file', 'Args', ':', 'name', ':', 'name', 'of', 'varibale', 'in', 'config', 'file', 'path_to_file', ':', 'path', 'to', 'config', 'file']",python,E,3,True,1,train
14110,pmacosta/pexdoc,pexdoc/pinspect.py,https://github.com/pmacosta/pexdoc/blob/201ac243e5781347feb75896a4231429fe6da4b1/pexdoc/pinspect.py#L611-L695,"def trace(self, fnames, _refresh=False):
        r""""""
        Generate a list of module callables and gets their attributes.

        Callables are functions, classes, methods and class properties; their
        attributes are callable type, file name, and lines span

        :param fnames: File names of the modules to trace
        :type  fnames: list

        :raises:
         * OSError (File *[fname]* could not be found)

         * RuntimeError (Argument \`fnames\` is not valid)
        """"""
        # pylint: disable=R0101
        if fnames and (not isinstance(fnames, list)):
            raise RuntimeError(""Argument `fnames` is not valid"")
        if fnames and any([not isinstance(item, str) for item in fnames]):
            raise RuntimeError(""Argument `fnames` is not valid"")
        for fname in fnames:
            if not os.path.exists(fname):
                raise OSError(""File {0} could not be found"".format(fname))
        fnames = [item.replace("".pyc"", "".py"") for item in fnames]
        bobj = collections.namedtuple(""Bundle"", [""lineno"", ""col_offset""])
        for fname in fnames:
            if (fname not in self._fnames) or (
                _refresh
                and (fname in self._fnames)
                and (self._fnames[fname][""date""] < os.path.getmtime(fname))
            ):
                module_name = (
                    _get_module_name_from_fname(fname)
                    if not _refresh
                    else self._fnames[fname][""name""]
                )
                # Remove old module information if it is going to be refreshed
                if _refresh:
                    self._module_names.pop(self._module_names.index(module_name))
                    for cls in self._fnames[fname][""classes""]:
                        self._class_names.pop(self._class_names.index(cls))
                    dlist = []
                    for key, value in self._reverse_callables_db.items():
                        if key[0] == fname:
                            dlist.append(key)
                            try:
                                del self._callables_db[value]
                            except KeyError:
                                pass
                    for item in set(dlist):
                        del self._reverse_callables_db[item]
                lines = _readlines(fname)
                # Eliminate all Unicode characters till the first ASCII
                # character is found in first line of file, to deal with
                # Unicode-encoded source files
                for num, char in enumerate(lines[0]):  # pragma: no cover
                    if not _unicode_char(char):
                        break
                lines[0] = lines[0][num:]
                tree = ast.parse("""".join(lines))
                aobj = _AstTreeScanner(module_name, fname, lines)
                aobj.visit(tree)
                # Create a fake callable at the end of the file to properly
                # 'close', i.e. assign a last line number to the last
                # callable in file
                fake_node = bobj(len(lines) + 1, -1)
                aobj._close_callable(fake_node, force=True)
                self._class_names += aobj._class_names[:]
                self._module_names.append(module_name)
                self._callables_db.update(aobj._callables_db)
                self._reverse_callables_db.update(aobj._reverse_callables_db)
                # Split into modules
                self._modules_dict[module_name] = []
                iobj = [
                    item
                    for item in self._callables_db.values()
                    if item[""name""].startswith(module_name + ""."")
                ]
                for entry in iobj:
                    self._modules_dict[module_name].append(entry)
                self._fnames[fname] = {
                    ""name"": module_name,
                    ""date"": os.path.getmtime(fname),
                    ""classes"": aobj._class_names[:],
                }","['def', 'trace', '(', 'self', ',', 'fnames', ',', '_refresh', '=', 'False', ')', ':', '# pylint: disable=R0101', 'if', 'fnames', 'and', '(', 'not', 'isinstance', '(', 'fnames', ',', 'list', ')', ')', ':', 'raise', 'RuntimeError', '(', '""Argument `fnames` is not valid""', ')', 'if', 'fnames', 'and', 'any', '(', '[', 'not', 'isinstance', '(', 'item', ',', 'str', ')', 'for', 'item', 'in', 'fnames', ']', ')', ':', 'raise', 'RuntimeError', '(', '""Argument `fnames` is not valid""', ')', 'for', 'fname', 'in', 'fnames', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'fname', ')', ':', 'raise', 'OSError', '(', '""File {0} could not be found""', '.', 'format', '(', 'fname', ')', ')', 'fnames', '=', '[', 'item', '.', 'replace', '(', '"".pyc""', ',', '"".py""', ')', 'for', 'item', 'in', 'fnames', ']', 'bobj', '=', 'collections', '.', 'namedtuple', '(', '""Bundle""', ',', '[', '""lineno""', ',', '""col_offset""', ']', ')', 'for', 'fname', 'in', 'fnames', ':', 'if', '(', 'fname', 'not', 'in', 'self', '.', '_fnames', ')', 'or', '(', '_refresh', 'and', '(', 'fname', 'in', 'self', '.', '_fnames', ')', 'and', '(', 'self', '.', '_fnames', '[', 'fname', ']', '[', '""date""', ']', '<', 'os', '.', 'path', '.', 'getmtime', '(', 'fname', ')', ')', ')', ':', 'module_name', '=', '(', '_get_module_name_from_fname', '(', 'fname', ')', 'if', 'not', '_refresh', 'else', 'self', '.', '_fnames', '[', 'fname', ']', '[', '""name""', ']', ')', '# Remove old module information if it is going to be refreshed', 'if', '_refresh', ':', 'self', '.', '_module_names', '.', 'pop', '(', 'self', '.', '_module_names', '.', 'index', '(', 'module_name', ')', ')', 'for', 'cls', 'in', 'self', '.', '_fnames', '[', 'fname', ']', '[', '""classes""', ']', ':', 'self', '.', '_class_names', '.', 'pop', '(', 'self', '.', '_class_names', '.', 'index', '(', 'cls', ')', ')', 'dlist', '=', '[', ']', 'for', 'key', ',', 'value', 'in', 'self', '.', '_reverse_callables_db', '.', 'items', '(', ')', ':', 'if', 'key', '[', '0', ']', '==', 'fname', ':', 'dlist', '.', 'append', '(', 'key', ')', 'try', ':', 'del', 'self', '.', '_callables_db', '[', 'value', ']', 'except', 'KeyError', ':', 'pass', 'for', 'item', 'in', 'set', '(', 'dlist', ')', ':', 'del', 'self', '.', '_reverse_callables_db', '[', 'item', ']', 'lines', '=', '_readlines', '(', 'fname', ')', '# Eliminate all Unicode characters till the first ASCII', '# character is found in first line of file, to deal with', '# Unicode-encoded source files', 'for', 'num', ',', 'char', 'in', 'enumerate', '(', 'lines', '[', '0', ']', ')', ':', '# pragma: no cover', 'if', 'not', '_unicode_char', '(', 'char', ')', ':', 'break', 'lines', '[', '0', ']', '=', 'lines', '[', '0', ']', '[', 'num', ':', ']', 'tree', '=', 'ast', '.', 'parse', '(', '""""', '.', 'join', '(', 'lines', ')', ')', 'aobj', '=', '_AstTreeScanner', '(', 'module_name', ',', 'fname', ',', 'lines', ')', 'aobj', '.', 'visit', '(', 'tree', ')', '# Create a fake callable at the end of the file to properly', ""# 'close', i.e. assign a last line number to the last"", '# callable in file', 'fake_node', '=', 'bobj', '(', 'len', '(', 'lines', ')', '+', '1', ',', '-', '1', ')', 'aobj', '.', '_close_callable', '(', 'fake_node', ',', 'force', '=', 'True', ')', 'self', '.', '_class_names', '+=', 'aobj', '.', '_class_names', '[', ':', ']', 'self', '.', '_module_names', '.', 'append', '(', 'module_name', ')', 'self', '.', '_callables_db', '.', 'update', '(', 'aobj', '.', '_callables_db', ')', 'self', '.', '_reverse_callables_db', '.', 'update', '(', 'aobj', '.', '_reverse_callables_db', ')', '# Split into modules', 'self', '.', '_modules_dict', '[', 'module_name', ']', '=', '[', ']', 'iobj', '=', '[', 'item', 'for', 'item', 'in', 'self', '.', '_callables_db', '.', 'values', '(', ')', 'if', 'item', '[', '""name""', ']', '.', 'startswith', '(', 'module_name', '+', '"".""', ')', ']', 'for', 'entry', 'in', 'iobj', ':', 'self', '.', '_modules_dict', '[', 'module_name', ']', '.', 'append', '(', 'entry', ')', 'self', '.', '_fnames', '[', 'fname', ']', '=', '{', '""name""', ':', 'module_name', ',', '""date""', ':', 'os', '.', 'path', '.', 'getmtime', '(', 'fname', ')', ',', '""classes""', ':', 'aobj', '.', '_class_names', '[', ':', ']', ',', '}']","r""""""
        Generate a list of module callables and gets their attributes.

        Callables are functions, classes, methods and class properties; their
        attributes are callable type, file name, and lines span

        :param fnames: File names of the modules to trace
        :type  fnames: list

        :raises:
         * OSError (File *[fname]* could not be found)

         * RuntimeError (Argument \`fnames\` is not valid)","['r', 'Generate', 'a', 'list', 'of', 'module', 'callables', 'and', 'gets', 'their', 'attributes', '.']",python,E,3,True,1,train
15656,btotharye/mattermostwrapper,mattermostwrapper/wrapper.py,https://github.com/btotharye/mattermostwrapper/blob/d1eedee40f697246dd56caf6df233e77c48ddbb3/mattermostwrapper/wrapper.py#L69-L79,"def get_channel_listing(self):
        """"""
        This function takes in display_name of your team and gets the channel listing for that team.
        :param display_name:
        :return:
        """"""
        teams = self.get('/teams')
        for team in teams:
            if team['name'].lower() == self.team:
                channel_listing = self.get('/teams/' + team['id'] + '/channels')
                return channel_listing","['def', 'get_channel_listing', '(', 'self', ')', ':', 'teams', '=', 'self', '.', 'get', '(', ""'/teams'"", ')', 'for', 'team', 'in', 'teams', ':', 'if', 'team', '[', ""'name'"", ']', '.', 'lower', '(', ')', '==', 'self', '.', 'team', ':', 'channel_listing', '=', 'self', '.', 'get', '(', ""'/teams/'"", '+', 'team', '[', ""'id'"", ']', '+', ""'/channels'"", ')', 'return', 'channel_listing']","This function takes in display_name of your team and gets the channel listing for that team.
        :param display_name:
        :return:","['This', 'function', 'takes', 'in', 'display_name', 'of', 'your', 'team', 'and', 'gets', 'the', 'channel', 'listing', 'for', 'that', 'team', '.', ':', 'param', 'display_name', ':', ':', 'return', ':']",python,E,3,True,1,train
16267,Erotemic/utool,utool/util_arg.py,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_arg.py#L1068-L1127,"def get_argv_tail(scriptname, prefer_main=None, argv=None):
    r""""""
    gets the rest of the arguments after a script has been invoked hack.
    accounts for python -m scripts.

    Args:
        scriptname (str):

    CommandLine:
        python -m utool.util_arg --test-get_argv_tail

    Example:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_arg import *  # NOQA
        >>> import utool as ut
        >>> from os.path import relpath, dirname
        >>> scriptname = 'utool.util_arg'
        >>> prefer_main = False
        >>> argv=['python', '-m', 'utool.util_arg', '--test-get_argv_tail']
        >>> tail = get_argv_tail(scriptname, prefer_main, argv)
        >>> # hack
        >>> tail[0] = ut.ensure_unixslash(relpath(tail[0], dirname(dirname(ut.__file__))))
        >>> result = ut.repr2(tail)
        >>> print(result)
        ['utool/util_arg.py', '--test-get_argv_tail']

    Example:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_arg import *  # NOQA
        >>> import utool as ut
        >>> from os.path import relpath, dirname
        >>> scriptname = 'utprof.py'
        >>> prefer_main = True
        >>> argv=['utprof.py', '-m', 'utool', '--tf', 'get_argv_tail']
        >>> tail = get_argv_tail(scriptname, prefer_main, argv)
        >>> # hack
        >>> tail[0] = ut.ensure_unixslash(relpath(tail[0], dirname(dirname(ut.__file__))))
        >>> result = ut.repr2(tail)
        >>> print(result)
        ['utool/__main__.py', '--tf', 'get_argv_tail']
    """"""
    if argv is None:
        argv = sys.argv
    import utool as ut
    modname = ut.get_argval('-m', help_='specify module name to profile', argv=argv)
    if modname is not None:
        # hack to account for -m scripts
        modpath = ut.get_modpath(modname, prefer_main=prefer_main)
        argvx = argv.index(modname) + 1
        argv_tail = [modpath] + argv[argvx:]
    else:
        try:
            argvx = argv.index(scriptname)
        except ValueError:
            for argvx, arg in enumerate(argv):
                # HACK
                if scriptname in arg:
                    break
        argv_tail = argv[(argvx + 1):]
    return argv_tail","['def', 'get_argv_tail', '(', 'scriptname', ',', 'prefer_main', '=', 'None', ',', 'argv', '=', 'None', ')', ':', 'if', 'argv', 'is', 'None', ':', 'argv', '=', 'sys', '.', 'argv', 'import', 'utool', 'as', 'ut', 'modname', '=', 'ut', '.', 'get_argval', '(', ""'-m'"", ',', 'help_', '=', ""'specify module name to profile'"", ',', 'argv', '=', 'argv', ')', 'if', 'modname', 'is', 'not', 'None', ':', '# hack to account for -m scripts', 'modpath', '=', 'ut', '.', 'get_modpath', '(', 'modname', ',', 'prefer_main', '=', 'prefer_main', ')', 'argvx', '=', 'argv', '.', 'index', '(', 'modname', ')', '+', '1', 'argv_tail', '=', '[', 'modpath', ']', '+', 'argv', '[', 'argvx', ':', ']', 'else', ':', 'try', ':', 'argvx', '=', 'argv', '.', 'index', '(', 'scriptname', ')', 'except', 'ValueError', ':', 'for', 'argvx', ',', 'arg', 'in', 'enumerate', '(', 'argv', ')', ':', '# HACK', 'if', 'scriptname', 'in', 'arg', ':', 'break', 'argv_tail', '=', 'argv', '[', '(', 'argvx', '+', '1', ')', ':', ']', 'return', 'argv_tail']","r""""""
    gets the rest of the arguments after a script has been invoked hack.
    accounts for python -m scripts.

    Args:
        scriptname (str):

    CommandLine:
        python -m utool.util_arg --test-get_argv_tail

    Example:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_arg import *  # NOQA
        >>> import utool as ut
        >>> from os.path import relpath, dirname
        >>> scriptname = 'utool.util_arg'
        >>> prefer_main = False
        >>> argv=['python', '-m', 'utool.util_arg', '--test-get_argv_tail']
        >>> tail = get_argv_tail(scriptname, prefer_main, argv)
        >>> # hack
        >>> tail[0] = ut.ensure_unixslash(relpath(tail[0], dirname(dirname(ut.__file__))))
        >>> result = ut.repr2(tail)
        >>> print(result)
        ['utool/util_arg.py', '--test-get_argv_tail']

    Example:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_arg import *  # NOQA
        >>> import utool as ut
        >>> from os.path import relpath, dirname
        >>> scriptname = 'utprof.py'
        >>> prefer_main = True
        >>> argv=['utprof.py', '-m', 'utool', '--tf', 'get_argv_tail']
        >>> tail = get_argv_tail(scriptname, prefer_main, argv)
        >>> # hack
        >>> tail[0] = ut.ensure_unixslash(relpath(tail[0], dirname(dirname(ut.__file__))))
        >>> result = ut.repr2(tail)
        >>> print(result)
        ['utool/__main__.py', '--tf', 'get_argv_tail']","['r', 'gets', 'the', 'rest', 'of', 'the', 'arguments', 'after', 'a', 'script', 'has', 'been', 'invoked', 'hack', '.', 'accounts', 'for', 'python', '-', 'm', 'scripts', '.']",python,E,3,True,1,train
16352,glormph/msstitch,src/app/drivers/startup.py,https://github.com/glormph/msstitch/blob/ded7e5cbd813d7797dc9d42805778266e59ff042/src/app/drivers/startup.py#L47-L53,"def start_msstitch(exec_drivers, sysargs):
    """"""Passed all drivers of executable, checks which command is passed to
    the executable and then gets the options for a driver, parses them from
    command line and runs the driver""""""
    parser = populate_parser(exec_drivers)
    args = parser.parse_args(sysargs[1:])
    args.func(**vars(args))","['def', 'start_msstitch', '(', 'exec_drivers', ',', 'sysargs', ')', ':', 'parser', '=', 'populate_parser', '(', 'exec_drivers', ')', 'args', '=', 'parser', '.', 'parse_args', '(', 'sysargs', '[', '1', ':', ']', ')', 'args', '.', 'func', '(', '*', '*', 'vars', '(', 'args', ')', ')']","Passed all drivers of executable, checks which command is passed to
    the executable and then gets the options for a driver, parses them from
    command line and runs the driver","['Passed', 'all', 'drivers', 'of', 'executable', 'checks', 'which', 'command', 'is', 'passed', 'to', 'the', 'executable', 'and', 'then', 'gets', 'the', 'options', 'for', 'a', 'driver', 'parses', 'them', 'from', 'command', 'line', 'and', 'runs', 'the', 'driver']",python,E,3,True,1,train
16951,Erotemic/utool,utool/util_str.py,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_str.py#L1885-L1910,"def str_between(str_, startstr, endstr):
    r""""""
    gets substring between two sentianl strings

    Example:
        >>> # DISABLE_DOCTEST
        >>> from utool.util_str import *  # NOQA
        >>> import utool as ut
        >>> str_ = '\n        INSERT INTO vsone(\n'
        >>> startstr = 'INSERT'
        >>> endstr = '('
        >>> result = str_between(str_, startstr, endstr)
        >>> print(result)
    """"""
    if startstr is None:
        startpos = 0
    else:
        startpos = str_.find(startstr) + len(startstr)
    if endstr is None:
        endpos = None
    else:
        endpos = str_.find(endstr)
        if endpos == -1:
            endpos = None
    newstr = str_[startpos:endpos]
    return newstr","['def', 'str_between', '(', 'str_', ',', 'startstr', ',', 'endstr', ')', ':', 'if', 'startstr', 'is', 'None', ':', 'startpos', '=', '0', 'else', ':', 'startpos', '=', 'str_', '.', 'find', '(', 'startstr', ')', '+', 'len', '(', 'startstr', ')', 'if', 'endstr', 'is', 'None', ':', 'endpos', '=', 'None', 'else', ':', 'endpos', '=', 'str_', '.', 'find', '(', 'endstr', ')', 'if', 'endpos', '==', '-', '1', ':', 'endpos', '=', 'None', 'newstr', '=', 'str_', '[', 'startpos', ':', 'endpos', ']', 'return', 'newstr']","r""""""
    gets substring between two sentianl strings

    Example:
        >>> # DISABLE_DOCTEST
        >>> from utool.util_str import *  # NOQA
        >>> import utool as ut
        >>> str_ = '\n        INSERT INTO vsone(\n'
        >>> startstr = 'INSERT'
        >>> endstr = '('
        >>> result = str_between(str_, startstr, endstr)
        >>> print(result)","['r', 'gets', 'substring', 'between', 'two', 'sentianl', 'strings']",python,E,3,True,1,train
17031,glormph/msstitch,src/app/actions/mzidtsv/quant.py,https://github.com/glormph/msstitch/blob/ded7e5cbd813d7797dc9d42805778266e59ff042/src/app/actions/mzidtsv/quant.py#L5-L36,"def generate_psms_quanted(quantdb, tsvfn, isob_header, oldheader,
                          isobaric=False, precursor=False):
    """"""Takes dbfn and connects, gets quants for each line in tsvfn, sorts
    them in line by using keys in quantheader list.""""""
    allquants, sqlfields = quantdb.select_all_psm_quants(isobaric, precursor)
    quant = next(allquants)
    for rownr, psm in enumerate(readers.generate_tsv_psms(tsvfn, oldheader)):
        outpsm = {x: y for x, y in psm.items()}
        if precursor:
            pquant = quant[sqlfields['precursor']]
            if pquant is None:
                pquant = 'NA'
            outpsm.update({mzidtsvdata.HEADER_PRECURSOR_QUANT: str(pquant)})
        if isobaric:
            isoquants = {}
            while quant[0] == rownr:
                isoquants.update({quant[sqlfields['isochan']]:
                                  str(quant[sqlfields['isoquant']])})
                try:
                    quant = next(allquants)
                except StopIteration:
                    # last PSM, break from while loop or it is not yielded at all
                    break
            outpsm.update(get_quant_NAs(isoquants, isob_header))
        else:
            try:
                quant = next(allquants)
            except StopIteration:
                # last PSM, needs explicit yield/break or it will not be yielded
                yield outpsm
                break
        yield outpsm","['def', 'generate_psms_quanted', '(', 'quantdb', ',', 'tsvfn', ',', 'isob_header', ',', 'oldheader', ',', 'isobaric', '=', 'False', ',', 'precursor', '=', 'False', ')', ':', 'allquants', ',', 'sqlfields', '=', 'quantdb', '.', 'select_all_psm_quants', '(', 'isobaric', ',', 'precursor', ')', 'quant', '=', 'next', '(', 'allquants', ')', 'for', 'rownr', ',', 'psm', 'in', 'enumerate', '(', 'readers', '.', 'generate_tsv_psms', '(', 'tsvfn', ',', 'oldheader', ')', ')', ':', 'outpsm', '=', '{', 'x', ':', 'y', 'for', 'x', ',', 'y', 'in', 'psm', '.', 'items', '(', ')', '}', 'if', 'precursor', ':', 'pquant', '=', 'quant', '[', 'sqlfields', '[', ""'precursor'"", ']', ']', 'if', 'pquant', 'is', 'None', ':', 'pquant', '=', ""'NA'"", 'outpsm', '.', 'update', '(', '{', 'mzidtsvdata', '.', 'HEADER_PRECURSOR_QUANT', ':', 'str', '(', 'pquant', ')', '}', ')', 'if', 'isobaric', ':', 'isoquants', '=', '{', '}', 'while', 'quant', '[', '0', ']', '==', 'rownr', ':', 'isoquants', '.', 'update', '(', '{', 'quant', '[', 'sqlfields', '[', ""'isochan'"", ']', ']', ':', 'str', '(', 'quant', '[', 'sqlfields', '[', ""'isoquant'"", ']', ']', ')', '}', ')', 'try', ':', 'quant', '=', 'next', '(', 'allquants', ')', 'except', 'StopIteration', ':', '# last PSM, break from while loop or it is not yielded at all', 'break', 'outpsm', '.', 'update', '(', 'get_quant_NAs', '(', 'isoquants', ',', 'isob_header', ')', ')', 'else', ':', 'try', ':', 'quant', '=', 'next', '(', 'allquants', ')', 'except', 'StopIteration', ':', '# last PSM, needs explicit yield/break or it will not be yielded', 'yield', 'outpsm', 'break', 'yield', 'outpsm']","Takes dbfn and connects, gets quants for each line in tsvfn, sorts
    them in line by using keys in quantheader list.","['Takes', 'dbfn', 'and', 'connects', 'gets', 'quants', 'for', 'each', 'line', 'in', 'tsvfn', 'sorts', 'them', 'in', 'line', 'by', 'using', 'keys', 'in', 'quantheader', 'list', '.']",python,E,3,True,1,train
17298,developmentseed/sentinel-s3,sentinel_s3/crawler.py,https://github.com/developmentseed/sentinel-s3/blob/02bf2f9cb6aff527e492b39518a54f0b4613ddda/sentinel_s3/crawler.py#L24-L36,"def get_product_metadata_path(product_name):
    """""" gets a single products metadata """"""

    string_date = product_name.split('_')[-1]
    date = datetime.datetime.strptime(string_date, '%Y%m%dT%H%M%S')
    path = 'products/{0}/{1}/{2}/{3}'.format(date.year, date.month, date.day, product_name)

    return {
        product_name: {
            'metadata': '{0}/{1}'.format(path, 'metadata.xml'),
            'tiles': get_tile_metadata_path('{0}/{1}'.format(path, 'productInfo.json'))
        }
    }","['def', 'get_product_metadata_path', '(', 'product_name', ')', ':', 'string_date', '=', 'product_name', '.', 'split', '(', ""'_'"", ')', '[', '-', '1', ']', 'date', '=', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'string_date', ',', ""'%Y%m%dT%H%M%S'"", ')', 'path', '=', ""'products/{0}/{1}/{2}/{3}'"", '.', 'format', '(', 'date', '.', 'year', ',', 'date', '.', 'month', ',', 'date', '.', 'day', ',', 'product_name', ')', 'return', '{', 'product_name', ':', '{', ""'metadata'"", ':', ""'{0}/{1}'"", '.', 'format', '(', 'path', ',', ""'metadata.xml'"", ')', ',', ""'tiles'"", ':', 'get_tile_metadata_path', '(', ""'{0}/{1}'"", '.', 'format', '(', 'path', ',', ""'productInfo.json'"", ')', ')', '}', '}']",gets a single products metadata,"['gets', 'a', 'single', 'products', 'metadata']",python,E,3,True,1,train
19055,BernardFW/bernard,src/bernard/platforms/management.py,https://github.com/BernardFW/bernard/blob/9c55703e5ffe5717c9fa39793df59dbfa5b4c5ab/src/bernard/platforms/management.py#L190-L199,"def get_class(self, platform) -> Type[Platform]:
        """"""
        For a given platform name, gets the matching class
        """"""

        if platform in self._classes:
            return self._classes[platform]

        raise PlatformDoesNotExist('Platform ""{}"" is not in configuration'
                                   .format(platform))","['def', 'get_class', '(', 'self', ',', 'platform', ')', '->', 'Type', '[', 'Platform', ']', ':', 'if', 'platform', 'in', 'self', '.', '_classes', ':', 'return', 'self', '.', '_classes', '[', 'platform', ']', 'raise', 'PlatformDoesNotExist', '(', '\'Platform ""{}"" is not in configuration\'', '.', 'format', '(', 'platform', ')', ')']","For a given platform name, gets the matching class","['For', 'a', 'given', 'platform', 'name', 'gets', 'the', 'matching', 'class']",python,E,3,True,1,train
23906,aacanakin/glim,glim/core.py,https://github.com/aacanakin/glim/blob/71a20ac149a1292c0d6c1dc7414985ea51854f7a/glim/core.py#L34-L54,"def get(self, key):
        """"""
        Function deeply gets the key with ""."" notation

        Args
        ----
          key (string): A key with the ""."" notation.

        Returns
        -------
          reg (unknown type): Returns a dict or a primitive
            type.
        """"""
        try:
            layers = key.split('.')
            value = self.registrar
            for key in layers:
                value = value[key]
            return value
        except:
            return None","['def', 'get', '(', 'self', ',', 'key', ')', ':', 'try', ':', 'layers', '=', 'key', '.', 'split', '(', ""'.'"", ')', 'value', '=', 'self', '.', 'registrar', 'for', 'key', 'in', 'layers', ':', 'value', '=', 'value', '[', 'key', ']', 'return', 'value', 'except', ':', 'return', 'None']","Function deeply gets the key with ""."" notation

        Args
        ----
          key (string): A key with the ""."" notation.

        Returns
        -------
          reg (unknown type): Returns a dict or a primitive
            type.","['Function', 'deeply', 'gets', 'the', 'key', 'with', '.', 'notation']",python,E,3,True,1,train
24822,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L252-L265,"def get_all_clients(self, params=None):
        """"""
        Get all clients
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_clients_per_page,
            resource=CLIENTS,
            **{'params': params}
        )","['def', 'get_all_clients', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_clients_per_page', ',', 'resource', '=', 'CLIENTS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all clients
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'clients', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24825,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L331-L344,"def get_all_client_properties(self, params=None):
        """"""
        Get all contacts of client
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_client_properties_per_page,
            resource=CLIENT_PROPERTIES,
            **{'params': params}
        )","['def', 'get_all_client_properties', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_client_properties_per_page', ',', 'resource', '=', 'CLIENT_PROPERTIES', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all contacts of client
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'contacts', 'of', 'client', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24827,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L389-L402,"def get_all_client_tags(self, params=None):
        """"""
        Get all client tags
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_client_tags_per_page,
            resource=CLIENT_TAGS,
            **{'params': params}
        )","['def', 'get_all_client_tags', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_client_tags_per_page', ',', 'resource', '=', 'CLIENT_TAGS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all client tags
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'client', 'tags', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24829,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L454-L467,"def get_all_contacts_of_client(self, client_id):
        """"""
        Get all contacts of client
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param client_id: The id of the client
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_contacts_of_client_per_page,
            resource=CONTACTS,
            **{'client_id': client_id}
        )","['def', 'get_all_contacts_of_client', '(', 'self', ',', 'client_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_contacts_of_client_per_page', ',', 'resource', '=', 'CONTACTS', ',', '*', '*', '{', ""'client_id'"", ':', 'client_id', '}', ')']","Get all contacts of client
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param client_id: The id of the client
        :return: list","['Get', 'all', 'contacts', 'of', 'client', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24832,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L524-L539,"def get_all_suppliers(self, params=None):
        """"""
        Get all suppliers
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_suppliers_per_page,
            resource=SUPPLIERS,
            **{'params': params}
        )","['def', 'get_all_suppliers', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_suppliers_per_page', ',', 'resource', '=', 'SUPPLIERS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all suppliers
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'suppliers', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24835,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L596-L611,"def get_all_supplier_properties(self, params=None):
        """"""
        Get all supplier properties
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_supplier_properties_per_page,
            resource=SUPPLIER_PROPERTIES,
            **{'params': params}
        )","['def', 'get_all_supplier_properties', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_supplier_properties_per_page', ',', 'resource', '=', 'SUPPLIER_PROPERTIES', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all supplier properties
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'supplier', 'properties', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24837,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L653-L666,"def get_all_tags_of_supplier(self, supplier_id):
        """"""
        Get all supplier properties
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param supplier_id: the supplier id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_tags_of_supplier_per_page,
            resource=SUPPLIER_TAGS,
            **{'supplier_id': supplier_id}
        )","['def', 'get_all_tags_of_supplier', '(', 'self', ',', 'supplier_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_tags_of_supplier_per_page', ',', 'resource', '=', 'SUPPLIER_TAGS', ',', '*', '*', '{', ""'supplier_id'"", ':', 'supplier_id', '}', ')']","Get all supplier properties
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param supplier_id: the supplier id
        :return: list","['Get', 'all', 'supplier', 'properties', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24839,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L712-L723,"def get_all_articles(self, params=None):
        """"""
        Get all articles
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_articles_per_page, resource=ARTICLES, **{'params': params})","['def', 'get_all_articles', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'self', '.', 'get_articles_per_page', ',', 'resource', '=', 'ARTICLES', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all articles
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'articles', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24842,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L780-L795,"def get_all_article_properties(self, params=None):
        """"""
        Get all article properties
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_article_properties_per_page,
            resource=ARTICLE_PROPERTIES,
            **{'params': params}
        )","['def', 'get_all_article_properties', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_article_properties_per_page', ',', 'resource', '=', 'ARTICLE_PROPERTIES', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all article properties
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'article', 'properties', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24844,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L837-L850,"def get_all_tags_of_article(self, article_id):
        """"""
        Get all tags of article
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param article_id: the article id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_tags_of_article_per_page,
            resource=ARTICLE_TAGS,
            **{'article_id': article_id}
        )","['def', 'get_all_tags_of_article', '(', 'self', ',', 'article_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_tags_of_article_per_page', ',', 'resource', '=', 'ARTICLE_TAGS', ',', '*', '*', '{', ""'article_id'"", ':', 'article_id', '}', ')']","Get all tags of article
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param article_id: the article id
        :return: list","['Get', 'all', 'tags', 'of', 'article', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24846,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L897-L908,"def get_all_units(self, params=None):
        """"""
        Get all units
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_units_per_page, resource=UNITS, **{'params': params})","['def', 'get_all_units', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'self', '.', 'get_units_per_page', ',', 'resource', '=', 'UNITS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all units
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'units', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24849,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L965-L976,"def get_all_invoices(self, params=None):
        """"""
        Get all invoices
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_invoices_per_page, resource=INVOICES, **{'params': params})","['def', 'get_all_invoices', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'self', '.', 'get_invoices_per_page', ',', 'resource', '=', 'INVOICES', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all invoices
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'invoices', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24858,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1120-L1133,"def get_all_items_of_invoice(self, invoice_id):
        """"""
        Get all items of invoice
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param invoice_id: the invoice id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_items_of_invoice_per_page,
            resource=INVOICE_ITEMS,
            **{'invoice_id': invoice_id}
        )","['def', 'get_all_items_of_invoice', '(', 'self', ',', 'invoice_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_items_of_invoice_per_page', ',', 'resource', '=', 'INVOICE_ITEMS', ',', '*', '*', '{', ""'invoice_id'"", ':', 'invoice_id', '}', ')']","Get all items of invoice
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param invoice_id: the invoice id
        :return: list","['Get', 'all', 'items', 'of', 'invoice', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24861,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1199-L1212,"def get_all_comments_of_invoice(self, invoice_id):
        """"""
        Get all invoice comments of invoice
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param invoice_id: the invoice id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_comments_of_invoice_per_page,
            resource=INVOICE_COMMENTS,
            **{'invoice_id': invoice_id}
        )","['def', 'get_all_comments_of_invoice', '(', 'self', ',', 'invoice_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_comments_of_invoice_per_page', ',', 'resource', '=', 'INVOICE_COMMENTS', ',', '*', '*', '{', ""'invoice_id'"", ':', 'invoice_id', '}', ')']","Get all invoice comments of invoice
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param invoice_id: the invoice id
        :return: list","['Get', 'all', 'invoice', 'comments', 'of', 'invoice', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24864,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1281-L1296,"def get_all_invoice_payments(self, params=None):
        """"""
        Get all invoice payments
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(
            get_function=self.get_invoice_payments_per_page,
            resource=INVOICE_PAYMENTS,
            **{'params': params}
        )","['def', 'get_all_invoice_payments', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_invoice_payments_per_page', ',', 'resource', '=', 'INVOICE_PAYMENTS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all invoice payments
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'invoice', 'payments', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24866,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1347-L1360,"def get_all_tags_of_invoice(self, invoice_id):
        """"""
        Get all tags of invoice
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param invoice_id: the invoice id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_tags_of_invoice_per_page,
            resource=INVOICE_TAGS,
            **{'invoice_id': invoice_id}
        )","['def', 'get_all_tags_of_invoice', '(', 'self', ',', 'invoice_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_tags_of_invoice_per_page', ',', 'resource', '=', 'INVOICE_TAGS', ',', '*', '*', '{', ""'invoice_id'"", ':', 'invoice_id', '}', ')']","Get all tags of invoice
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param invoice_id: the invoice id
        :return: list","['Get', 'all', 'tags', 'of', 'invoice', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24868,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1407-L1418,"def get_all_recurrings(self, params=None):
        """"""
        Get all recurrings
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_recurrings_per_page, resource=RECURRINGS, **{'params': params})","['def', 'get_all_recurrings', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'self', '.', 'get_recurrings_per_page', ',', 'resource', '=', 'RECURRINGS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all recurrings
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'recurrings', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24871,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1480-L1493,"def get_all_items_of_recurring(self, recurring_id):
        """"""
        Get all items of recurring
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param recurring_id: the recurring id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_items_of_recurring_per_page,
            resource=RECURRING_ITEMS,
            **{'recurring_id': recurring_id}
        )","['def', 'get_all_items_of_recurring', '(', 'self', ',', 'recurring_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_items_of_recurring_per_page', ',', 'resource', '=', 'RECURRING_ITEMS', ',', '*', '*', '{', ""'recurring_id'"", ':', 'recurring_id', '}', ')']","Get all items of recurring
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param recurring_id: the recurring id
        :return: list","['Get', 'all', 'items', 'of', 'recurring', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24874,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1558-L1571,"def get_all_tags_of_recurring(self, recurring_id):
        """"""
        Get all tags of recurring
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param recurring_id: the recurring id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_tags_of_recurring_per_page,
            resource=RECURRING_TAGS,
            **{'recurring_id': recurring_id}
        )","['def', 'get_all_tags_of_recurring', '(', 'self', ',', 'recurring_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_tags_of_recurring_per_page', ',', 'resource', '=', 'RECURRING_TAGS', ',', '*', '*', '{', ""'recurring_id'"", ':', 'recurring_id', '}', ')']","Get all tags of recurring
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param recurring_id: the recurring id
        :return: list","['Get', 'all', 'tags', 'of', 'recurring', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24876,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1622-L1635,"def get_all_email_receivers_of_recurring(self, recurring_id):
        """"""
        Get all email receivers of recurring
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param recurring_id: the recurring id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_email_receivers_of_recurring_per_page,
            resource=RECURRING_EMAIL_RECEIVERS,
            **{'recurring_id': recurring_id}
        )","['def', 'get_all_email_receivers_of_recurring', '(', 'self', ',', 'recurring_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_email_receivers_of_recurring_per_page', ',', 'resource', '=', 'RECURRING_EMAIL_RECEIVERS', ',', '*', '*', '{', ""'recurring_id'"", ':', 'recurring_id', '}', ')']","Get all email receivers of recurring
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param recurring_id: the recurring id
        :return: list","['Get', 'all', 'email', 'receivers', 'of', 'recurring', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24878,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1682-L1693,"def get_all_incomings(self, params=None):
        """"""
        Get all incomings
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list
        """"""
        if not params:
            params = {}
        return self._iterate_through_pages(self.get_incomings_per_page, resource=INCOMINGS, **{'params': params})","['def', 'get_all_incomings', '(', 'self', ',', 'params', '=', 'None', ')', ':', 'if', 'not', 'params', ':', 'params', '=', '{', '}', 'return', 'self', '.', '_iterate_through_pages', '(', 'self', '.', 'get_incomings_per_page', ',', 'resource', '=', 'INCOMINGS', ',', '*', '*', '{', ""'params'"", ':', 'params', '}', ')']","Get all incomings
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param params: search params
        :return: list","['Get', 'all', 'incomings', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
24881,bykof/billomapy,billomapy/billomapy.py,https://github.com/bykof/billomapy/blob/a28ba69fd37654fa145d0411d52c200e7f8984ab/billomapy/billomapy.py#L1755-L1768,"def get_all_comments_of_incoming(self, incoming_id):
        """"""
        Get all comments of incoming
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param incoming_id: the incoming id
        :return: list
        """"""
        return self._iterate_through_pages(
            get_function=self.get_comments_of_incoming_per_page,
            resource=INCOMING_COMMENTS,
            **{'incoming_id': incoming_id}
        )","['def', 'get_all_comments_of_incoming', '(', 'self', ',', 'incoming_id', ')', ':', 'return', 'self', '.', '_iterate_through_pages', '(', 'get_function', '=', 'self', '.', 'get_comments_of_incoming_per_page', ',', 'resource', '=', 'INCOMING_COMMENTS', ',', '*', '*', '{', ""'incoming_id'"", ':', 'incoming_id', '}', ')']","Get all comments of incoming
        This will iterate over all pages until it gets all elements.
        So if the rate limit exceeded it will throw an Exception and you will get nothing

        :param incoming_id: the incoming id
        :return: list","['Get', 'all', 'comments', 'of', 'incoming', 'This', 'will', 'iterate', 'over', 'all', 'pages', 'until', 'it', 'gets', 'all', 'elements', '.', 'So', 'if', 'the', 'rate', 'limit', 'exceeded', 'it', 'will', 'throw', 'an', 'Exception', 'and', 'you', 'will', 'get', 'nothing']",python,E,3,True,1,train
25738,praekeltfoundation/seed-message-sender,message_sender/factory.py,https://github.com/praekeltfoundation/seed-message-sender/blob/257b01635171b9dbe1f5f13baa810c971bb2620e/message_sender/factory.py#L72-L85,"def _get_filename(self, path):
        """"""
        This function gets the base filename from the path, if a language code
        is present the filename will start from there.
        """"""
        match = re.search(""[a-z]{2,3}_[A-Z]{2}"", path)

        if match:
            start = match.start(0)
            filename = path[start:]
        else:
            filename = os.path.basename(path)

        return filename","['def', '_get_filename', '(', 'self', ',', 'path', ')', ':', 'match', '=', 're', '.', 'search', '(', '""[a-z]{2,3}_[A-Z]{2}""', ',', 'path', ')', 'if', 'match', ':', 'start', '=', 'match', '.', 'start', '(', '0', ')', 'filename', '=', 'path', '[', 'start', ':', ']', 'else', ':', 'filename', '=', 'os', '.', 'path', '.', 'basename', '(', 'path', ')', 'return', 'filename']","This function gets the base filename from the path, if a language code
        is present the filename will start from there.","['This', 'function', 'gets', 'the', 'base', 'filename', 'from', 'the', 'path', 'if', 'a', 'language', 'code', 'is', 'present', 'the', 'filename', 'will', 'start', 'from', 'there', '.']",python,E,3,True,1,train
26737,rraadd88/rohan,rohan/dandage/align/align_annot.py,https://github.com/rraadd88/rohan/blob/b0643a3582a2fffc0165ace69fb80880d92bfb10/rohan/dandage/align/align_annot.py#L25-L72,"def dqueries2queriessam(cfg,dqueries):    
    """"""
    Aligns queries to genome and gets SAM file
    step#1

    :param cfg: configuration dict
    :param dqueries: dataframe of queries
    """"""
    datatmpd=cfg['datatmpd']
    dqueries=set_index(dqueries,'query id')
    queryls=dqueries.loc[:,'query sequence'].apply(len).unique()
    for queryl in queryls:
        logging.debug(f""now aligning queries of length {queryl}"")
        queriesfap = f'{datatmpd}/01_queries_queryl{queryl:02}.fa'
        logging.info(basename(queriesfap))
        if not exists(queriesfap) or cfg['force']:
            with open(queriesfap,'w') as f:
                for gi in dqueries.index:
                    f.write('>{}\n{}\n'.format(gi.replace(' ','_'),dqueries.loc[gi,'query sequence']))
        ## BWA alignment command is adapted from cripror 
        ## https://github.com/rraadd88/crisporWebsite/blob/master/crispor.py
        # BWA allow up to X mismatches
        # maximum number of occurences in the genome to get flagged as repeats. 
        # This is used in bwa samse, when converting the sam file
        # and for warnings in the table output.
        MAXOCC = 60000

        # the BWA queue size is 2M by default. We derive the queue size from MAXOCC
        MFAC = 2000000/MAXOCC

        genomep=cfg['genomep']
        genomed = dirname(genomep) # make var local, see below
        genomegffp=cfg['genomegffp']

        # increase MAXOCC if there is only a single query, but only in CGI mode
        bwaM = MFAC*MAXOCC # -m is queue size in bwa
        queriessap = f'{datatmpd}/01_queries_queryl{queryl:02}.sa'
        logging.info(basename(queriessap))
        if not exists(queriessap) or cfg['force']:
            cmd=f""{cfg['bwa']} aln -t 1 -o 0 -m {bwaM} -n {cfg['mismatches_max']} -k {cfg['mismatches_max']} -N -l {queryl} {genomep} {queriesfap} > {queriessap} 2> {queriessap}.log""
            runbashcmd(cmd)

        queriessamp = f'{datatmpd}/01_queries_queryl{queryl:02}.sam'
        logging.info(basename(queriessamp))        
        if not exists(queriessamp) or cfg['force']:
            cmd=f""{cfg['bwa']} samse -n {MAXOCC} {genomep} {queriessap} {queriesfap} > {queriessamp} 2> {queriessamp}.log""
            runbashcmd(cmd)
    return cfg","['def', 'dqueries2queriessam', '(', 'cfg', ',', 'dqueries', ')', ':', 'datatmpd', '=', 'cfg', '[', ""'datatmpd'"", ']', 'dqueries', '=', 'set_index', '(', 'dqueries', ',', ""'query id'"", ')', 'queryls', '=', 'dqueries', '.', 'loc', '[', ':', ',', ""'query sequence'"", ']', '.', 'apply', '(', 'len', ')', '.', 'unique', '(', ')', 'for', 'queryl', 'in', 'queryls', ':', 'logging', '.', 'debug', '(', 'f""now aligning queries of length {queryl}""', ')', 'queriesfap', '=', ""f'{datatmpd}/01_queries_queryl{queryl:02}.fa'"", 'logging', '.', 'info', '(', 'basename', '(', 'queriesfap', ')', ')', 'if', 'not', 'exists', '(', 'queriesfap', ')', 'or', 'cfg', '[', ""'force'"", ']', ':', 'with', 'open', '(', 'queriesfap', ',', ""'w'"", ')', 'as', 'f', ':', 'for', 'gi', 'in', 'dqueries', '.', 'index', ':', 'f', '.', 'write', '(', ""'>{}\\n{}\\n'"", '.', 'format', '(', 'gi', '.', 'replace', '(', ""' '"", ',', ""'_'"", ')', ',', 'dqueries', '.', 'loc', '[', 'gi', ',', ""'query sequence'"", ']', ')', ')', '## BWA alignment command is adapted from cripror ', '## https://github.com/rraadd88/crisporWebsite/blob/master/crispor.py', '# BWA allow up to X mismatches', '# maximum number of occurences in the genome to get flagged as repeats. ', '# This is used in bwa samse, when converting the sam file', '# and for warnings in the table output.', 'MAXOCC', '=', '60000', '# the BWA queue size is 2M by default. We derive the queue size from MAXOCC', 'MFAC', '=', '2000000', '/', 'MAXOCC', 'genomep', '=', 'cfg', '[', ""'genomep'"", ']', 'genomed', '=', 'dirname', '(', 'genomep', ')', '# make var local, see below', 'genomegffp', '=', 'cfg', '[', ""'genomegffp'"", ']', '# increase MAXOCC if there is only a single query, but only in CGI mode', 'bwaM', '=', 'MFAC', '*', 'MAXOCC', '# -m is queue size in bwa', 'queriessap', '=', ""f'{datatmpd}/01_queries_queryl{queryl:02}.sa'"", 'logging', '.', 'info', '(', 'basename', '(', 'queriessap', ')', ')', 'if', 'not', 'exists', '(', 'queriessap', ')', 'or', 'cfg', '[', ""'force'"", ']', ':', 'cmd', '=', 'f""{cfg[\'bwa\']} aln -t 1 -o 0 -m {bwaM} -n {cfg[\'mismatches_max\']} -k {cfg[\'mismatches_max\']} -N -l {queryl} {genomep} {queriesfap} > {queriessap} 2> {queriessap}.log""', 'runbashcmd', '(', 'cmd', ')', 'queriessamp', '=', ""f'{datatmpd}/01_queries_queryl{queryl:02}.sam'"", 'logging', '.', 'info', '(', 'basename', '(', 'queriessamp', ')', ')', 'if', 'not', 'exists', '(', 'queriessamp', ')', 'or', 'cfg', '[', ""'force'"", ']', ':', 'cmd', '=', 'f""{cfg[\'bwa\']} samse -n {MAXOCC} {genomep} {queriessap} {queriesfap} > {queriessamp} 2> {queriessamp}.log""', 'runbashcmd', '(', 'cmd', ')', 'return', 'cfg']","Aligns queries to genome and gets SAM file
    step#1

    :param cfg: configuration dict
    :param dqueries: dataframe of queries","['Aligns', 'queries', 'to', 'genome', 'and', 'gets', 'SAM', 'file', 'step#1']",python,E,3,True,1,train
28214,sirfoga/pyhal,hal/internet/parser.py,https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/internet/parser.py#L22-L42,"def _get_row_tag(row, tag):
        """"""Parses row and gets columns matching tag

        :param row: HTML row
        :param tag: tag to get
        :return: list of labels in row
        """"""

        is_empty = True
        data = []
        for column_label in row.find_all(tag):  # cycle through all labels
            data.append(
                String(column_label.text).strip_bad_html()
            )
            if data[-1]:
                is_empty = False

        if not is_empty:
            return data

        return None","['def', '_get_row_tag', '(', 'row', ',', 'tag', ')', ':', 'is_empty', '=', 'True', 'data', '=', '[', ']', 'for', 'column_label', 'in', 'row', '.', 'find_all', '(', 'tag', ')', ':', '# cycle through all labels', 'data', '.', 'append', '(', 'String', '(', 'column_label', '.', 'text', ')', '.', 'strip_bad_html', '(', ')', ')', 'if', 'data', '[', '-', '1', ']', ':', 'is_empty', '=', 'False', 'if', 'not', 'is_empty', ':', 'return', 'data', 'return', 'None']","Parses row and gets columns matching tag

        :param row: HTML row
        :param tag: tag to get
        :return: list of labels in row","['Parses', 'row', 'and', 'gets', 'columns', 'matching', 'tag']",python,E,3,True,1,train
28627,sirfoga/pyhal,hal/streams/user.py,https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/streams/user.py#L79-L87,"def get_answer(self, question):
        """"""Asks user a question, then gets user answer

        :param question: Question: to ask user
        :return: User answer
        """"""
        self.last_question = str(question).strip()
        user_answer = input(self.last_question)
        return user_answer.strip()","['def', 'get_answer', '(', 'self', ',', 'question', ')', ':', 'self', '.', 'last_question', '=', 'str', '(', 'question', ')', '.', 'strip', '(', ')', 'user_answer', '=', 'input', '(', 'self', '.', 'last_question', ')', 'return', 'user_answer', '.', 'strip', '(', ')']","Asks user a question, then gets user answer

        :param question: Question: to ask user
        :return: User answer","['Asks', 'user', 'a', 'question', 'then', 'gets', 'user', 'answer']",python,E,3,True,1,train
28629,sirfoga/pyhal,hal/streams/user.py,https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/streams/user.py#L116-L151,"def get_number(self, question, min_i=float(""-inf""), max_i=float(""inf""),
                   just_these=None):
        """"""Parses answer and gets number

        :param question: Question: to ask user
        :param min_i: min acceptable number
        :param max_i: max acceptable number
        :param just_these: Accept only these numbers
        :return: User answer
        """"""
        try:
            user_answer = self.get_answer(question)
            user_answer = float(user_answer)

            if min_i < user_answer < max_i:
                if just_these:
                    if user_answer in just_these:
                        return user_answer

                    exc = ""Number cannot be accepted. Just these: ""
                    exc += str(just_these)
                    raise Exception(exc)

                return user_answer

            exc = ""Number is not within limits. ""
            exc += ""Min is "" + str(min_i) + "". Max is "" + str(max_i) + """"
            raise Exception(exc)
        except Exception as exc:
            print(str(exc))
            return self.get_number(
                self.last_question,
                min_i=min_i,
                max_i=max_i,
                just_these=just_these
            )","['def', 'get_number', '(', 'self', ',', 'question', ',', 'min_i', '=', 'float', '(', '""-inf""', ')', ',', 'max_i', '=', 'float', '(', '""inf""', ')', ',', 'just_these', '=', 'None', ')', ':', 'try', ':', 'user_answer', '=', 'self', '.', 'get_answer', '(', 'question', ')', 'user_answer', '=', 'float', '(', 'user_answer', ')', 'if', 'min_i', '<', 'user_answer', '<', 'max_i', ':', 'if', 'just_these', ':', 'if', 'user_answer', 'in', 'just_these', ':', 'return', 'user_answer', 'exc', '=', '""Number cannot be accepted. Just these: ""', 'exc', '+=', 'str', '(', 'just_these', ')', 'raise', 'Exception', '(', 'exc', ')', 'return', 'user_answer', 'exc', '=', '""Number is not within limits. ""', 'exc', '+=', '""Min is ""', '+', 'str', '(', 'min_i', ')', '+', '"". Max is ""', '+', 'str', '(', 'max_i', ')', '+', '""""', 'raise', 'Exception', '(', 'exc', ')', 'except', 'Exception', 'as', 'exc', ':', 'print', '(', 'str', '(', 'exc', ')', ')', 'return', 'self', '.', 'get_number', '(', 'self', '.', 'last_question', ',', 'min_i', '=', 'min_i', ',', 'max_i', '=', 'max_i', ',', 'just_these', '=', 'just_these', ')']","Parses answer and gets number

        :param question: Question: to ask user
        :param min_i: min acceptable number
        :param max_i: max acceptable number
        :param just_these: Accept only these numbers
        :return: User answer","['Parses', 'answer', 'and', 'gets', 'number']",python,E,3,True,1,train
28630,sirfoga/pyhal,hal/streams/user.py,https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/streams/user.py#L153-L182,"def get_list(self, question,
                 splitter="","", at_least=0, at_most=float(""inf"")):
        """"""Parses answer and gets list

        :param question: Question: to ask user
        :param splitter: Split list elements with this char
        :param at_least: List must have at least this amount of elements
        :param at_most: List must have at most this amount of elements
        :return: User answer
        """"""
        try:
            user_answer = self.get_answer(question)  # ask question
            user_answer = user_answer.split(splitter)  # split items
            user_answer = [str(item).strip() for item in user_answer]  # strip

            if at_least < len(user_answer) < at_most:
                return user_answer

            exc = ""List is not correct. ""
            exc += ""There must be at least "" + str(at_least) + "" items, ""
            exc += ""and at most "" + str(at_most) + "". ""
            exc += ""Use '"" + str(splitter) + ""' to separate items""
            raise Exception(exc)
        except Exception as exc:
            print(str(exc))
            return self.get_list(
                self.last_question,
                at_least=at_least,
                at_most=at_most
            )","['def', 'get_list', '(', 'self', ',', 'question', ',', 'splitter', '=', '"",""', ',', 'at_least', '=', '0', ',', 'at_most', '=', 'float', '(', '""inf""', ')', ')', ':', 'try', ':', 'user_answer', '=', 'self', '.', 'get_answer', '(', 'question', ')', '# ask question', 'user_answer', '=', 'user_answer', '.', 'split', '(', 'splitter', ')', '# split items', 'user_answer', '=', '[', 'str', '(', 'item', ')', '.', 'strip', '(', ')', 'for', 'item', 'in', 'user_answer', ']', '# strip', 'if', 'at_least', '<', 'len', '(', 'user_answer', ')', '<', 'at_most', ':', 'return', 'user_answer', 'exc', '=', '""List is not correct. ""', 'exc', '+=', '""There must be at least ""', '+', 'str', '(', 'at_least', ')', '+', '"" items, ""', 'exc', '+=', '""and at most ""', '+', 'str', '(', 'at_most', ')', '+', '"". ""', 'exc', '+=', '""Use \'""', '+', 'str', '(', 'splitter', ')', '+', '""\' to separate items""', 'raise', 'Exception', '(', 'exc', ')', 'except', 'Exception', 'as', 'exc', ':', 'print', '(', 'str', '(', 'exc', ')', ')', 'return', 'self', '.', 'get_list', '(', 'self', '.', 'last_question', ',', 'at_least', '=', 'at_least', ',', 'at_most', '=', 'at_most', ')']","Parses answer and gets list

        :param question: Question: to ask user
        :param splitter: Split list elements with this char
        :param at_least: List must have at least this amount of elements
        :param at_most: List must have at most this amount of elements
        :return: User answer","['Parses', 'answer', 'and', 'gets', 'list']",python,E,3,True,1,train
383,helto4real/python-packages,smhi/smhi/smhi_lib.py,https://github.com/helto4real/python-packages/blob/8b65342eea34e370ea6fc5abdcb55e544c51fec5/smhi/smhi/smhi_lib.py#L197-L205,"def get_forecast_api(self, longitude: str, latitude: str) -> {}:
        """"""gets data from API""""""
        api_url = APIURL_TEMPLATE.format(longitude, latitude)

        response = urlopen(api_url)
        data = response.read().decode('utf-8')
        json_data = json.loads(data)

        return json_data","['def', 'get_forecast_api', '(', 'self', ',', 'longitude', ':', 'str', ',', 'latitude', ':', 'str', ')', '->', '{', '}', ':', 'api_url', '=', 'APIURL_TEMPLATE', '.', 'format', '(', 'longitude', ',', 'latitude', ')', 'response', '=', 'urlopen', '(', 'api_url', ')', 'data', '=', 'response', '.', 'read', '(', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'json_data', '=', 'json', '.', 'loads', '(', 'data', ')', 'return', 'json_data']",gets data from API,"['gets', 'data', 'from', 'API']",python,E,3,True,1,train
384,helto4real/python-packages,smhi/smhi/smhi_lib.py,https://github.com/helto4real/python-packages/blob/8b65342eea34e370ea6fc5abdcb55e544c51fec5/smhi/smhi/smhi_lib.py#L207-L222,"async def async_get_forecast_api(self, longitude: str,
                                     latitude: str) -> {}:
        """"""gets data from API asyncronious""""""
        api_url = APIURL_TEMPLATE.format(longitude, latitude)

        if self.session is None:
            self.session = aiohttp.ClientSession()

        async with self.session.get(api_url) as response:
            if response.status != 200:
                raise SmhiForecastException(
                    ""Failed to access weather API with status code {}"".format(
                        response.status)
                )
            data = await response.text()
            return json.loads(data)","['async', 'def', 'async_get_forecast_api', '(', 'self', ',', 'longitude', ':', 'str', ',', 'latitude', ':', 'str', ')', '->', '{', '}', ':', 'api_url', '=', 'APIURL_TEMPLATE', '.', 'format', '(', 'longitude', ',', 'latitude', ')', 'if', 'self', '.', 'session', 'is', 'None', ':', 'self', '.', 'session', '=', 'aiohttp', '.', 'ClientSession', '(', ')', 'async', 'with', 'self', '.', 'session', '.', 'get', '(', 'api_url', ')', 'as', 'response', ':', 'if', 'response', '.', 'status', '!=', '200', ':', 'raise', 'SmhiForecastException', '(', '""Failed to access weather API with status code {}""', '.', 'format', '(', 'response', '.', 'status', ')', ')', 'data', '=', 'await', 'response', '.', 'text', '(', ')', 'return', 'json', '.', 'loads', '(', 'data', ')']",gets data from API asyncronious,"['gets', 'data', 'from', 'API', 'asyncronious']",python,E,3,True,1,train
2925,AshleySetter/optoanalysis,optoanalysis/optoanalysis/optoanalysis.py,https://github.com/AshleySetter/optoanalysis/blob/9d390acc834d70024d47b574aea14189a5a5714e/optoanalysis/optoanalysis/optoanalysis.py#L3442-L3466,"def count_collisions(Collisions):
    """"""
    Counts the number of unique collisions and gets the collision index.

    Parameters
    ----------
    Collisions : array_like
        Array of booleans, containing true if during a collision event, false otherwise.

    Returns
    -------
    CollisionCount : int
        Number of unique collisions
    CollisionIndicies : list
        Indicies of collision occurance
    """"""
    CollisionCount = 0
    CollisionIndicies = []
    lastval = True
    for i, val in enumerate(Collisions):
        if val == True and lastval == False:
            CollisionIndicies.append(i)
            CollisionCount += 1
        lastval = val
    return CollisionCount, CollisionIndicies","['def', 'count_collisions', '(', 'Collisions', ')', ':', 'CollisionCount', '=', '0', 'CollisionIndicies', '=', '[', ']', 'lastval', '=', 'True', 'for', 'i', ',', 'val', 'in', 'enumerate', '(', 'Collisions', ')', ':', 'if', 'val', '==', 'True', 'and', 'lastval', '==', 'False', ':', 'CollisionIndicies', '.', 'append', '(', 'i', ')', 'CollisionCount', '+=', '1', 'lastval', '=', 'val', 'return', 'CollisionCount', ',', 'CollisionIndicies']","Counts the number of unique collisions and gets the collision index.

    Parameters
    ----------
    Collisions : array_like
        Array of booleans, containing true if during a collision event, false otherwise.

    Returns
    -------
    CollisionCount : int
        Number of unique collisions
    CollisionIndicies : list
        Indicies of collision occurance","['Counts', 'the', 'number', 'of', 'unique', 'collisions', 'and', 'gets', 'the', 'collision', 'index', '.']",python,E,3,True,1,train
3421,Equitable/trump,trump/tools/reprobj.py,https://github.com/Equitable/trump/blob/a2802692bc642fa32096374159eea7ceca2947b4/trump/tools/reprobj.py#L43-L51,"def process_result_value(self, value, dialect):
        """"""
        When SQLAlchemy gets the string representation from a ReprObjType
        column, it converts it to the python equivalent via exec.
        """"""
        if value is not None:
            cmd = ""value = {}"".format(value)
            exec(cmd)
        return value","['def', 'process_result_value', '(', 'self', ',', 'value', ',', 'dialect', ')', ':', 'if', 'value', 'is', 'not', 'None', ':', 'cmd', '=', '""value = {}""', '.', 'format', '(', 'value', ')', 'exec', '(', 'cmd', ')', 'return', 'value']","When SQLAlchemy gets the string representation from a ReprObjType
        column, it converts it to the python equivalent via exec.","['When', 'SQLAlchemy', 'gets', 'the', 'string', 'representation', 'from', 'a', 'ReprObjType', 'column', 'it', 'converts', 'it', 'to', 'the', 'python', 'equivalent', 'via', 'exec', '.']",python,E,3,True,1,train
7993,agrc/agrc.python,agrc/logging.py,https://github.com/agrc/agrc.python/blob/be427e919bd4cdd6f19524b7f7fe18882429c25b/agrc/logging.py#L61-L71,"def logError(self):
        """"""
        gets traceback info and logs it
        """"""
        # got from http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Error_handling_with_Python
        import traceback

        self.logMsg('ERROR!!!')
        errMsg = traceback.format_exc()
        self.logMsg(errMsg)
        return errMsg","['def', 'logError', '(', 'self', ')', ':', '# got from http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Error_handling_with_Python', 'import', 'traceback', 'self', '.', 'logMsg', '(', ""'ERROR!!!'"", ')', 'errMsg', '=', 'traceback', '.', 'format_exc', '(', ')', 'self', '.', 'logMsg', '(', 'errMsg', ')', 'return', 'errMsg']",gets traceback info and logs it,"['gets', 'traceback', 'info', 'and', 'logs', 'it']",python,E,3,True,1,train
8952,capless/kev,kev/backends/__init__.py,https://github.com/capless/kev/blob/902f4d5d89482b2eedcf9a396d57be7083357024/kev/backends/__init__.py#L52-L70,"def prep_doc(self, doc_obj):
        """"""
        This method Validates, gets the Python value, checks unique indexes, 
        gets the db value, and then returns the prepared doc dict object. 
        Useful for save and backup functions.
        @param doc_obj: 
        @return: 
        """"""
        doc = doc_obj._data.copy()
        for key, prop in list(doc_obj._base_properties.items()):
            prop.validate(doc.get(key), key)
            raw_value = prop.get_python_value(doc.get(key))
            if prop.unique:
                self.check_unique(doc_obj, key, raw_value)
            value = prop.get_db_value(raw_value)
            doc[key] = value

        doc['_doc_type'] = get_doc_type(doc_obj.__class__)
        return doc","['def', 'prep_doc', '(', 'self', ',', 'doc_obj', ')', ':', 'doc', '=', 'doc_obj', '.', '_data', '.', 'copy', '(', ')', 'for', 'key', ',', 'prop', 'in', 'list', '(', 'doc_obj', '.', '_base_properties', '.', 'items', '(', ')', ')', ':', 'prop', '.', 'validate', '(', 'doc', '.', 'get', '(', 'key', ')', ',', 'key', ')', 'raw_value', '=', 'prop', '.', 'get_python_value', '(', 'doc', '.', 'get', '(', 'key', ')', ')', 'if', 'prop', '.', 'unique', ':', 'self', '.', 'check_unique', '(', 'doc_obj', ',', 'key', ',', 'raw_value', ')', 'value', '=', 'prop', '.', 'get_db_value', '(', 'raw_value', ')', 'doc', '[', 'key', ']', '=', 'value', 'doc', '[', ""'_doc_type'"", ']', '=', 'get_doc_type', '(', 'doc_obj', '.', '__class__', ')', 'return', 'doc']","This method Validates, gets the Python value, checks unique indexes, 
        gets the db value, and then returns the prepared doc dict object. 
        Useful for save and backup functions.
        @param doc_obj: 
        @return:","['This', 'method', 'Validates', 'gets', 'the', 'Python', 'value', 'checks', 'unique', 'indexes', 'gets', 'the', 'db', 'value', 'and', 'then', 'returns', 'the', 'prepared', 'doc', 'dict', 'object', '.', 'Useful', 'for', 'save', 'and', 'backup', 'functions', '.']",python,E,3,True,1,train
9529,iclab/centinel,centinel/primitives/http.py,https://github.com/iclab/centinel/blob/9a25dcf30c6a1db3c046f7ccb8ab8873e455c1a4/centinel/primitives/http.py#L46-L98,"def _get_http_request(netloc, path=""/"", headers=None, ssl=False):
    """"""
    Actually gets the http. Moved this to it's own private method since
    it is called several times for following redirects

    :param host:
    :param path:
    :param headers:
    :param ssl:
    :return:
    """"""
    if ssl:
        port = 443
    else:
        port = 80

    host = netloc

    if len(netloc.split("":"")) == 2:
        host, port = netloc.split("":"")

    request = {""host"": host,
               ""port"": port,
               ""path"": path,
               ""ssl"": ssl,
               ""method"": ""GET""}
    if headers:
        request[""headers""] = headers

    response = {}

    try:
        conn = ICHTTPConnection(host=host, port=port, timeout=10)

        conn.request(path, headers, ssl, timeout=10)
        response[""status""] = conn.status
        response[""reason""] = conn.reason
        response[""headers""] = conn.headers
        body = conn.body

        try:
            response[""body""] = body.encode('utf-8')
        except UnicodeDecodeError:
            # if utf-8 fails to encode, just use base64
            response[""body.b64""] = body.encode('base64')

    except Exception as err:
        response[""failure""] = str(err)

    result = {""response"": response,
              ""request"": request}

    return result","['def', '_get_http_request', '(', 'netloc', ',', 'path', '=', '""/""', ',', 'headers', '=', 'None', ',', 'ssl', '=', 'False', ')', ':', 'if', 'ssl', ':', 'port', '=', '443', 'else', ':', 'port', '=', '80', 'host', '=', 'netloc', 'if', 'len', '(', 'netloc', '.', 'split', '(', '"":""', ')', ')', '==', '2', ':', 'host', ',', 'port', '=', 'netloc', '.', 'split', '(', '"":""', ')', 'request', '=', '{', '""host""', ':', 'host', ',', '""port""', ':', 'port', ',', '""path""', ':', 'path', ',', '""ssl""', ':', 'ssl', ',', '""method""', ':', '""GET""', '}', 'if', 'headers', ':', 'request', '[', '""headers""', ']', '=', 'headers', 'response', '=', '{', '}', 'try', ':', 'conn', '=', 'ICHTTPConnection', '(', 'host', '=', 'host', ',', 'port', '=', 'port', ',', 'timeout', '=', '10', ')', 'conn', '.', 'request', '(', 'path', ',', 'headers', ',', 'ssl', ',', 'timeout', '=', '10', ')', 'response', '[', '""status""', ']', '=', 'conn', '.', 'status', 'response', '[', '""reason""', ']', '=', 'conn', '.', 'reason', 'response', '[', '""headers""', ']', '=', 'conn', '.', 'headers', 'body', '=', 'conn', '.', 'body', 'try', ':', 'response', '[', '""body""', ']', '=', 'body', '.', 'encode', '(', ""'utf-8'"", ')', 'except', 'UnicodeDecodeError', ':', '# if utf-8 fails to encode, just use base64', 'response', '[', '""body.b64""', ']', '=', 'body', '.', 'encode', '(', ""'base64'"", ')', 'except', 'Exception', 'as', 'err', ':', 'response', '[', '""failure""', ']', '=', 'str', '(', 'err', ')', 'result', '=', '{', '""response""', ':', 'response', ',', '""request""', ':', 'request', '}', 'return', 'result']","Actually gets the http. Moved this to it's own private method since
    it is called several times for following redirects

    :param host:
    :param path:
    :param headers:
    :param ssl:
    :return:","['Actually', 'gets', 'the', 'http', '.', 'Moved', 'this', 'to', 'it', 's', 'own', 'private', 'method', 'since', 'it', 'is', 'called', 'several', 'times', 'for', 'following', 'redirects']",python,E,3,True,1,train
6595,bakwc/PySyncObj,pysyncobj/transport.py,https://github.com/bakwc/PySyncObj/blob/be3b0aaa932d5156f5df140c23c962430f51b7b8/pysyncobj/transport.py#L532-L549,"def send(self, node, message):
        """"""
        Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.

        :param node: target node
        :type node: Node
        :param message: message
        :param message: any
        :returns success
        :rtype bool
        """"""

        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:
            return False
        self._connections[node].send(message)
        if self._connections[node].state != CONNECTION_STATE.CONNECTED:
            return False
        return True","['def', 'send', '(', 'self', ',', 'node', ',', 'message', ')', ':', 'if', 'node', 'not', 'in', 'self', '.', '_connections', 'or', 'self', '.', '_connections', '[', 'node', ']', '.', 'state', '!=', 'CONNECTION_STATE', '.', 'CONNECTED', ':', 'return', 'False', 'self', '.', '_connections', '[', 'node', ']', '.', 'send', '(', 'message', ')', 'if', 'self', '.', '_connections', '[', 'node', ']', '.', 'state', '!=', 'CONNECTION_STATE', '.', 'CONNECTED', ':', 'return', 'False', 'return', 'True']","Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.

        :param node: target node
        :type node: Node
        :param message: message
        :param message: any
        :returns success
        :rtype bool","['Send', 'a', 'message', 'to', 'a', 'node', '.', 'Returns', 'False', 'if', 'the', 'connection', 'appears', 'to', 'be', 'dead', 'either', 'before', 'or', 'after', 'actually', 'trying', 'to', 'send', 'the', 'message', '.']",python,X,2,True,1,test
7355,pgjones/hypercorn,hypercorn/asgi/h11.py,https://github.com/pgjones/hypercorn/blob/ef93d741fe246846a127f9318b54505ac65f1ae7/hypercorn/asgi/h11.py#L123-L151,"async def asgi_send(self, message: dict) -> None:
        """"""Called by the ASGI instance to send a message.""""""
        if message[""type""] == ""http.response.start"" and self.state == ASGIHTTPState.REQUEST:
            self.response = message
        elif message[""type""] == ""http.response.body"" and self.state in {
            ASGIHTTPState.REQUEST,
            ASGIHTTPState.RESPONSE,
        }:
            if self.state == ASGIHTTPState.REQUEST:
                headers = build_and_validate_headers(self.response[""headers""])
                headers.extend(self.response_headers())
                await self.asend(
                    h11.Response(status_code=int(self.response[""status""]), headers=headers)
                )
                self.state = ASGIHTTPState.RESPONSE

            if (
                not suppress_body(self.scope[""method""], int(self.response[""status""]))
                and message.get(""body"", b"""") != b""""
            ):
                await self.asend(h11.Data(data=bytes(message[""body""])))

            if not message.get(""more_body"", False):
                if self.state != ASGIHTTPState.CLOSED:
                    await self.asend(h11.EndOfMessage())
                    await self.asgi_put({""type"": ""http.disconnect""})
                    self.state = ASGIHTTPState.CLOSED
        else:
            raise UnexpectedMessage(self.state, message[""type""])","['async', 'def', 'asgi_send', '(', 'self', ',', 'message', ':', 'dict', ')', '->', 'None', ':', 'if', 'message', '[', '""type""', ']', '==', '""http.response.start""', 'and', 'self', '.', 'state', '==', 'ASGIHTTPState', '.', 'REQUEST', ':', 'self', '.', 'response', '=', 'message', 'elif', 'message', '[', '""type""', ']', '==', '""http.response.body""', 'and', 'self', '.', 'state', 'in', '{', 'ASGIHTTPState', '.', 'REQUEST', ',', 'ASGIHTTPState', '.', 'RESPONSE', ',', '}', ':', 'if', 'self', '.', 'state', '==', 'ASGIHTTPState', '.', 'REQUEST', ':', 'headers', '=', 'build_and_validate_headers', '(', 'self', '.', 'response', '[', '""headers""', ']', ')', 'headers', '.', 'extend', '(', 'self', '.', 'response_headers', '(', ')', ')', 'await', 'self', '.', 'asend', '(', 'h11', '.', 'Response', '(', 'status_code', '=', 'int', '(', 'self', '.', 'response', '[', '""status""', ']', ')', ',', 'headers', '=', 'headers', ')', ')', 'self', '.', 'state', '=', 'ASGIHTTPState', '.', 'RESPONSE', 'if', '(', 'not', 'suppress_body', '(', 'self', '.', 'scope', '[', '""method""', ']', ',', 'int', '(', 'self', '.', 'response', '[', '""status""', ']', ')', ')', 'and', 'message', '.', 'get', '(', '""body""', ',', 'b""""', ')', '!=', 'b""""', ')', ':', 'await', 'self', '.', 'asend', '(', 'h11', '.', 'Data', '(', 'data', '=', 'bytes', '(', 'message', '[', '""body""', ']', ')', ')', ')', 'if', 'not', 'message', '.', 'get', '(', '""more_body""', ',', 'False', ')', ':', 'if', 'self', '.', 'state', '!=', 'ASGIHTTPState', '.', 'CLOSED', ':', 'await', 'self', '.', 'asend', '(', 'h11', '.', 'EndOfMessage', '(', ')', ')', 'await', 'self', '.', 'asgi_put', '(', '{', '""type""', ':', '""http.disconnect""', '}', ')', 'self', '.', 'state', '=', 'ASGIHTTPState', '.', 'CLOSED', 'else', ':', 'raise', 'UnexpectedMessage', '(', 'self', '.', 'state', ',', 'message', '[', '""type""', ']', ')']",Called by the ASGI instance to send a message.,"['Called', 'by', 'the', 'ASGI', 'instance', 'to', 'send', 'a', 'message', '.']",python,X,2,True,1,test
7356,pgjones/hypercorn,hypercorn/asgi/wsproto.py,https://github.com/pgjones/hypercorn/blob/ef93d741fe246846a127f9318b54505ac65f1ae7/hypercorn/asgi/wsproto.py#L108-L151,"async def asgi_send(self, message: dict) -> None:
        """"""Called by the ASGI instance to send a message.""""""
        if message[""type""] == ""websocket.accept"" and self.state == ASGIWebsocketState.HANDSHAKE:
            headers = build_and_validate_headers(message.get(""headers"", []))
            raise_if_subprotocol_present(headers)
            headers.extend(self.response_headers())
            await self.asend(
                AcceptConnection(
                    extensions=[PerMessageDeflate()],
                    extra_headers=headers,
                    subprotocol=message.get(""subprotocol""),
                )
            )
            self.state = ASGIWebsocketState.CONNECTED
            self.config.access_logger.access(
                self.scope, {""status"": 101, ""headers"": []}, time() - self.start_time
            )
        elif (
            message[""type""] == ""websocket.http.response.start""
            and self.state == ASGIWebsocketState.HANDSHAKE
        ):
            self.response = message
            self.config.access_logger.access(self.scope, self.response, time() - self.start_time)
        elif message[""type""] == ""websocket.http.response.body"" and self.state in {
            ASGIWebsocketState.HANDSHAKE,
            ASGIWebsocketState.RESPONSE,
        }:
            await self._asgi_send_rejection(message)
        elif message[""type""] == ""websocket.send"" and self.state == ASGIWebsocketState.CONNECTED:
            data: Union[bytes, str]
            if message.get(""bytes"") is not None:
                await self.asend(BytesMessage(data=bytes(message[""bytes""])))
            elif not isinstance(message[""text""], str):
                raise TypeError(f""{message['text']} should be a str"")
            else:
                await self.asend(TextMessage(data=message[""text""]))
        elif message[""type""] == ""websocket.close"" and self.state == ASGIWebsocketState.HANDSHAKE:
            await self.send_http_error(403)
            self.state = ASGIWebsocketState.HTTPCLOSED
        elif message[""type""] == ""websocket.close"":
            await self.asend(CloseConnection(code=int(message[""code""])))
            self.state = ASGIWebsocketState.CLOSED
        else:
            raise UnexpectedMessage(self.state, message[""type""])","['async', 'def', 'asgi_send', '(', 'self', ',', 'message', ':', 'dict', ')', '->', 'None', ':', 'if', 'message', '[', '""type""', ']', '==', '""websocket.accept""', 'and', 'self', '.', 'state', '==', 'ASGIWebsocketState', '.', 'HANDSHAKE', ':', 'headers', '=', 'build_and_validate_headers', '(', 'message', '.', 'get', '(', '""headers""', ',', '[', ']', ')', ')', 'raise_if_subprotocol_present', '(', 'headers', ')', 'headers', '.', 'extend', '(', 'self', '.', 'response_headers', '(', ')', ')', 'await', 'self', '.', 'asend', '(', 'AcceptConnection', '(', 'extensions', '=', '[', 'PerMessageDeflate', '(', ')', ']', ',', 'extra_headers', '=', 'headers', ',', 'subprotocol', '=', 'message', '.', 'get', '(', '""subprotocol""', ')', ',', ')', ')', 'self', '.', 'state', '=', 'ASGIWebsocketState', '.', 'CONNECTED', 'self', '.', 'config', '.', 'access_logger', '.', 'access', '(', 'self', '.', 'scope', ',', '{', '""status""', ':', '101', ',', '""headers""', ':', '[', ']', '}', ',', 'time', '(', ')', '-', 'self', '.', 'start_time', ')', 'elif', '(', 'message', '[', '""type""', ']', '==', '""websocket.http.response.start""', 'and', 'self', '.', 'state', '==', 'ASGIWebsocketState', '.', 'HANDSHAKE', ')', ':', 'self', '.', 'response', '=', 'message', 'self', '.', 'config', '.', 'access_logger', '.', 'access', '(', 'self', '.', 'scope', ',', 'self', '.', 'response', ',', 'time', '(', ')', '-', 'self', '.', 'start_time', ')', 'elif', 'message', '[', '""type""', ']', '==', '""websocket.http.response.body""', 'and', 'self', '.', 'state', 'in', '{', 'ASGIWebsocketState', '.', 'HANDSHAKE', ',', 'ASGIWebsocketState', '.', 'RESPONSE', ',', '}', ':', 'await', 'self', '.', '_asgi_send_rejection', '(', 'message', ')', 'elif', 'message', '[', '""type""', ']', '==', '""websocket.send""', 'and', 'self', '.', 'state', '==', 'ASGIWebsocketState', '.', 'CONNECTED', ':', 'data', ':', 'Union', '[', 'bytes', ',', 'str', ']', 'if', 'message', '.', 'get', '(', '""bytes""', ')', 'is', 'not', 'None', ':', 'await', 'self', '.', 'asend', '(', 'BytesMessage', '(', 'data', '=', 'bytes', '(', 'message', '[', '""bytes""', ']', ')', ')', ')', 'elif', 'not', 'isinstance', '(', 'message', '[', '""text""', ']', ',', 'str', ')', ':', 'raise', 'TypeError', '(', 'f""{message[\'text\']} should be a str""', ')', 'else', ':', 'await', 'self', '.', 'asend', '(', 'TextMessage', '(', 'data', '=', 'message', '[', '""text""', ']', ')', ')', 'elif', 'message', '[', '""type""', ']', '==', '""websocket.close""', 'and', 'self', '.', 'state', '==', 'ASGIWebsocketState', '.', 'HANDSHAKE', ':', 'await', 'self', '.', 'send_http_error', '(', '403', ')', 'self', '.', 'state', '=', 'ASGIWebsocketState', '.', 'HTTPCLOSED', 'elif', 'message', '[', '""type""', ']', '==', '""websocket.close""', ':', 'await', 'self', '.', 'asend', '(', 'CloseConnection', '(', 'code', '=', 'int', '(', 'message', '[', '""code""', ']', ')', ')', ')', 'self', '.', 'state', '=', 'ASGIWebsocketState', '.', 'CLOSED', 'else', ':', 'raise', 'UnexpectedMessage', '(', 'self', '.', 'state', ',', 'message', '[', '""type""', ']', ')']",Called by the ASGI instance to send a message.,"['Called', 'by', 'the', 'ASGI', 'instance', 'to', 'send', 'a', 'message', '.']",python,X,2,True,1,test
17319,trp07/messages,messages/api.py,https://github.com/trp07/messages/blob/7789ebc960335a59ea5d319fceed3dd349023648/messages/api.py#L29-L62,"def send(msg_type, send_async=False, *args, **kwargs):
    """"""
    Constructs a message class and sends the message.
    Defaults to sending synchronously.  Set send_async=True to send
    asynchronously.

    Args:
        :msg_type: (str) the type of message to send, i.e. 'Email'
        :send_async: (bool) default is False, set True to send asynchronously.
        :kwargs: (dict) keywords arguments that are required for the
            various message types.  See docstrings for each type.
            i.e. help(messages.Email), help(messages.Twilio), etc.

    Example:
        >>> kwargs = {
                  from_: 'me@here.com',
                  to: 'you@there.com',
                  auth: 'yourPassword',
                  subject: 'Email Subject',
                  body: 'Your message to send',
                  attachments: ['filepath1', 'filepath2'],
            }
        >>> messages.send('email', **kwargs)
        Message sent...
    """"""
    message = message_factory(msg_type, *args, **kwargs)

    try:
        if send_async:
            message.send_async()
        else:
            message.send()
    except MessageSendError as e:
        err_exit(""Unable to send message: "", e)","['def', 'send', '(', 'msg_type', ',', 'send_async', '=', 'False', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'message', '=', 'message_factory', '(', 'msg_type', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', 'try', ':', 'if', 'send_async', ':', 'message', '.', 'send_async', '(', ')', 'else', ':', 'message', '.', 'send', '(', ')', 'except', 'MessageSendError', 'as', 'e', ':', 'err_exit', '(', '""Unable to send message: ""', ',', 'e', ')']","Constructs a message class and sends the message.
    Defaults to sending synchronously.  Set send_async=True to send
    asynchronously.

    Args:
        :msg_type: (str) the type of message to send, i.e. 'Email'
        :send_async: (bool) default is False, set True to send asynchronously.
        :kwargs: (dict) keywords arguments that are required for the
            various message types.  See docstrings for each type.
            i.e. help(messages.Email), help(messages.Twilio), etc.

    Example:
        >>> kwargs = {
                  from_: 'me@here.com',
                  to: 'you@there.com',
                  auth: 'yourPassword',
                  subject: 'Email Subject',
                  body: 'Your message to send',
                  attachments: ['filepath1', 'filepath2'],
            }
        >>> messages.send('email', **kwargs)
        Message sent...","['Constructs', 'a', 'message', 'class', 'and', 'sends', 'the', 'message', '.', 'Defaults', 'to', 'sending', 'synchronously', '.', 'Set', 'send_async', '=', 'True', 'to', 'send', 'asynchronously', '.']",python,X,2,True,1,test
17328,trp07/messages,messages/_eventloop.py,https://github.com/trp07/messages/blob/7789ebc960335a59ea5d319fceed3dd349023648/messages/_eventloop.py#L11-L20,"def _send_coroutine():
    """"""
    Creates a running coroutine to receive message instances and send
    them in a futures executor.
    """"""
    with PoolExecutor() as executor:
        while True:
            msg = yield
            future = executor.submit(msg.send)
            future.add_done_callback(_exception_handler)","['def', '_send_coroutine', '(', ')', ':', 'with', 'PoolExecutor', '(', ')', 'as', 'executor', ':', 'while', 'True', ':', 'msg', '=', 'yield', 'future', '=', 'executor', '.', 'submit', '(', 'msg', '.', 'send', ')', 'future', '.', 'add_done_callback', '(', '_exception_handler', ')']","Creates a running coroutine to receive message instances and send
    them in a futures executor.","['Creates', 'a', 'running', 'coroutine', 'to', 'receive', 'message', 'instances', 'and', 'send', 'them', 'in', 'a', 'futures', 'executor', '.']",python,X,2,True,1,test
17332,trp07/messages,messages/cli.py,https://github.com/trp07/messages/blob/7789ebc960335a59ea5d319fceed3dd349023648/messages/cli.py#L40-L45,"def send_message(msg_type, kwds):
    """"""Do some final preprocessing and send the message.""""""
    if kwds[""file""]:
        get_body_from_file(kwds)
    kwargs = trim_args(kwds)
    send(msg_type, send_async=False, **kwargs)","['def', 'send_message', '(', 'msg_type', ',', 'kwds', ')', ':', 'if', 'kwds', '[', '""file""', ']', ':', 'get_body_from_file', '(', 'kwds', ')', 'kwargs', '=', 'trim_args', '(', 'kwds', ')', 'send', '(', 'msg_type', ',', 'send_async', '=', 'False', ',', '*', '*', 'kwargs', ')']",Do some final preprocessing and send the message.,"['Do', 'some', 'final', 'preprocessing', 'and', 'send', 'the', 'message', '.']",python,X,2,True,1,test
20455,cloud9ers/gurumate,environment/share/doc/ipython/examples/parallel/interengine/interengine.py,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/share/doc/ipython/examples/parallel/interengine/interengine.py#L30-L39,"def send(client, sender, targets, msg_name, dest_name=None, block=None):
    """"""send a message from one to one-or-more engines.""""""
    dest_name = msg_name if dest_name is None else dest_name
    def _send(targets, m_name):
        msg = globals()[m_name]
        return com.send(targets, msg)
        
    client[sender].apply_async(_send, targets, msg_name)
    
    return client[targets].execute('%s=com.recv()'%dest_name, block=None)","['def', 'send', '(', 'client', ',', 'sender', ',', 'targets', ',', 'msg_name', ',', 'dest_name', '=', 'None', ',', 'block', '=', 'None', ')', ':', 'dest_name', '=', 'msg_name', 'if', 'dest_name', 'is', 'None', 'else', 'dest_name', 'def', '_send', '(', 'targets', ',', 'm_name', ')', ':', 'msg', '=', 'globals', '(', ')', '[', 'm_name', ']', 'return', 'com', '.', 'send', '(', 'targets', ',', 'msg', ')', 'client', '[', 'sender', ']', '.', 'apply_async', '(', '_send', ',', 'targets', ',', 'msg_name', ')', 'return', 'client', '[', 'targets', ']', '.', 'execute', '(', ""'%s=com.recv()'"", '%', 'dest_name', ',', 'block', '=', 'None', ')']",send a message from one to one-or-more engines.,"['send', 'a', 'message', 'from', 'one', 'to', 'one', '-', 'or', '-', 'more', 'engines', '.']",python,X,2,True,1,test
20788,mrtazz/notifo.py,notifo/notifo.py,https://github.com/mrtazz/notifo.py/blob/26079db3b40c26661155af20a9f16a0eca06dbde/notifo/notifo.py#L33-L56,"def send_notification(self, to=None, msg=None, label=None,
                          title=None, uri=None):
        """""" method to send a message to a user

            Parameters:
                to -> recipient
                msg -> message to send
                label -> application description
                title -> name of the notification event
                uri -> callback uri
        """"""
        url = self.root_url + ""send_notification""
        values = {}
        if to is not None:
            values[""to""] = to
        if msg is not None:
            values[""msg""] = msg
        if label is not None:
            values[""label""] = label
        if title is not None:
            values[""title""] = title
        if uri is not None:
            values[""uri""] = uri
        return self._query(url, values)","['def', 'send_notification', '(', 'self', ',', 'to', '=', 'None', ',', 'msg', '=', 'None', ',', 'label', '=', 'None', ',', 'title', '=', 'None', ',', 'uri', '=', 'None', ')', ':', 'url', '=', 'self', '.', 'root_url', '+', '""send_notification""', 'values', '=', '{', '}', 'if', 'to', 'is', 'not', 'None', ':', 'values', '[', '""to""', ']', '=', 'to', 'if', 'msg', 'is', 'not', 'None', ':', 'values', '[', '""msg""', ']', '=', 'msg', 'if', 'label', 'is', 'not', 'None', ':', 'values', '[', '""label""', ']', '=', 'label', 'if', 'title', 'is', 'not', 'None', ':', 'values', '[', '""title""', ']', '=', 'title', 'if', 'uri', 'is', 'not', 'None', ':', 'values', '[', '""uri""', ']', '=', 'uri', 'return', 'self', '.', '_query', '(', 'url', ',', 'values', ')']","method to send a message to a user

            Parameters:
                to -> recipient
                msg -> message to send
                label -> application description
                title -> name of the notification event
                uri -> callback uri","['method', 'to', 'send', 'a', 'message', 'to', 'a', 'user']",python,X,2,True,1,test
20789,mrtazz/notifo.py,notifo/notifo.py,https://github.com/mrtazz/notifo.py/blob/26079db3b40c26661155af20a9f16a0eca06dbde/notifo/notifo.py#L58-L71,"def send_message(self, to=None, msg=None):
        """""" method to send a message to a user

            Parameters:
                to -> recipient
                msg -> message to send
        """"""
        url = self.root_url + ""send_message""
        values = {}
        if to is not None:
            values[""to""] = to
        if msg is not None:
            values[""msg""] = msg
        return self._query(url, values)","['def', 'send_message', '(', 'self', ',', 'to', '=', 'None', ',', 'msg', '=', 'None', ')', ':', 'url', '=', 'self', '.', 'root_url', '+', '""send_message""', 'values', '=', '{', '}', 'if', 'to', 'is', 'not', 'None', ':', 'values', '[', '""to""', ']', '=', 'to', 'if', 'msg', 'is', 'not', 'None', ':', 'values', '[', '""msg""', ']', '=', 'msg', 'return', 'self', '.', '_query', '(', 'url', ',', 'values', ')']","method to send a message to a user

            Parameters:
                to -> recipient
                msg -> message to send","['method', 'to', 'send', 'a', 'message', 'to', 'a', 'user']",python,X,2,True,1,test
21276,cloud9ers/gurumate,environment/lib/python2.7/site-packages/IPython/parallel/client/client.py,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/client/client.py#L1204-L1246,"def send_apply_request(self, socket, f, args=None, kwargs=None, subheader=None, track=False,
                            ident=None):
        """"""construct and send an apply message via a socket.

        This is the principal method with which all engine execution is performed by views.
        """"""

        if self._closed:
            raise RuntimeError(""Client cannot be used after its sockets have been closed"")
        
        # defaults:
        args = args if args is not None else []
        kwargs = kwargs if kwargs is not None else {}
        subheader = subheader if subheader is not None else {}

        # validate arguments
        if not callable(f) and not isinstance(f, Reference):
            raise TypeError(""f must be callable, not %s""%type(f))
        if not isinstance(args, (tuple, list)):
            raise TypeError(""args must be tuple or list, not %s""%type(args))
        if not isinstance(kwargs, dict):
            raise TypeError(""kwargs must be dict, not %s""%type(kwargs))
        if not isinstance(subheader, dict):
            raise TypeError(""subheader must be dict, not %s""%type(subheader))

        bufs = util.pack_apply_message(f,args,kwargs)

        msg = self.session.send(socket, ""apply_request"", buffers=bufs, ident=ident,
                            subheader=subheader, track=track)

        msg_id = msg['header']['msg_id']
        self.outstanding.add(msg_id)
        if ident:
            # possibly routed to a specific engine
            if isinstance(ident, list):
                ident = ident[-1]
            if ident in self._engines.values():
                # save for later, in case of engine death
                self._outstanding_dict[ident].add(msg_id)
        self.history.append(msg_id)
        self.metadata[msg_id]['submitted'] = datetime.now()

        return msg","['def', 'send_apply_request', '(', 'self', ',', 'socket', ',', 'f', ',', 'args', '=', 'None', ',', 'kwargs', '=', 'None', ',', 'subheader', '=', 'None', ',', 'track', '=', 'False', ',', 'ident', '=', 'None', ')', ':', 'if', 'self', '.', '_closed', ':', 'raise', 'RuntimeError', '(', '""Client cannot be used after its sockets have been closed""', ')', '# defaults:', 'args', '=', 'args', 'if', 'args', 'is', 'not', 'None', 'else', '[', ']', 'kwargs', '=', 'kwargs', 'if', 'kwargs', 'is', 'not', 'None', 'else', '{', '}', 'subheader', '=', 'subheader', 'if', 'subheader', 'is', 'not', 'None', 'else', '{', '}', '# validate arguments', 'if', 'not', 'callable', '(', 'f', ')', 'and', 'not', 'isinstance', '(', 'f', ',', 'Reference', ')', ':', 'raise', 'TypeError', '(', '""f must be callable, not %s""', '%', 'type', '(', 'f', ')', ')', 'if', 'not', 'isinstance', '(', 'args', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'raise', 'TypeError', '(', '""args must be tuple or list, not %s""', '%', 'type', '(', 'args', ')', ')', 'if', 'not', 'isinstance', '(', 'kwargs', ',', 'dict', ')', ':', 'raise', 'TypeError', '(', '""kwargs must be dict, not %s""', '%', 'type', '(', 'kwargs', ')', ')', 'if', 'not', 'isinstance', '(', 'subheader', ',', 'dict', ')', ':', 'raise', 'TypeError', '(', '""subheader must be dict, not %s""', '%', 'type', '(', 'subheader', ')', ')', 'bufs', '=', 'util', '.', 'pack_apply_message', '(', 'f', ',', 'args', ',', 'kwargs', ')', 'msg', '=', 'self', '.', 'session', '.', 'send', '(', 'socket', ',', '""apply_request""', ',', 'buffers', '=', 'bufs', ',', 'ident', '=', 'ident', ',', 'subheader', '=', 'subheader', ',', 'track', '=', 'track', ')', 'msg_id', '=', 'msg', '[', ""'header'"", ']', '[', ""'msg_id'"", ']', 'self', '.', 'outstanding', '.', 'add', '(', 'msg_id', ')', 'if', 'ident', ':', '# possibly routed to a specific engine', 'if', 'isinstance', '(', 'ident', ',', 'list', ')', ':', 'ident', '=', 'ident', '[', '-', '1', ']', 'if', 'ident', 'in', 'self', '.', '_engines', '.', 'values', '(', ')', ':', '# save for later, in case of engine death', 'self', '.', '_outstanding_dict', '[', 'ident', ']', '.', 'add', '(', 'msg_id', ')', 'self', '.', 'history', '.', 'append', '(', 'msg_id', ')', 'self', '.', 'metadata', '[', 'msg_id', ']', '[', ""'submitted'"", ']', '=', 'datetime', '.', 'now', '(', ')', 'return', 'msg']","construct and send an apply message via a socket.

        This is the principal method with which all engine execution is performed by views.","['construct', 'and', 'send', 'an', 'apply', 'message', 'via', 'a', 'socket', '.']",python,X,2,True,1,test
22047,cloud9ers/gurumate,environment/lib/python2.7/site-packages/IPython/zmq/session.py,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/zmq/session.py#L494-L586,"def send(self, stream, msg_or_type, content=None, parent=None, ident=None,
             buffers=None, subheader=None, track=False, header=None):
        """"""Build and send a message via stream or socket.

        The message format used by this function internally is as follows:

        [ident1,ident2,...,DELIM,HMAC,p_header,p_parent,p_content,
         buffer1,buffer2,...]

        The serialize/unserialize methods convert the nested message dict into this
        format.

        Parameters
        ----------

        stream : zmq.Socket or ZMQStream
            The socket-like object used to send the data.
        msg_or_type : str or Message/dict
            Normally, msg_or_type will be a msg_type unless a message is being
            sent more than once. If a header is supplied, this can be set to
            None and the msg_type will be pulled from the header.

        content : dict or None
            The content of the message (ignored if msg_or_type is a message).
        header : dict or None
            The header dict for the message (ignores if msg_to_type is a message).
        parent : Message or dict or None
            The parent or parent header describing the parent of this message
            (ignored if msg_or_type is a message).
        ident : bytes or list of bytes
            The zmq.IDENTITY routing path.
        subheader : dict or None
            Extra header keys for this message's header (ignored if msg_or_type
            is a message).
        buffers : list or None
            The already-serialized buffers to be appended to the message.
        track : bool
            Whether to track.  Only for use with Sockets, because ZMQStream
            objects cannot track messages.

        Returns
        -------
        msg : dict
            The constructed message.
        (msg,tracker) : (dict, MessageTracker)
            if track=True, then a 2-tuple will be returned,
            the first element being the constructed
            message, and the second being the MessageTracker

        """"""

        if not isinstance(stream, (zmq.Socket, ZMQStream)):
            raise TypeError(""stream must be Socket or ZMQStream, not %r""%type(stream))
        elif track and isinstance(stream, ZMQStream):
            raise TypeError(""ZMQStream cannot track messages"")

        if isinstance(msg_or_type, (Message, dict)):
            # We got a Message or message dict, not a msg_type so don't
            # build a new Message.
            msg = msg_or_type
        else:
            msg = self.msg(msg_or_type, content=content, parent=parent,
                           subheader=subheader, header=header)

        buffers = [] if buffers is None else buffers
        to_send = self.serialize(msg, ident)
        flag = 0
        if buffers:
            flag = zmq.SNDMORE
            _track = False
        else:
            _track=track
        if track:
            tracker = stream.send_multipart(to_send, flag, copy=False, track=_track)
        else:
            tracker = stream.send_multipart(to_send, flag, copy=False)
        for b in buffers[:-1]:
            stream.send(b, flag, copy=False)
        if buffers:
            if track:
                tracker = stream.send(buffers[-1], copy=False, track=track)
            else:
                tracker = stream.send(buffers[-1], copy=False)

        # omsg = Message(msg)
        if self.debug:
            pprint.pprint(msg)
            pprint.pprint(to_send)
            pprint.pprint(buffers)

        msg['tracker'] = tracker

        return msg","['def', 'send', '(', 'self', ',', 'stream', ',', 'msg_or_type', ',', 'content', '=', 'None', ',', 'parent', '=', 'None', ',', 'ident', '=', 'None', ',', 'buffers', '=', 'None', ',', 'subheader', '=', 'None', ',', 'track', '=', 'False', ',', 'header', '=', 'None', ')', ':', 'if', 'not', 'isinstance', '(', 'stream', ',', '(', 'zmq', '.', 'Socket', ',', 'ZMQStream', ')', ')', ':', 'raise', 'TypeError', '(', '""stream must be Socket or ZMQStream, not %r""', '%', 'type', '(', 'stream', ')', ')', 'elif', 'track', 'and', 'isinstance', '(', 'stream', ',', 'ZMQStream', ')', ':', 'raise', 'TypeError', '(', '""ZMQStream cannot track messages""', ')', 'if', 'isinstance', '(', 'msg_or_type', ',', '(', 'Message', ',', 'dict', ')', ')', ':', ""# We got a Message or message dict, not a msg_type so don't"", '# build a new Message.', 'msg', '=', 'msg_or_type', 'else', ':', 'msg', '=', 'self', '.', 'msg', '(', 'msg_or_type', ',', 'content', '=', 'content', ',', 'parent', '=', 'parent', ',', 'subheader', '=', 'subheader', ',', 'header', '=', 'header', ')', 'buffers', '=', '[', ']', 'if', 'buffers', 'is', 'None', 'else', 'buffers', 'to_send', '=', 'self', '.', 'serialize', '(', 'msg', ',', 'ident', ')', 'flag', '=', '0', 'if', 'buffers', ':', 'flag', '=', 'zmq', '.', 'SNDMORE', '_track', '=', 'False', 'else', ':', '_track', '=', 'track', 'if', 'track', ':', 'tracker', '=', 'stream', '.', 'send_multipart', '(', 'to_send', ',', 'flag', ',', 'copy', '=', 'False', ',', 'track', '=', '_track', ')', 'else', ':', 'tracker', '=', 'stream', '.', 'send_multipart', '(', 'to_send', ',', 'flag', ',', 'copy', '=', 'False', ')', 'for', 'b', 'in', 'buffers', '[', ':', '-', '1', ']', ':', 'stream', '.', 'send', '(', 'b', ',', 'flag', ',', 'copy', '=', 'False', ')', 'if', 'buffers', ':', 'if', 'track', ':', 'tracker', '=', 'stream', '.', 'send', '(', 'buffers', '[', '-', '1', ']', ',', 'copy', '=', 'False', ',', 'track', '=', 'track', ')', 'else', ':', 'tracker', '=', 'stream', '.', 'send', '(', 'buffers', '[', '-', '1', ']', ',', 'copy', '=', 'False', ')', '# omsg = Message(msg)', 'if', 'self', '.', 'debug', ':', 'pprint', '.', 'pprint', '(', 'msg', ')', 'pprint', '.', 'pprint', '(', 'to_send', ')', 'pprint', '.', 'pprint', '(', 'buffers', ')', 'msg', '[', ""'tracker'"", ']', '=', 'tracker', 'return', 'msg']","Build and send a message via stream or socket.

        The message format used by this function internally is as follows:

        [ident1,ident2,...,DELIM,HMAC,p_header,p_parent,p_content,
         buffer1,buffer2,...]

        The serialize/unserialize methods convert the nested message dict into this
        format.

        Parameters
        ----------

        stream : zmq.Socket or ZMQStream
            The socket-like object used to send the data.
        msg_or_type : str or Message/dict
            Normally, msg_or_type will be a msg_type unless a message is being
            sent more than once. If a header is supplied, this can be set to
            None and the msg_type will be pulled from the header.

        content : dict or None
            The content of the message (ignored if msg_or_type is a message).
        header : dict or None
            The header dict for the message (ignores if msg_to_type is a message).
        parent : Message or dict or None
            The parent or parent header describing the parent of this message
            (ignored if msg_or_type is a message).
        ident : bytes or list of bytes
            The zmq.IDENTITY routing path.
        subheader : dict or None
            Extra header keys for this message's header (ignored if msg_or_type
            is a message).
        buffers : list or None
            The already-serialized buffers to be appended to the message.
        track : bool
            Whether to track.  Only for use with Sockets, because ZMQStream
            objects cannot track messages.

        Returns
        -------
        msg : dict
            The constructed message.
        (msg,tracker) : (dict, MessageTracker)
            if track=True, then a 2-tuple will be returned,
            the first element being the constructed
            message, and the second being the MessageTracker","['Build', 'and', 'send', 'a', 'message', 'via', 'stream', 'or', 'socket', '.']",python,X,2,True,1,test
22522,saltstack/salt,salt/returners/pushover_returner.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/returners/pushover_returner.py#L151-L198,"def _post_message(user,
                  device,
                  message,
                  title,
                  priority,
                  expire,
                  retry,
                  sound,
                  api_version=1,
                  token=None):
    '''
    Send a message to a Pushover user or group.
    :param user:        The user or group to send to, must be key of user or group not email address.
    :param message:     The message to send to the PushOver user or group.
    :param title:       Specify who the message is from.
    :param priority     The priority of the message, defaults to 0.
    :param api_version: The PushOver API version, if not specified in the configuration.
    :param notify:      Whether to notify the room, default: False.
    :param token:       The PushOver token, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.
    '''

    user_validate = salt.utils.pushover.validate_user(user, device, token)
    if not user_validate['result']:
        return user_validate

    parameters = dict()
    parameters['user'] = user
    parameters['device'] = device
    parameters['token'] = token
    parameters['title'] = title
    parameters['priority'] = priority
    parameters['expire'] = expire
    parameters['retry'] = retry
    parameters['message'] = message

    if sound:
        sound_validate = salt.utils.pushover.validate_sound(sound, token)
        if sound_validate['res']:
            parameters['sound'] = sound

    result = salt.utils.pushover.query(function='message',
                                       method='POST',
                                       header_dict={'Content-Type': 'application/x-www-form-urlencoded'},
                                       data=_urlencode(parameters),
                                       opts=__opts__)

    return result","['def', '_post_message', '(', 'user', ',', 'device', ',', 'message', ',', 'title', ',', 'priority', ',', 'expire', ',', 'retry', ',', 'sound', ',', 'api_version', '=', '1', ',', 'token', '=', 'None', ')', ':', 'user_validate', '=', 'salt', '.', 'utils', '.', 'pushover', '.', 'validate_user', '(', 'user', ',', 'device', ',', 'token', ')', 'if', 'not', 'user_validate', '[', ""'result'"", ']', ':', 'return', 'user_validate', 'parameters', '=', 'dict', '(', ')', 'parameters', '[', ""'user'"", ']', '=', 'user', 'parameters', '[', ""'device'"", ']', '=', 'device', 'parameters', '[', ""'token'"", ']', '=', 'token', 'parameters', '[', ""'title'"", ']', '=', 'title', 'parameters', '[', ""'priority'"", ']', '=', 'priority', 'parameters', '[', ""'expire'"", ']', '=', 'expire', 'parameters', '[', ""'retry'"", ']', '=', 'retry', 'parameters', '[', ""'message'"", ']', '=', 'message', 'if', 'sound', ':', 'sound_validate', '=', 'salt', '.', 'utils', '.', 'pushover', '.', 'validate_sound', '(', 'sound', ',', 'token', ')', 'if', 'sound_validate', '[', ""'res'"", ']', ':', 'parameters', '[', ""'sound'"", ']', '=', 'sound', 'result', '=', 'salt', '.', 'utils', '.', 'pushover', '.', 'query', '(', 'function', '=', ""'message'"", ',', 'method', '=', ""'POST'"", ',', 'header_dict', '=', '{', ""'Content-Type'"", ':', ""'application/x-www-form-urlencoded'"", '}', ',', 'data', '=', '_urlencode', '(', 'parameters', ')', ',', 'opts', '=', '__opts__', ')', 'return', 'result']","Send a message to a Pushover user or group.
    :param user:        The user or group to send to, must be key of user or group not email address.
    :param message:     The message to send to the PushOver user or group.
    :param title:       Specify who the message is from.
    :param priority     The priority of the message, defaults to 0.
    :param api_version: The PushOver API version, if not specified in the configuration.
    :param notify:      Whether to notify the room, default: False.
    :param token:       The PushOver token, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.","['Send', 'a', 'message', 'to', 'a', 'Pushover', 'user', 'or', 'group', '.', ':', 'param', 'user', ':', 'The', 'user', 'or', 'group', 'to', 'send', 'to', 'must', 'be', 'key', 'of', 'user', 'or', 'group', 'not', 'email', 'address', '.', ':', 'param', 'message', ':', 'The', 'message', 'to', 'send', 'to', 'the', 'PushOver', 'user', 'or', 'group', '.', ':', 'param', 'title', ':', 'Specify', 'who', 'the', 'message', 'is', 'from', '.', ':', 'param', 'priority', 'The', 'priority', 'of', 'the', 'message', 'defaults', 'to', '0', '.', ':', 'param', 'api_version', ':', 'The', 'PushOver', 'API', 'version', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'param', 'notify', ':', 'Whether', 'to', 'notify', 'the', 'room', 'default', ':', 'False', '.', ':', 'param', 'token', ':', 'The', 'PushOver', 'token', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'return', ':', 'Boolean', 'if', 'message', 'was', 'sent', 'successfully', '.']",python,X,2,True,1,train
24123,saltstack/salt,salt/modules/msteams.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/msteams.py#L54-L96,"def post_card(message,
              hook_url=None,
              title=None,
              theme_color=None):
    '''
    Send a message to an MS Teams channel.
    :param message:     The message to send to the MS Teams channel.
    :param hook_url:    The Teams webhook URL, if not specified in the configuration.
    :param title:       Optional title for the posted card
    :param theme_color:  Optional hex color highlight for the posted card
    :return:            Boolean if message was sent successfully.

    CLI Example:

    .. code-block:: bash

        salt '*' msteams.post_card message=""Build is done""
    '''

    if not hook_url:
        hook_url = _get_hook_url()

    if not message:
        log.error('message is a required option.')

    payload = {
        ""text"": message,
        ""title"": title,
        ""themeColor"": theme_color
    }

    result = salt.utils.http.query(hook_url,
                                   method='POST',
                                   data=salt.utils.json.dumps(payload),
                                   status=True)

    if result['status'] <= 201:
        return True
    else:
        return {
            'res': False,
            'message': result.get('body', result['status'])
        }","['def', 'post_card', '(', 'message', ',', 'hook_url', '=', 'None', ',', 'title', '=', 'None', ',', 'theme_color', '=', 'None', ')', ':', 'if', 'not', 'hook_url', ':', 'hook_url', '=', '_get_hook_url', '(', ')', 'if', 'not', 'message', ':', 'log', '.', 'error', '(', ""'message is a required option.'"", ')', 'payload', '=', '{', '""text""', ':', 'message', ',', '""title""', ':', 'title', ',', '""themeColor""', ':', 'theme_color', '}', 'result', '=', 'salt', '.', 'utils', '.', 'http', '.', 'query', '(', 'hook_url', ',', 'method', '=', ""'POST'"", ',', 'data', '=', 'salt', '.', 'utils', '.', 'json', '.', 'dumps', '(', 'payload', ')', ',', 'status', '=', 'True', ')', 'if', 'result', '[', ""'status'"", ']', '<=', '201', ':', 'return', 'True', 'else', ':', 'return', '{', ""'res'"", ':', 'False', ',', ""'message'"", ':', 'result', '.', 'get', '(', ""'body'"", ',', 'result', '[', ""'status'"", ']', ')', '}']","Send a message to an MS Teams channel.
    :param message:     The message to send to the MS Teams channel.
    :param hook_url:    The Teams webhook URL, if not specified in the configuration.
    :param title:       Optional title for the posted card
    :param theme_color:  Optional hex color highlight for the posted card
    :return:            Boolean if message was sent successfully.

    CLI Example:

    .. code-block:: bash

        salt '*' msteams.post_card message=""Build is done""","['Send', 'a', 'message', 'to', 'an', 'MS', 'Teams', 'channel', '.', ':', 'param', 'message', ':', 'The', 'message', 'to', 'send', 'to', 'the', 'MS', 'Teams', 'channel', '.', ':', 'param', 'hook_url', ':', 'The', 'Teams', 'webhook', 'URL', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'param', 'title', ':', 'Optional', 'title', 'for', 'the', 'posted', 'card', ':', 'param', 'theme_color', ':', 'Optional', 'hex', 'color', 'highlight', 'for', 'the', 'posted', 'card', ':', 'return', ':', 'Boolean', 'if', 'message', 'was', 'sent', 'successfully', '.']",python,X,2,True,1,train
24476,saltstack/salt,salt/returners/slack_returner.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/returners/slack_returner.py#L145-L178,"def _post_message(channel,
                  message,
                  username,
                  as_user,
                  api_key=None):
    '''
    Send a message to a Slack room.
    :param channel:     The room name.
    :param message:     The message to send to the Slack room.
    :param username:    Specify who the message is from.
    :param as_user:     Sets the profile picture which have been added through Slack itself.
    :param api_key:     The Slack api key, if not specified in the configuration.
    :param api_version: The Slack api version, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.
    '''

    parameters = dict()
    parameters['channel'] = channel
    parameters['username'] = username
    parameters['as_user'] = as_user
    parameters['text'] = '```' + message + '```'  # pre-formatted, fixed-width text

    # Slack wants the body on POST to be urlencoded.
    result = salt.utils.slack.query(function='message',
                                    api_key=api_key,
                                    method='POST',
                                    header_dict={'Content-Type': 'application/x-www-form-urlencoded'},
                                    data=_urlencode(parameters))

    log.debug('Slack message post result: %s', result)
    if result:
        return True
    else:
        return False","['def', '_post_message', '(', 'channel', ',', 'message', ',', 'username', ',', 'as_user', ',', 'api_key', '=', 'None', ')', ':', 'parameters', '=', 'dict', '(', ')', 'parameters', '[', ""'channel'"", ']', '=', 'channel', 'parameters', '[', ""'username'"", ']', '=', 'username', 'parameters', '[', ""'as_user'"", ']', '=', 'as_user', 'parameters', '[', ""'text'"", ']', '=', ""'```'"", '+', 'message', '+', ""'```'"", '# pre-formatted, fixed-width text', '# Slack wants the body on POST to be urlencoded.', 'result', '=', 'salt', '.', 'utils', '.', 'slack', '.', 'query', '(', 'function', '=', ""'message'"", ',', 'api_key', '=', 'api_key', ',', 'method', '=', ""'POST'"", ',', 'header_dict', '=', '{', ""'Content-Type'"", ':', ""'application/x-www-form-urlencoded'"", '}', ',', 'data', '=', '_urlencode', '(', 'parameters', ')', ')', 'log', '.', 'debug', '(', ""'Slack message post result: %s'"", ',', 'result', ')', 'if', 'result', ':', 'return', 'True', 'else', ':', 'return', 'False']","Send a message to a Slack room.
    :param channel:     The room name.
    :param message:     The message to send to the Slack room.
    :param username:    Specify who the message is from.
    :param as_user:     Sets the profile picture which have been added through Slack itself.
    :param api_key:     The Slack api key, if not specified in the configuration.
    :param api_version: The Slack api version, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.","['Send', 'a', 'message', 'to', 'a', 'Slack', 'room', '.', ':', 'param', 'channel', ':', 'The', 'room', 'name', '.', ':', 'param', 'message', ':', 'The', 'message', 'to', 'send', 'to', 'the', 'Slack', 'room', '.', ':', 'param', 'username', ':', 'Specify', 'who', 'the', 'message', 'is', 'from', '.', ':', 'param', 'as_user', ':', 'Sets', 'the', 'profile', 'picture', 'which', 'have', 'been', 'added', 'through', 'Slack', 'itself', '.', ':', 'param', 'api_key', ':', 'The', 'Slack', 'api', 'key', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'param', 'api_version', ':', 'The', 'Slack', 'api', 'version', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'return', ':', 'Boolean', 'if', 'message', 'was', 'sent', 'successfully', '.']",python,X,2,True,1,train
26122,saltstack/salt,salt/runners/mattermost.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/runners/mattermost.py#L95-L146,"def post_message(message,
                 channel=None,
                 username=None,
                 api_url=None,
                 hook=None):
    '''
    Send a message to a Mattermost channel.
    :param channel:     The channel name, either will work.
    :param username:    The username of the poster.
    :param message:     The message to send to the Mattermost channel.
    :param api_url:     The Mattermost api url, if not specified in the configuration.
    :param hook:        The Mattermost hook, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.

    CLI Example:

    .. code-block:: bash

        salt-run mattermost.post_message message='Build is done'
    '''
    if not api_url:
        api_url = _get_api_url()

    if not hook:
        hook = _get_hook()

    if not username:
        username = _get_username()

    if not channel:
        channel = _get_channel()

    if not message:
        log.error('message is a required option.')

    parameters = dict()
    if channel:
        parameters['channel'] = channel
    if username:
        parameters['username'] = username
    parameters['text'] = '```' + message + '```'  # pre-formatted, fixed-width text
    log.debug('Parameters: %s', parameters)
    data = salt.utils.json.dumps(parameters)
    result = salt.utils.mattermost.query(
        api_url=api_url,
        hook=hook,
        data=str('payload={0}').format(data))  # future lint: blacklisted-function

    if result:
        return True
    else:
        return result","['def', 'post_message', '(', 'message', ',', 'channel', '=', 'None', ',', 'username', '=', 'None', ',', 'api_url', '=', 'None', ',', 'hook', '=', 'None', ')', ':', 'if', 'not', 'api_url', ':', 'api_url', '=', '_get_api_url', '(', ')', 'if', 'not', 'hook', ':', 'hook', '=', '_get_hook', '(', ')', 'if', 'not', 'username', ':', 'username', '=', '_get_username', '(', ')', 'if', 'not', 'channel', ':', 'channel', '=', '_get_channel', '(', ')', 'if', 'not', 'message', ':', 'log', '.', 'error', '(', ""'message is a required option.'"", ')', 'parameters', '=', 'dict', '(', ')', 'if', 'channel', ':', 'parameters', '[', ""'channel'"", ']', '=', 'channel', 'if', 'username', ':', 'parameters', '[', ""'username'"", ']', '=', 'username', 'parameters', '[', ""'text'"", ']', '=', ""'```'"", '+', 'message', '+', ""'```'"", '# pre-formatted, fixed-width text', 'log', '.', 'debug', '(', ""'Parameters: %s'"", ',', 'parameters', ')', 'data', '=', 'salt', '.', 'utils', '.', 'json', '.', 'dumps', '(', 'parameters', ')', 'result', '=', 'salt', '.', 'utils', '.', 'mattermost', '.', 'query', '(', 'api_url', '=', 'api_url', ',', 'hook', '=', 'hook', ',', 'data', '=', 'str', '(', ""'payload={0}'"", ')', '.', 'format', '(', 'data', ')', ')', '# future lint: blacklisted-function', 'if', 'result', ':', 'return', 'True', 'else', ':', 'return', 'result']","Send a message to a Mattermost channel.
    :param channel:     The channel name, either will work.
    :param username:    The username of the poster.
    :param message:     The message to send to the Mattermost channel.
    :param api_url:     The Mattermost api url, if not specified in the configuration.
    :param hook:        The Mattermost hook, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.

    CLI Example:

    .. code-block:: bash

        salt-run mattermost.post_message message='Build is done'","['Send', 'a', 'message', 'to', 'a', 'Mattermost', 'channel', '.', ':', 'param', 'channel', ':', 'The', 'channel', 'name', 'either', 'will', 'work', '.', ':', 'param', 'username', ':', 'The', 'username', 'of', 'the', 'poster', '.', ':', 'param', 'message', ':', 'The', 'message', 'to', 'send', 'to', 'the', 'Mattermost', 'channel', '.', ':', 'param', 'api_url', ':', 'The', 'Mattermost', 'api', 'url', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'param', 'hook', ':', 'The', 'Mattermost', 'hook', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'return', ':', 'Boolean', 'if', 'message', 'was', 'sent', 'successfully', '.']",python,X,2,True,1,train
26123,saltstack/salt,salt/runners/mattermost.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/runners/mattermost.py#L149-L188,"def post_event(event,
               channel=None,
               username=None,
               api_url=None,
               hook=None):
    '''
    Send an event to a Mattermost channel.
    :param channel:     The channel name, either will work.
    :param username:    The username of the poster.
    :param event:       The event to send to the Mattermost channel.
    :param api_url:     The Mattermost api url, if not specified in the configuration.
    :param hook:        The Mattermost hook, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.
    '''
    if not api_url:
        api_url = _get_api_url()

    if not hook:
        hook = _get_hook()

    if not username:
        username = _get_username()

    if not channel:
        channel = _get_channel()

    if not event:
        log.error('message is a required option.')

    log.debug('Event: %s', event)
    log.debug('Event data: %s', event['data'])
    message = 'tag: {0}\r\n'.format(event['tag'])
    for key, value in six.iteritems(event['data']):
        message += '{0}: {1}\r\n'.format(key, value)
    result = post_message(channel,
                          username,
                          message,
                          api_url,
                          hook)
    return bool(result)","['def', 'post_event', '(', 'event', ',', 'channel', '=', 'None', ',', 'username', '=', 'None', ',', 'api_url', '=', 'None', ',', 'hook', '=', 'None', ')', ':', 'if', 'not', 'api_url', ':', 'api_url', '=', '_get_api_url', '(', ')', 'if', 'not', 'hook', ':', 'hook', '=', '_get_hook', '(', ')', 'if', 'not', 'username', ':', 'username', '=', '_get_username', '(', ')', 'if', 'not', 'channel', ':', 'channel', '=', '_get_channel', '(', ')', 'if', 'not', 'event', ':', 'log', '.', 'error', '(', ""'message is a required option.'"", ')', 'log', '.', 'debug', '(', ""'Event: %s'"", ',', 'event', ')', 'log', '.', 'debug', '(', ""'Event data: %s'"", ',', 'event', '[', ""'data'"", ']', ')', 'message', '=', ""'tag: {0}\\r\\n'"", '.', 'format', '(', 'event', '[', ""'tag'"", ']', ')', 'for', 'key', ',', 'value', 'in', 'six', '.', 'iteritems', '(', 'event', '[', ""'data'"", ']', ')', ':', 'message', '+=', ""'{0}: {1}\\r\\n'"", '.', 'format', '(', 'key', ',', 'value', ')', 'result', '=', 'post_message', '(', 'channel', ',', 'username', ',', 'message', ',', 'api_url', ',', 'hook', ')', 'return', 'bool', '(', 'result', ')']","Send an event to a Mattermost channel.
    :param channel:     The channel name, either will work.
    :param username:    The username of the poster.
    :param event:       The event to send to the Mattermost channel.
    :param api_url:     The Mattermost api url, if not specified in the configuration.
    :param hook:        The Mattermost hook, if not specified in the configuration.
    :return:            Boolean if message was sent successfully.","['Send', 'an', 'event', 'to', 'a', 'Mattermost', 'channel', '.', ':', 'param', 'channel', ':', 'The', 'channel', 'name', 'either', 'will', 'work', '.', ':', 'param', 'username', ':', 'The', 'username', 'of', 'the', 'poster', '.', ':', 'param', 'event', ':', 'The', 'event', 'to', 'send', 'to', 'the', 'Mattermost', 'channel', '.', ':', 'param', 'api_url', ':', 'The', 'Mattermost', 'api', 'url', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'param', 'hook', ':', 'The', 'Mattermost', 'hook', 'if', 'not', 'specified', 'in', 'the', 'configuration', '.', ':', 'return', ':', 'Boolean', 'if', 'message', 'was', 'sent', 'successfully', '.']",python,X,2,True,1,train
29995,saltstack/salt,salt/transport/zeromq.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/transport/zeromq.py#L370-L378,"def send(self, load, tries=3, timeout=60, raw=False):
        '''
        Send a request, return a future which will complete when we send the message
        '''
        if self.crypt == 'clear':
            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)
        else:
            ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout, raw=raw)
        raise tornado.gen.Return(ret)","['def', 'send', '(', 'self', ',', 'load', ',', 'tries', '=', '3', ',', 'timeout', '=', '60', ',', 'raw', '=', 'False', ')', ':', 'if', 'self', '.', 'crypt', '==', ""'clear'"", ':', 'ret', '=', 'yield', 'self', '.', '_uncrypted_transfer', '(', 'load', ',', 'tries', '=', 'tries', ',', 'timeout', '=', 'timeout', ')', 'else', ':', 'ret', '=', 'yield', 'self', '.', '_crypted_transfer', '(', 'load', ',', 'tries', '=', 'tries', ',', 'timeout', '=', 'timeout', ',', 'raw', '=', 'raw', ')', 'raise', 'tornado', '.', 'gen', '.', 'Return', '(', 'ret', ')']","Send a request, return a future which will complete when we send the message","['Send', 'a', 'request', 'return', 'a', 'future', 'which', 'will', 'complete', 'when', 'we', 'send', 'the', 'message']",python,X,2,True,1,train
2265,saltstack/salt,salt/modules/xmpp.py,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/xmpp.py#L149-L197,"def send_msg_multi(message,
                   recipients=None,
                   rooms=None,
                   jid=None,
                   password=None,
                   nick=""SaltStack Bot"",
                   profile=None):
    '''
    Send a message to an XMPP recipient, support send message to
    multiple recipients or chat room.

    CLI Examples:

    .. code-block:: bash

        xmpp.send_msg recipients=['admins@xmpp.example.com'] \
            rooms=['secret@conference.xmpp.example.com'] \
            'This is a salt module test' \
            profile='my-xmpp-account'
        xmpp.send_msg recipients=['admins@xmpp.example.com'] \
            rooms=['secret@conference.xmpp.example.com'] \
           'This is a salt module test' \
            jid='myuser@xmpp.example.com/salt' password='verybadpass'

    '''

    # Remove: [WARNING ] Use of send mask waiters is deprecated.
    for handler in logging.root.handlers:
        handler.addFilter(SleekXMPPMUC())

    if profile:
        creds = __salt__['config.option'](profile)
        jid = creds.get('xmpp.jid')
        password = creds.get('xmpp.password')

    xmpp = SendMsgBot.create_multi(
        jid, password, message, recipients=recipients, rooms=rooms, nick=nick)

    if rooms:
        xmpp.register_plugin('xep_0045')  # MUC plugin
    if xmpp.connect():
        try:
            xmpp.process(block=True)
            return True
        except XMPPError as err:
            log.error(""Could not send message, error: %s"", err)
    else:
        log.error(""Could not connect to XMPP server"")
    return False","['def', 'send_msg_multi', '(', 'message', ',', 'recipients', '=', 'None', ',', 'rooms', '=', 'None', ',', 'jid', '=', 'None', ',', 'password', '=', 'None', ',', 'nick', '=', '""SaltStack Bot""', ',', 'profile', '=', 'None', ')', ':', '# Remove: [WARNING ] Use of send mask waiters is deprecated.', 'for', 'handler', 'in', 'logging', '.', 'root', '.', 'handlers', ':', 'handler', '.', 'addFilter', '(', 'SleekXMPPMUC', '(', ')', ')', 'if', 'profile', ':', 'creds', '=', '__salt__', '[', ""'config.option'"", ']', '(', 'profile', ')', 'jid', '=', 'creds', '.', 'get', '(', ""'xmpp.jid'"", ')', 'password', '=', 'creds', '.', 'get', '(', ""'xmpp.password'"", ')', 'xmpp', '=', 'SendMsgBot', '.', 'create_multi', '(', 'jid', ',', 'password', ',', 'message', ',', 'recipients', '=', 'recipients', ',', 'rooms', '=', 'rooms', ',', 'nick', '=', 'nick', ')', 'if', 'rooms', ':', 'xmpp', '.', 'register_plugin', '(', ""'xep_0045'"", ')', '# MUC plugin', 'if', 'xmpp', '.', 'connect', '(', ')', ':', 'try', ':', 'xmpp', '.', 'process', '(', 'block', '=', 'True', ')', 'return', 'True', 'except', 'XMPPError', 'as', 'err', ':', 'log', '.', 'error', '(', '""Could not send message, error: %s""', ',', 'err', ')', 'else', ':', 'log', '.', 'error', '(', '""Could not connect to XMPP server""', ')', 'return', 'False']","Send a message to an XMPP recipient, support send message to
    multiple recipients or chat room.

    CLI Examples:

    .. code-block:: bash

        xmpp.send_msg recipients=['admins@xmpp.example.com'] \
            rooms=['secret@conference.xmpp.example.com'] \
            'This is a salt module test' \
            profile='my-xmpp-account'
        xmpp.send_msg recipients=['admins@xmpp.example.com'] \
            rooms=['secret@conference.xmpp.example.com'] \
           'This is a salt module test' \
            jid='myuser@xmpp.example.com/salt' password='verybadpass'","['Send', 'a', 'message', 'to', 'an', 'XMPP', 'recipient', 'support', 'send', 'message', 'to', 'multiple', 'recipients', 'or', 'chat', 'room', '.']",python,X,2,True,1,train
6003,secdev/scapy,scapy/layers/inet6.py,https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/layers/inet6.py#L3916-L3994,"def NDP_Attack_Fake_Router(ra, iface=None, mac_src_filter=None,
                           ip_src_filter=None):
    """"""
    The purpose of this function is to send provided RA message at layer 2
    (i.e. providing a packet starting with IPv6 will not work) in response
    to received RS messages. In the end, the function is a simple wrapper
    around sendp() that monitor the link for RS messages.

    It is probably better explained with an example:

      >>> ra  = Ether()/IPv6()/ICMPv6ND_RA()
      >>> ra /= ICMPv6NDOptPrefixInfo(prefix=""2001:db8:1::"", prefixlen=64)
      >>> ra /= ICMPv6NDOptPrefixInfo(prefix=""2001:db8:2::"", prefixlen=64)
      >>> ra /= ICMPv6NDOptSrcLLAddr(lladdr=""00:11:22:33:44:55"")
      >>> NDP_Attack_Fake_Router(ra, iface=""eth0"")
      Fake RA sent in response to RS from fe80::213:58ff:fe8c:b573
      Fake RA sent in response to RS from fe80::213:72ff:fe8c:b9ae
      ...

    Following arguments can be used to change the behavior:

      ra: the RA message to send in response to received RS message.

      iface: a specific interface (e.g. ""eth0"") of the system on which the
             DoS should be launched. If none is provided, conf.iface is
             used.

      mac_src_filter: a mac address (e.g ""00:13:72:8c:b5:69"") to filter on.
         Only RS messages received from this source will trigger a reply.
         Note that no changes to provided RA is done which imply that if
         you intend to target only the source of the RS using this option,
         you will have to set the Ethernet destination address to the same
         value in your RA.
         The default value for this parameter is None: no filtering on the
         source of RS is done.

    ip_src_filter: an IPv6 address (e.g. fe80::21e:bff:fe4e:3b2) to filter
         on. Only RS messages received from this source address will trigger
         replies. Same comment as for previous argument apply: if you use
         the option, you will probably want to set a specific Ethernet
         destination address in the RA.
    """"""

    def is_request(req, mac_src_filter, ip_src_filter):
        """"""
        Check if packet req is a request
        """"""

        if not (Ether in req and IPv6 in req and ICMPv6ND_RS in req):
            return 0

        mac_src = req[Ether].src
        if mac_src_filter and mac_src != mac_src_filter:
            return 0

        ip_src = req[IPv6].src
        if ip_src_filter and ip_src != ip_src_filter:
            return 0

        return 1

    def ra_reply_callback(req, iface):
        """"""
        Callback that sends an RA in reply to an RS
        """"""

        src = req[IPv6].src
        sendp(ra, iface=iface, verbose=0)
        print(""Fake RA sent in response to RS from %s"" % src)

    if not iface:
        iface = conf.iface
    sniff_filter = ""icmp6""

    sniff(store=0,
          filter=sniff_filter,
          lfilter=lambda x: is_request(x, mac_src_filter, ip_src_filter),
          prn=lambda x: ra_reply_callback(x, iface),
          iface=iface)","['def', 'NDP_Attack_Fake_Router', '(', 'ra', ',', 'iface', '=', 'None', ',', 'mac_src_filter', '=', 'None', ',', 'ip_src_filter', '=', 'None', ')', ':', 'def', 'is_request', '(', 'req', ',', 'mac_src_filter', ',', 'ip_src_filter', ')', ':', '""""""\n        Check if packet req is a request\n        """"""', 'if', 'not', '(', 'Ether', 'in', 'req', 'and', 'IPv6', 'in', 'req', 'and', 'ICMPv6ND_RS', 'in', 'req', ')', ':', 'return', '0', 'mac_src', '=', 'req', '[', 'Ether', ']', '.', 'src', 'if', 'mac_src_filter', 'and', 'mac_src', '!=', 'mac_src_filter', ':', 'return', '0', 'ip_src', '=', 'req', '[', 'IPv6', ']', '.', 'src', 'if', 'ip_src_filter', 'and', 'ip_src', '!=', 'ip_src_filter', ':', 'return', '0', 'return', '1', 'def', 'ra_reply_callback', '(', 'req', ',', 'iface', ')', ':', '""""""\n        Callback that sends an RA in reply to an RS\n        """"""', 'src', '=', 'req', '[', 'IPv6', ']', '.', 'src', 'sendp', '(', 'ra', ',', 'iface', '=', 'iface', ',', 'verbose', '=', '0', ')', 'print', '(', '""Fake RA sent in response to RS from %s""', '%', 'src', ')', 'if', 'not', 'iface', ':', 'iface', '=', 'conf', '.', 'iface', 'sniff_filter', '=', '""icmp6""', 'sniff', '(', 'store', '=', '0', ',', 'filter', '=', 'sniff_filter', ',', 'lfilter', '=', 'lambda', 'x', ':', 'is_request', '(', 'x', ',', 'mac_src_filter', ',', 'ip_src_filter', ')', ',', 'prn', '=', 'lambda', 'x', ':', 'ra_reply_callback', '(', 'x', ',', 'iface', ')', ',', 'iface', '=', 'iface', ')']","The purpose of this function is to send provided RA message at layer 2
    (i.e. providing a packet starting with IPv6 will not work) in response
    to received RS messages. In the end, the function is a simple wrapper
    around sendp() that monitor the link for RS messages.

    It is probably better explained with an example:

      >>> ra  = Ether()/IPv6()/ICMPv6ND_RA()
      >>> ra /= ICMPv6NDOptPrefixInfo(prefix=""2001:db8:1::"", prefixlen=64)
      >>> ra /= ICMPv6NDOptPrefixInfo(prefix=""2001:db8:2::"", prefixlen=64)
      >>> ra /= ICMPv6NDOptSrcLLAddr(lladdr=""00:11:22:33:44:55"")
      >>> NDP_Attack_Fake_Router(ra, iface=""eth0"")
      Fake RA sent in response to RS from fe80::213:58ff:fe8c:b573
      Fake RA sent in response to RS from fe80::213:72ff:fe8c:b9ae
      ...

    Following arguments can be used to change the behavior:

      ra: the RA message to send in response to received RS message.

      iface: a specific interface (e.g. ""eth0"") of the system on which the
             DoS should be launched. If none is provided, conf.iface is
             used.

      mac_src_filter: a mac address (e.g ""00:13:72:8c:b5:69"") to filter on.
         Only RS messages received from this source will trigger a reply.
         Note that no changes to provided RA is done which imply that if
         you intend to target only the source of the RS using this option,
         you will have to set the Ethernet destination address to the same
         value in your RA.
         The default value for this parameter is None: no filtering on the
         source of RS is done.

    ip_src_filter: an IPv6 address (e.g. fe80::21e:bff:fe4e:3b2) to filter
         on. Only RS messages received from this source address will trigger
         replies. Same comment as for previous argument apply: if you use
         the option, you will probably want to set a specific Ethernet
         destination address in the RA.","['The', 'purpose', 'of', 'this', 'function', 'is', 'to', 'send', 'provided', 'RA', 'message', 'at', 'layer', '2', '(', 'i', '.', 'e', '.', 'providing', 'a', 'packet', 'starting', 'with', 'IPv6', 'will', 'not', 'work', ')', 'in', 'response', 'to', 'received', 'RS', 'messages', '.', 'In', 'the', 'end', 'the', 'function', 'is', 'a', 'simple', 'wrapper', 'around', 'sendp', '()', 'that', 'monitor', 'the', 'link', 'for', 'RS', 'messages', '.']",python,X,2,True,1,train
6114,secdev/scapy,scapy/contrib/isotp.py,https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/isotp.py#L602-L610,"def begin_send(self, p):
        """"""Begin the transmission of message p. This method returns after
        sending the first frame. If multiple frames are necessary to send the
        message, this socket will unable to send other messages until either
        the transmission of this frame succeeds or it fails.""""""
        if hasattr(p, ""sent_time""):
            p.sent_time = time.time()

        return self.outs.begin_send(bytes(p))","['def', 'begin_send', '(', 'self', ',', 'p', ')', ':', 'if', 'hasattr', '(', 'p', ',', '""sent_time""', ')', ':', 'p', '.', 'sent_time', '=', 'time', '.', 'time', '(', ')', 'return', 'self', '.', 'outs', '.', 'begin_send', '(', 'bytes', '(', 'p', ')', ')']","Begin the transmission of message p. This method returns after
        sending the first frame. If multiple frames are necessary to send the
        message, this socket will unable to send other messages until either
        the transmission of this frame succeeds or it fails.","['Begin', 'the', 'transmission', 'of', 'message', 'p', '.', 'This', 'method', 'returns', 'after', 'sending', 'the', 'first', 'frame', '.', 'If', 'multiple', 'frames', 'are', 'necessary', 'to', 'send', 'the', 'message', 'this', 'socket', 'will', 'unable', 'to', 'send', 'other', 'messages', 'until', 'either', 'the', 'transmission', 'of', 'this', 'frame', 'succeeds', 'or', 'it', 'fails', '.']",python,X,2,True,1,train
6131,secdev/scapy,scapy/layers/tls/automaton_cli.py,https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/layers/tls/automaton_cli.py#L312-L328,"def should_add_ClientCertificate(self):
        """"""
        If the server sent a CertificateRequest, we send a Certificate message.
        If no certificate is available, an empty Certificate message is sent:
        - this is a SHOULD in RFC 4346 (Section 7.4.6)
        - this is a MUST in RFC 5246 (Section 7.4.6)

        XXX We may want to add a complete chain.
        """"""
        hs_msg = [type(m) for m in self.cur_session.handshake_messages_parsed]
        if TLSCertificateRequest not in hs_msg:
            return
        certs = []
        if self.mycert:
            certs = [self.mycert]
        self.add_msg(TLSCertificate(certs=certs))
        raise self.ADDED_CLIENTCERTIFICATE()","['def', 'should_add_ClientCertificate', '(', 'self', ')', ':', 'hs_msg', '=', '[', 'type', '(', 'm', ')', 'for', 'm', 'in', 'self', '.', 'cur_session', '.', 'handshake_messages_parsed', ']', 'if', 'TLSCertificateRequest', 'not', 'in', 'hs_msg', ':', 'return', 'certs', '=', '[', ']', 'if', 'self', '.', 'mycert', ':', 'certs', '=', '[', 'self', '.', 'mycert', ']', 'self', '.', 'add_msg', '(', 'TLSCertificate', '(', 'certs', '=', 'certs', ')', ')', 'raise', 'self', '.', 'ADDED_CLIENTCERTIFICATE', '(', ')']","If the server sent a CertificateRequest, we send a Certificate message.
        If no certificate is available, an empty Certificate message is sent:
        - this is a SHOULD in RFC 4346 (Section 7.4.6)
        - this is a MUST in RFC 5246 (Section 7.4.6)

        XXX We may want to add a complete chain.","['If', 'the', 'server', 'sent', 'a', 'CertificateRequest', 'we', 'send', 'a', 'Certificate', 'message', '.', 'If', 'no', 'certificate', 'is', 'available', 'an', 'empty', 'Certificate', 'message', 'is', 'sent', ':', '-', 'this', 'is', 'a', 'SHOULD', 'in', 'RFC', '4346', '(', 'Section', '7', '.', '4', '.', '6', ')', '-', 'this', 'is', 'a', 'MUST', 'in', 'RFC', '5246', '(', 'Section', '7', '.', '4', '.', '6', ')']",python,X,2,True,1,train
7083,eternnoir/pyTelegramBotAPI,telebot/__init__.py,https://github.com/eternnoir/pyTelegramBotAPI/blob/47b53b88123097f1b9562a6cd5d4e080b86185d1/telebot/__init__.py#L639-L656,"def send_audio(self, chat_id, audio, caption=None, duration=None, performer=None, title=None,
                   reply_to_message_id=None, reply_markup=None, parse_mode=None, disable_notification=None,
                   timeout=None):
        """"""
        Use this method to send audio files, if you want Telegram clients to display them in the music player. Your audio must be in the .mp3 format.
        :param chat_id:Unique identifier for the message recipient
        :param audio:Audio file to send.
        :param duration:Duration of the audio in seconds
        :param performer:Performer
        :param title:Track name
        :param parse_mode
        :param reply_to_message_id:If the message is a reply, ID of the original message
        :param reply_markup:
        :return: Message
        """"""
        return types.Message.de_json(
            apihelper.send_audio(self.token, chat_id, audio, caption, duration, performer, title, reply_to_message_id,
                                 reply_markup, parse_mode, disable_notification, timeout))","['def', 'send_audio', '(', 'self', ',', 'chat_id', ',', 'audio', ',', 'caption', '=', 'None', ',', 'duration', '=', 'None', ',', 'performer', '=', 'None', ',', 'title', '=', 'None', ',', 'reply_to_message_id', '=', 'None', ',', 'reply_markup', '=', 'None', ',', 'parse_mode', '=', 'None', ',', 'disable_notification', '=', 'None', ',', 'timeout', '=', 'None', ')', ':', 'return', 'types', '.', 'Message', '.', 'de_json', '(', 'apihelper', '.', 'send_audio', '(', 'self', '.', 'token', ',', 'chat_id', ',', 'audio', ',', 'caption', ',', 'duration', ',', 'performer', ',', 'title', ',', 'reply_to_message_id', ',', 'reply_markup', ',', 'parse_mode', ',', 'disable_notification', ',', 'timeout', ')', ')']","Use this method to send audio files, if you want Telegram clients to display them in the music player. Your audio must be in the .mp3 format.
        :param chat_id:Unique identifier for the message recipient
        :param audio:Audio file to send.
        :param duration:Duration of the audio in seconds
        :param performer:Performer
        :param title:Track name
        :param parse_mode
        :param reply_to_message_id:If the message is a reply, ID of the original message
        :param reply_markup:
        :return: Message","['Use', 'this', 'method', 'to', 'send', 'audio', 'files', 'if', 'you', 'want', 'Telegram', 'clients', 'to', 'display', 'them', 'in', 'the', 'music', 'player', '.', 'Your', 'audio', 'must', 'be', 'in', 'the', '.', 'mp3', 'format', '.', ':', 'param', 'chat_id', ':', 'Unique', 'identifier', 'for', 'the', 'message', 'recipient', ':', 'param', 'audio', ':', 'Audio', 'file', 'to', 'send', '.', ':', 'param', 'duration', ':', 'Duration', 'of', 'the', 'audio', 'in', 'seconds', ':', 'param', 'performer', ':', 'Performer', ':', 'param', 'title', ':', 'Track', 'name', ':', 'param', 'parse_mode', ':', 'param', 'reply_to_message_id', ':', 'If', 'the', 'message', 'is', 'a', 'reply', 'ID', 'of', 'the', 'original', 'message', ':', 'param', 'reply_markup', ':', ':', 'return', ':', 'Message']",python,X,2,True,1,train
7087,eternnoir/pyTelegramBotAPI,telebot/__init__.py,https://github.com/eternnoir/pyTelegramBotAPI/blob/47b53b88123097f1b9562a6cd5d4e080b86185d1/telebot/__init__.py#L704-L720,"def send_video(self, chat_id, data, duration=None, caption=None, reply_to_message_id=None, reply_markup=None,
                   parse_mode=None, supports_streaming=None, disable_notification=None, timeout=None):
        """"""
        Use this method to send video files, Telegram clients support mp4 videos.
        :param chat_id: Integer : Unique identifier for the message recipient  User or GroupChat id
        :param data: InputFile or String : Video to send. You can either pass a file_id as String to resend a video that is already on the Telegram server
        :param duration: Integer : Duration of sent video in seconds
        :param caption: String : Video caption (may also be used when resending videos by file_id).
        :param parse_mode:
        :param supports_streaming:
        :param reply_to_message_id:
        :param reply_markup:
        :return:
        """"""
        return types.Message.de_json(
            apihelper.send_video(self.token, chat_id, data, duration, caption, reply_to_message_id, reply_markup,
                                 parse_mode, supports_streaming, disable_notification, timeout))","['def', 'send_video', '(', 'self', ',', 'chat_id', ',', 'data', ',', 'duration', '=', 'None', ',', 'caption', '=', 'None', ',', 'reply_to_message_id', '=', 'None', ',', 'reply_markup', '=', 'None', ',', 'parse_mode', '=', 'None', ',', 'supports_streaming', '=', 'None', ',', 'disable_notification', '=', 'None', ',', 'timeout', '=', 'None', ')', ':', 'return', 'types', '.', 'Message', '.', 'de_json', '(', 'apihelper', '.', 'send_video', '(', 'self', '.', 'token', ',', 'chat_id', ',', 'data', ',', 'duration', ',', 'caption', ',', 'reply_to_message_id', ',', 'reply_markup', ',', 'parse_mode', ',', 'supports_streaming', ',', 'disable_notification', ',', 'timeout', ')', ')']","Use this method to send video files, Telegram clients support mp4 videos.
        :param chat_id: Integer : Unique identifier for the message recipient  User or GroupChat id
        :param data: InputFile or String : Video to send. You can either pass a file_id as String to resend a video that is already on the Telegram server
        :param duration: Integer : Duration of sent video in seconds
        :param caption: String : Video caption (may also be used when resending videos by file_id).
        :param parse_mode:
        :param supports_streaming:
        :param reply_to_message_id:
        :param reply_markup:
        :return:","['Use', 'this', 'method', 'to', 'send', 'video', 'files', 'Telegram', 'clients', 'support', 'mp4', 'videos', '.', ':', 'param', 'chat_id', ':', 'Integer', ':', 'Unique', 'identifier', 'for', 'the', 'message', 'recipient', '', 'User', 'or', 'GroupChat', 'id', ':', 'param', 'data', ':', 'InputFile', 'or', 'String', ':', 'Video', 'to', 'send', '.', 'You', 'can', 'either', 'pass', 'a', 'file_id', 'as', 'String', 'to', 'resend', 'a', 'video', 'that', 'is', 'already', 'on', 'the', 'Telegram', 'server', ':', 'param', 'duration', ':', 'Integer', ':', 'Duration', 'of', 'sent', 'video', 'in', 'seconds', ':', 'param', 'caption', ':', 'String', ':', 'Video', 'caption', '(', 'may', 'also', 'be', 'used', 'when', 'resending', 'videos', 'by', 'file_id', ')', '.', ':', 'param', 'parse_mode', ':', ':', 'param', 'supports_streaming', ':', ':', 'param', 'reply_to_message_id', ':', ':', 'param', 'reply_markup', ':', ':', 'return', ':']",python,X,2,True,1,train
7088,eternnoir/pyTelegramBotAPI,telebot/__init__.py,https://github.com/eternnoir/pyTelegramBotAPI/blob/47b53b88123097f1b9562a6cd5d4e080b86185d1/telebot/__init__.py#L722-L736,"def send_video_note(self, chat_id, data, duration=None, length=None, reply_to_message_id=None, reply_markup=None,
                        disable_notification=None, timeout=None):
        """"""
        Use this method to send video files, Telegram clients support mp4 videos.
        :param chat_id: Integer : Unique identifier for the message recipient  User or GroupChat id
        :param data: InputFile or String : Video note to send. You can either pass a file_id as String to resend a video that is already on the Telegram server
        :param duration: Integer : Duration of sent video in seconds
        :param length: Integer : Video width and height, Can't be None and should be in range of (0, 640)
        :param reply_to_message_id:
        :param reply_markup:
        :return:
        """"""
        return types.Message.de_json(
            apihelper.send_video_note(self.token, chat_id, data, duration, length, reply_to_message_id, reply_markup,
                                      disable_notification, timeout))","['def', 'send_video_note', '(', 'self', ',', 'chat_id', ',', 'data', ',', 'duration', '=', 'None', ',', 'length', '=', 'None', ',', 'reply_to_message_id', '=', 'None', ',', 'reply_markup', '=', 'None', ',', 'disable_notification', '=', 'None', ',', 'timeout', '=', 'None', ')', ':', 'return', 'types', '.', 'Message', '.', 'de_json', '(', 'apihelper', '.', 'send_video_note', '(', 'self', '.', 'token', ',', 'chat_id', ',', 'data', ',', 'duration', ',', 'length', ',', 'reply_to_message_id', ',', 'reply_markup', ',', 'disable_notification', ',', 'timeout', ')', ')']","Use this method to send video files, Telegram clients support mp4 videos.
        :param chat_id: Integer : Unique identifier for the message recipient  User or GroupChat id
        :param data: InputFile or String : Video note to send. You can either pass a file_id as String to resend a video that is already on the Telegram server
        :param duration: Integer : Duration of sent video in seconds
        :param length: Integer : Video width and height, Can't be None and should be in range of (0, 640)
        :param reply_to_message_id:
        :param reply_markup:
        :return:","['Use', 'this', 'method', 'to', 'send', 'video', 'files', 'Telegram', 'clients', 'support', 'mp4', 'videos', '.', ':', 'param', 'chat_id', ':', 'Integer', ':', 'Unique', 'identifier', 'for', 'the', 'message', 'recipient', '', 'User', 'or', 'GroupChat', 'id', ':', 'param', 'data', ':', 'InputFile', 'or', 'String', ':', 'Video', 'note', 'to', 'send', '.', 'You', 'can', 'either', 'pass', 'a', 'file_id', 'as', 'String', 'to', 'resend', 'a', 'video', 'that', 'is', 'already', 'on', 'the', 'Telegram', 'server', ':', 'param', 'duration', ':', 'Integer', ':', 'Duration', 'of', 'sent', 'video', 'in', 'seconds', ':', 'param', 'length', ':', 'Integer', ':', 'Video', 'width', 'and', 'height', 'Can', 't', 'be', 'None', 'and', 'should', 'be', 'in', 'range', 'of', '(', '0', '640', ')', ':', 'param', 'reply_to_message_id', ':', ':', 'param', 'reply_markup', ':', ':', 'return', ':']",python,X,2,True,1,train
7103,eternnoir/pyTelegramBotAPI,telebot/__init__.py,https://github.com/eternnoir/pyTelegramBotAPI/blob/47b53b88123097f1b9562a6cd5d4e080b86185d1/telebot/__init__.py#L1067-L1083,"def answer_inline_query(self, inline_query_id, results, cache_time=None, is_personal=None, next_offset=None,
                            switch_pm_text=None, switch_pm_parameter=None):
        """"""
        Use this method to send answers to an inline query. On success, True is returned.
        No more than 50 results per query are allowed.
        :param inline_query_id: Unique identifier for the answered query
        :param results: Array of results for the inline query
        :param cache_time: The maximum amount of time in seconds that the result of the inline query may be cached on the server.
        :param is_personal: Pass True, if results may be cached on the server side only for the user that sent the query.
        :param next_offset: Pass the offset that a client should send in the next query with the same text to receive more results.
        :param switch_pm_parameter: If passed, clients will display a button with specified text that switches the user
         to a private chat with the bot and sends the bot a start message with the parameter switch_pm_parameter
        :param switch_pm_text: 	Parameter for the start message sent to the bot when user presses the switch button
        :return: True means success.
        """"""
        return apihelper.answer_inline_query(self.token, inline_query_id, results, cache_time, is_personal, next_offset,
                                             switch_pm_text, switch_pm_parameter)","['def', 'answer_inline_query', '(', 'self', ',', 'inline_query_id', ',', 'results', ',', 'cache_time', '=', 'None', ',', 'is_personal', '=', 'None', ',', 'next_offset', '=', 'None', ',', 'switch_pm_text', '=', 'None', ',', 'switch_pm_parameter', '=', 'None', ')', ':', 'return', 'apihelper', '.', 'answer_inline_query', '(', 'self', '.', 'token', ',', 'inline_query_id', ',', 'results', ',', 'cache_time', ',', 'is_personal', ',', 'next_offset', ',', 'switch_pm_text', ',', 'switch_pm_parameter', ')']","Use this method to send answers to an inline query. On success, True is returned.
        No more than 50 results per query are allowed.
        :param inline_query_id: Unique identifier for the answered query
        :param results: Array of results for the inline query
        :param cache_time: The maximum amount of time in seconds that the result of the inline query may be cached on the server.
        :param is_personal: Pass True, if results may be cached on the server side only for the user that sent the query.
        :param next_offset: Pass the offset that a client should send in the next query with the same text to receive more results.
        :param switch_pm_parameter: If passed, clients will display a button with specified text that switches the user
         to a private chat with the bot and sends the bot a start message with the parameter switch_pm_parameter
        :param switch_pm_text: 	Parameter for the start message sent to the bot when user presses the switch button
        :return: True means success.","['Use', 'this', 'method', 'to', 'send', 'answers', 'to', 'an', 'inline', 'query', '.', 'On', 'success', 'True', 'is', 'returned', '.', 'No', 'more', 'than', '50', 'results', 'per', 'query', 'are', 'allowed', '.', ':', 'param', 'inline_query_id', ':', 'Unique', 'identifier', 'for', 'the', 'answered', 'query', ':', 'param', 'results', ':', 'Array', 'of', 'results', 'for', 'the', 'inline', 'query', ':', 'param', 'cache_time', ':', 'The', 'maximum', 'amount', 'of', 'time', 'in', 'seconds', 'that', 'the', 'result', 'of', 'the', 'inline', 'query', 'may', 'be', 'cached', 'on', 'the', 'server', '.', ':', 'param', 'is_personal', ':', 'Pass', 'True', 'if', 'results', 'may', 'be', 'cached', 'on', 'the', 'server', 'side', 'only', 'for', 'the', 'user', 'that', 'sent', 'the', 'query', '.', ':', 'param', 'next_offset', ':', 'Pass', 'the', 'offset', 'that', 'a', 'client', 'should', 'send', 'in', 'the', 'next', 'query', 'with', 'the', 'same', 'text', 'to', 'receive', 'more', 'results', '.', ':', 'param', 'switch_pm_parameter', ':', 'If', 'passed', 'clients', 'will', 'display', 'a', 'button', 'with', 'specified', 'text', 'that', 'switches', 'the', 'user', 'to', 'a', 'private', 'chat', 'with', 'the', 'bot', 'and', 'sends', 'the', 'bot', 'a', 'start', 'message', 'with', 'the', 'parameter', 'switch_pm_parameter', ':', 'param', 'switch_pm_text', ':', 'Parameter', 'for', 'the', 'start', 'message', 'sent', 'to', 'the', 'bot', 'when', 'user', 'presses', 'the', 'switch', 'button', ':', 'return', ':', 'True', 'means', 'success', '.']",python,X,2,True,1,train
15709,openthread/openthread,tools/harness-thci/OpenThread.py,https://github.com/openthread/openthread/blob/0208d10563aa21c518092985c78ecf9cd223ab74/tools/harness-thci/OpenThread.py#L2173-L2196,"def MGMT_PANID_QUERY(self, sAddr, xCommissionerSessionId, listChannelMask, xPanId):
        """"""send MGMT_PANID_QUERY message to a given destination

        Args:
            xPanId: a given PAN ID to check the conflicts

        Returns:
            True: successful to send MGMT_PANID_QUERY message.
            False: fail to send MGMT_PANID_QUERY message.
        """"""
        print '%s call MGMT_PANID_QUERY' % self.port
        panid = ''
        channelMask = ''
        channelMask = '0x' + self.__convertLongToString(self.__convertChannelMask(listChannelMask))

        if not isinstance(xPanId, str):
            panid = str(hex(xPanId))

        try:
            cmd = 'commissioner panid %s %s %s' % (panid, channelMask, sAddr)
            print cmd
            return self.__sendCommand(cmd) == 'Done'
        except Exception, e:
            ModuleHelper.writeintodebuglogger(""MGMT_PANID_QUERY() error: "" + str(e))","['def', 'MGMT_PANID_QUERY', '(', 'self', ',', 'sAddr', ',', 'xCommissionerSessionId', ',', 'listChannelMask', ',', 'xPanId', ')', ':', 'print', ""'%s call MGMT_PANID_QUERY'"", '%', 'self', '.', 'port', 'panid', '=', ""''"", 'channelMask', '=', ""''"", 'channelMask', '=', ""'0x'"", '+', 'self', '.', '__convertLongToString', '(', 'self', '.', '__convertChannelMask', '(', 'listChannelMask', ')', ')', 'if', 'not', 'isinstance', '(', 'xPanId', ',', 'str', ')', ':', 'panid', '=', 'str', '(', 'hex', '(', 'xPanId', ')', ')', 'try', ':', 'cmd', '=', ""'commissioner panid %s %s %s'"", '%', '(', 'panid', ',', 'channelMask', ',', 'sAddr', ')', 'print', 'cmd', 'return', 'self', '.', '__sendCommand', '(', 'cmd', ')', '==', ""'Done'"", 'except', 'Exception', ',', 'e', ':', 'ModuleHelper', '.', 'writeintodebuglogger', '(', '""MGMT_PANID_QUERY() error: ""', '+', 'str', '(', 'e', ')', ')']","send MGMT_PANID_QUERY message to a given destination

        Args:
            xPanId: a given PAN ID to check the conflicts

        Returns:
            True: successful to send MGMT_PANID_QUERY message.
            False: fail to send MGMT_PANID_QUERY message.","['send', 'MGMT_PANID_QUERY', 'message', 'to', 'a', 'given', 'destination']",python,X,2,True,1,train
15778,openthread/openthread,tools/harness-thci/OpenThread_WpanCtl.py,https://github.com/openthread/openthread/blob/0208d10563aa21c518092985c78ecf9cd223ab74/tools/harness-thci/OpenThread_WpanCtl.py#L2196-L2219,"def MGMT_PANID_QUERY(self, sAddr, xCommissionerSessionId, listChannelMask, xPanId):
        """"""send MGMT_PANID_QUERY message to a given destination

        Args:
            xPanId: a given PAN ID to check the conflicts

        Returns:
            True: successful to send MGMT_PANID_QUERY message.
            False: fail to send MGMT_PANID_QUERY message.
        """"""
        print '%s call MGMT_PANID_QUERY' % self.port
        panid = ''
        channelMask = ''
        channelMask = self.__ChannelMaskListToStr(listChannelMask)

        if not isinstance(xPanId, str):
            panid = str(hex(xPanId))

        try:
            cmd = WPANCTL_CMD + 'commissioner pan-id-query %s %s %s' % (panid, channelMask, sAddr)
            print cmd
            return self.__sendCommand(cmd) != 'Fail'
        except Exception, e:
            ModuleHelper.WriteIntoDebugLogger('MGMT_PANID_QUERY() error: ' + str(e))","['def', 'MGMT_PANID_QUERY', '(', 'self', ',', 'sAddr', ',', 'xCommissionerSessionId', ',', 'listChannelMask', ',', 'xPanId', ')', ':', 'print', ""'%s call MGMT_PANID_QUERY'"", '%', 'self', '.', 'port', 'panid', '=', ""''"", 'channelMask', '=', ""''"", 'channelMask', '=', 'self', '.', '__ChannelMaskListToStr', '(', 'listChannelMask', ')', 'if', 'not', 'isinstance', '(', 'xPanId', ',', 'str', ')', ':', 'panid', '=', 'str', '(', 'hex', '(', 'xPanId', ')', ')', 'try', ':', 'cmd', '=', 'WPANCTL_CMD', '+', ""'commissioner pan-id-query %s %s %s'"", '%', '(', 'panid', ',', 'channelMask', ',', 'sAddr', ')', 'print', 'cmd', 'return', 'self', '.', '__sendCommand', '(', 'cmd', ')', '!=', ""'Fail'"", 'except', 'Exception', ',', 'e', ':', 'ModuleHelper', '.', 'WriteIntoDebugLogger', '(', ""'MGMT_PANID_QUERY() error: '"", '+', 'str', '(', 'e', ')', ')']","send MGMT_PANID_QUERY message to a given destination

        Args:
            xPanId: a given PAN ID to check the conflicts

        Returns:
            True: successful to send MGMT_PANID_QUERY message.
            False: fail to send MGMT_PANID_QUERY message.","['send', 'MGMT_PANID_QUERY', 'message', 'to', 'a', 'given', 'destination']",python,X,2,True,1,train
15779,openthread/openthread,tools/harness-thci/OpenThread_WpanCtl.py,https://github.com/openthread/openthread/blob/0208d10563aa21c518092985c78ecf9cd223ab74/tools/harness-thci/OpenThread_WpanCtl.py#L2221-L2236,"def MGMT_ANNOUNCE_BEGIN(self, sAddr, xCommissionerSessionId, listChannelMask, xCount, xPeriod):
        """"""send MGMT_ANNOUNCE_BEGIN message to a given destination

        Returns:
            True: successful to send MGMT_ANNOUNCE_BEGIN message.
            False: fail to send MGMT_ANNOUNCE_BEGIN message.
        """"""
        print '%s call MGMT_ANNOUNCE_BEGIN' % self.port
        channelMask = ''
        channelMask = self.__ChannelMaskListToStr(listChannelMask)
        try:
            cmd = WPANCTL_CMD + 'commissioner announce-begin %s %s %s %s' % (channelMask, xCount, xPeriod, sAddr)
            print cmd
            return self.__sendCommand(cmd) != 'Fail'
        except Exception, e:
            ModuleHelper.WriteIntoDebugLogger('MGMT_ANNOUNCE_BEGIN() error: ' + str(e))","['def', 'MGMT_ANNOUNCE_BEGIN', '(', 'self', ',', 'sAddr', ',', 'xCommissionerSessionId', ',', 'listChannelMask', ',', 'xCount', ',', 'xPeriod', ')', ':', 'print', ""'%s call MGMT_ANNOUNCE_BEGIN'"", '%', 'self', '.', 'port', 'channelMask', '=', ""''"", 'channelMask', '=', 'self', '.', '__ChannelMaskListToStr', '(', 'listChannelMask', ')', 'try', ':', 'cmd', '=', 'WPANCTL_CMD', '+', ""'commissioner announce-begin %s %s %s %s'"", '%', '(', 'channelMask', ',', 'xCount', ',', 'xPeriod', ',', 'sAddr', ')', 'print', 'cmd', 'return', 'self', '.', '__sendCommand', '(', 'cmd', ')', '!=', ""'Fail'"", 'except', 'Exception', ',', 'e', ':', 'ModuleHelper', '.', 'WriteIntoDebugLogger', '(', ""'MGMT_ANNOUNCE_BEGIN() error: '"", '+', 'str', '(', 'e', ')', ')']","send MGMT_ANNOUNCE_BEGIN message to a given destination

        Returns:
            True: successful to send MGMT_ANNOUNCE_BEGIN message.
            False: fail to send MGMT_ANNOUNCE_BEGIN message.","['send', 'MGMT_ANNOUNCE_BEGIN', 'message', 'to', 'a', 'given', 'destination']",python,X,2,True,1,train
17078,google/grr,grr/server/grr_response_server/flow_base.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/flow_base.py#L324-L359,"def SendReply(self, response, tag=None):
    """"""Allows this flow to send a message to its parent flow.

    If this flow does not have a parent, the message is ignored.

    Args:
      response: An RDFValue() instance to be sent to the parent.
      tag: If specified, tag the result with this tag.

    Raises:
      ValueError: If responses is not of the correct type.
    """"""
    if not isinstance(response, rdfvalue.RDFValue):
      raise ValueError(""SendReply can only send RDFValues"")

    if self.rdf_flow.parent_flow_id:
      response = rdf_flow_objects.FlowResponse(
          client_id=self.rdf_flow.client_id,
          request_id=self.rdf_flow.parent_request_id,
          response_id=self.GetNextResponseId(),
          payload=response,
          flow_id=self.rdf_flow.parent_flow_id,
          tag=tag)

      self.flow_responses.append(response)
    else:
      reply = rdf_flow_objects.FlowResult(
          client_id=self.rdf_flow.client_id,
          flow_id=self.rdf_flow.flow_id,
          hunt_id=self.rdf_flow.parent_hunt_id,
          payload=response,
          tag=tag)
      self.replies_to_write.append(reply)
      self.replies_to_process.append(reply)

    self.rdf_flow.num_replies_sent += 1","['def', 'SendReply', '(', 'self', ',', 'response', ',', 'tag', '=', 'None', ')', ':', 'if', 'not', 'isinstance', '(', 'response', ',', 'rdfvalue', '.', 'RDFValue', ')', ':', 'raise', 'ValueError', '(', '""SendReply can only send RDFValues""', ')', 'if', 'self', '.', 'rdf_flow', '.', 'parent_flow_id', ':', 'response', '=', 'rdf_flow_objects', '.', 'FlowResponse', '(', 'client_id', '=', 'self', '.', 'rdf_flow', '.', 'client_id', ',', 'request_id', '=', 'self', '.', 'rdf_flow', '.', 'parent_request_id', ',', 'response_id', '=', 'self', '.', 'GetNextResponseId', '(', ')', ',', 'payload', '=', 'response', ',', 'flow_id', '=', 'self', '.', 'rdf_flow', '.', 'parent_flow_id', ',', 'tag', '=', 'tag', ')', 'self', '.', 'flow_responses', '.', 'append', '(', 'response', ')', 'else', ':', 'reply', '=', 'rdf_flow_objects', '.', 'FlowResult', '(', 'client_id', '=', 'self', '.', 'rdf_flow', '.', 'client_id', ',', 'flow_id', '=', 'self', '.', 'rdf_flow', '.', 'flow_id', ',', 'hunt_id', '=', 'self', '.', 'rdf_flow', '.', 'parent_hunt_id', ',', 'payload', '=', 'response', ',', 'tag', '=', 'tag', ')', 'self', '.', 'replies_to_write', '.', 'append', '(', 'reply', ')', 'self', '.', 'replies_to_process', '.', 'append', '(', 'reply', ')', 'self', '.', 'rdf_flow', '.', 'num_replies_sent', '+=', '1']","Allows this flow to send a message to its parent flow.

    If this flow does not have a parent, the message is ignored.

    Args:
      response: An RDFValue() instance to be sent to the parent.
      tag: If specified, tag the result with this tag.

    Raises:
      ValueError: If responses is not of the correct type.","['Allows', 'this', 'flow', 'to', 'send', 'a', 'message', 'to', 'its', 'parent', 'flow', '.']",python,X,2,True,1,train
17375,google/grr,grr/server/grr_response_server/flow_runner.py,https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/flow_runner.py#L774-L819,"def SendReply(self, response, tag=None):
    """"""Allows this flow to send a message to its parent flow.

    If this flow does not have a parent, the message is ignored.

    Args:
      response: An RDFValue() instance to be sent to the parent.
      tag: If specified, tag the result with the following tag. NOTE: supported
        in REL_DB implementation only.

    Raises:
      ValueError: If responses is not of the correct type.
    """"""
    del tag

    if not isinstance(response, rdfvalue.RDFValue):
      raise ValueError(""SendReply can only send a Semantic Value"")

    # Only send the reply if we have a parent, indicated by knowing our parent's
    # request state.
    if self.runner_args.request_state.session_id:

      request_state = self.runner_args.request_state

      request_state.response_count += 1

      # Make a response message
      msg = rdf_flows.GrrMessage(
          session_id=request_state.session_id,
          request_id=request_state.id,
          response_id=request_state.response_count,
          auth_state=rdf_flows.GrrMessage.AuthorizationState.AUTHENTICATED,
          type=rdf_flows.GrrMessage.Type.MESSAGE,
          payload=response,
          args_rdf_name=response.__class__.__name__,
          args_age=int(response.age))

      # Queue the response now
      self.queue_manager.QueueResponse(msg)

      if self.runner_args.write_intermediate_results:
        self.QueueReplyForResultCollection(response)

    else:
      # Only write the reply to the collection if we are the parent flow.
      self.QueueReplyForResultCollection(response)","['def', 'SendReply', '(', 'self', ',', 'response', ',', 'tag', '=', 'None', ')', ':', 'del', 'tag', 'if', 'not', 'isinstance', '(', 'response', ',', 'rdfvalue', '.', 'RDFValue', ')', ':', 'raise', 'ValueError', '(', '""SendReply can only send a Semantic Value""', ')', ""# Only send the reply if we have a parent, indicated by knowing our parent's"", '# request state.', 'if', 'self', '.', 'runner_args', '.', 'request_state', '.', 'session_id', ':', 'request_state', '=', 'self', '.', 'runner_args', '.', 'request_state', 'request_state', '.', 'response_count', '+=', '1', '# Make a response message', 'msg', '=', 'rdf_flows', '.', 'GrrMessage', '(', 'session_id', '=', 'request_state', '.', 'session_id', ',', 'request_id', '=', 'request_state', '.', 'id', ',', 'response_id', '=', 'request_state', '.', 'response_count', ',', 'auth_state', '=', 'rdf_flows', '.', 'GrrMessage', '.', 'AuthorizationState', '.', 'AUTHENTICATED', ',', 'type', '=', 'rdf_flows', '.', 'GrrMessage', '.', 'Type', '.', 'MESSAGE', ',', 'payload', '=', 'response', ',', 'args_rdf_name', '=', 'response', '.', '__class__', '.', '__name__', ',', 'args_age', '=', 'int', '(', 'response', '.', 'age', ')', ')', '# Queue the response now', 'self', '.', 'queue_manager', '.', 'QueueResponse', '(', 'msg', ')', 'if', 'self', '.', 'runner_args', '.', 'write_intermediate_results', ':', 'self', '.', 'QueueReplyForResultCollection', '(', 'response', ')', 'else', ':', '# Only write the reply to the collection if we are the parent flow.', 'self', '.', 'QueueReplyForResultCollection', '(', 'response', ')']","Allows this flow to send a message to its parent flow.

    If this flow does not have a parent, the message is ignored.

    Args:
      response: An RDFValue() instance to be sent to the parent.
      tag: If specified, tag the result with the following tag. NOTE: supported
        in REL_DB implementation only.

    Raises:
      ValueError: If responses is not of the correct type.","['Allows', 'this', 'flow', 'to', 'send', 'a', 'message', 'to', 'its', 'parent', 'flow', '.']",python,X,2,True,1,train
17998,hugapi/hug,hug/use.py,https://github.com/hugapi/hug/blob/080901c81576657f82e2432fd4a82f1d0d2f370c/hug/use.py#L251-L267,"def request(self, message, timeout=False, *args, **kwargs):
        """"""Populate connection pool, send message, return BytesIO, and cleanup""""""
        if not self.connection_pool.full():
            self.connection_pool.put(self._register_socket())

        _socket = self.connection_pool.get()

        # setting timeout to None enables the socket to block.
        if timeout or timeout is None:
            _socket.settimeout(timeout)

        data = self.send_and_receive(_socket, message, *args, **kwargs)

        if self.connection.proto in Socket.streams:
            _socket.shutdown(socket.SHUT_RDWR)

        return Response(data, None, None)","['def', 'request', '(', 'self', ',', 'message', ',', 'timeout', '=', 'False', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', 'not', 'self', '.', 'connection_pool', '.', 'full', '(', ')', ':', 'self', '.', 'connection_pool', '.', 'put', '(', 'self', '.', '_register_socket', '(', ')', ')', '_socket', '=', 'self', '.', 'connection_pool', '.', 'get', '(', ')', '# setting timeout to None enables the socket to block.', 'if', 'timeout', 'or', 'timeout', 'is', 'None', ':', '_socket', '.', 'settimeout', '(', 'timeout', ')', 'data', '=', 'self', '.', 'send_and_receive', '(', '_socket', ',', 'message', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', 'if', 'self', '.', 'connection', '.', 'proto', 'in', 'Socket', '.', 'streams', ':', '_socket', '.', 'shutdown', '(', 'socket', '.', 'SHUT_RDWR', ')', 'return', 'Response', '(', 'data', ',', 'None', ',', 'None', ')']","Populate connection pool, send message, return BytesIO, and cleanup","['Populate', 'connection', 'pool', 'send', 'message', 'return', 'BytesIO', 'and', 'cleanup']",python,X,2,True,1,train
23118,slackapi/python-slackclient,tutorial/PythOnBoardingBot/app.py,https://github.com/slackapi/python-slackclient/blob/901341c0284fd81e6d2719d6a0502308760d83e4/tutorial/PythOnBoardingBot/app.py#L38-L53,"def onboarding_message(**payload):
    """"""Create and send an onboarding welcome message to new users. Save the
    time stamp of this message so we can update this message in the future.
    """"""
    # Get WebClient so you can communicate back to Slack.
    web_client = payload[""web_client""]

    # Get the id of the Slack user associated with the incoming event
    user_id = payload[""data""][""user""][""id""]

    # Open a DM with the new user.
    response = web_client.im_open(user_id)
    channel = response[""channel""][""id""]

    # Post the onboarding message.
    start_onboarding(web_client, user_id, channel)","['def', 'onboarding_message', '(', '*', '*', 'payload', ')', ':', '# Get WebClient so you can communicate back to Slack.', 'web_client', '=', 'payload', '[', '""web_client""', ']', '# Get the id of the Slack user associated with the incoming event', 'user_id', '=', 'payload', '[', '""data""', ']', '[', '""user""', ']', '[', '""id""', ']', '# Open a DM with the new user.', 'response', '=', 'web_client', '.', 'im_open', '(', 'user_id', ')', 'channel', '=', 'response', '[', '""channel""', ']', '[', '""id""', ']', '# Post the onboarding message.', 'start_onboarding', '(', 'web_client', ',', 'user_id', ',', 'channel', ')']","Create and send an onboarding welcome message to new users. Save the
    time stamp of this message so we can update this message in the future.","['Create', 'and', 'send', 'an', 'onboarding', 'welcome', 'message', 'to', 'new', 'users', '.', 'Save', 'the', 'time', 'stamp', 'of', 'this', 'message', 'so', 'we', 'can', 'update', 'this', 'message', 'in', 'the', 'future', '.']",python,X,2,True,1,train
26370,osrg/ryu,ryu/services/protocols/bgp/speaker.py,https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/services/protocols/bgp/speaker.py#L442-L512,"def _handle_msg(self, msg):
        """"""When a BGP message is received, send it to peer.

        Open messages are validated here. Peer handler is called to handle each
        message except for *Open* and *Notification* message. On receiving
        *Notification* message we close connection with peer.
        """"""
        LOG.debug('Received msg from %s << %s', self._remotename, msg)

        # If we receive open message we try to bind to protocol
        if msg.type == BGP_MSG_OPEN:
            if self.state == BGP_FSM_OPEN_SENT:
                # Validate open message.
                self._validate_open_msg(msg)
                self.recv_open_msg = msg
                self.state = BGP_FSM_OPEN_CONFIRM
                self._peer.state.bgp_state = self.state

                # Try to bind this protocol to peer.
                self._is_bound = self._peer.bind_protocol(self)

                # If this protocol failed to bind to peer.
                if not self._is_bound:
                    # Failure to bind to peer indicates connection collision
                    # resolution choose different instance of protocol and this
                    # instance has to close. Before closing it sends
                    # appropriate notification msg. to peer.
                    raise bgp.CollisionResolution()

                # If peer sends Hold Time as zero, then according to RFC we do
                # not set Hold Time and Keep Alive timer.
                if msg.hold_time == 0:
                    LOG.info('The Hold Time sent by the peer is zero, hence '
                             'not setting any Hold Time and Keep Alive'
                             ' timers.')
                else:
                    # Start Keep Alive timer considering Hold Time preference
                    # of the peer.
                    self._start_timers(msg.hold_time)
                    self._send_keepalive()

                # Peer does not see open message.
                return
            else:
                # If we receive a Open message out of order
                LOG.error('Open message received when current state is not '
                          'OpenSent')
                # Received out-of-order open message
                # We raise Finite state machine error
                raise bgp.FiniteStateMachineError()
        elif msg.type == BGP_MSG_NOTIFICATION:
            if self._peer:
                self._signal_bus.bgp_notification_received(self._peer, msg)
            # If we receive notification message
            LOG.error('Received notification message, hence closing '
                      'connection %s', msg)
            self._socket.close()
            return

        # If we receive keepalive or update message, we reset expire timer.
        if (msg.type == BGP_MSG_KEEPALIVE or
                msg.type == BGP_MSG_UPDATE):
            if self._expiry:
                self._expiry.reset()

        # Call peer message handler for appropriate messages.
        if (msg.type in
                (BGP_MSG_UPDATE, BGP_MSG_KEEPALIVE, BGP_MSG_ROUTE_REFRESH)):
            self._peer.handle_msg(msg)
        # We give chance to other threads to run.
        self.pause(0)","['def', '_handle_msg', '(', 'self', ',', 'msg', ')', ':', 'LOG', '.', 'debug', '(', ""'Received msg from %s << %s'"", ',', 'self', '.', '_remotename', ',', 'msg', ')', '# If we receive open message we try to bind to protocol', 'if', 'msg', '.', 'type', '==', 'BGP_MSG_OPEN', ':', 'if', 'self', '.', 'state', '==', 'BGP_FSM_OPEN_SENT', ':', '# Validate open message.', 'self', '.', '_validate_open_msg', '(', 'msg', ')', 'self', '.', 'recv_open_msg', '=', 'msg', 'self', '.', 'state', '=', 'BGP_FSM_OPEN_CONFIRM', 'self', '.', '_peer', '.', 'state', '.', 'bgp_state', '=', 'self', '.', 'state', '# Try to bind this protocol to peer.', 'self', '.', '_is_bound', '=', 'self', '.', '_peer', '.', 'bind_protocol', '(', 'self', ')', '# If this protocol failed to bind to peer.', 'if', 'not', 'self', '.', '_is_bound', ':', '# Failure to bind to peer indicates connection collision', '# resolution choose different instance of protocol and this', '# instance has to close. Before closing it sends', '# appropriate notification msg. to peer.', 'raise', 'bgp', '.', 'CollisionResolution', '(', ')', '# If peer sends Hold Time as zero, then according to RFC we do', '# not set Hold Time and Keep Alive timer.', 'if', 'msg', '.', 'hold_time', '==', '0', ':', 'LOG', '.', 'info', '(', ""'The Hold Time sent by the peer is zero, hence '"", ""'not setting any Hold Time and Keep Alive'"", ""' timers.'"", ')', 'else', ':', '# Start Keep Alive timer considering Hold Time preference', '# of the peer.', 'self', '.', '_start_timers', '(', 'msg', '.', 'hold_time', ')', 'self', '.', '_send_keepalive', '(', ')', '# Peer does not see open message.', 'return', 'else', ':', '# If we receive a Open message out of order', 'LOG', '.', 'error', '(', ""'Open message received when current state is not '"", ""'OpenSent'"", ')', '# Received out-of-order open message', '# We raise Finite state machine error', 'raise', 'bgp', '.', 'FiniteStateMachineError', '(', ')', 'elif', 'msg', '.', 'type', '==', 'BGP_MSG_NOTIFICATION', ':', 'if', 'self', '.', '_peer', ':', 'self', '.', '_signal_bus', '.', 'bgp_notification_received', '(', 'self', '.', '_peer', ',', 'msg', ')', '# If we receive notification message', 'LOG', '.', 'error', '(', ""'Received notification message, hence closing '"", ""'connection %s'"", ',', 'msg', ')', 'self', '.', '_socket', '.', 'close', '(', ')', 'return', '# If we receive keepalive or update message, we reset expire timer.', 'if', '(', 'msg', '.', 'type', '==', 'BGP_MSG_KEEPALIVE', 'or', 'msg', '.', 'type', '==', 'BGP_MSG_UPDATE', ')', ':', 'if', 'self', '.', '_expiry', ':', 'self', '.', '_expiry', '.', 'reset', '(', ')', '# Call peer message handler for appropriate messages.', 'if', '(', 'msg', '.', 'type', 'in', '(', 'BGP_MSG_UPDATE', ',', 'BGP_MSG_KEEPALIVE', ',', 'BGP_MSG_ROUTE_REFRESH', ')', ')', ':', 'self', '.', '_peer', '.', 'handle_msg', '(', 'msg', ')', '# We give chance to other threads to run.', 'self', '.', 'pause', '(', '0', ')']","When a BGP message is received, send it to peer.

        Open messages are validated here. Peer handler is called to handle each
        message except for *Open* and *Notification* message. On receiving
        *Notification* message we close connection with peer.","['When', 'a', 'BGP', 'message', 'is', 'received', 'send', 'it', 'to', 'peer', '.']",python,X,2,True,1,train
26505,osrg/ryu,ryu/lib/igmplib.py,https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/igmplib.py#L326-L396,"def _send_query(self):
        """""" send a QUERY message periodically.""""""
        timeout = 60
        ofproto = self._datapath.ofproto
        parser = self._datapath.ofproto_parser
        if ofproto_v1_0.OFP_VERSION == ofproto.OFP_VERSION:
            send_port = ofproto.OFPP_NONE
        else:
            send_port = ofproto.OFPP_ANY

        # create a general query.
        res_igmp = igmp.igmp(
            msgtype=igmp.IGMP_TYPE_QUERY,
            maxresp=igmp.QUERY_RESPONSE_INTERVAL * 10,
            csum=0,
            address='0.0.0.0')
        res_ipv4 = ipv4.ipv4(
            total_length=len(ipv4.ipv4()) + len(res_igmp),
            proto=inet.IPPROTO_IGMP, ttl=1,
            src='0.0.0.0',
            dst=igmp.MULTICAST_IP_ALL_HOST)
        res_ether = ethernet.ethernet(
            dst=igmp.MULTICAST_MAC_ALL_HOST,
            src=self._datapath.ports[ofproto.OFPP_LOCAL].hw_addr,
            ethertype=ether.ETH_TYPE_IP)
        res_pkt = packet.Packet()
        res_pkt.add_protocol(res_ether)
        res_pkt.add_protocol(res_ipv4)
        res_pkt.add_protocol(res_igmp)
        res_pkt.serialize()

        flood = [parser.OFPActionOutput(ofproto.OFPP_FLOOD)]

        while True:
            # reset reply status.
            for status in self._mcast.values():
                for port in status.keys():
                    status[port] = False

            # send a general query to the host that sent this message.
            self._do_packet_out(
                self._datapath, res_pkt.data, send_port, flood)
            hub.sleep(igmp.QUERY_RESPONSE_INTERVAL)

            # QUERY timeout expired.
            del_groups = []
            for group, status in self._mcast.items():
                del_ports = []
                actions = []
                for port in status.keys():
                    if not status[port]:
                        del_ports.append(port)
                    else:
                        actions.append(parser.OFPActionOutput(port))
                if len(actions) and len(del_ports):
                    self._set_flow_entry(
                        self._datapath, actions, self.server_port, group)
                if not len(actions):
                    self._del_flow_entry(
                        self._datapath, self.server_port, group)
                    del_groups.append(group)
                if len(del_ports):
                    for port in del_ports:
                        self._del_flow_entry(self._datapath, port, group)
                for port in del_ports:
                    del status[port]
            for group in del_groups:
                del self._mcast[group]

            rest_time = timeout - igmp.QUERY_RESPONSE_INTERVAL
            hub.sleep(rest_time)","['def', '_send_query', '(', 'self', ')', ':', 'timeout', '=', '60', 'ofproto', '=', 'self', '.', '_datapath', '.', 'ofproto', 'parser', '=', 'self', '.', '_datapath', '.', 'ofproto_parser', 'if', 'ofproto_v1_0', '.', 'OFP_VERSION', '==', 'ofproto', '.', 'OFP_VERSION', ':', 'send_port', '=', 'ofproto', '.', 'OFPP_NONE', 'else', ':', 'send_port', '=', 'ofproto', '.', 'OFPP_ANY', '# create a general query.', 'res_igmp', '=', 'igmp', '.', 'igmp', '(', 'msgtype', '=', 'igmp', '.', 'IGMP_TYPE_QUERY', ',', 'maxresp', '=', 'igmp', '.', 'QUERY_RESPONSE_INTERVAL', '*', '10', ',', 'csum', '=', '0', ',', 'address', '=', ""'0.0.0.0'"", ')', 'res_ipv4', '=', 'ipv4', '.', 'ipv4', '(', 'total_length', '=', 'len', '(', 'ipv4', '.', 'ipv4', '(', ')', ')', '+', 'len', '(', 'res_igmp', ')', ',', 'proto', '=', 'inet', '.', 'IPPROTO_IGMP', ',', 'ttl', '=', '1', ',', 'src', '=', ""'0.0.0.0'"", ',', 'dst', '=', 'igmp', '.', 'MULTICAST_IP_ALL_HOST', ')', 'res_ether', '=', 'ethernet', '.', 'ethernet', '(', 'dst', '=', 'igmp', '.', 'MULTICAST_MAC_ALL_HOST', ',', 'src', '=', 'self', '.', '_datapath', '.', 'ports', '[', 'ofproto', '.', 'OFPP_LOCAL', ']', '.', 'hw_addr', ',', 'ethertype', '=', 'ether', '.', 'ETH_TYPE_IP', ')', 'res_pkt', '=', 'packet', '.', 'Packet', '(', ')', 'res_pkt', '.', 'add_protocol', '(', 'res_ether', ')', 'res_pkt', '.', 'add_protocol', '(', 'res_ipv4', ')', 'res_pkt', '.', 'add_protocol', '(', 'res_igmp', ')', 'res_pkt', '.', 'serialize', '(', ')', 'flood', '=', '[', 'parser', '.', 'OFPActionOutput', '(', 'ofproto', '.', 'OFPP_FLOOD', ')', ']', 'while', 'True', ':', '# reset reply status.', 'for', 'status', 'in', 'self', '.', '_mcast', '.', 'values', '(', ')', ':', 'for', 'port', 'in', 'status', '.', 'keys', '(', ')', ':', 'status', '[', 'port', ']', '=', 'False', '# send a general query to the host that sent this message.', 'self', '.', '_do_packet_out', '(', 'self', '.', '_datapath', ',', 'res_pkt', '.', 'data', ',', 'send_port', ',', 'flood', ')', 'hub', '.', 'sleep', '(', 'igmp', '.', 'QUERY_RESPONSE_INTERVAL', ')', '# QUERY timeout expired.', 'del_groups', '=', '[', ']', 'for', 'group', ',', 'status', 'in', 'self', '.', '_mcast', '.', 'items', '(', ')', ':', 'del_ports', '=', '[', ']', 'actions', '=', '[', ']', 'for', 'port', 'in', 'status', '.', 'keys', '(', ')', ':', 'if', 'not', 'status', '[', 'port', ']', ':', 'del_ports', '.', 'append', '(', 'port', ')', 'else', ':', 'actions', '.', 'append', '(', 'parser', '.', 'OFPActionOutput', '(', 'port', ')', ')', 'if', 'len', '(', 'actions', ')', 'and', 'len', '(', 'del_ports', ')', ':', 'self', '.', '_set_flow_entry', '(', 'self', '.', '_datapath', ',', 'actions', ',', 'self', '.', 'server_port', ',', 'group', ')', 'if', 'not', 'len', '(', 'actions', ')', ':', 'self', '.', '_del_flow_entry', '(', 'self', '.', '_datapath', ',', 'self', '.', 'server_port', ',', 'group', ')', 'del_groups', '.', 'append', '(', 'group', ')', 'if', 'len', '(', 'del_ports', ')', ':', 'for', 'port', 'in', 'del_ports', ':', 'self', '.', '_del_flow_entry', '(', 'self', '.', '_datapath', ',', 'port', ',', 'group', ')', 'for', 'port', 'in', 'del_ports', ':', 'del', 'status', '[', 'port', ']', 'for', 'group', 'in', 'del_groups', ':', 'del', 'self', '.', '_mcast', '[', 'group', ']', 'rest_time', '=', 'timeout', '-', 'igmp', '.', 'QUERY_RESPONSE_INTERVAL', 'hub', '.', 'sleep', '(', 'rest_time', ')']",send a QUERY message periodically.,"['send', 'a', 'QUERY', 'message', 'periodically', '.']",python,X,2,True,1,train
27380,balloob/pychromecast,pychromecast/controllers/youtube.py,https://github.com/balloob/pychromecast/blob/831b09c4fed185a7bffe0ea330b7849d5f4e36b6/pychromecast/controllers/youtube.py#L77-L91,"def update_screen_id(self):
        """"""
        Sends a getMdxSessionStatus to get the screenId and waits for response.
        This function is blocking
        If connected we should always get a response
        (send message will launch app if it is not running).
        """"""
        self.status_update_event.clear()
        # This gets the screenId but always throws. Couldn't find a better way.
        try:
            self.send_message({MESSAGE_TYPE: TYPE_GET_SCREEN_ID})
        except UnsupportedNamespace:
            pass
        self.status_update_event.wait()
        self.status_update_event.clear()","['def', 'update_screen_id', '(', 'self', ')', ':', 'self', '.', 'status_update_event', '.', 'clear', '(', ')', ""# This gets the screenId but always throws. Couldn't find a better way."", 'try', ':', 'self', '.', 'send_message', '(', '{', 'MESSAGE_TYPE', ':', 'TYPE_GET_SCREEN_ID', '}', ')', 'except', 'UnsupportedNamespace', ':', 'pass', 'self', '.', 'status_update_event', '.', 'wait', '(', ')', 'self', '.', 'status_update_event', '.', 'clear', '(', ')']","Sends a getMdxSessionStatus to get the screenId and waits for response.
        This function is blocking
        If connected we should always get a response
        (send message will launch app if it is not running).","['Sends', 'a', 'getMdxSessionStatus', 'to', 'get', 'the', 'screenId', 'and', 'waits', 'for', 'response', '.', 'This', 'function', 'is', 'blocking', 'If', 'connected', 'we', 'should', 'always', 'get', 'a', 'response', '(', 'send', 'message', 'will', 'launch', 'app', 'if', 'it', 'is', 'not', 'running', ')', '.']",python,X,2,True,1,train
28373,hyperledger/sawtooth-core,validator/sawtooth_validator/networking/interconnect.py,https://github.com/hyperledger/sawtooth-core/blob/8cf473bc2207e51f02bd182d825158a57d72b098/validator/sawtooth_validator/networking/interconnect.py#L1038-L1076,"def send(self, message_type, data, connection_id, callback=None,
             one_way=False):
        """"""
        Send a message of message_type
        :param connection_id: the identity for the connection to send to
        :param message_type: validator_pb2.Message.* enum value
        :param data: bytes serialized protobuf
        :return: future.Future
        """"""
        if connection_id not in self._connections:
            raise ValueError(""Unknown connection id: {}"".format(connection_id))
        connection_info = self._connections.get(connection_id)
        if connection_info.connection_type == \
                ConnectionType.ZMQ_IDENTITY:
            message = validator_pb2.Message(
                correlation_id=_generate_id(),
                content=data,
                message_type=message_type)

            timer_tag = get_enum_name(message.message_type)
            timer_ctx = self._get_send_response_timer(timer_tag).time()
            fut = future.Future(
                message.correlation_id,
                message.content,
                callback,
                timeout=self._connection_timeout,
                timer_ctx=timer_ctx)
            if not one_way:
                self._futures.put(fut)

            self._send_receive_thread.send_message(msg=message,
                                                   connection_id=connection_id)
            return fut

        return connection_info.connection.send(
            message_type,
            data,
            callback=callback,
            one_way=one_way)","['def', 'send', '(', 'self', ',', 'message_type', ',', 'data', ',', 'connection_id', ',', 'callback', '=', 'None', ',', 'one_way', '=', 'False', ')', ':', 'if', 'connection_id', 'not', 'in', 'self', '.', '_connections', ':', 'raise', 'ValueError', '(', '""Unknown connection id: {}""', '.', 'format', '(', 'connection_id', ')', ')', 'connection_info', '=', 'self', '.', '_connections', '.', 'get', '(', 'connection_id', ')', 'if', 'connection_info', '.', 'connection_type', '==', 'ConnectionType', '.', 'ZMQ_IDENTITY', ':', 'message', '=', 'validator_pb2', '.', 'Message', '(', 'correlation_id', '=', '_generate_id', '(', ')', ',', 'content', '=', 'data', ',', 'message_type', '=', 'message_type', ')', 'timer_tag', '=', 'get_enum_name', '(', 'message', '.', 'message_type', ')', 'timer_ctx', '=', 'self', '.', '_get_send_response_timer', '(', 'timer_tag', ')', '.', 'time', '(', ')', 'fut', '=', 'future', '.', 'Future', '(', 'message', '.', 'correlation_id', ',', 'message', '.', 'content', ',', 'callback', ',', 'timeout', '=', 'self', '.', '_connection_timeout', ',', 'timer_ctx', '=', 'timer_ctx', ')', 'if', 'not', 'one_way', ':', 'self', '.', '_futures', '.', 'put', '(', 'fut', ')', 'self', '.', '_send_receive_thread', '.', 'send_message', '(', 'msg', '=', 'message', ',', 'connection_id', '=', 'connection_id', ')', 'return', 'fut', 'return', 'connection_info', '.', 'connection', '.', 'send', '(', 'message_type', ',', 'data', ',', 'callback', '=', 'callback', ',', 'one_way', '=', 'one_way', ')']","Send a message of message_type
        :param connection_id: the identity for the connection to send to
        :param message_type: validator_pb2.Message.* enum value
        :param data: bytes serialized protobuf
        :return: future.Future","['Send', 'a', 'message', 'of', 'message_type', ':', 'param', 'connection_id', ':', 'the', 'identity', 'for', 'the', 'connection', 'to', 'send', 'to', ':', 'param', 'message_type', ':', 'validator_pb2', '.', 'Message', '.', '*', 'enum', 'value', ':', 'param', 'data', ':', 'bytes', 'serialized', 'protobuf', ':', 'return', ':', 'future', '.', 'Future']",python,X,2,True,1,train
28377,hyperledger/sawtooth-core,validator/sawtooth_validator/networking/interconnect.py,https://github.com/hyperledger/sawtooth-core/blob/8cf473bc2207e51f02bd182d825158a57d72b098/validator/sawtooth_validator/networking/interconnect.py#L1217-L1251,"def send_last_message(self, message_type, data,
                          connection_id, callback=None, one_way=False):
        """"""
        Send a message of message_type and close the connection.
        :param connection_id: the identity for the connection to send to
        :param message_type: validator_pb2.Message.* enum value
        :param data: bytes serialized protobuf
        :return: future.Future
        """"""
        if connection_id not in self._connections:
            raise ValueError(""Unknown connection id: {}"".format(connection_id))
        connection_info = self._connections.get(connection_id)
        if connection_info.connection_type == \
                ConnectionType.ZMQ_IDENTITY:
            message = validator_pb2.Message(
                correlation_id=_generate_id(),
                content=data,
                message_type=message_type)

            fut = future.Future(message.correlation_id, message.content,
                                callback, timeout=self._connection_timeout)

            if not one_way:
                self._futures.put(fut)

            self._send_receive_thread.send_last_message(
                msg=message,
                connection_id=connection_id)
            return fut

        del self._connections[connection_id]
        return connection_info.connection.send_last_message(
            message_type,
            data,
            callback=callback)","['def', 'send_last_message', '(', 'self', ',', 'message_type', ',', 'data', ',', 'connection_id', ',', 'callback', '=', 'None', ',', 'one_way', '=', 'False', ')', ':', 'if', 'connection_id', 'not', 'in', 'self', '.', '_connections', ':', 'raise', 'ValueError', '(', '""Unknown connection id: {}""', '.', 'format', '(', 'connection_id', ')', ')', 'connection_info', '=', 'self', '.', '_connections', '.', 'get', '(', 'connection_id', ')', 'if', 'connection_info', '.', 'connection_type', '==', 'ConnectionType', '.', 'ZMQ_IDENTITY', ':', 'message', '=', 'validator_pb2', '.', 'Message', '(', 'correlation_id', '=', '_generate_id', '(', ')', ',', 'content', '=', 'data', ',', 'message_type', '=', 'message_type', ')', 'fut', '=', 'future', '.', 'Future', '(', 'message', '.', 'correlation_id', ',', 'message', '.', 'content', ',', 'callback', ',', 'timeout', '=', 'self', '.', '_connection_timeout', ')', 'if', 'not', 'one_way', ':', 'self', '.', '_futures', '.', 'put', '(', 'fut', ')', 'self', '.', '_send_receive_thread', '.', 'send_last_message', '(', 'msg', '=', 'message', ',', 'connection_id', '=', 'connection_id', ')', 'return', 'fut', 'del', 'self', '.', '_connections', '[', 'connection_id', ']', 'return', 'connection_info', '.', 'connection', '.', 'send_last_message', '(', 'message_type', ',', 'data', ',', 'callback', '=', 'callback', ')']","Send a message of message_type and close the connection.
        :param connection_id: the identity for the connection to send to
        :param message_type: validator_pb2.Message.* enum value
        :param data: bytes serialized protobuf
        :return: future.Future","['Send', 'a', 'message', 'of', 'message_type', 'and', 'close', 'the', 'connection', '.', ':', 'param', 'connection_id', ':', 'the', 'identity', 'for', 'the', 'connection', 'to', 'send', 'to', ':', 'param', 'message_type', ':', 'validator_pb2', '.', 'Message', '.', '*', 'enum', 'value', ':', 'param', 'data', ':', 'bytes', 'serialized', 'protobuf', ':', 'return', ':', 'future', '.', 'Future']",python,X,2,True,1,train
28430,websocket-client/websocket-client,websocket/_app.py,https://github.com/websocket-client/websocket-client/blob/3c25814664fef5b78716ed8841123ed1c0d17824/websocket/_app.py#L145-L155,"def send(self, data, opcode=ABNF.OPCODE_TEXT):
        """"""
        send message.
        data: message to send. If you set opcode to OPCODE_TEXT,
              data must be utf-8 string or unicode.
        opcode: operation code of data. default is OPCODE_TEXT.
        """"""

        if not self.sock or self.sock.send(data, opcode) == 0:
            raise WebSocketConnectionClosedException(
                ""Connection is already closed."")","['def', 'send', '(', 'self', ',', 'data', ',', 'opcode', '=', 'ABNF', '.', 'OPCODE_TEXT', ')', ':', 'if', 'not', 'self', '.', 'sock', 'or', 'self', '.', 'sock', '.', 'send', '(', 'data', ',', 'opcode', ')', '==', '0', ':', 'raise', 'WebSocketConnectionClosedException', '(', '""Connection is already closed.""', ')']","send message.
        data: message to send. If you set opcode to OPCODE_TEXT,
              data must be utf-8 string or unicode.
        opcode: operation code of data. default is OPCODE_TEXT.","['send', 'message', '.', 'data', ':', 'message', 'to', 'send', '.', 'If', 'you', 'set', 'opcode', 'to', 'OPCODE_TEXT', 'data', 'must', 'be', 'utf', '-', '8', 'string', 'or', 'unicode', '.', 'opcode', ':', 'operation', 'code', 'of', 'data', '.', 'default', 'is', 'OPCODE_TEXT', '.']",python,X,2,True,1,train
11032,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L78-L90,"def send_message(self, text: str, reply: int=None, link_preview: bool=None,
                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send message to this peer.
        :param text: Text to send.
        :param reply: Message object or message_id to reply to.
        :param link_preview: Whether or not to show the link preview for this message
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        self.twx.send_message(self, text=text, reply=reply, link_preview=link_preview, on_success=on_success,
                              reply_markup=reply_markup)","['def', 'send_message', '(', 'self', ',', 'text', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'link_preview', ':', 'bool', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'self', '.', 'twx', '.', 'send_message', '(', 'self', ',', 'text', '=', 'text', ',', 'reply', '=', 'reply', ',', 'link_preview', '=', 'link_preview', ',', 'on_success', '=', 'on_success', ',', 'reply_markup', '=', 'reply_markup', ')']","Send message to this peer.
        :param text: Text to send.
        :param reply: Message object or message_id to reply to.
        :param link_preview: Whether or not to show the link preview for this message
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'message', 'to', 'this', 'peer', '.', ':', 'param', 'text', ':', 'Text', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'link_preview', ':', 'Whether', 'or', 'not', 'to', 'show', 'the', 'link', 'preview', 'for', 'this', 'message', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11040,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L297-L306,"def send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None, on_success: callable=None):
        """"""
        Send message to peer.
        :param peer: Peer to send message to.
        :param text: Text to send.
        :param reply: Message object or message_id to reply to.
        :param link_preview: Whether or not to show the link preview for this message
        :param on_success: Callback to call when call is complete.
        """"""
        pass","['def', 'send_message', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'text', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'link_preview', ':', 'bool', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ')', ':', 'pass']","Send message to peer.
        :param peer: Peer to send message to.
        :param text: Text to send.
        :param reply: Message object or message_id to reply to.
        :param link_preview: Whether or not to show the link preview for this message
        :param on_success: Callback to call when call is complete.","['Send', 'message', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'text', ':', 'Text', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'link_preview', ':', 'Whether', 'or', 'not', 'to', 'show', 'the', 'link', 'preview', 'for', 'this', 'message', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11041,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L321-L333,"def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send photo to peer.
        :param peer: Peer to send message to.
        :param photo: File path to photo to send.
        :param caption: Caption for photo
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        pass","['def', 'send_photo', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'photo', ':', 'str', ',', 'caption', ':', 'str', '=', 'None', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'pass']","Send photo to peer.
        :param peer: Peer to send message to.
        :param photo: File path to photo to send.
        :param caption: Caption for photo
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'photo', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'photo', ':', 'File', 'path', 'to', 'photo', 'to', 'send', '.', ':', 'param', 'caption', ':', 'Caption', 'for', 'photo', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11042,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L336-L347,"def send_audio(self, peer: Peer, audio: str, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send audio clip to peer.
        :param peer: Peer to send message to.
        :param audio: File path to audio to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        pass","['def', 'send_audio', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'audio', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'pass']","Send audio clip to peer.
        :param peer: Peer to send message to.
        :param audio: File path to audio to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'audio', 'clip', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'audio', ':', 'File', 'path', 'to', 'audio', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11043,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L350-L361,"def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,
                      reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send document to peer.
        :param peer: Peer to send message to.
        :param document: File path to document to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        pass","['def', 'send_document', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'document', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'pass']","Send document to peer.
        :param peer: Peer to send message to.
        :param document: File path to document to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'document', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'document', ':', 'File', 'path', 'to', 'document', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11044,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L364-L375,"def send_sticker(self, peer: Peer, sticker: str, reply: int=None, on_success: callable=None,
                     reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send sticker to peer.
        :param peer: Peer to send message to.
        :param sticker: File path to sticker to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        pass","['def', 'send_sticker', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'sticker', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'pass']","Send sticker to peer.
        :param peer: Peer to send message to.
        :param sticker: File path to sticker to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'sticker', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'sticker', ':', 'File', 'path', 'to', 'sticker', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11045,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L378-L389,"def send_video(self, peer: Peer, video: str, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send video to peer.
        :param peer: Peer to send message to.
        :param video: File path to video to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        pass","['def', 'send_video', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'video', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'pass']","Send video to peer.
        :param peer: Peer to send message to.
        :param video: File path to video to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'video', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'video', ':', 'File', 'path', 'to', 'video', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11046,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L392-L404,"def send_location(self, peer: Peer, latitude: float, longitude: float, reply: int=None, on_success: callable=None,
                      reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send location to peer.
        :param peer: Peer to send message to.
        :param latitude: Latitude of the location.
        :param longitude: Longitude of the location.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        pass","['def', 'send_location', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'latitude', ':', 'float', ',', 'longitude', ':', 'float', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'pass']","Send location to peer.
        :param peer: Peer to send message to.
        :param latitude: Latitude of the location.
        :param longitude: Longitude of the location.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'location', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'latitude', ':', 'Latitude', 'of', 'the', 'location', '.', ':', 'param', 'longitude', ':', 'Longitude', 'of', 'the', 'location', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11049,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L568-L585,"def send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None,
                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send message to peer.
        :param peer: Peer to send message to.
        :param text: Text to send.
        :param reply: Message object or message_id to reply to.
        :param link_preview: Whether or not to show the link preview for this message
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        if isinstance(reply, Message):
            reply = reply.id

        botapi.send_message(chat_id=peer.id, text=text, disable_web_page_preview=not link_preview,
                            reply_to_message_id=reply, on_success=on_success, reply_markup=reply_markup,
                            **self.request_args).run()","['def', 'send_message', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'text', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'link_preview', ':', 'bool', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'reply', ',', 'Message', ')', ':', 'reply', '=', 'reply', '.', 'id', 'botapi', '.', 'send_message', '(', 'chat_id', '=', 'peer', '.', 'id', ',', 'text', '=', 'text', ',', 'disable_web_page_preview', '=', 'not', 'link_preview', ',', 'reply_to_message_id', '=', 'reply', ',', 'on_success', '=', 'on_success', ',', 'reply_markup', '=', 'reply_markup', ',', '*', '*', 'self', '.', 'request_args', ')', '.', 'run', '(', ')']","Send message to peer.
        :param peer: Peer to send message to.
        :param text: Text to send.
        :param reply: Message object or message_id to reply to.
        :param link_preview: Whether or not to show the link preview for this message
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'message', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'text', ':', 'Text', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'link_preview', ':', 'Whether', 'or', 'not', 'to', 'show', 'the', 'link', 'preview', 'for', 'this', 'message', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11051,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L599-L617,"def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send photo to peer.
        :param peer: Peer to send message to.
        :param photo: File path to photo to send.
        :param caption: Caption for photo
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        if isinstance(reply, Message):
            reply = reply.id

        photo = botapi.InputFile('photo', botapi.InputFileInfo(photo, open(photo, 'rb'), get_mimetype(photo)))

        botapi.send_photo(chat_id=peer.id, photo=photo, caption=caption, reply_to_message_id=reply, on_success=on_success,
                          reply_markup=reply_markup, **self.request_args).run()","['def', 'send_photo', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'photo', ':', 'str', ',', 'caption', ':', 'str', '=', 'None', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'reply', ',', 'Message', ')', ':', 'reply', '=', 'reply', '.', 'id', 'photo', '=', 'botapi', '.', 'InputFile', '(', ""'photo'"", ',', 'botapi', '.', 'InputFileInfo', '(', 'photo', ',', 'open', '(', 'photo', ',', ""'rb'"", ')', ',', 'get_mimetype', '(', 'photo', ')', ')', ')', 'botapi', '.', 'send_photo', '(', 'chat_id', '=', 'peer', '.', 'id', ',', 'photo', '=', 'photo', ',', 'caption', '=', 'caption', ',', 'reply_to_message_id', '=', 'reply', ',', 'on_success', '=', 'on_success', ',', 'reply_markup', '=', 'reply_markup', ',', '*', '*', 'self', '.', 'request_args', ')', '.', 'run', '(', ')']","Send photo to peer.
        :param peer: Peer to send message to.
        :param photo: File path to photo to send.
        :param caption: Caption for photo
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'photo', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'photo', ':', 'File', 'path', 'to', 'photo', 'to', 'send', '.', ':', 'param', 'caption', ':', 'Caption', 'for', 'photo', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11052,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L619-L636,"def send_audio(self, peer: Peer, audio: str, reply: int=None, on_success: callable=None,
                   reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send audio clip to peer.
        :param peer: Peer to send message to.
        :param audio: File path to audio to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        if isinstance(reply, Message):
            reply = reply.id

        audio = botapi.InputFile('audio', botapi.InputFileInfo(audio, open(audio, 'rb'), get_mimetype(audio)))

        botapi.send_audio(chat_id=peer.id, audio=audio, reply_to_message_id=reply, on_success=on_success,
                          reply_markup=reply_markup, **self.request_args).run()","['def', 'send_audio', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'audio', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'reply', ',', 'Message', ')', ':', 'reply', '=', 'reply', '.', 'id', 'audio', '=', 'botapi', '.', 'InputFile', '(', ""'audio'"", ',', 'botapi', '.', 'InputFileInfo', '(', 'audio', ',', 'open', '(', 'audio', ',', ""'rb'"", ')', ',', 'get_mimetype', '(', 'audio', ')', ')', ')', 'botapi', '.', 'send_audio', '(', 'chat_id', '=', 'peer', '.', 'id', ',', 'audio', '=', 'audio', ',', 'reply_to_message_id', '=', 'reply', ',', 'on_success', '=', 'on_success', ',', 'reply_markup', '=', 'reply_markup', ',', '*', '*', 'self', '.', 'request_args', ')', '.', 'run', '(', ')']","Send audio clip to peer.
        :param peer: Peer to send message to.
        :param audio: File path to audio to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'audio', 'clip', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'audio', ':', 'File', 'path', 'to', 'audio', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
11053,datamachine/twx,twx/twx.py,https://github.com/datamachine/twx/blob/d9633f12f3647b1e54ba87b70b39df3b7e02b4eb/twx/twx.py#L638-L656,"def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,
                      reply_markup: botapi.ReplyMarkup=None):
        """"""
        Send document to peer.
        :param peer: Peer to send message to.
        :param document: File path to document to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message
        """"""
        if isinstance(reply, Message):
            reply = reply.id

        document = botapi.InputFile('document', botapi.InputFileInfo(document, open(document, 'rb'),
                                                                     get_mimetype(document)))

        botapi.send_document(chat_id=peer.id, document=document, reply_to_message_id=reply, on_success=on_success,
                             reply_markup=reply_markup, **self.request_args).run()","['def', 'send_document', '(', 'self', ',', 'peer', ':', 'Peer', ',', 'document', ':', 'str', ',', 'reply', ':', 'int', '=', 'None', ',', 'on_success', ':', 'callable', '=', 'None', ',', 'reply_markup', ':', 'botapi', '.', 'ReplyMarkup', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'reply', ',', 'Message', ')', ':', 'reply', '=', 'reply', '.', 'id', 'document', '=', 'botapi', '.', 'InputFile', '(', ""'document'"", ',', 'botapi', '.', 'InputFileInfo', '(', 'document', ',', 'open', '(', 'document', ',', ""'rb'"", ')', ',', 'get_mimetype', '(', 'document', ')', ')', ')', 'botapi', '.', 'send_document', '(', 'chat_id', '=', 'peer', '.', 'id', ',', 'document', '=', 'document', ',', 'reply_to_message_id', '=', 'reply', ',', 'on_success', '=', 'on_success', ',', 'reply_markup', '=', 'reply_markup', ',', '*', '*', 'self', '.', 'request_args', ')', '.', 'run', '(', ')']","Send document to peer.
        :param peer: Peer to send message to.
        :param document: File path to document to send.
        :param reply: Message object or message_id to reply to.
        :param on_success: Callback to call when call is complete.

        :type reply: int or Message","['Send', 'document', 'to', 'peer', '.', ':', 'param', 'peer', ':', 'Peer', 'to', 'send', 'message', 'to', '.', ':', 'param', 'document', ':', 'File', 'path', 'to', 'document', 'to', 'send', '.', ':', 'param', 'reply', ':', 'Message', 'object', 'or', 'message_id', 'to', 'reply', 'to', '.', ':', 'param', 'on_success', ':', 'Callback', 'to', 'call', 'when', 'call', 'is', 'complete', '.']",python,X,2,True,1,train
15071,frawau/aiolifx,aiolifx/aiolifx.py,https://github.com/frawau/aiolifx/blob/9bd8c5e6d291f4c79314989402f7e2c6476d5851/aiolifx/aiolifx.py#L253-L293,"async def try_sending(self,msg,timeout_secs, max_attempts):
        """"""Coroutine used to send message to the device when a response or ack is needed.

        This coroutine will try to send up to max_attempts time the message, waiting timeout_secs
        for an answer. If no answer is received, it will consider that the device is no longer
        accessible and will unregister it.

            :param msg: The message to send
            :type msg: aiolifx.Message
            :param timeout_secs: Number of seconds to wait for a response or ack
            :type timeout_secs: int
            :param max_attempts: .
            :type max_attempts: int
            :returns: a coroutine to be scheduled
            :rtype: coroutine
        """"""
        if timeout_secs is None:
            timeout_secs = self.timeout
        if max_attempts is None:
            max_attempts = self.retry_count

        attempts = 0
        while attempts < max_attempts:
            if msg.seq_num not in self.message: return
            event = aio.Event()
            self.message[msg.seq_num][1]= event
            attempts += 1
            if self.transport:
                self.transport.sendto(msg.packed_message)
            try:
                myresult = await aio.wait_for(event.wait(),timeout_secs)
                break
            except Exception as inst:
                if attempts >= max_attempts:
                    if msg.seq_num in self.message:
                        callb = self.message[msg.seq_num][2]
                        if callb:
                            callb(self, None)
                        del(self.message[msg.seq_num])
                    #It's dead Jim
                    self.unregister()","['async', 'def', 'try_sending', '(', 'self', ',', 'msg', ',', 'timeout_secs', ',', 'max_attempts', ')', ':', 'if', 'timeout_secs', 'is', 'None', ':', 'timeout_secs', '=', 'self', '.', 'timeout', 'if', 'max_attempts', 'is', 'None', ':', 'max_attempts', '=', 'self', '.', 'retry_count', 'attempts', '=', '0', 'while', 'attempts', '<', 'max_attempts', ':', 'if', 'msg', '.', 'seq_num', 'not', 'in', 'self', '.', 'message', ':', 'return', 'event', '=', 'aio', '.', 'Event', '(', ')', 'self', '.', 'message', '[', 'msg', '.', 'seq_num', ']', '[', '1', ']', '=', 'event', 'attempts', '+=', '1', 'if', 'self', '.', 'transport', ':', 'self', '.', 'transport', '.', 'sendto', '(', 'msg', '.', 'packed_message', ')', 'try', ':', 'myresult', '=', 'await', 'aio', '.', 'wait_for', '(', 'event', '.', 'wait', '(', ')', ',', 'timeout_secs', ')', 'break', 'except', 'Exception', 'as', 'inst', ':', 'if', 'attempts', '>=', 'max_attempts', ':', 'if', 'msg', '.', 'seq_num', 'in', 'self', '.', 'message', ':', 'callb', '=', 'self', '.', 'message', '[', 'msg', '.', 'seq_num', ']', '[', '2', ']', 'if', 'callb', ':', 'callb', '(', 'self', ',', 'None', ')', 'del', '(', 'self', '.', 'message', '[', 'msg', '.', 'seq_num', ']', ')', ""#It's dead Jim"", 'self', '.', 'unregister', '(', ')']","Coroutine used to send message to the device when a response or ack is needed.

        This coroutine will try to send up to max_attempts time the message, waiting timeout_secs
        for an answer. If no answer is received, it will consider that the device is no longer
        accessible and will unregister it.

            :param msg: The message to send
            :type msg: aiolifx.Message
            :param timeout_secs: Number of seconds to wait for a response or ack
            :type timeout_secs: int
            :param max_attempts: .
            :type max_attempts: int
            :returns: a coroutine to be scheduled
            :rtype: coroutine","['Coroutine', 'used', 'to', 'send', 'message', 'to', 'the', 'device', 'when', 'a', 'response', 'or', 'ack', 'is', 'needed', '.']",python,X,2,True,1,train
15072,frawau/aiolifx,aiolifx/aiolifx.py,https://github.com/frawau/aiolifx/blob/9bd8c5e6d291f4c79314989402f7e2c6476d5851/aiolifx/aiolifx.py#L296-L315,"def req_with_ack(self, msg_type, payload, callb = None, timeout_secs=None, max_attempts=None):
        """"""Method to send a message expecting to receive an ACK.

            :param msg_type: The type of the message to send, a subclass of aiolifx.Message
            :type msg_type: class
            :param payload: value to use when instantiating msg_type
            :type payload: dict
            :param callb: A callback that will be executed when the ACK is received in datagram_received
            :type callb: callable
            :param timeout_secs: Number of seconds to wait for an ack
            :type timeout_secs: int
            :param max_attempts: .
            :type max_attempts: int
            :returns: True
            :rtype: bool
        """"""
        msg = msg_type(self.mac_addr, self.source_id, seq_num=self.seq_next(), payload=payload, ack_requested=True, response_requested=False)
        self.message[msg.seq_num]=[Acknowledgement,None,callb]
        xx=self.loop.create_task(self.try_sending(msg,timeout_secs, max_attempts))
        return True","['def', 'req_with_ack', '(', 'self', ',', 'msg_type', ',', 'payload', ',', 'callb', '=', 'None', ',', 'timeout_secs', '=', 'None', ',', 'max_attempts', '=', 'None', ')', ':', 'msg', '=', 'msg_type', '(', 'self', '.', 'mac_addr', ',', 'self', '.', 'source_id', ',', 'seq_num', '=', 'self', '.', 'seq_next', '(', ')', ',', 'payload', '=', 'payload', ',', 'ack_requested', '=', 'True', ',', 'response_requested', '=', 'False', ')', 'self', '.', 'message', '[', 'msg', '.', 'seq_num', ']', '=', '[', 'Acknowledgement', ',', 'None', ',', 'callb', ']', 'xx', '=', 'self', '.', 'loop', '.', 'create_task', '(', 'self', '.', 'try_sending', '(', 'msg', ',', 'timeout_secs', ',', 'max_attempts', ')', ')', 'return', 'True']","Method to send a message expecting to receive an ACK.

            :param msg_type: The type of the message to send, a subclass of aiolifx.Message
            :type msg_type: class
            :param payload: value to use when instantiating msg_type
            :type payload: dict
            :param callb: A callback that will be executed when the ACK is received in datagram_received
            :type callb: callable
            :param timeout_secs: Number of seconds to wait for an ack
            :type timeout_secs: int
            :param max_attempts: .
            :type max_attempts: int
            :returns: True
            :rtype: bool","['Method', 'to', 'send', 'a', 'message', 'expecting', 'to', 'receive', 'an', 'ACK', '.']",python,X,2,True,1,train
15113,frawau/aiolifx,aiolifx/aiolifx.py,https://github.com/frawau/aiolifx/blob/9bd8c5e6d291f4c79314989402f7e2c6476d5851/aiolifx/aiolifx.py#L1224-L1234,"def discover(self):
        """"""Method to send a discovery message
        """"""
        if self.transport:
            if self.discovery_countdown <= 0:
                self.discovery_countdown = self.discovery_interval
                msg = GetService(BROADCAST_MAC, self.source_id, seq_num=0, payload={}, ack_requested=False, response_requested=True)
                self.transport.sendto(msg.generate_packed_message(), (self.broadcast_ip, UDP_BROADCAST_PORT))
            else:
                self.discovery_countdown -= self.discovery_step
            self.loop.call_later(self.discovery_step, self.discover)","['def', 'discover', '(', 'self', ')', ':', 'if', 'self', '.', 'transport', ':', 'if', 'self', '.', 'discovery_countdown', '<=', '0', ':', 'self', '.', 'discovery_countdown', '=', 'self', '.', 'discovery_interval', 'msg', '=', 'GetService', '(', 'BROADCAST_MAC', ',', 'self', '.', 'source_id', ',', 'seq_num', '=', '0', ',', 'payload', '=', '{', '}', ',', 'ack_requested', '=', 'False', ',', 'response_requested', '=', 'True', ')', 'self', '.', 'transport', '.', 'sendto', '(', 'msg', '.', 'generate_packed_message', '(', ')', ',', '(', 'self', '.', 'broadcast_ip', ',', 'UDP_BROADCAST_PORT', ')', ')', 'else', ':', 'self', '.', 'discovery_countdown', '-=', 'self', '.', 'discovery_step', 'self', '.', 'loop', '.', 'call_later', '(', 'self', '.', 'discovery_step', ',', 'self', '.', 'discover', ')']",Method to send a discovery message,"['Method', 'to', 'send', 'a', 'discovery', 'message']",python,X,2,True,1,train
17573,tamasgal/km3pipe,km3pipe/srv.py,https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/srv.py#L114-L125,"def raw_message_to(self, token, message):
        """"""Convert message to JSON and send it to the client with token""""""
        if token not in self._clients:
            log.critical(""Client with token '{0}' not found!"".format(token))
            return
        client = self._clients[token]
        try:
            client.write_message(message)
        except (AttributeError, tornado.websocket.WebSocketClosedError):
            log.error(""Lost connection to client '{0}'"".format(client))
        else:
            print(""Sent {0} bytes."".format(len(message)))","['def', 'raw_message_to', '(', 'self', ',', 'token', ',', 'message', ')', ':', 'if', 'token', 'not', 'in', 'self', '.', '_clients', ':', 'log', '.', 'critical', '(', '""Client with token \'{0}\' not found!""', '.', 'format', '(', 'token', ')', ')', 'return', 'client', '=', 'self', '.', '_clients', '[', 'token', ']', 'try', ':', 'client', '.', 'write_message', '(', 'message', ')', 'except', '(', 'AttributeError', ',', 'tornado', '.', 'websocket', '.', 'WebSocketClosedError', ')', ':', 'log', '.', 'error', '(', '""Lost connection to client \'{0}\'""', '.', 'format', '(', 'client', ')', ')', 'else', ':', 'print', '(', '""Sent {0} bytes.""', '.', 'format', '(', 'len', '(', 'message', ')', ')', ')']",Convert message to JSON and send it to the client with token,"['Convert', 'message', 'to', 'JSON', 'and', 'send', 'it', 'to', 'the', 'client', 'with', 'token']",python,X,2,True,1,train
17574,tamasgal/km3pipe,km3pipe/srv.py,https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/srv.py#L282-L286,"def message(self, data, kind=""info""):
        """"""Convert message to json and send it to the clients""""""
        message = pd.io.json.dumps({'kind': kind, 'data': data})
        print(""Sent {0} bytes."".format(len(message)))
        self.write_message(message)","['def', 'message', '(', 'self', ',', 'data', ',', 'kind', '=', '""info""', ')', ':', 'message', '=', 'pd', '.', 'io', '.', 'json', '.', 'dumps', '(', '{', ""'kind'"", ':', 'kind', ',', ""'data'"", ':', 'data', '}', ')', 'print', '(', '""Sent {0} bytes.""', '.', 'format', '(', 'len', '(', 'message', ')', ')', ')', 'self', '.', 'write_message', '(', 'message', ')']",Convert message to json and send it to the clients,"['Convert', 'message', 'to', 'json', 'and', 'send', 'it', 'to', 'the', 'clients']",python,X,2,True,1,train
18928,rsgalloway/grit,grit/server/cherrypy/__init__.py,https://github.com/rsgalloway/grit/blob/e6434ad8a1f4ac5d0903ebad630c81f8a5164d78/grit/server/cherrypy/__init__.py#L811-L875,"def send_headers(self):
        """"""Assert, process, and send the HTTP response message-headers.
        
        You must set self.status, and self.outheaders before calling this.
        """"""
        hkeys = [key.lower() for key, value in self.outheaders]
        status = int(self.status[:3])
        
        if status == 413:
            # Request Entity Too Large. Close conn to avoid garbage.
            self.close_connection = True
        elif ""content-length"" not in hkeys:
            # ""All 1xx (informational), 204 (no content),
            # and 304 (not modified) responses MUST NOT
            # include a message-body."" So no point chunking.
            if status < 200 or status in (204, 205, 304):
                pass
            else:
                if (self.response_protocol == 'HTTP/1.1'
                    and self.method != 'HEAD'):
                    # Use the chunked transfer-coding
                    self.chunked_write = True
                    self.outheaders.append((""Transfer-Encoding"", ""chunked""))
                else:
                    # Closing the conn is the only way to determine len.
                    self.close_connection = True
        
        if ""connection"" not in hkeys:
            if self.response_protocol == 'HTTP/1.1':
                # Both server and client are HTTP/1.1 or better
                if self.close_connection:
                    self.outheaders.append((""Connection"", ""close""))
            else:
                # Server and/or client are HTTP/1.0
                if not self.close_connection:
                    self.outheaders.append((""Connection"", ""Keep-Alive""))
        
        if (not self.close_connection) and (not self.chunked_read):
            # Read any remaining request body data on the socket.
            # ""If an origin server receives a request that does not include an
            # Expect request-header field with the ""100-continue"" expectation,
            # the request includes a request body, and the server responds
            # with a final status code before reading the entire request body
            # from the transport connection, then the server SHOULD NOT close
            # the transport connection until it has read the entire request,
            # or until the client closes the connection. Otherwise, the client
            # might not reliably receive the response message. However, this
            # requirement is not be construed as preventing a server from
            # defending itself against denial-of-service attacks, or from
            # badly broken client implementations.""
            remaining = getattr(self.rfile, 'remaining', 0)
            if remaining > 0:
                self.rfile.read(remaining)
        
        if ""date"" not in hkeys:
            self.outheaders.append((""Date"", rfc822.formatdate()))
        
        if ""server"" not in hkeys:
            self.outheaders.append((""Server"", self.server.server_name))
        
        buf = [self.server.protocol + "" "" + self.status + CRLF]
        for k, v in self.outheaders:
            buf.append(k + "": "" + v + CRLF)
        buf.append(CRLF)
        self.conn.wfile.sendall("""".join(buf))","['def', 'send_headers', '(', 'self', ')', ':', 'hkeys', '=', '[', 'key', '.', 'lower', '(', ')', 'for', 'key', ',', 'value', 'in', 'self', '.', 'outheaders', ']', 'status', '=', 'int', '(', 'self', '.', 'status', '[', ':', '3', ']', ')', 'if', 'status', '==', '413', ':', '# Request Entity Too Large. Close conn to avoid garbage.', 'self', '.', 'close_connection', '=', 'True', 'elif', '""content-length""', 'not', 'in', 'hkeys', ':', '# ""All 1xx (informational), 204 (no content),', '# and 304 (not modified) responses MUST NOT', '# include a message-body."" So no point chunking.', 'if', 'status', '<', '200', 'or', 'status', 'in', '(', '204', ',', '205', ',', '304', ')', ':', 'pass', 'else', ':', 'if', '(', 'self', '.', 'response_protocol', '==', ""'HTTP/1.1'"", 'and', 'self', '.', 'method', '!=', ""'HEAD'"", ')', ':', '# Use the chunked transfer-coding', 'self', '.', 'chunked_write', '=', 'True', 'self', '.', 'outheaders', '.', 'append', '(', '(', '""Transfer-Encoding""', ',', '""chunked""', ')', ')', 'else', ':', '# Closing the conn is the only way to determine len.', 'self', '.', 'close_connection', '=', 'True', 'if', '""connection""', 'not', 'in', 'hkeys', ':', 'if', 'self', '.', 'response_protocol', '==', ""'HTTP/1.1'"", ':', '# Both server and client are HTTP/1.1 or better', 'if', 'self', '.', 'close_connection', ':', 'self', '.', 'outheaders', '.', 'append', '(', '(', '""Connection""', ',', '""close""', ')', ')', 'else', ':', '# Server and/or client are HTTP/1.0', 'if', 'not', 'self', '.', 'close_connection', ':', 'self', '.', 'outheaders', '.', 'append', '(', '(', '""Connection""', ',', '""Keep-Alive""', ')', ')', 'if', '(', 'not', 'self', '.', 'close_connection', ')', 'and', '(', 'not', 'self', '.', 'chunked_read', ')', ':', '# Read any remaining request body data on the socket.', '# ""If an origin server receives a request that does not include an', '# Expect request-header field with the ""100-continue"" expectation,', '# the request includes a request body, and the server responds', '# with a final status code before reading the entire request body', '# from the transport connection, then the server SHOULD NOT close', '# the transport connection until it has read the entire request,', '# or until the client closes the connection. Otherwise, the client', '# might not reliably receive the response message. However, this', '# requirement is not be construed as preventing a server from', '# defending itself against denial-of-service attacks, or from', '# badly broken client implementations.""', 'remaining', '=', 'getattr', '(', 'self', '.', 'rfile', ',', ""'remaining'"", ',', '0', ')', 'if', 'remaining', '>', '0', ':', 'self', '.', 'rfile', '.', 'read', '(', 'remaining', ')', 'if', '""date""', 'not', 'in', 'hkeys', ':', 'self', '.', 'outheaders', '.', 'append', '(', '(', '""Date""', ',', 'rfc822', '.', 'formatdate', '(', ')', ')', ')', 'if', '""server""', 'not', 'in', 'hkeys', ':', 'self', '.', 'outheaders', '.', 'append', '(', '(', '""Server""', ',', 'self', '.', 'server', '.', 'server_name', ')', ')', 'buf', '=', '[', 'self', '.', 'server', '.', 'protocol', '+', '"" ""', '+', 'self', '.', 'status', '+', 'CRLF', ']', 'for', 'k', ',', 'v', 'in', 'self', '.', 'outheaders', ':', 'buf', '.', 'append', '(', 'k', '+', '"": ""', '+', 'v', '+', 'CRLF', ')', 'buf', '.', 'append', '(', 'CRLF', ')', 'self', '.', 'conn', '.', 'wfile', '.', 'sendall', '(', '""""', '.', 'join', '(', 'buf', ')', ')']","Assert, process, and send the HTTP response message-headers.
        
        You must set self.status, and self.outheaders before calling this.","['Assert', 'process', 'and', 'send', 'the', 'HTTP', 'response', 'message', '-', 'headers', '.', 'You', 'must', 'set', 'self', '.', 'status', 'and', 'self', '.', 'outheaders', 'before', 'calling', 'this', '.']",python,X,2,True,1,train
19838,swappsco/django-email,django_email/djemail.py,https://github.com/swappsco/django-email/blob/bc89a9b0bfa35748f5022abcd4cb3cfce0a10de2/django_email/djemail.py#L10-L64,"def send_email(to=None, message=None, template='base',
               context={}, subject=None):
    """"""
    Generic Method to Send Emails from template in an easier and modular way
    :param to: Email Address to send the email message
    :param message: Message content that is added to context
    :param template: Path of the email template (Without extension)
    :param context: Dict context variables to send to the template
    :param subject: Email subject
    """"""
    from_email = settings.DEFAULT_FROM_EMAIL
    if to is None:
        if len(settings.ADMINS) > 0:
            to = settings.ADMINS[0][1]
        else:
            raise AttributeError(""Not Admins defined"")

    if isinstance(to, (tuple, str)) or isinstance(to, (list, str)):
        pass
    elif unicode:
        if not isinstance(to, unicode):
            raise TypeError(
                ""email_to parameter has to be a List, Tuple or a String"")
    else:
            raise TypeError(
                ""email_to parameter has to be a List, Tuple or a String"")

    email_to = to if isinstance(to, tuple) else (to,)

    context.update(get_default_context())

    if message is not None:
        context.update({'message': message})

    try:
        email_template = get_email_template(template)
    except EmailTemplateNotFound:
        email_template = get_email_template('email/base')

    email_subject = subject or ""System Notification""

    if email_template.get('txt'):
        template_txt = email_template.get('txt')
        msg = EmailMultiAlternatives(
            email_subject,
            template_txt.render(context), from_email, email_to)
        if email_template.get('html'):
            template_html = email_template.get('html')
            html_content = template_html.render(context)
            msg.attach_alternative(html_content, 'text/html')
        return msg.send()
    else:
        raise AttributeError("".txt template does not exist"")

    raise Exception(""Could Not Send Email"")","['def', 'send_email', '(', 'to', '=', 'None', ',', 'message', '=', 'None', ',', 'template', '=', ""'base'"", ',', 'context', '=', '{', '}', ',', 'subject', '=', 'None', ')', ':', 'from_email', '=', 'settings', '.', 'DEFAULT_FROM_EMAIL', 'if', 'to', 'is', 'None', ':', 'if', 'len', '(', 'settings', '.', 'ADMINS', ')', '>', '0', ':', 'to', '=', 'settings', '.', 'ADMINS', '[', '0', ']', '[', '1', ']', 'else', ':', 'raise', 'AttributeError', '(', '""Not Admins defined""', ')', 'if', 'isinstance', '(', 'to', ',', '(', 'tuple', ',', 'str', ')', ')', 'or', 'isinstance', '(', 'to', ',', '(', 'list', ',', 'str', ')', ')', ':', 'pass', 'elif', 'unicode', ':', 'if', 'not', 'isinstance', '(', 'to', ',', 'unicode', ')', ':', 'raise', 'TypeError', '(', '""email_to parameter has to be a List, Tuple or a String""', ')', 'else', ':', 'raise', 'TypeError', '(', '""email_to parameter has to be a List, Tuple or a String""', ')', 'email_to', '=', 'to', 'if', 'isinstance', '(', 'to', ',', 'tuple', ')', 'else', '(', 'to', ',', ')', 'context', '.', 'update', '(', 'get_default_context', '(', ')', ')', 'if', 'message', 'is', 'not', 'None', ':', 'context', '.', 'update', '(', '{', ""'message'"", ':', 'message', '}', ')', 'try', ':', 'email_template', '=', 'get_email_template', '(', 'template', ')', 'except', 'EmailTemplateNotFound', ':', 'email_template', '=', 'get_email_template', '(', ""'email/base'"", ')', 'email_subject', '=', 'subject', 'or', '""System Notification""', 'if', 'email_template', '.', 'get', '(', ""'txt'"", ')', ':', 'template_txt', '=', 'email_template', '.', 'get', '(', ""'txt'"", ')', 'msg', '=', 'EmailMultiAlternatives', '(', 'email_subject', ',', 'template_txt', '.', 'render', '(', 'context', ')', ',', 'from_email', ',', 'email_to', ')', 'if', 'email_template', '.', 'get', '(', ""'html'"", ')', ':', 'template_html', '=', 'email_template', '.', 'get', '(', ""'html'"", ')', 'html_content', '=', 'template_html', '.', 'render', '(', 'context', ')', 'msg', '.', 'attach_alternative', '(', 'html_content', ',', ""'text/html'"", ')', 'return', 'msg', '.', 'send', '(', ')', 'else', ':', 'raise', 'AttributeError', '(', '"".txt template does not exist""', ')', 'raise', 'Exception', '(', '""Could Not Send Email""', ')']","Generic Method to Send Emails from template in an easier and modular way
    :param to: Email Address to send the email message
    :param message: Message content that is added to context
    :param template: Path of the email template (Without extension)
    :param context: Dict context variables to send to the template
    :param subject: Email subject","['Generic', 'Method', 'to', 'Send', 'Emails', 'from', 'template', 'in', 'an', 'easier', 'and', 'modular', 'way', ':', 'param', 'to', ':', 'Email', 'Address', 'to', 'send', 'the', 'email', 'message', ':', 'param', 'message', ':', 'Message', 'content', 'that', 'is', 'added', 'to', 'context', ':', 'param', 'template', ':', 'Path', 'of', 'the', 'email', 'template', '(', 'Without', 'extension', ')', ':', 'param', 'context', ':', 'Dict', 'context', 'variables', 'to', 'send', 'to', 'the', 'template', ':', 'param', 'subject', ':', 'Email', 'subject']",python,X,2,True,1,train
20636,wrboyce/telegrambot,telegrambot/api/__init__.py,https://github.com/wrboyce/telegrambot/blob/c35ce19886df4c306a2a19851cc1f63e3066d70d/telegrambot/api/__init__.py#L155-L167,"def send_voice(self, chat_id, voice, duration=None, reply_to_message_id=None, reply_markup=None):
        """"""
            Use this method to send audio files, if you want Telegram clients to display the file as a playable voice
            message. For this to work, your audio must be in an .ogg file encoded with OPUS (other formats may be sent
            as Audio or Document). On success, the sent Message is returned. Bots can currently send voice messages of
            up to 50 MB in size, this limit may be changed in the future.
        """"""
        payload = dict(chat_id=chat_id,
                       duration=duration,
                       reply_to_message_id=reply_to_message_id,
                       reply_markup=reply_markup)
        files = dict(voice=open(voice, 'rb'))
        return Message.from_api(self, **self._post('sendVoice', payload, files))","['def', 'send_voice', '(', 'self', ',', 'chat_id', ',', 'voice', ',', 'duration', '=', 'None', ',', 'reply_to_message_id', '=', 'None', ',', 'reply_markup', '=', 'None', ')', ':', 'payload', '=', 'dict', '(', 'chat_id', '=', 'chat_id', ',', 'duration', '=', 'duration', ',', 'reply_to_message_id', '=', 'reply_to_message_id', ',', 'reply_markup', '=', 'reply_markup', ')', 'files', '=', 'dict', '(', 'voice', '=', 'open', '(', 'voice', ',', ""'rb'"", ')', ')', 'return', 'Message', '.', 'from_api', '(', 'self', ',', '*', '*', 'self', '.', '_post', '(', ""'sendVoice'"", ',', 'payload', ',', 'files', ')', ')']","Use this method to send audio files, if you want Telegram clients to display the file as a playable voice
            message. For this to work, your audio must be in an .ogg file encoded with OPUS (other formats may be sent
            as Audio or Document). On success, the sent Message is returned. Bots can currently send voice messages of
            up to 50 MB in size, this limit may be changed in the future.","['Use', 'this', 'method', 'to', 'send', 'audio', 'files', 'if', 'you', 'want', 'Telegram', 'clients', 'to', 'display', 'the', 'file', 'as', 'a', 'playable', 'voice', 'message', '.', 'For', 'this', 'to', 'work', 'your', 'audio', 'must', 'be', 'in', 'an', '.', 'ogg', 'file', 'encoded', 'with', 'OPUS', '(', 'other', 'formats', 'may', 'be', 'sent', 'as', 'Audio', 'or', 'Document', ')', '.', 'On', 'success', 'the', 'sent', 'Message', 'is', 'returned', '.', 'Bots', 'can', 'currently', 'send', 'voice', 'messages', 'of', 'up', 'to', '50', 'MB', 'in', 'size', 'this', 'limit', 'may', 'be', 'changed', 'in', 'the', 'future', '.']",python,X,2,True,1,train
21953,miguelgrinberg/Flask-MarrowMailer,flask_marrowmailer.py,https://github.com/miguelgrinberg/Flask-MarrowMailer/blob/daf1ac0745fb31db2f43f4f7dc24c6f50ae96764/flask_marrowmailer.py#L71-L82,"def send(self, msg):
        '''Send the message. If message is an iterable, then send 
        all the messages.'''
        app = self.app or current_app
        mailer = app.extensions['marrowmailer']
        mailer.start()
        if not hasattr(msg, '__iter__'):
            result = mailer.send(msg)
        else:
            result = map(lambda message: mailer.send(message), msg)
        mailer.stop()
        return result","['def', 'send', '(', 'self', ',', 'msg', ')', ':', 'app', '=', 'self', '.', 'app', 'or', 'current_app', 'mailer', '=', 'app', '.', 'extensions', '[', ""'marrowmailer'"", ']', 'mailer', '.', 'start', '(', ')', 'if', 'not', 'hasattr', '(', 'msg', ',', ""'__iter__'"", ')', ':', 'result', '=', 'mailer', '.', 'send', '(', 'msg', ')', 'else', ':', 'result', '=', 'map', '(', 'lambda', 'message', ':', 'mailer', '.', 'send', '(', 'message', ')', ',', 'msg', ')', 'mailer', '.', 'stop', '(', ')', 'return', 'result']","Send the message. If message is an iterable, then send 
        all the messages.","['Send', 'the', 'message', '.', 'If', 'message', 'is', 'an', 'iterable', 'then', 'send', 'all', 'the', 'messages', '.']",python,X,2,True,1,train
23808,reillysiemens/layabout,examples/early-connection/example.py,https://github.com/reillysiemens/layabout/blob/a146c47f2558e66bb51cf708d39909b93eaea7f4/examples/early-connection/example.py#L31-L38,"def send_message(slack):
    """""" Prompt for and send a message to a channel. """"""
    channel = input('Which channel would you like to message? ')
    message = input('What should the message be? ')
    channel_id = channel_to_id(slack, channel)

    print(f""Sending message to #{channel} (id: {channel_id})!"")
    slack.rtm_send_message(channel_id, message)","['def', 'send_message', '(', 'slack', ')', ':', 'channel', '=', 'input', '(', ""'Which channel would you like to message? '"", ')', 'message', '=', 'input', '(', ""'What should the message be? '"", ')', 'channel_id', '=', 'channel_to_id', '(', 'slack', ',', 'channel', ')', 'print', '(', 'f""Sending message to #{channel} (id: {channel_id})!""', ')', 'slack', '.', 'rtm_send_message', '(', 'channel_id', ',', 'message', ')']",Prompt for and send a message to a channel.,"['Prompt', 'for', 'and', 'send', 'a', 'message', 'to', 'a', 'channel', '.']",python,X,2,True,1,train
25212,projectshift/shift-boiler,boiler/user/user_service.py,https://github.com/projectshift/shift-boiler/blob/8e6f3a3e4b9493fb6c8bd16bed160ede153bfb0b/boiler/user/user_service.py#L358-L385,"def register(self, user_data, base_confirm_url='', send_welcome=True):
        """"""
        Register user
        Accepts user data, validates it and performs registration. Will send
        a welcome message with a confirmation link on success.

        :param user_data: dic, populate user with data
        :param send_welcome: bool, whether to send welcome or skip it (testing)
        :param base_confirm_url: str, base confirmation link url
        :return: boiler.user.models.User
        """"""
        user = self.__model__(**user_data)
        schema = RegisterSchema()
        valid = schema.process(user)
        if not valid:
            return valid

        db.session.add(user)
        db.session.commit()
        if not user.id:
            return False

        # send welcome message
        if send_welcome:
            self.send_welcome_message(user, base_confirm_url)

        events.register_event.send(user)
        return user","['def', 'register', '(', 'self', ',', 'user_data', ',', 'base_confirm_url', '=', ""''"", ',', 'send_welcome', '=', 'True', ')', ':', 'user', '=', 'self', '.', '__model__', '(', '*', '*', 'user_data', ')', 'schema', '=', 'RegisterSchema', '(', ')', 'valid', '=', 'schema', '.', 'process', '(', 'user', ')', 'if', 'not', 'valid', ':', 'return', 'valid', 'db', '.', 'session', '.', 'add', '(', 'user', ')', 'db', '.', 'session', '.', 'commit', '(', ')', 'if', 'not', 'user', '.', 'id', ':', 'return', 'False', '# send welcome message', 'if', 'send_welcome', ':', 'self', '.', 'send_welcome_message', '(', 'user', ',', 'base_confirm_url', ')', 'events', '.', 'register_event', '.', 'send', '(', 'user', ')', 'return', 'user']","Register user
        Accepts user data, validates it and performs registration. Will send
        a welcome message with a confirmation link on success.

        :param user_data: dic, populate user with data
        :param send_welcome: bool, whether to send welcome or skip it (testing)
        :param base_confirm_url: str, base confirmation link url
        :return: boiler.user.models.User","['Register', 'user', 'Accepts', 'user', 'data', 'validates', 'it', 'and', 'performs', 'registration', '.', 'Will', 'send', 'a', 'welcome', 'message', 'with', 'a', 'confirmation', 'link', 'on', 'success', '.']",python,X,2,True,1,train
25218,projectshift/shift-boiler,boiler/user/user_service.py,https://github.com/projectshift/shift-boiler/blob/8e6f3a3e4b9493fb6c8bd16bed160ede153bfb0b/boiler/user/user_service.py#L532-L538,"def request_password_reset(self, user, base_url):
        """""" Regenerate password link and send message """"""
        user.generate_password_link()
        db.session.add(user)
        db.session.commit()
        events.password_change_requested_event.send(user)
        self.send_password_change_message(user, base_url)","['def', 'request_password_reset', '(', 'self', ',', 'user', ',', 'base_url', ')', ':', 'user', '.', 'generate_password_link', '(', ')', 'db', '.', 'session', '.', 'add', '(', 'user', ')', 'db', '.', 'session', '.', 'commit', '(', ')', 'events', '.', 'password_change_requested_event', '.', 'send', '(', 'user', ')', 'self', '.', 'send_password_change_message', '(', 'user', ',', 'base_url', ')']",Regenerate password link and send message,"['Regenerate', 'password', 'link', 'and', 'send', 'message']",python,X,2,True,1,train
26622,praekeltfoundation/seed-message-sender,message_sender/tasks.py,https://github.com/praekeltfoundation/seed-message-sender/blob/257b01635171b9dbe1f5f13baa810c971bb2620e/message_sender/tasks.py#L225-L383,"def run(self, message_id, **kwargs):
        """"""
        Load and contruct message and send them off
        """"""
        log = self.get_logger(**kwargs)

        error_retry_count = kwargs.get(""error_retry_count"", 0)
        if error_retry_count >= self.max_error_retries:
            raise MaxRetriesExceededError(
                ""Can't retry {0}[{1}] args:{2} kwargs:{3}"".format(
                    self.name, self.request.id, self.request.args, kwargs
                )
            )

        log.info(""Loading Outbound Message <%s>"" % message_id)
        try:
            message = Outbound.objects.select_related(""channel"").get(id=message_id)
        except ObjectDoesNotExist:
            logger.error(""Missing Outbound message"", exc_info=True)
            return

        if message.attempts < settings.MESSAGE_SENDER_MAX_RETRIES:
            if error_retry_count > 0:
                retry_delay = calculate_retry_delay(error_retry_count)
            else:
                retry_delay = self.default_retry_delay
            log.info(""Attempts: %s"" % message.attempts)
            # send or resend
            try:
                if not message.channel:
                    channel = Channel.objects.get(default=True)
                else:
                    channel = message.channel

                sender = self.get_client(channel)
                ConcurrencyLimiter.manage_limit(self, channel)

                if not message.to_addr and message.to_identity:
                    message.to_addr = get_identity_address(
                        message.to_identity, use_communicate_through=True
                    )
                    if not message.to_addr:
                        self.fire_failed_msisdn_lookup(message.to_identity)
                        return

                if message.to_addr and not message.to_identity:
                    result = get_identity_by_address(message.to_addr)

                    if result:
                        message.to_identity = result[0][""id""]
                    else:
                        identity = {
                            ""details"": {
                                ""default_addr_type"": ""msisdn"",
                                ""addresses"": {
                                    ""msisdn"": {message.to_addr: {""default"": True}}
                                },
                            }
                        }
                        identity = create_identity(identity)
                        message.to_identity = identity[""id""]

                if ""voice_speech_url"" in message.metadata:
                    # OBD number of tries metric
                    fire_metric.apply_async(
                        kwargs={
                            ""metric_name"": ""vumimessage.obd.tries.sum"",
                            ""metric_value"": 1.0,
                        }
                    )

                    # Voice message
                    speech_url = message.metadata[""voice_speech_url""]
                    vumiresponse = sender.send_voice(
                        voice_to_addr_formatter(message.to_addr),
                        message.content,
                        speech_url=speech_url,
                        session_event=""new"",
                    )
                    log.info(""Sent voice message to <%s>"" % message.to_addr)

                elif ""image_url"" in message.metadata:
                    # Image message
                    image_url = message.metadata[""image_url""]
                    vumiresponse = sender.send_image(
                        text_to_addr_formatter(message.to_addr),
                        message.content,
                        image_url=image_url,
                    )
                    log.info(""Sent image message to <%s>"" % (message.to_addr,))

                else:
                    # Plain content
                    vumiresponse = sender.send_text(
                        text_to_addr_formatter(message.to_addr),
                        message.content,
                        metadata=message.metadata,
                        session_event=""new"",
                    )
                    log.info(""Sent text message to <%s>"" % (message.to_addr,))

                message.last_sent_time = timezone.now()
                message.attempts += 1
                message.vumi_message_id = vumiresponse[""message_id""]
                message.save()
                fire_metric.apply_async(
                    kwargs={""metric_name"": ""vumimessage.tries.sum"", ""metric_value"": 1.0}
                )
            except requests_exceptions.ConnectionError as exc:
                log.info(""Connection Error sending message"")
                fire_metric.delay(""sender.send_message.connection_error.sum"", 1)
                kwargs[""error_retry_count""] = error_retry_count + 1
                self.retry(
                    exc=exc, countdown=retry_delay, args=(message_id,), kwargs=kwargs
                )
            except requests_exceptions.Timeout as exc:
                log.info(""Sending message failed due to timeout"")
                fire_metric.delay(""sender.send_message.timeout.sum"", 1)
                kwargs[""error_retry_count""] = error_retry_count + 1
                self.retry(
                    exc=exc, countdown=retry_delay, args=(message_id,), kwargs=kwargs
                )
            except (requests_exceptions.HTTPError, HTTPServiceError) as exc:
                # retry message sending if in 500 range (3 default
                # retries)
                log.info(
                    ""Sending message failed due to status: %s""
                    % exc.response.status_code
                )
                metric_name = (
                    ""sender.send_message.http_error.%s.sum"" % exc.response.status_code
                )
                fire_metric.delay(metric_name, 1)
                kwargs[""error_retry_count""] = error_retry_count + 1
                self.retry(
                    exc=exc, countdown=retry_delay, args=(message_id,), kwargs=kwargs
                )

            # If we've gotten this far the message send was successful.
            fire_metric.apply_async(
                kwargs={""metric_name"": ""message.sent.sum"", ""metric_value"": 1.0}
            )
            return vumiresponse

        else:
            # This is for retries based on async nacks from the transport.
            log.info(""Message <%s> at max retries."" % str(message_id))
            message.to_addr = """"
            message.save(update_fields=[""to_addr""])
            fire_metric.apply_async(
                kwargs={
                    ""metric_name"": ""vumimessage.maxretries.sum"",
                    ""metric_value"": 1.0,
                }
            )
            # Count failures on exhausted tries.
            fire_metric.apply_async(
                kwargs={""metric_name"": ""message.failures.sum"", ""metric_value"": 1.0}
            )","['def', 'run', '(', 'self', ',', 'message_id', ',', '*', '*', 'kwargs', ')', ':', 'log', '=', 'self', '.', 'get_logger', '(', '*', '*', 'kwargs', ')', 'error_retry_count', '=', 'kwargs', '.', 'get', '(', '""error_retry_count""', ',', '0', ')', 'if', 'error_retry_count', '>=', 'self', '.', 'max_error_retries', ':', 'raise', 'MaxRetriesExceededError', '(', '""Can\'t retry {0}[{1}] args:{2} kwargs:{3}""', '.', 'format', '(', 'self', '.', 'name', ',', 'self', '.', 'request', '.', 'id', ',', 'self', '.', 'request', '.', 'args', ',', 'kwargs', ')', ')', 'log', '.', 'info', '(', '""Loading Outbound Message <%s>""', '%', 'message_id', ')', 'try', ':', 'message', '=', 'Outbound', '.', 'objects', '.', 'select_related', '(', '""channel""', ')', '.', 'get', '(', 'id', '=', 'message_id', ')', 'except', 'ObjectDoesNotExist', ':', 'logger', '.', 'error', '(', '""Missing Outbound message""', ',', 'exc_info', '=', 'True', ')', 'return', 'if', 'message', '.', 'attempts', '<', 'settings', '.', 'MESSAGE_SENDER_MAX_RETRIES', ':', 'if', 'error_retry_count', '>', '0', ':', 'retry_delay', '=', 'calculate_retry_delay', '(', 'error_retry_count', ')', 'else', ':', 'retry_delay', '=', 'self', '.', 'default_retry_delay', 'log', '.', 'info', '(', '""Attempts: %s""', '%', 'message', '.', 'attempts', ')', '# send or resend', 'try', ':', 'if', 'not', 'message', '.', 'channel', ':', 'channel', '=', 'Channel', '.', 'objects', '.', 'get', '(', 'default', '=', 'True', ')', 'else', ':', 'channel', '=', 'message', '.', 'channel', 'sender', '=', 'self', '.', 'get_client', '(', 'channel', ')', 'ConcurrencyLimiter', '.', 'manage_limit', '(', 'self', ',', 'channel', ')', 'if', 'not', 'message', '.', 'to_addr', 'and', 'message', '.', 'to_identity', ':', 'message', '.', 'to_addr', '=', 'get_identity_address', '(', 'message', '.', 'to_identity', ',', 'use_communicate_through', '=', 'True', ')', 'if', 'not', 'message', '.', 'to_addr', ':', 'self', '.', 'fire_failed_msisdn_lookup', '(', 'message', '.', 'to_identity', ')', 'return', 'if', 'message', '.', 'to_addr', 'and', 'not', 'message', '.', 'to_identity', ':', 'result', '=', 'get_identity_by_address', '(', 'message', '.', 'to_addr', ')', 'if', 'result', ':', 'message', '.', 'to_identity', '=', 'result', '[', '0', ']', '[', '""id""', ']', 'else', ':', 'identity', '=', '{', '""details""', ':', '{', '""default_addr_type""', ':', '""msisdn""', ',', '""addresses""', ':', '{', '""msisdn""', ':', '{', 'message', '.', 'to_addr', ':', '{', '""default""', ':', 'True', '}', '}', '}', ',', '}', '}', 'identity', '=', 'create_identity', '(', 'identity', ')', 'message', '.', 'to_identity', '=', 'identity', '[', '""id""', ']', 'if', '""voice_speech_url""', 'in', 'message', '.', 'metadata', ':', '# OBD number of tries metric', 'fire_metric', '.', 'apply_async', '(', 'kwargs', '=', '{', '""metric_name""', ':', '""vumimessage.obd.tries.sum""', ',', '""metric_value""', ':', '1.0', ',', '}', ')', '# Voice message', 'speech_url', '=', 'message', '.', 'metadata', '[', '""voice_speech_url""', ']', 'vumiresponse', '=', 'sender', '.', 'send_voice', '(', 'voice_to_addr_formatter', '(', 'message', '.', 'to_addr', ')', ',', 'message', '.', 'content', ',', 'speech_url', '=', 'speech_url', ',', 'session_event', '=', '""new""', ',', ')', 'log', '.', 'info', '(', '""Sent voice message to <%s>""', '%', 'message', '.', 'to_addr', ')', 'elif', '""image_url""', 'in', 'message', '.', 'metadata', ':', '# Image message', 'image_url', '=', 'message', '.', 'metadata', '[', '""image_url""', ']', 'vumiresponse', '=', 'sender', '.', 'send_image', '(', 'text_to_addr_formatter', '(', 'message', '.', 'to_addr', ')', ',', 'message', '.', 'content', ',', 'image_url', '=', 'image_url', ',', ')', 'log', '.', 'info', '(', '""Sent image message to <%s>""', '%', '(', 'message', '.', 'to_addr', ',', ')', ')', 'else', ':', '# Plain content', 'vumiresponse', '=', 'sender', '.', 'send_text', '(', 'text_to_addr_formatter', '(', 'message', '.', 'to_addr', ')', ',', 'message', '.', 'content', ',', 'metadata', '=', 'message', '.', 'metadata', ',', 'session_event', '=', '""new""', ',', ')', 'log', '.', 'info', '(', '""Sent text message to <%s>""', '%', '(', 'message', '.', 'to_addr', ',', ')', ')', 'message', '.', 'last_sent_time', '=', 'timezone', '.', 'now', '(', ')', 'message', '.', 'attempts', '+=', '1', 'message', '.', 'vumi_message_id', '=', 'vumiresponse', '[', '""message_id""', ']', 'message', '.', 'save', '(', ')', 'fire_metric', '.', 'apply_async', '(', 'kwargs', '=', '{', '""metric_name""', ':', '""vumimessage.tries.sum""', ',', '""metric_value""', ':', '1.0', '}', ')', 'except', 'requests_exceptions', '.', 'ConnectionError', 'as', 'exc', ':', 'log', '.', 'info', '(', '""Connection Error sending message""', ')', 'fire_metric', '.', 'delay', '(', '""sender.send_message.connection_error.sum""', ',', '1', ')', 'kwargs', '[', '""error_retry_count""', ']', '=', 'error_retry_count', '+', '1', 'self', '.', 'retry', '(', 'exc', '=', 'exc', ',', 'countdown', '=', 'retry_delay', ',', 'args', '=', '(', 'message_id', ',', ')', ',', 'kwargs', '=', 'kwargs', ')', 'except', 'requests_exceptions', '.', 'Timeout', 'as', 'exc', ':', 'log', '.', 'info', '(', '""Sending message failed due to timeout""', ')', 'fire_metric', '.', 'delay', '(', '""sender.send_message.timeout.sum""', ',', '1', ')', 'kwargs', '[', '""error_retry_count""', ']', '=', 'error_retry_count', '+', '1', 'self', '.', 'retry', '(', 'exc', '=', 'exc', ',', 'countdown', '=', 'retry_delay', ',', 'args', '=', '(', 'message_id', ',', ')', ',', 'kwargs', '=', 'kwargs', ')', 'except', '(', 'requests_exceptions', '.', 'HTTPError', ',', 'HTTPServiceError', ')', 'as', 'exc', ':', '# retry message sending if in 500 range (3 default', '# retries)', 'log', '.', 'info', '(', '""Sending message failed due to status: %s""', '%', 'exc', '.', 'response', '.', 'status_code', ')', 'metric_name', '=', '(', '""sender.send_message.http_error.%s.sum""', '%', 'exc', '.', 'response', '.', 'status_code', ')', 'fire_metric', '.', 'delay', '(', 'metric_name', ',', '1', ')', 'kwargs', '[', '""error_retry_count""', ']', '=', 'error_retry_count', '+', '1', 'self', '.', 'retry', '(', 'exc', '=', 'exc', ',', 'countdown', '=', 'retry_delay', ',', 'args', '=', '(', 'message_id', ',', ')', ',', 'kwargs', '=', 'kwargs', ')', ""# If we've gotten this far the message send was successful."", 'fire_metric', '.', 'apply_async', '(', 'kwargs', '=', '{', '""metric_name""', ':', '""message.sent.sum""', ',', '""metric_value""', ':', '1.0', '}', ')', 'return', 'vumiresponse', 'else', ':', '# This is for retries based on async nacks from the transport.', 'log', '.', 'info', '(', '""Message <%s> at max retries.""', '%', 'str', '(', 'message_id', ')', ')', 'message', '.', 'to_addr', '=', '""""', 'message', '.', 'save', '(', 'update_fields', '=', '[', '""to_addr""', ']', ')', 'fire_metric', '.', 'apply_async', '(', 'kwargs', '=', '{', '""metric_name""', ':', '""vumimessage.maxretries.sum""', ',', '""metric_value""', ':', '1.0', ',', '}', ')', '# Count failures on exhausted tries.', 'fire_metric', '.', 'apply_async', '(', 'kwargs', '=', '{', '""metric_name""', ':', '""message.failures.sum""', ',', '""metric_value""', ':', '1.0', '}', ')']",Load and contruct message and send them off,"['Load', 'and', 'contruct', 'message', 'and', 'send', 'them', 'off']",python,X,2,True,1,train
397,supercoderz/pyzmq-wrapper,zmqwrapper/sockets.py,https://github.com/supercoderz/pyzmq-wrapper/blob/b16c0313dd10febd5060ee0589285025a09fa26a/zmqwrapper/sockets.py#L10-L32,"def send(self,message,message_type,topic=''):
        """"""
        Send the message on the socket.
        
        Args:
            - message: the message to publish
            - message_type: the type of message being sent
            - topic: the topic on which to send the message. Defaults to ''.
        """"""
        if message_type == RAW:
            self._sock.send(message)
        elif message_type == PYOBJ:
            self._sock.send_pyobj(message)
        elif message_type == JSON:
            self._sock.send_json(message)
        elif message_type == MULTIPART:
            self._sock.send_multipart([topic, message])
        elif message_type == STRING:
            self._sock.send_string(message)
        elif message_type == UNICODE:
            self._sock.send_unicode(message)
        else:
            raise Exception(""Unknown message type %s""%(message_type,))","['def', 'send', '(', 'self', ',', 'message', ',', 'message_type', ',', 'topic', '=', ""''"", ')', ':', 'if', 'message_type', '==', 'RAW', ':', 'self', '.', '_sock', '.', 'send', '(', 'message', ')', 'elif', 'message_type', '==', 'PYOBJ', ':', 'self', '.', '_sock', '.', 'send_pyobj', '(', 'message', ')', 'elif', 'message_type', '==', 'JSON', ':', 'self', '.', '_sock', '.', 'send_json', '(', 'message', ')', 'elif', 'message_type', '==', 'MULTIPART', ':', 'self', '.', '_sock', '.', 'send_multipart', '(', '[', 'topic', ',', 'message', ']', ')', 'elif', 'message_type', '==', 'STRING', ':', 'self', '.', '_sock', '.', 'send_string', '(', 'message', ')', 'elif', 'message_type', '==', 'UNICODE', ':', 'self', '.', '_sock', '.', 'send_unicode', '(', 'message', ')', 'else', ':', 'raise', 'Exception', '(', '""Unknown message type %s""', '%', '(', 'message_type', ',', ')', ')']","Send the message on the socket.
        
        Args:
            - message: the message to publish
            - message_type: the type of message being sent
            - topic: the topic on which to send the message. Defaults to ''.","['Send', 'the', 'message', 'on', 'the', 'socket', '.', 'Args', ':', '-', 'message', ':', 'the', 'message', 'to', 'publish', '-', 'message_type', ':', 'the', 'type', 'of', 'message', 'being', 'sent', '-', 'topic', ':', 'the', 'topic', 'on', 'which', 'to', 'send', 'the', 'message', '.', 'Defaults', 'to', '.']",python,X,2,True,1,train
691,supercoderz/pyzmq-wrapper,zmqwrapper/publishers.py,https://github.com/supercoderz/pyzmq-wrapper/blob/b16c0313dd10febd5060ee0589285025a09fa26a/zmqwrapper/publishers.py#L26-L38,"def publish(self,message,message_type,topic=''):
        """"""
        Publish the message on the PUB socket with the given topic name.
        
        Args:
            - message: the message to publish
            - message_type: the type of message being sent
            - topic: the topic on which to send the message. Defaults to ''.
        """"""
        if message_type == MULTIPART:
            raise Exception(""Unsupported request type"")

        super(Publisher,self).send(message,message_type,topic)","['def', 'publish', '(', 'self', ',', 'message', ',', 'message_type', ',', 'topic', '=', ""''"", ')', ':', 'if', 'message_type', '==', 'MULTIPART', ':', 'raise', 'Exception', '(', '""Unsupported request type""', ')', 'super', '(', 'Publisher', ',', 'self', ')', '.', 'send', '(', 'message', ',', 'message_type', ',', 'topic', ')']","Publish the message on the PUB socket with the given topic name.
        
        Args:
            - message: the message to publish
            - message_type: the type of message being sent
            - topic: the topic on which to send the message. Defaults to ''.","['Publish', 'the', 'message', 'on', 'the', 'PUB', 'socket', 'with', 'the', 'given', 'topic', 'name', '.', 'Args', ':', '-', 'message', ':', 'the', 'message', 'to', 'publish', '-', 'message_type', ':', 'the', 'type', 'of', 'message', 'being', 'sent', '-', 'topic', ':', 'the', 'topic', 'on', 'which', 'to', 'send', 'the', 'message', '.', 'Defaults', 'to', '.']",python,X,2,True,1,train
3650,botstory/botstory,botstory/chat.py,https://github.com/botstory/botstory/blob/9c5b2fc7f7a14dbd467d70f60d5ba855ef89dac3/botstory/chat.py#L65-L75,"async def send_audio(self, url, user, options=None):
        """"""
        send audio message

        :param url: link to the audio file
        :param user: target user
        :param options:
        :return:
        """"""
        tasks = [interface.send_audio(user, url, options) for _, interface in self.interfaces.items()]
        return [body for body in await asyncio.gather(*tasks)]","['async', 'def', 'send_audio', '(', 'self', ',', 'url', ',', 'user', ',', 'options', '=', 'None', ')', ':', 'tasks', '=', '[', 'interface', '.', 'send_audio', '(', 'user', ',', 'url', ',', 'options', ')', 'for', '_', ',', 'interface', 'in', 'self', '.', 'interfaces', '.', 'items', '(', ')', ']', 'return', '[', 'body', 'for', 'body', 'in', 'await', 'asyncio', '.', 'gather', '(', '*', 'tasks', ')', ']']","send audio message

        :param url: link to the audio file
        :param user: target user
        :param options:
        :return:","['send', 'audio', 'message']",python,X,2,True,1,train
3672,botstory/botstory,botstory/story.py,https://github.com/botstory/botstory/blob/9c5b2fc7f7a14dbd467d70f60d5ba855ef89dac3/botstory/story.py#L92-L101,"async def send_audio(self, url, user, options=None):
        """"""
        send audio message

        :param url: link to the audio file
        :param user: target user
        :param options:
        :return:
        """"""
        return await self.chat.send_audio(url, user, options)","['async', 'def', 'send_audio', '(', 'self', ',', 'url', ',', 'user', ',', 'options', '=', 'None', ')', ':', 'return', 'await', 'self', '.', 'chat', '.', 'send_audio', '(', 'url', ',', 'user', ',', 'options', ')']","send audio message

        :param url: link to the audio file
        :param user: target user
        :param options:
        :return:","['send', 'audio', 'message']",python,X,2,True,1,train
6500,AtomHash/evernode,evernode/scripts/sendemail.py,https://github.com/AtomHash/evernode/blob/b2fb91555fb937a3f3eba41db56dee26f9b034be/evernode/scripts/sendemail.py#L109-L117,"def send(self, email=None):
        """""" send email message """"""
        if email is None and self.send_as_one:
            self.smtp.send_message(
                self.multipart, self.config['EMAIL'], self.addresses)
        elif email is not None and self.send_as_one is False:
            self.smtp.send_message(
                self.multipart, self.config['EMAIL'], email)
        self.multipart = MIMEMultipart('alternative')","['def', 'send', '(', 'self', ',', 'email', '=', 'None', ')', ':', 'if', 'email', 'is', 'None', 'and', 'self', '.', 'send_as_one', ':', 'self', '.', 'smtp', '.', 'send_message', '(', 'self', '.', 'multipart', ',', 'self', '.', 'config', '[', ""'EMAIL'"", ']', ',', 'self', '.', 'addresses', ')', 'elif', 'email', 'is', 'not', 'None', 'and', 'self', '.', 'send_as_one', 'is', 'False', ':', 'self', '.', 'smtp', '.', 'send_message', '(', 'self', '.', 'multipart', ',', 'self', '.', 'config', '[', ""'EMAIL'"", ']', ',', 'email', ')', 'self', '.', 'multipart', '=', 'MIMEMultipart', '(', ""'alternative'"", ')']",send email message,"['send', 'email', 'message']",python,X,2,True,1,train
8418,Duke-GCB/lando-messaging,lando_messaging/workqueue.py,https://github.com/Duke-GCB/lando-messaging/blob/b90ccc79a874714e0776af8badf505bb2b56c0ec/lando_messaging/workqueue.py#L109-L125,"def send_durable_exchange_message(self, exchange_name, body):
        """"""
        Send a message with the specified body to an exchange.
        :param exchange_name: str: name of the exchange to send the message into
        :param body: str: contents of the message
        :return Bool: True when delivery confirmed
        """"""
        self.connect()
        channel = self.connection.channel()
        # Fanout will send message to multiple subscribers
        channel.exchange_declare(exchange=exchange_name, type='fanout')
        result = channel.basic_publish(exchange=exchange_name, routing_key='', body=body,
                                       properties=pika.BasicProperties(
                                           delivery_mode=2,  # make message persistent
                                       ))
        self.close()
        return result","['def', 'send_durable_exchange_message', '(', 'self', ',', 'exchange_name', ',', 'body', ')', ':', 'self', '.', 'connect', '(', ')', 'channel', '=', 'self', '.', 'connection', '.', 'channel', '(', ')', '# Fanout will send message to multiple subscribers', 'channel', '.', 'exchange_declare', '(', 'exchange', '=', 'exchange_name', ',', 'type', '=', ""'fanout'"", ')', 'result', '=', 'channel', '.', 'basic_publish', '(', 'exchange', '=', 'exchange_name', ',', 'routing_key', '=', ""''"", ',', 'body', '=', 'body', ',', 'properties', '=', 'pika', '.', 'BasicProperties', '(', 'delivery_mode', '=', '2', ',', '# make message persistent', ')', ')', 'self', '.', 'close', '(', ')', 'return', 'result']","Send a message with the specified body to an exchange.
        :param exchange_name: str: name of the exchange to send the message into
        :param body: str: contents of the message
        :return Bool: True when delivery confirmed","['Send', 'a', 'message', 'with', 'the', 'specified', 'body', 'to', 'an', 'exchange', '.', ':', 'param', 'exchange_name', ':', 'str', ':', 'name', 'of', 'the', 'exchange', 'to', 'send', 'the', 'message', 'into', ':', 'param', 'body', ':', 'str', ':', 'contents', 'of', 'the', 'message', ':', 'return', 'Bool', ':', 'True', 'when', 'delivery', 'confirmed']",python,X,2,True,1,train
11811,symphonyoss/python-symphony,symphony/Agent/base.py,https://github.com/symphonyoss/python-symphony/blob/b939f35fbda461183ec0c01790c754f89a295be0/symphony/Agent/base.py#L49-L60,"def send_message(self, threadid, msgFormat, message):
        ''' send message to threadid/stream '''
        # using deprecated v3 message create because of bug in codegen of v4 ( multipart/form-data )
        response, status_code = self.__agent__.Messages.post_v3_stream_sid_message_create(
            sessionToken=self.__session__,
            keyManagerToken=self.__keymngr__,
            sid=threadid,
            message={""format"": msgFormat,
                     ""message"": message}
        ).result()
        self.logger.debug('%s: %s' % (status_code, response))
        return status_code, response","['def', 'send_message', '(', 'self', ',', 'threadid', ',', 'msgFormat', ',', 'message', ')', ':', '# using deprecated v3 message create because of bug in codegen of v4 ( multipart/form-data )', 'response', ',', 'status_code', '=', 'self', '.', '__agent__', '.', 'Messages', '.', 'post_v3_stream_sid_message_create', '(', 'sessionToken', '=', 'self', '.', '__session__', ',', 'keyManagerToken', '=', 'self', '.', '__keymngr__', ',', 'sid', '=', 'threadid', ',', 'message', '=', '{', '""format""', ':', 'msgFormat', ',', '""message""', ':', 'message', '}', ')', '.', 'result', '(', ')', 'self', '.', 'logger', '.', 'debug', '(', ""'%s: %s'"", '%', '(', 'status_code', ',', 'response', ')', ')', 'return', 'status_code', ',', 'response']",send message to threadid/stream,"['send', 'message', 'to', 'threadid', '/', 'stream']",python,X,2,True,1,train
16459,blockstack/virtualchain,virtualchain/lib/blockchain/bitcoin_blockchain/blocks.py,https://github.com/blockstack/virtualchain/blob/fcfc970064ca7dfcab26ebd3ab955870a763ea39/virtualchain/lib/blockchain/bitcoin_blockchain/blocks.py#L305-L317,"def begin(self):
        """"""
        This method will implement the handshake of the
        Bitcoin protocol. It will send the Version message,
        and block until it receives a VerAck.
        Once we receive the version, we'll send the verack,
        and begin downloading.
        """"""
        log.debug(""handshake (version %s)"" % PROTOCOL_VERSION)
        version = Version()
        version.services = 0    # can't send blocks
        log.debug(""send Version"")
        self.send_message(version)","['def', 'begin', '(', 'self', ')', ':', 'log', '.', 'debug', '(', '""handshake (version %s)""', '%', 'PROTOCOL_VERSION', ')', 'version', '=', 'Version', '(', ')', 'version', '.', 'services', '=', '0', ""# can't send blocks"", 'log', '.', 'debug', '(', '""send Version""', ')', 'self', '.', 'send_message', '(', 'version', ')']","This method will implement the handshake of the
        Bitcoin protocol. It will send the Version message,
        and block until it receives a VerAck.
        Once we receive the version, we'll send the verack,
        and begin downloading.","['This', 'method', 'will', 'implement', 'the', 'handshake', 'of', 'the', 'Bitcoin', 'protocol', '.', 'It', 'will', 'send', 'the', 'Version', 'message', 'and', 'block', 'until', 'it', 'receives', 'a', 'VerAck', '.', 'Once', 'we', 'receive', 'the', 'version', 'we', 'll', 'send', 'the', 'verack', 'and', 'begin', 'downloading', '.']",python,X,2,True,1,train
16460,blockstack/virtualchain,virtualchain/lib/blockchain/bitcoin_blockchain/blocks.py,https://github.com/blockstack/virtualchain/blob/fcfc970064ca7dfcab26ebd3ab955870a763ea39/virtualchain/lib/blockchain/bitcoin_blockchain/blocks.py#L320-L360,"def handle_version(self, message_header, message):
        """"""
        This method will handle the Version message and
        will send a VerAck message when it receives the
        Version message.

        :param message_header: The Version message header
        :param message: The Version message
        """"""
        log.debug(""handle version"")
        verack = VerAck()
        log.debug(""send VerAck"")
        self.send_message(verack)
        self.verack = True

        start_block_height = sorted(self.blocks.keys())[0]
        if start_block_height < 1:
            start_block_height = 1

        # ask for all blocks
        block_hashes = []
        for height in sorted(self.blocks.keys()):
            block_hashes.append( int(self.blocks[height], 16) )

        start_block_height = sorted(self.blocks.keys())[0]
        end_block_height = sorted(self.blocks.keys())[-1]
    
        log.debug(""send getdata for %s-%s (%064x-%064x)"" % (start_block_height, end_block_height, block_hashes[0], block_hashes[-1]))

        # send off the getdata
        getdata = GetData()
        block_inv_vec = []
        for block_hash in block_hashes:
            block_inv = Inventory()
            block_inv.inv_type = INVENTORY_TYPE[""MSG_BLOCK""]
            block_inv.inv_hash = block_hash

            block_inv_vec.append(block_inv)

        getdata.inventory = block_inv_vec
        self.send_message(getdata)","['def', 'handle_version', '(', 'self', ',', 'message_header', ',', 'message', ')', ':', 'log', '.', 'debug', '(', '""handle version""', ')', 'verack', '=', 'VerAck', '(', ')', 'log', '.', 'debug', '(', '""send VerAck""', ')', 'self', '.', 'send_message', '(', 'verack', ')', 'self', '.', 'verack', '=', 'True', 'start_block_height', '=', 'sorted', '(', 'self', '.', 'blocks', '.', 'keys', '(', ')', ')', '[', '0', ']', 'if', 'start_block_height', '<', '1', ':', 'start_block_height', '=', '1', '# ask for all blocks', 'block_hashes', '=', '[', ']', 'for', 'height', 'in', 'sorted', '(', 'self', '.', 'blocks', '.', 'keys', '(', ')', ')', ':', 'block_hashes', '.', 'append', '(', 'int', '(', 'self', '.', 'blocks', '[', 'height', ']', ',', '16', ')', ')', 'start_block_height', '=', 'sorted', '(', 'self', '.', 'blocks', '.', 'keys', '(', ')', ')', '[', '0', ']', 'end_block_height', '=', 'sorted', '(', 'self', '.', 'blocks', '.', 'keys', '(', ')', ')', '[', '-', '1', ']', 'log', '.', 'debug', '(', '""send getdata for %s-%s (%064x-%064x)""', '%', '(', 'start_block_height', ',', 'end_block_height', ',', 'block_hashes', '[', '0', ']', ',', 'block_hashes', '[', '-', '1', ']', ')', ')', '# send off the getdata', 'getdata', '=', 'GetData', '(', ')', 'block_inv_vec', '=', '[', ']', 'for', 'block_hash', 'in', 'block_hashes', ':', 'block_inv', '=', 'Inventory', '(', ')', 'block_inv', '.', 'inv_type', '=', 'INVENTORY_TYPE', '[', '""MSG_BLOCK""', ']', 'block_inv', '.', 'inv_hash', '=', 'block_hash', 'block_inv_vec', '.', 'append', '(', 'block_inv', ')', 'getdata', '.', 'inventory', '=', 'block_inv_vec', 'self', '.', 'send_message', '(', 'getdata', ')']","This method will handle the Version message and
        will send a VerAck message when it receives the
        Version message.

        :param message_header: The Version message header
        :param message: The Version message","['This', 'method', 'will', 'handle', 'the', 'Version', 'message', 'and', 'will', 'send', 'a', 'VerAck', 'message', 'when', 'it', 'receives', 'the', 'Version', 'message', '.']",python,X,2,True,1,train
19833,bogdal/django-gcm,gcm/api.py,https://github.com/bogdal/django-gcm/blob/d11f8fcb038677e292bf8ffb4057ef51cf3a2938/gcm/api.py#L26-L68,"def send(self, data, registration_ids=None, **kwargs):
        """"""
        Send a GCM message for one or more devices, using json data
        registration_ids: A list with the devices which will be receiving a message
        data: The dict data which will be send
        Optional params e.g.:
            collapse_key: A string to group messages
        For more info see the following documentation:
        https://developer.android.com/google/gcm/server-ref.html#send-downstream
        """"""

        if not isinstance(data, dict):
            data = {'msg': data}

        registration_ids = registration_ids or []

        if len(registration_ids) > conf.GCM_MAX_RECIPIENTS:
            ret = []
            for chunk in self._chunks(
                    registration_ids, conf.GCM_MAX_RECIPIENTS):
                ret.append(self.send(data, registration_ids=chunk, **kwargs))
            return ret

        values = {
            'data': data,
            'collapse_key': 'message'}
        if registration_ids:
            values.update({'registration_ids': registration_ids})
        values.update(kwargs)

        values = json.dumps(values)

        headers = {
            'UserAgent': ""GCM-Server"",
            'Content-Type': 'application/json',
            'Authorization': 'key=' + self.api_key}

        response = requests.post(
            url=""https://gcm-http.googleapis.com/gcm/send"",
            data=values, headers=headers)

        response.raise_for_status()
        return registration_ids, json.loads(force_text(response.content))","['def', 'send', '(', 'self', ',', 'data', ',', 'registration_ids', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'if', 'not', 'isinstance', '(', 'data', ',', 'dict', ')', ':', 'data', '=', '{', ""'msg'"", ':', 'data', '}', 'registration_ids', '=', 'registration_ids', 'or', '[', ']', 'if', 'len', '(', 'registration_ids', ')', '>', 'conf', '.', 'GCM_MAX_RECIPIENTS', ':', 'ret', '=', '[', ']', 'for', 'chunk', 'in', 'self', '.', '_chunks', '(', 'registration_ids', ',', 'conf', '.', 'GCM_MAX_RECIPIENTS', ')', ':', 'ret', '.', 'append', '(', 'self', '.', 'send', '(', 'data', ',', 'registration_ids', '=', 'chunk', ',', '*', '*', 'kwargs', ')', ')', 'return', 'ret', 'values', '=', '{', ""'data'"", ':', 'data', ',', ""'collapse_key'"", ':', ""'message'"", '}', 'if', 'registration_ids', ':', 'values', '.', 'update', '(', '{', ""'registration_ids'"", ':', 'registration_ids', '}', ')', 'values', '.', 'update', '(', 'kwargs', ')', 'values', '=', 'json', '.', 'dumps', '(', 'values', ')', 'headers', '=', '{', ""'UserAgent'"", ':', '""GCM-Server""', ',', ""'Content-Type'"", ':', ""'application/json'"", ',', ""'Authorization'"", ':', ""'key='"", '+', 'self', '.', 'api_key', '}', 'response', '=', 'requests', '.', 'post', '(', 'url', '=', '""https://gcm-http.googleapis.com/gcm/send""', ',', 'data', '=', 'values', ',', 'headers', '=', 'headers', ')', 'response', '.', 'raise_for_status', '(', ')', 'return', 'registration_ids', ',', 'json', '.', 'loads', '(', 'force_text', '(', 'response', '.', 'content', ')', ')']","Send a GCM message for one or more devices, using json data
        registration_ids: A list with the devices which will be receiving a message
        data: The dict data which will be send
        Optional params e.g.:
            collapse_key: A string to group messages
        For more info see the following documentation:
        https://developer.android.com/google/gcm/server-ref.html#send-downstream","['Send', 'a', 'GCM', 'message', 'for', 'one', 'or', 'more', 'devices', 'using', 'json', 'data', 'registration_ids', ':', 'A', 'list', 'with', 'the', 'devices', 'which', 'will', 'be', 'receiving', 'a', 'message', 'data', ':', 'The', 'dict', 'data', 'which', 'will', 'be', 'send', 'Optional', 'params', 'e', '.', 'g', '.', ':', 'collapse_key', ':', 'A', 'string', 'to', 'group', 'messages', 'For', 'more', 'info', 'see', 'the', 'following', 'documentation', ':', 'https', ':', '//', 'developer', '.', 'android', '.', 'com', '/', 'google', '/', 'gcm', '/', 'server', '-', 'ref', '.', 'html#send', '-', 'downstream']",python,X,2,True,1,train
4128,matrix-org/pushbaby,pushbaby/__init__.py,https://github.com/matrix-org/pushbaby/blob/d3265e32dba12cb25474cb9383481def4a8b3bbe/pushbaby/__init__.py#L81-L111,"def send(self, payload, token, expiration=None, priority=None, identifier=None):
        """"""
        Attempts to send a push message. On network failures, progagates the exception.
        It is advised to make all text in the payload dictionary unicode objects and not
        mix unicode objects and str objects. If str objects are used, they must be
        in UTF-8 encoding.
        Args:
            payload (dict): The dictionary payload of the push to send
            token (str): token to send the push to (raw, unencoded bytes)
            expiration (int, seconds): When the message becomes irrelevant (time in seconds, as from time.time())
            priority (int): Integer priority for the message as per Apple's documentation
            identifier (any): optional identifier that will be returned if the push fails.
                        This is opaque to the library and not limited to 4 bytes.
        Throws:
            BodyTooLongException: If the payload body is too long and cannot be truncated to fit
        """"""

        # we only use one conn at a time currently but we may as well do this...
        created_conn = False
        while not created_conn:
            if len(self.conns) == 0:
                self.conns.append(PushConnection(self, self.address, self.certfile, self.keyfile))
                created_conn = True
            conn = random.choice(self.conns)
            try:
                conn.send(payload, token, expiration=expiration, priority=priority, identifier=identifier)
                return
            except:
                logger.info(""Connection died: removing"")
                self.conns.remove(conn)
        raise SendFailedException()","['def', 'send', '(', 'self', ',', 'payload', ',', 'token', ',', 'expiration', '=', 'None', ',', 'priority', '=', 'None', ',', 'identifier', '=', 'None', ')', ':', '# we only use one conn at a time currently but we may as well do this...', 'created_conn', '=', 'False', 'while', 'not', 'created_conn', ':', 'if', 'len', '(', 'self', '.', 'conns', ')', '==', '0', ':', 'self', '.', 'conns', '.', 'append', '(', 'PushConnection', '(', 'self', ',', 'self', '.', 'address', ',', 'self', '.', 'certfile', ',', 'self', '.', 'keyfile', ')', ')', 'created_conn', '=', 'True', 'conn', '=', 'random', '.', 'choice', '(', 'self', '.', 'conns', ')', 'try', ':', 'conn', '.', 'send', '(', 'payload', ',', 'token', ',', 'expiration', '=', 'expiration', ',', 'priority', '=', 'priority', ',', 'identifier', '=', 'identifier', ')', 'return', 'except', ':', 'logger', '.', 'info', '(', '""Connection died: removing""', ')', 'self', '.', 'conns', '.', 'remove', '(', 'conn', ')', 'raise', 'SendFailedException', '(', ')']","Attempts to send a push message. On network failures, progagates the exception.
        It is advised to make all text in the payload dictionary unicode objects and not
        mix unicode objects and str objects. If str objects are used, they must be
        in UTF-8 encoding.
        Args:
            payload (dict): The dictionary payload of the push to send
            token (str): token to send the push to (raw, unencoded bytes)
            expiration (int, seconds): When the message becomes irrelevant (time in seconds, as from time.time())
            priority (int): Integer priority for the message as per Apple's documentation
            identifier (any): optional identifier that will be returned if the push fails.
                        This is opaque to the library and not limited to 4 bytes.
        Throws:
            BodyTooLongException: If the payload body is too long and cannot be truncated to fit","['Attempts', 'to', 'send', 'a', 'push', 'message', '.', 'On', 'network', 'failures', 'progagates', 'the', 'exception', '.', 'It', 'is', 'advised', 'to', 'make', 'all', 'text', 'in', 'the', 'payload', 'dictionary', 'unicode', 'objects', 'and', 'not', 'mix', 'unicode', 'objects', 'and', 'str', 'objects', '.', 'If', 'str', 'objects', 'are', 'used', 'they', 'must', 'be', 'in', 'UTF', '-', '8', 'encoding', '.', 'Args', ':', 'payload', '(', 'dict', ')', ':', 'The', 'dictionary', 'payload', 'of', 'the', 'push', 'to', 'send', 'token', '(', 'str', ')', ':', 'token', 'to', 'send', 'the', 'push', 'to', '(', 'raw', 'unencoded', 'bytes', ')', 'expiration', '(', 'int', 'seconds', ')', ':', 'When', 'the', 'message', 'becomes', 'irrelevant', '(', 'time', 'in', 'seconds', 'as', 'from', 'time', '.', 'time', '()', ')', 'priority', '(', 'int', ')', ':', 'Integer', 'priority', 'for', 'the', 'message', 'as', 'per', 'Apple', 's', 'documentation', 'identifier', '(', 'any', ')', ':', 'optional', 'identifier', 'that', 'will', 'be', 'returned', 'if', 'the', 'push', 'fails', '.', 'This', 'is', 'opaque', 'to', 'the', 'library', 'and', 'not', 'limited', 'to', '4', 'bytes', '.', 'Throws', ':', 'BodyTooLongException', ':', 'If', 'the', 'payload', 'body', 'is', 'too', 'long', 'and', 'cannot', 'be', 'truncated', 'to', 'fit']",python,X,2,True,1,train
6711,volker48/flask-mandrill,flask_mandrill.py,https://github.com/volker48/flask-mandrill/blob/85ec7756bf29a6b3c58da36b12f5ec9325975def/flask_mandrill.py#L17-L62,"def send_email(self, **kwargs):
        """"""
        Sends an email using Mandrill's API. Returns a
        Requests :class:`Response` object.
        At a minimum kwargs must contain the keys to, from_email, and text.
        Everything passed as kwargs except for the keywords 'key', 'async',
        and 'ip_pool' will be sent as key-value pairs in the message object.
        Reference https://mandrillapp.com/api/docs/messages.JSON.html#method=send
        for all the available options.
        """"""
        endpoint = self.messages_endpoint

        data = {
            'async': kwargs.pop('async', False),
            'ip_pool': kwargs.pop('ip_pool', ''),
            'key': kwargs.pop('key', self.api_key),
        }

        if not data.get('key', None):
            raise ValueError('No Mandrill API key has been configured')

        # Sending a template through Mandrill requires a couple extra args
        # and a different endpoint.
        if kwargs.get('template_name', None):
            data['template_name'] = kwargs.pop('template_name')
            data['template_content'] = kwargs.pop('template_content', [])
            endpoint = self.templates_endpoint

        data['message'] = kwargs

        if self.app:
            data['message'].setdefault(
                'from_email',
                self.app.config.get('MANDRILL_DEFAULT_FROM', None)
            )

        if endpoint != self.templates_endpoint and not data['message'].get('from_email', None):
            raise ValueError(
                'No from email was specified and no default was configured')


        response = requests.post(endpoint,
                                 data=json.dumps(data),
                                 headers={'Content-Type': 'application/json'})
        response.raise_for_status()
        return response","['def', 'send_email', '(', 'self', ',', '*', '*', 'kwargs', ')', ':', 'endpoint', '=', 'self', '.', 'messages_endpoint', 'data', '=', '{', ""'async'"", ':', 'kwargs', '.', 'pop', '(', ""'async'"", ',', 'False', ')', ',', ""'ip_pool'"", ':', 'kwargs', '.', 'pop', '(', ""'ip_pool'"", ',', ""''"", ')', ',', ""'key'"", ':', 'kwargs', '.', 'pop', '(', ""'key'"", ',', 'self', '.', 'api_key', ')', ',', '}', 'if', 'not', 'data', '.', 'get', '(', ""'key'"", ',', 'None', ')', ':', 'raise', 'ValueError', '(', ""'No Mandrill API key has been configured'"", ')', '# Sending a template through Mandrill requires a couple extra args', '# and a different endpoint.', 'if', 'kwargs', '.', 'get', '(', ""'template_name'"", ',', 'None', ')', ':', 'data', '[', ""'template_name'"", ']', '=', 'kwargs', '.', 'pop', '(', ""'template_name'"", ')', 'data', '[', ""'template_content'"", ']', '=', 'kwargs', '.', 'pop', '(', ""'template_content'"", ',', '[', ']', ')', 'endpoint', '=', 'self', '.', 'templates_endpoint', 'data', '[', ""'message'"", ']', '=', 'kwargs', 'if', 'self', '.', 'app', ':', 'data', '[', ""'message'"", ']', '.', 'setdefault', '(', ""'from_email'"", ',', 'self', '.', 'app', '.', 'config', '.', 'get', '(', ""'MANDRILL_DEFAULT_FROM'"", ',', 'None', ')', ')', 'if', 'endpoint', '!=', 'self', '.', 'templates_endpoint', 'and', 'not', 'data', '[', ""'message'"", ']', '.', 'get', '(', ""'from_email'"", ',', 'None', ')', ':', 'raise', 'ValueError', '(', ""'No from email was specified and no default was configured'"", ')', 'response', '=', 'requests', '.', 'post', '(', 'endpoint', ',', 'data', '=', 'json', '.', 'dumps', '(', 'data', ')', ',', 'headers', '=', '{', ""'Content-Type'"", ':', ""'application/json'"", '}', ')', 'response', '.', 'raise_for_status', '(', ')', 'return', 'response']","Sends an email using Mandrill's API. Returns a
        Requests :class:`Response` object.
        At a minimum kwargs must contain the keys to, from_email, and text.
        Everything passed as kwargs except for the keywords 'key', 'async',
        and 'ip_pool' will be sent as key-value pairs in the message object.
        Reference https://mandrillapp.com/api/docs/messages.JSON.html#method=send
        for all the available options.","['Sends', 'an', 'email', 'using', 'Mandrill', 's', 'API', '.', 'Returns', 'a', 'Requests', ':', 'class', ':', 'Response', 'object', '.', 'At', 'a', 'minimum', 'kwargs', 'must', 'contain', 'the', 'keys', 'to', 'from_email', 'and', 'text', '.', 'Everything', 'passed', 'as', 'kwargs', 'except', 'for', 'the', 'keywords', 'key', 'async', 'and', 'ip_pool', 'will', 'be', 'sent', 'as', 'key', '-', 'value', 'pairs', 'in', 'the', 'message', 'object', '.', 'Reference', 'https', ':', '//', 'mandrillapp', '.', 'com', '/', 'api', '/', 'docs', '/', 'messages', '.', 'JSON', '.', 'html#method', '=', 'send', 'for', 'all', 'the', 'available', 'options', '.']",python,X,2,True,1,train
10579,larryng/narwal,narwal/reddit.py,https://github.com/larryng/narwal/blob/58c409a475c8ed865579a61d7010162ed8cef597/narwal/reddit.py#L645-L658,"def compose(self, to, subject, text):
        """"""Login required.  Sends POST to send a message to a user.  Returns True or raises :class:`exceptions.UnexpectedResponse` if non-""truthy"" value in response.
        
        URL: ``http://www.reddit.com/api/compose/``
        
        :param to: username or :class`things.Account` of user to send to
        :param subject: subject of message
        :param text: message body text
        """"""
        if isinstance(to, Account):
            to = to.name
        data = dict(to=to, subject=subject, text=text)
        j = self.post('api', 'compose', data=data)
        return assert_truthy(j)","['def', 'compose', '(', 'self', ',', 'to', ',', 'subject', ',', 'text', ')', ':', 'if', 'isinstance', '(', 'to', ',', 'Account', ')', ':', 'to', '=', 'to', '.', 'name', 'data', '=', 'dict', '(', 'to', '=', 'to', ',', 'subject', '=', 'subject', ',', 'text', '=', 'text', ')', 'j', '=', 'self', '.', 'post', '(', ""'api'"", ',', ""'compose'"", ',', 'data', '=', 'data', ')', 'return', 'assert_truthy', '(', 'j', ')']","Login required.  Sends POST to send a message to a user.  Returns True or raises :class:`exceptions.UnexpectedResponse` if non-""truthy"" value in response.
        
        URL: ``http://www.reddit.com/api/compose/``
        
        :param to: username or :class`things.Account` of user to send to
        :param subject: subject of message
        :param text: message body text","['Login', 'required', '.', 'Sends', 'POST', 'to', 'send', 'a', 'message', 'to', 'a', 'user', '.', 'Returns', 'True', 'or', 'raises', ':', 'class', ':', 'exceptions', '.', 'UnexpectedResponse', 'if', 'non', '-', 'truthy', 'value', 'in', 'response', '.', 'URL', ':', 'http', ':', '//', 'www', '.', 'reddit', '.', 'com', '/', 'api', '/', 'compose', '/', ':', 'param', 'to', ':', 'username', 'or', ':', 'class', 'things', '.', 'Account', 'of', 'user', 'to', 'send', 'to', ':', 'param', 'subject', ':', 'subject', 'of', 'message', ':', 'param', 'text', ':', 'message', 'body', 'text']",python,X,2,True,1,train
26196,f3at/feat,src/feat/agencies/messaging/messaging.py,https://github.com/f3at/feat/blob/15da93fc9d6ec8154f52a9172824e25821195ef8/src/feat/agencies/messaging/messaging.py#L100-L167,"def on_message(self, msg):
        '''
        When a message with an already known traversal_id is received,
        we try to build a duplication message and send it in to a protocol
        dependent recipient. This is used in contracts traversing
        the graph, when the contract has reached again the same shard.
        This message is necessary, as silently ignoring the incoming bids
        adds a lot of latency to the nested contracts (it is waiting to receive
        message from all the recipients).
        '''
        self.log('Received message: %r', msg)

        # Check if it isn't expired message
        time_left = time.left(msg.expiration_time)
        if time_left < 0:
            self.log('Throwing away expired message. Time left: %s, '
                     'msg_class: %r', time_left, msg.get_msg_class())
            return False

        # Check for duplicated message
        if msg.message_id in self._message_ids:
            self.log(""Throwing away duplicated message %r"",
                     msg.get_msg_class())
            return False
        else:
            self._message_ids.set(msg.message_id, True, msg.expiration_time)

        # Check for known traversal ids:
        if IFirstMessage.providedBy(msg):
            t_id = msg.traversal_id
            if t_id is None:
                self.warning(
                    ""Received corrupted message. The traversal_id is None ! ""
                    ""Message: %r"", msg)
                return False
            if t_id in self._traversal_ids:
                self.log('Throwing away already known traversal id %r, '
                         'msg_class: %r', t_id, msg.get_msg_class())
                recp = msg.duplication_recipient()
                if recp:
                    resp = msg.duplication_message()
                    self.post(recp, resp)
                return False
            else:
                self._traversal_ids.set(t_id, True, msg.expiration_time)

        # Handle registered dialog
        if IDialogMessage.providedBy(msg):
            recv_id = msg.receiver_id
            if recv_id is not None and \
               recv_id in self._agency_agent._protocols:
                protocol = self._agency_agent._protocols[recv_id]
                protocol.on_message(msg)
                return True

        # Handle new conversation coming in (interest)
        # if msg.protocol_id == 'alert':
        #     print self._agency_agent._interests
        p_type = msg.protocol_type
        if p_type in self._agency_agent._interests:
            p_id = msg.protocol_id
            interest = self._agency_agent._interests[p_type].get(p_id)
            if interest and interest.schedule_message(msg):
                return True

        self.debug(""Couldn't find appropriate protocol for message: ""
                   ""%s"", msg.get_msg_class())
        return False","['def', 'on_message', '(', 'self', ',', 'msg', ')', ':', 'self', '.', 'log', '(', ""'Received message: %r'"", ',', 'msg', ')', ""# Check if it isn't expired message"", 'time_left', '=', 'time', '.', 'left', '(', 'msg', '.', 'expiration_time', ')', 'if', 'time_left', '<', '0', ':', 'self', '.', 'log', '(', ""'Throwing away expired message. Time left: %s, '"", ""'msg_class: %r'"", ',', 'time_left', ',', 'msg', '.', 'get_msg_class', '(', ')', ')', 'return', 'False', '# Check for duplicated message', 'if', 'msg', '.', 'message_id', 'in', 'self', '.', '_message_ids', ':', 'self', '.', 'log', '(', '""Throwing away duplicated message %r""', ',', 'msg', '.', 'get_msg_class', '(', ')', ')', 'return', 'False', 'else', ':', 'self', '.', '_message_ids', '.', 'set', '(', 'msg', '.', 'message_id', ',', 'True', ',', 'msg', '.', 'expiration_time', ')', '# Check for known traversal ids:', 'if', 'IFirstMessage', '.', 'providedBy', '(', 'msg', ')', ':', 't_id', '=', 'msg', '.', 'traversal_id', 'if', 't_id', 'is', 'None', ':', 'self', '.', 'warning', '(', '""Received corrupted message. The traversal_id is None ! ""', '""Message: %r""', ',', 'msg', ')', 'return', 'False', 'if', 't_id', 'in', 'self', '.', '_traversal_ids', ':', 'self', '.', 'log', '(', ""'Throwing away already known traversal id %r, '"", ""'msg_class: %r'"", ',', 't_id', ',', 'msg', '.', 'get_msg_class', '(', ')', ')', 'recp', '=', 'msg', '.', 'duplication_recipient', '(', ')', 'if', 'recp', ':', 'resp', '=', 'msg', '.', 'duplication_message', '(', ')', 'self', '.', 'post', '(', 'recp', ',', 'resp', ')', 'return', 'False', 'else', ':', 'self', '.', '_traversal_ids', '.', 'set', '(', 't_id', ',', 'True', ',', 'msg', '.', 'expiration_time', ')', '# Handle registered dialog', 'if', 'IDialogMessage', '.', 'providedBy', '(', 'msg', ')', ':', 'recv_id', '=', 'msg', '.', 'receiver_id', 'if', 'recv_id', 'is', 'not', 'None', 'and', 'recv_id', 'in', 'self', '.', '_agency_agent', '.', '_protocols', ':', 'protocol', '=', 'self', '.', '_agency_agent', '.', '_protocols', '[', 'recv_id', ']', 'protocol', '.', 'on_message', '(', 'msg', ')', 'return', 'True', '# Handle new conversation coming in (interest)', ""# if msg.protocol_id == 'alert':"", '#     print self._agency_agent._interests', 'p_type', '=', 'msg', '.', 'protocol_type', 'if', 'p_type', 'in', 'self', '.', '_agency_agent', '.', '_interests', ':', 'p_id', '=', 'msg', '.', 'protocol_id', 'interest', '=', 'self', '.', '_agency_agent', '.', '_interests', '[', 'p_type', ']', '.', 'get', '(', 'p_id', ')', 'if', 'interest', 'and', 'interest', '.', 'schedule_message', '(', 'msg', ')', ':', 'return', 'True', 'self', '.', 'debug', '(', '""Couldn\'t find appropriate protocol for message: ""', '""%s""', ',', 'msg', '.', 'get_msg_class', '(', ')', ')', 'return', 'False']","When a message with an already known traversal_id is received,
        we try to build a duplication message and send it in to a protocol
        dependent recipient. This is used in contracts traversing
        the graph, when the contract has reached again the same shard.
        This message is necessary, as silently ignoring the incoming bids
        adds a lot of latency to the nested contracts (it is waiting to receive
        message from all the recipients).","['When', 'a', 'message', 'with', 'an', 'already', 'known', 'traversal_id', 'is', 'received', 'we', 'try', 'to', 'build', 'a', 'duplication', 'message', 'and', 'send', 'it', 'in', 'to', 'a', 'protocol', 'dependent', 'recipient', '.', 'This', 'is', 'used', 'in', 'contracts', 'traversing', 'the', 'graph', 'when', 'the', 'contract', 'has', 'reached', 'again', 'the', 'same', 'shard', '.', 'This', 'message', 'is', 'necessary', 'as', 'silently', 'ignoring', 'the', 'incoming', 'bids', 'adds', 'a', 'lot', 'of', 'latency', 'to', 'the', 'nested', 'contracts', '(', 'it', 'is', 'waiting', 'to', 'receive', 'message', 'from', 'all', 'the', 'recipients', ')', '.']",python,X,2,True,1,train
27762,edeposit/edeposit.amqp,edeposit/amqp/pikadaemon.py,https://github.com/edeposit/edeposit.amqp/blob/7804b52028b90ab96302d54bc2430f88dc2ebf64/edeposit/amqp/pikadaemon.py#L101-L135,"def sendMessage(self, exchange, routing_key, message, properties=None,
                    UUID=None):
        """"""
        With this function, you can send message to `exchange`.

        Args:
            exchange (str): name of exchange you want to message to be
                            delivered
            routing_key (str): which routing key to use in headers of message
            message (str): body of message
            properties (dict ,optional): properties of message - if not used,
                                      or set to ``None``, ``self.content_type``
                                      and ``delivery_mode=2`` (persistent) is
                                      used
            UUID (str, optional): UUID of the message. If set, it is included
                                  into ``properties`` of the message.
        """"""
        if properties is None:
            properties = pika.BasicProperties(
                content_type=self.content_type,
                delivery_mode=1,
                headers={}
            )

        if UUID is not None:
            if properties.headers is None:
                properties.headers = {}
            properties.headers[""UUID""] = UUID

        self.channel.basic_publish(
            exchange=exchange,
            routing_key=routing_key,
            properties=properties,
            body=message
        )","['def', 'sendMessage', '(', 'self', ',', 'exchange', ',', 'routing_key', ',', 'message', ',', 'properties', '=', 'None', ',', 'UUID', '=', 'None', ')', ':', 'if', 'properties', 'is', 'None', ':', 'properties', '=', 'pika', '.', 'BasicProperties', '(', 'content_type', '=', 'self', '.', 'content_type', ',', 'delivery_mode', '=', '1', ',', 'headers', '=', '{', '}', ')', 'if', 'UUID', 'is', 'not', 'None', ':', 'if', 'properties', '.', 'headers', 'is', 'None', ':', 'properties', '.', 'headers', '=', '{', '}', 'properties', '.', 'headers', '[', '""UUID""', ']', '=', 'UUID', 'self', '.', 'channel', '.', 'basic_publish', '(', 'exchange', '=', 'exchange', ',', 'routing_key', '=', 'routing_key', ',', 'properties', '=', 'properties', ',', 'body', '=', 'message', ')']","With this function, you can send message to `exchange`.

        Args:
            exchange (str): name of exchange you want to message to be
                            delivered
            routing_key (str): which routing key to use in headers of message
            message (str): body of message
            properties (dict ,optional): properties of message - if not used,
                                      or set to ``None``, ``self.content_type``
                                      and ``delivery_mode=2`` (persistent) is
                                      used
            UUID (str, optional): UUID of the message. If set, it is included
                                  into ``properties`` of the message.","['With', 'this', 'function', 'you', 'can', 'send', 'message', 'to', 'exchange', '.']",python,X,2,True,1,train
28235,wesyoung/pyzyre,czmq/_czmq_ctypes.py,https://github.com/wesyoung/pyzyre/blob/22d4c757acefcfdb700d3802adaf30b402bb9eea/czmq/_czmq_ctypes.py#L5683-L5712,"def send(self, picture, *args):
        """"""
        Send a 'picture' message to the socket (or actor). The picture is a
string that defines the type of each frame. This makes it easy to send
a complex multiframe message in one call. The picture can contain any
of these characters, each corresponding to one or two arguments:

    i = int (signed)
    1 = uint8_t
    2 = uint16_t
    4 = uint32_t
    8 = uint64_t
    s = char *
    b = byte *, size_t (2 arguments)
    c = zchunk_t *
    f = zframe_t *
    h = zhashx_t *
    U = zuuid_t *
    p = void * (sends the pointer value, only meaningful over inproc)
    m = zmsg_t * (sends all frames in the zmsg)
    z = sends zero-sized frame (0 arguments)
    u = uint (deprecated)

Note that s, b, c, and f are encoded the same way and the choice is
offered as a convenience to the sender, which may or may not already
have data in a zchunk or zframe. Does not change or take ownership of
any arguments. Returns 0 if successful, -1 if sending failed for any
reason.
        """"""
        return lib.zsock_send(self._as_parameter_, picture, *args)","['def', 'send', '(', 'self', ',', 'picture', ',', '*', 'args', ')', ':', 'return', 'lib', '.', 'zsock_send', '(', 'self', '.', '_as_parameter_', ',', 'picture', ',', '*', 'args', ')']","Send a 'picture' message to the socket (or actor). The picture is a
string that defines the type of each frame. This makes it easy to send
a complex multiframe message in one call. The picture can contain any
of these characters, each corresponding to one or two arguments:

    i = int (signed)
    1 = uint8_t
    2 = uint16_t
    4 = uint32_t
    8 = uint64_t
    s = char *
    b = byte *, size_t (2 arguments)
    c = zchunk_t *
    f = zframe_t *
    h = zhashx_t *
    U = zuuid_t *
    p = void * (sends the pointer value, only meaningful over inproc)
    m = zmsg_t * (sends all frames in the zmsg)
    z = sends zero-sized frame (0 arguments)
    u = uint (deprecated)

Note that s, b, c, and f are encoded the same way and the choice is
offered as a convenience to the sender, which may or may not already
have data in a zchunk or zframe. Does not change or take ownership of
any arguments. Returns 0 if successful, -1 if sending failed for any
reason.","['Send', 'a', 'picture', 'message', 'to', 'the', 'socket', '(', 'or', 'actor', ')', '.', 'The', 'picture', 'is', 'a', 'string', 'that', 'defines', 'the', 'type', 'of', 'each', 'frame', '.', 'This', 'makes', 'it', 'easy', 'to', 'send', 'a', 'complex', 'multiframe', 'message', 'in', 'one', 'call', '.', 'The', 'picture', 'can', 'contain', 'any', 'of', 'these', 'characters', 'each', 'corresponding', 'to', 'one', 'or', 'two', 'arguments', ':']",python,X,2,True,1,train
29317,edeposit/edeposit.amqp,edeposit/amqp/amqpdaemon.py,https://github.com/edeposit/edeposit.amqp/blob/7804b52028b90ab96302d54bc2430f88dc2ebf64/edeposit/amqp/amqpdaemon.py#L106-L161,"def onMessageReceived(self, method_frame, properties, body):
        """"""
        React to received message - deserialize it, add it to users reaction
        function stored in ``self.react_fn`` and send back result.

        If `Exception` is thrown during process, it is sent back instead of
        message.

        Note:
            In case of `Exception`, response message doesn't have useful `body`,
            but in headers is stored following (string) parameters:

            - ``exception``, where the Exception's message is stored
            - ``exception_type``, where ``e.__class__`` is stored
            - ``exception_name``, where ``e.__class__.__name__`` is stored
            - ``traceback`` where the full traceback is stored (contains line
              number)

            This allows you to react to unexpected cases at the other end of
            the AMQP communication.
        """"""
        # if UUID is not in headers, just ack the message and ignore it
        if ""UUID"" not in properties.headers:
            self.process_exception(
                e=ValueError(""No UUID provided, message ignored.""),
                uuid="""",
                routing_key=self.parseKey(method_frame),
                body=body
            )
            return True  # ack message

        key = self.parseKey(method_frame)
        uuid = properties.headers[""UUID""]
        try:
            result = self.react_fn(
                serializers.deserialize(body, self.globals),
                self.get_sendback(uuid, key)
            )

            print ""sending response"", key

            self.sendResponse(
                serializers.serialize(result),
                uuid,
                key
            )
        except Exception, e:
            self.process_exception(
                e=e,
                uuid=uuid,
                routing_key=key,
                body=str(e),
                tb=traceback.format_exc().strip()
            )

        return True","['def', 'onMessageReceived', '(', 'self', ',', 'method_frame', ',', 'properties', ',', 'body', ')', ':', '# if UUID is not in headers, just ack the message and ignore it', 'if', '""UUID""', 'not', 'in', 'properties', '.', 'headers', ':', 'self', '.', 'process_exception', '(', 'e', '=', 'ValueError', '(', '""No UUID provided, message ignored.""', ')', ',', 'uuid', '=', '""""', ',', 'routing_key', '=', 'self', '.', 'parseKey', '(', 'method_frame', ')', ',', 'body', '=', 'body', ')', 'return', 'True', '# ack message', 'key', '=', 'self', '.', 'parseKey', '(', 'method_frame', ')', 'uuid', '=', 'properties', '.', 'headers', '[', '""UUID""', ']', 'try', ':', 'result', '=', 'self', '.', 'react_fn', '(', 'serializers', '.', 'deserialize', '(', 'body', ',', 'self', '.', 'globals', ')', ',', 'self', '.', 'get_sendback', '(', 'uuid', ',', 'key', ')', ')', 'print', '""sending response""', ',', 'key', 'self', '.', 'sendResponse', '(', 'serializers', '.', 'serialize', '(', 'result', ')', ',', 'uuid', ',', 'key', ')', 'except', 'Exception', ',', 'e', ':', 'self', '.', 'process_exception', '(', 'e', '=', 'e', ',', 'uuid', '=', 'uuid', ',', 'routing_key', '=', 'key', ',', 'body', '=', 'str', '(', 'e', ')', ',', 'tb', '=', 'traceback', '.', 'format_exc', '(', ')', '.', 'strip', '(', ')', ')', 'return', 'True']","React to received message - deserialize it, add it to users reaction
        function stored in ``self.react_fn`` and send back result.

        If `Exception` is thrown during process, it is sent back instead of
        message.

        Note:
            In case of `Exception`, response message doesn't have useful `body`,
            but in headers is stored following (string) parameters:

            - ``exception``, where the Exception's message is stored
            - ``exception_type``, where ``e.__class__`` is stored
            - ``exception_name``, where ``e.__class__.__name__`` is stored
            - ``traceback`` where the full traceback is stored (contains line
              number)

            This allows you to react to unexpected cases at the other end of
            the AMQP communication.","['React', 'to', 'received', 'message', '-', 'deserialize', 'it', 'add', 'it', 'to', 'users', 'reaction', 'function', 'stored', 'in', 'self', '.', 'react_fn', 'and', 'send', 'back', 'result', '.']",python,X,2,True,1,train
849,fiesta/fiesta-python,fiesta/fiesta.py,https://github.com/fiesta/fiesta-python/blob/cfcc11e4ae4c76b1007794604c33dde877f62cfb/fiesta/fiesta.py#L210-L215,"def send_message(self, subject=None, text=None, markdown=None, message_dict=None):
        """"""
        Helper function to send a message to a group
        """"""
        message = FiestaMessage(self.api, self, subject, text, markdown, message_dict)
        return message.send()","['def', 'send_message', '(', 'self', ',', 'subject', '=', 'None', ',', 'text', '=', 'None', ',', 'markdown', '=', 'None', ',', 'message_dict', '=', 'None', ')', ':', 'message', '=', 'FiestaMessage', '(', 'self', '.', 'api', ',', 'self', ',', 'subject', ',', 'text', ',', 'markdown', ',', 'message_dict', ')', 'return', 'message', '.', 'send', '(', ')']",Helper function to send a message to a group,"['Helper', 'function', 'to', 'send', 'a', 'message', 'to', 'a', 'group']",python,X,2,True,1,train
4370,decryptus/httpdis,httpdis/httpdis.py,https://github.com/decryptus/httpdis/blob/5d198cdc5558f416634602689b3df2c8aeb34984/httpdis/httpdis.py#L552-L560,"def send_error_json(self, code, message, headers=None):
        ""send an error to the client. text message is formatted in a json stream""
        if headers is None:
            headers = {}

        self.end_response(HttpResponseJson(code,
                                           {'code':    code,
                                            'message': message},
                                           headers))","['def', 'send_error_json', '(', 'self', ',', 'code', ',', 'message', ',', 'headers', '=', 'None', ')', ':', 'if', 'headers', 'is', 'None', ':', 'headers', '=', '{', '}', 'self', '.', 'end_response', '(', 'HttpResponseJson', '(', 'code', ',', '{', ""'code'"", ':', 'code', ',', ""'message'"", ':', 'message', '}', ',', 'headers', ')', ')']",send an error to the client. text message is formatted in a json stream,"['send', 'an', 'error', 'to', 'the', 'client', '.', 'text', 'message', 'is', 'formatted', 'in', 'a', 'json', 'stream']",python,X,2,True,1,train
9114,tommilligan/pypersonalassistant,pypersonalassistant/secure.py,https://github.com/tommilligan/pypersonalassistant/blob/123903189be3f3d73a6f480de5e4442ffd099058/pypersonalassistant/secure.py#L213-L223,"def SMS(self, to, body):
        """"""
        Quickly send an SMS from a default number. Calls :py:meth:`twilio_SMS`.
        
        * *stored credential name: TWILIO_PHONE_NUMBER*
        
        :param string to: The phone number to send the SMS message to. Full international format.
        :param string body: The content of the SMS message. 
        """"""
        logging.debug('Texting someone')
        return self.twilio_SMS(self._credentials['TWILIO_PHONE_NUMBER'], to, body)","['def', 'SMS', '(', 'self', ',', 'to', ',', 'body', ')', ':', 'logging', '.', 'debug', '(', ""'Texting someone'"", ')', 'return', 'self', '.', 'twilio_SMS', '(', 'self', '.', '_credentials', '[', ""'TWILIO_PHONE_NUMBER'"", ']', ',', 'to', ',', 'body', ')']","Quickly send an SMS from a default number. Calls :py:meth:`twilio_SMS`.
        
        * *stored credential name: TWILIO_PHONE_NUMBER*
        
        :param string to: The phone number to send the SMS message to. Full international format.
        :param string body: The content of the SMS message.","['Quickly', 'send', 'an', 'SMS', 'from', 'a', 'default', 'number', '.', 'Calls', ':', 'py', ':', 'meth', ':', 'twilio_SMS', '.', '*', '*', 'stored', 'credential', 'name', ':', 'TWILIO_PHONE_NUMBER', '*', ':', 'param', 'string', 'to', ':', 'The', 'phone', 'number', 'to', 'send', 'the', 'SMS', 'message', 'to', '.', 'Full', 'international', 'format', '.', ':', 'param', 'string', 'body', ':', 'The', 'content', 'of', 'the', 'SMS', 'message', '.']",python,X,2,True,1,train
9115,tommilligan/pypersonalassistant,pypersonalassistant/secure.py,https://github.com/tommilligan/pypersonalassistant/blob/123903189be3f3d73a6f480de5e4442ffd099058/pypersonalassistant/secure.py#L225-L234,"def SMS_me(self, body):
        """"""
        Quickly send an SMS to yourself. Calls :py:meth:`SMS`.
        
        * *stored credential name: PERSONAL_PHONE_NUMBER*
        
        :param string body: The content of the SMS message. 
        """"""
        logging.debug('Texting myself')
        return self.text(self._credentials['PERSONAL_PHONE_NUMBER'], body)","['def', 'SMS_me', '(', 'self', ',', 'body', ')', ':', 'logging', '.', 'debug', '(', ""'Texting myself'"", ')', 'return', 'self', '.', 'text', '(', 'self', '.', '_credentials', '[', ""'PERSONAL_PHONE_NUMBER'"", ']', ',', 'body', ')']","Quickly send an SMS to yourself. Calls :py:meth:`SMS`.
        
        * *stored credential name: PERSONAL_PHONE_NUMBER*
        
        :param string body: The content of the SMS message.","['Quickly', 'send', 'an', 'SMS', 'to', 'yourself', '.', 'Calls', ':', 'py', ':', 'meth', ':', 'SMS', '.', '*', '*', 'stored', 'credential', 'name', ':', 'PERSONAL_PHONE_NUMBER', '*', ':', 'param', 'string', 'body', ':', 'The', 'content', 'of', 'the', 'SMS', 'message', '.']",python,X,2,True,1,train
9116,tommilligan/pypersonalassistant,pypersonalassistant/secure.py,https://github.com/tommilligan/pypersonalassistant/blob/123903189be3f3d73a6f480de5e4442ffd099058/pypersonalassistant/secure.py#L238-L289,"def gmail_email(self, from_, to, msg):
        """"""
        Send an email from your `gmail`_ account.
        
        .. _gmail: https://mail.google.com/
        
        msg can either be:
        
        * A string, in which case:
        
            * At the first newline (\\n) the string will be split into subject and body
            * If no newline is present, the entire string will be body.
        
        * An `email.message.Message`_ object
        
        .. _email.message.Message: https://docs.python.org/3/library/email.message.html
        
        Login will be performed using stored credentials.
        
        * *stored credential name: GMAIL_EMAIL*
        * *stored credential name: GMAIL_EMAIL_PASSWORD*
        
        :param string from_: The phone number in your twilio account to send the SMS message from. Full international format.        
        :param string to: The email address to send the email to.
        :param body: The content of the email. See above. 
        """"""
        logging.debug('Emailing from Gmail')
        smtpConn = smtplib.SMTP('smtp.gmail.com', 587)
        smtpConn.ehlo()
        smtpConn.starttls()
        login_response = smtpConn.login(self._credentials['GMAIL_EMAIL'], self._credentials['GMAIL_EMAIL_PASSWORD'])
        
        # if email is of type email.message.Message, flatten and send
        # if anything else, convert to string and try and send
        if isinstance(msg, email.message.Message):
            logging.debug('Flattening MIME to string')
            # If From is already set, overwrite
            msg['From'] = from_
            # If To is string, convert to list and add each to header
            if isinstance(to, str):
                to = [to]
            for x in to:
                msg['To'] = x
            msg = msg.as_string()
        else:
            msg = str(msg)
        
        logging.debug(msg.replace('\n', ' '))
        response = smtpConn.sendmail(from_, to, msg)
        logging.info('Response from Gmail: {0}'.format(response))
        smtpConn.quit()
        return response","['def', 'gmail_email', '(', 'self', ',', 'from_', ',', 'to', ',', 'msg', ')', ':', 'logging', '.', 'debug', '(', ""'Emailing from Gmail'"", ')', 'smtpConn', '=', 'smtplib', '.', 'SMTP', '(', ""'smtp.gmail.com'"", ',', '587', ')', 'smtpConn', '.', 'ehlo', '(', ')', 'smtpConn', '.', 'starttls', '(', ')', 'login_response', '=', 'smtpConn', '.', 'login', '(', 'self', '.', '_credentials', '[', ""'GMAIL_EMAIL'"", ']', ',', 'self', '.', '_credentials', '[', ""'GMAIL_EMAIL_PASSWORD'"", ']', ')', '# if email is of type email.message.Message, flatten and send', '# if anything else, convert to string and try and send', 'if', 'isinstance', '(', 'msg', ',', 'email', '.', 'message', '.', 'Message', ')', ':', 'logging', '.', 'debug', '(', ""'Flattening MIME to string'"", ')', '# If From is already set, overwrite', 'msg', '[', ""'From'"", ']', '=', 'from_', '# If To is string, convert to list and add each to header', 'if', 'isinstance', '(', 'to', ',', 'str', ')', ':', 'to', '=', '[', 'to', ']', 'for', 'x', 'in', 'to', ':', 'msg', '[', ""'To'"", ']', '=', 'x', 'msg', '=', 'msg', '.', 'as_string', '(', ')', 'else', ':', 'msg', '=', 'str', '(', 'msg', ')', 'logging', '.', 'debug', '(', 'msg', '.', 'replace', '(', ""'\\n'"", ',', ""' '"", ')', ')', 'response', '=', 'smtpConn', '.', 'sendmail', '(', 'from_', ',', 'to', ',', 'msg', ')', 'logging', '.', 'info', '(', ""'Response from Gmail: {0}'"", '.', 'format', '(', 'response', ')', ')', 'smtpConn', '.', 'quit', '(', ')', 'return', 'response']","Send an email from your `gmail`_ account.
        
        .. _gmail: https://mail.google.com/
        
        msg can either be:
        
        * A string, in which case:
        
            * At the first newline (\\n) the string will be split into subject and body
            * If no newline is present, the entire string will be body.
        
        * An `email.message.Message`_ object
        
        .. _email.message.Message: https://docs.python.org/3/library/email.message.html
        
        Login will be performed using stored credentials.
        
        * *stored credential name: GMAIL_EMAIL*
        * *stored credential name: GMAIL_EMAIL_PASSWORD*
        
        :param string from_: The phone number in your twilio account to send the SMS message from. Full international format.        
        :param string to: The email address to send the email to.
        :param body: The content of the email. See above.","['Send', 'an', 'email', 'from', 'your', 'gmail', '_', 'account', '.', '..', '_gmail', ':', 'https', ':', '//', 'mail', '.', 'google', '.', 'com', '/', 'msg', 'can', 'either', 'be', ':', '*', 'A', 'string', 'in', 'which', 'case', ':', '*', 'At', 'the', 'first', 'newline', '(', '\\\\', 'n', ')', 'the', 'string', 'will', 'be', 'split', 'into', 'subject', 'and', 'body', '*', 'If', 'no', 'newline', 'is', 'present', 'the', 'entire', 'string', 'will', 'be', 'body', '.', '*', 'An', 'email', '.', 'message', '.', 'Message', '_', 'object', '..', '_email', '.', 'message', '.', 'Message', ':', 'https', ':', '//', 'docs', '.', 'python', '.', 'org', '/', '3', '/', 'library', '/', 'email', '.', 'message', '.', 'html', 'Login', 'will', 'be', 'performed', 'using', 'stored', 'credentials', '.', '*', '*', 'stored', 'credential', 'name', ':', 'GMAIL_EMAIL', '*', '*', '*', 'stored', 'credential', 'name', ':', 'GMAIL_EMAIL_PASSWORD', '*', ':', 'param', 'string', 'from_', ':', 'The', 'phone', 'number', 'in', 'your', 'twilio', 'account', 'to', 'send', 'the', 'SMS', 'message', 'from', '.', 'Full', 'international', 'format', '.', ':', 'param', 'string', 'to', ':', 'The', 'email', 'address', 'to', 'send', 'the', 'email', 'to', '.', ':', 'param', 'body', ':', 'The', 'content', 'of', 'the', 'email', '.', 'See', 'above', '.']",python,X,2,True,1,train
9511,rameshg87/pyremotevbox,pyremotevbox/ZSI/client.py,https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/client.py#L193-L292,"def Send(self, url, opname, obj, nsdict={}, soapaction=None, wsaction=None, 
             endPointReference=None, soapheaders=(), **kw):
        '''Send a message.  If url is None, use the value from the
        constructor (else error). obj is the object (data) to send.
        Data may be described with a requesttypecode keyword, the default 
        is the class's typecode (if there is one), else Any.

        Try to serialize as a Struct, if this is not possible serialize an Array.  If 
        data is a sequence of built-in python data types, it will be serialized as an
        Array, unless requesttypecode is specified.

        arguments:
            url -- 
            opname -- struct wrapper
            obj -- python instance

        key word arguments:
            nsdict -- 
            soapaction --
            wsaction -- WS-Address Action, goes in SOAP Header.
            endPointReference --  set by calling party, must be an 
                EndPointReference type instance.
            soapheaders -- list of pyobj, typically w/typecode attribute.
                serialized in the SOAP:Header.
            requesttypecode -- 

        '''
        url = url or self.url
        endPointReference = endPointReference or self.endPointReference

        # Serialize the object.
        d = {}
        d.update(self.nsdict)
        d.update(nsdict)

        sw = SoapWriter(nsdict=d, header=True, outputclass=self.writerclass, 
                 encodingStyle=kw.get('encodingStyle'),)
        
        requesttypecode = kw.get('requesttypecode')
        if kw.has_key('_args'): #NamedParamBinding
            tc = requesttypecode or TC.Any(pname=opname, aslist=False)
            sw.serialize(kw['_args'], tc)
        elif not requesttypecode:
            tc = getattr(obj, 'typecode', None) or TC.Any(pname=opname, aslist=False)
            try:
                if type(obj) in _seqtypes:
                    obj = dict(map(lambda i: (i.typecode.pname,i), obj))
            except AttributeError:
                # can't do anything but serialize this in a SOAP:Array
                tc = TC.Any(pname=opname, aslist=True)
            else:
                tc = TC.Any(pname=opname, aslist=False)

            sw.serialize(obj, tc)
        else:
            sw.serialize(obj, requesttypecode)
            
        for i in soapheaders:
           sw.serialize_header(i) 
            
        # 
        # Determine the SOAP auth element.  SOAP:Header element
        if self.auth_style & AUTH.zsibasic:
            sw.serialize_header(_AuthHeader(self.auth_user, self.auth_pass),
                _AuthHeader.typecode)

        # 
        # Serialize WS-Address
        if self.wsAddressURI is not None:
            if self.soapaction and wsaction.strip('\'""') != self.soapaction:
                raise WSActionException, 'soapAction(%s) and WS-Action(%s) must match'\
                    %(self.soapaction,wsaction)

            self.address = Address(url, self.wsAddressURI)
            self.address.setRequest(endPointReference, wsaction)
            self.address.serialize(sw)

        # 
        # WS-Security Signature Handler
        if self.sig_handler is not None:
            self.sig_handler.sign(sw)

        scheme,netloc,path,nil,nil,nil = urlparse.urlparse(url)
        transport = self.transport
        if transport is None and url is not None:
            if scheme == 'https':
                transport = self.defaultHttpsTransport
            elif scheme == 'http':
                transport = self.defaultHttpTransport
            else:
                raise RuntimeError, 'must specify transport or url startswith https/http'

        # Send the request.
        if issubclass(transport, httplib.HTTPConnection) is False:
            raise TypeError, 'transport must be a HTTPConnection'

        soapdata = str(sw)
        self.h = transport(netloc, None, **self.transdict)
        self.h.connect()
        self.SendSOAPData(soapdata, url, soapaction, **kw)","['def', 'Send', '(', 'self', ',', 'url', ',', 'opname', ',', 'obj', ',', 'nsdict', '=', '{', '}', ',', 'soapaction', '=', 'None', ',', 'wsaction', '=', 'None', ',', 'endPointReference', '=', 'None', ',', 'soapheaders', '=', '(', ')', ',', '*', '*', 'kw', ')', ':', 'url', '=', 'url', 'or', 'self', '.', 'url', 'endPointReference', '=', 'endPointReference', 'or', 'self', '.', 'endPointReference', '# Serialize the object.', 'd', '=', '{', '}', 'd', '.', 'update', '(', 'self', '.', 'nsdict', ')', 'd', '.', 'update', '(', 'nsdict', ')', 'sw', '=', 'SoapWriter', '(', 'nsdict', '=', 'd', ',', 'header', '=', 'True', ',', 'outputclass', '=', 'self', '.', 'writerclass', ',', 'encodingStyle', '=', 'kw', '.', 'get', '(', ""'encodingStyle'"", ')', ',', ')', 'requesttypecode', '=', 'kw', '.', 'get', '(', ""'requesttypecode'"", ')', 'if', 'kw', '.', 'has_key', '(', ""'_args'"", ')', ':', '#NamedParamBinding', 'tc', '=', 'requesttypecode', 'or', 'TC', '.', 'Any', '(', 'pname', '=', 'opname', ',', 'aslist', '=', 'False', ')', 'sw', '.', 'serialize', '(', 'kw', '[', ""'_args'"", ']', ',', 'tc', ')', 'elif', 'not', 'requesttypecode', ':', 'tc', '=', 'getattr', '(', 'obj', ',', ""'typecode'"", ',', 'None', ')', 'or', 'TC', '.', 'Any', '(', 'pname', '=', 'opname', ',', 'aslist', '=', 'False', ')', 'try', ':', 'if', 'type', '(', 'obj', ')', 'in', '_seqtypes', ':', 'obj', '=', 'dict', '(', 'map', '(', 'lambda', 'i', ':', '(', 'i', '.', 'typecode', '.', 'pname', ',', 'i', ')', ',', 'obj', ')', ')', 'except', 'AttributeError', ':', ""# can't do anything but serialize this in a SOAP:Array"", 'tc', '=', 'TC', '.', 'Any', '(', 'pname', '=', 'opname', ',', 'aslist', '=', 'True', ')', 'else', ':', 'tc', '=', 'TC', '.', 'Any', '(', 'pname', '=', 'opname', ',', 'aslist', '=', 'False', ')', 'sw', '.', 'serialize', '(', 'obj', ',', 'tc', ')', 'else', ':', 'sw', '.', 'serialize', '(', 'obj', ',', 'requesttypecode', ')', 'for', 'i', 'in', 'soapheaders', ':', 'sw', '.', 'serialize_header', '(', 'i', ')', '# ', '# Determine the SOAP auth element.  SOAP:Header element', 'if', 'self', '.', 'auth_style', '&', 'AUTH', '.', 'zsibasic', ':', 'sw', '.', 'serialize_header', '(', '_AuthHeader', '(', 'self', '.', 'auth_user', ',', 'self', '.', 'auth_pass', ')', ',', '_AuthHeader', '.', 'typecode', ')', '# ', '# Serialize WS-Address', 'if', 'self', '.', 'wsAddressURI', 'is', 'not', 'None', ':', 'if', 'self', '.', 'soapaction', 'and', 'wsaction', '.', 'strip', '(', '\'\\\'""\'', ')', '!=', 'self', '.', 'soapaction', ':', 'raise', 'WSActionException', ',', ""'soapAction(%s) and WS-Action(%s) must match'"", '%', '(', 'self', '.', 'soapaction', ',', 'wsaction', ')', 'self', '.', 'address', '=', 'Address', '(', 'url', ',', 'self', '.', 'wsAddressURI', ')', 'self', '.', 'address', '.', 'setRequest', '(', 'endPointReference', ',', 'wsaction', ')', 'self', '.', 'address', '.', 'serialize', '(', 'sw', ')', '# ', '# WS-Security Signature Handler', 'if', 'self', '.', 'sig_handler', 'is', 'not', 'None', ':', 'self', '.', 'sig_handler', '.', 'sign', '(', 'sw', ')', 'scheme', ',', 'netloc', ',', 'path', ',', 'nil', ',', 'nil', ',', 'nil', '=', 'urlparse', '.', 'urlparse', '(', 'url', ')', 'transport', '=', 'self', '.', 'transport', 'if', 'transport', 'is', 'None', 'and', 'url', 'is', 'not', 'None', ':', 'if', 'scheme', '==', ""'https'"", ':', 'transport', '=', 'self', '.', 'defaultHttpsTransport', 'elif', 'scheme', '==', ""'http'"", ':', 'transport', '=', 'self', '.', 'defaultHttpTransport', 'else', ':', 'raise', 'RuntimeError', ',', ""'must specify transport or url startswith https/http'"", '# Send the request.', 'if', 'issubclass', '(', 'transport', ',', 'httplib', '.', 'HTTPConnection', ')', 'is', 'False', ':', 'raise', 'TypeError', ',', ""'transport must be a HTTPConnection'"", 'soapdata', '=', 'str', '(', 'sw', ')', 'self', '.', 'h', '=', 'transport', '(', 'netloc', ',', 'None', ',', '*', '*', 'self', '.', 'transdict', ')', 'self', '.', 'h', '.', 'connect', '(', ')', 'self', '.', 'SendSOAPData', '(', 'soapdata', ',', 'url', ',', 'soapaction', ',', '*', '*', 'kw', ')']","Send a message.  If url is None, use the value from the
        constructor (else error). obj is the object (data) to send.
        Data may be described with a requesttypecode keyword, the default 
        is the class's typecode (if there is one), else Any.

        Try to serialize as a Struct, if this is not possible serialize an Array.  If 
        data is a sequence of built-in python data types, it will be serialized as an
        Array, unless requesttypecode is specified.

        arguments:
            url -- 
            opname -- struct wrapper
            obj -- python instance

        key word arguments:
            nsdict -- 
            soapaction --
            wsaction -- WS-Address Action, goes in SOAP Header.
            endPointReference --  set by calling party, must be an 
                EndPointReference type instance.
            soapheaders -- list of pyobj, typically w/typecode attribute.
                serialized in the SOAP:Header.
            requesttypecode --","['Send', 'a', 'message', '.', 'If', 'url', 'is', 'None', 'use', 'the', 'value', 'from', 'the', 'constructor', '(', 'else', 'error', ')', '.', 'obj', 'is', 'the', 'object', '(', 'data', ')', 'to', 'send', '.', 'Data', 'may', 'be', 'described', 'with', 'a', 'requesttypecode', 'keyword', 'the', 'default', 'is', 'the', 'class', 's', 'typecode', '(', 'if', 'there', 'is', 'one', ')', 'else', 'Any', '.']",python,X,2,True,1,train
10242,collectiveacuity/labPack,labpack/messaging/twilio.py,https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/messaging/twilio.py#L20-L61,"def send_message(self, phone_number, message_text):

        ''' send an SMS from the Twilio account to phone number

        :param phone_number: string with phone number with country and area code
        :param message_text: string with message text
        :return: dictionary with details of response

        {
            'direction': 'outbound-api',
            'to': '+18001234567',
            'status': 'queued',
            'dt': 1479603651.0,
            'body': 'good times',
            'error_code': None,
            'from': '+19001234567'
        }

        '''

        response = self.client.messages.save(
            to=phone_number,
            from_=self.twilio_phone,
            body=message_text
        )

        keys = ['body', 'status', 'error_code', 'direction', 'date_updated', 'to', 'from_']
        response_details = {}
        import re
        builtin_pattern = re.compile('^_')
        for method in dir(response):
            if not builtin_pattern.findall(method):
                if method == 'date_updated':
                    from labpack.records.time import labDT
                    python_dt = getattr(response, method)
                    from tzlocal import get_localzone
                    python_dt = python_dt.replace(tzinfo=get_localzone())
                    response_details[method] = labDT.fromPython(python_dt).epoch()
                elif method in keys:
                    response_details[method] = getattr(response, method)

        return response_details","['def', 'send_message', '(', 'self', ',', 'phone_number', ',', 'message_text', ')', ':', 'response', '=', 'self', '.', 'client', '.', 'messages', '.', 'save', '(', 'to', '=', 'phone_number', ',', 'from_', '=', 'self', '.', 'twilio_phone', ',', 'body', '=', 'message_text', ')', 'keys', '=', '[', ""'body'"", ',', ""'status'"", ',', ""'error_code'"", ',', ""'direction'"", ',', ""'date_updated'"", ',', ""'to'"", ',', ""'from_'"", ']', 'response_details', '=', '{', '}', 'import', 're', 'builtin_pattern', '=', 're', '.', 'compile', '(', ""'^_'"", ')', 'for', 'method', 'in', 'dir', '(', 'response', ')', ':', 'if', 'not', 'builtin_pattern', '.', 'findall', '(', 'method', ')', ':', 'if', 'method', '==', ""'date_updated'"", ':', 'from', 'labpack', '.', 'records', '.', 'time', 'import', 'labDT', 'python_dt', '=', 'getattr', '(', 'response', ',', 'method', ')', 'from', 'tzlocal', 'import', 'get_localzone', 'python_dt', '=', 'python_dt', '.', 'replace', '(', 'tzinfo', '=', 'get_localzone', '(', ')', ')', 'response_details', '[', 'method', ']', '=', 'labDT', '.', 'fromPython', '(', 'python_dt', ')', '.', 'epoch', '(', ')', 'elif', 'method', 'in', 'keys', ':', 'response_details', '[', 'method', ']', '=', 'getattr', '(', 'response', ',', 'method', ')', 'return', 'response_details']","send an SMS from the Twilio account to phone number

        :param phone_number: string with phone number with country and area code
        :param message_text: string with message text
        :return: dictionary with details of response

        {
            'direction': 'outbound-api',
            'to': '+18001234567',
            'status': 'queued',
            'dt': 1479603651.0,
            'body': 'good times',
            'error_code': None,
            'from': '+19001234567'
        }","['send', 'an', 'SMS', 'from', 'the', 'Twilio', 'account', 'to', 'phone', 'number', ':', 'param', 'phone_number', ':', 'string', 'with', 'phone', 'number', 'with', 'country', 'and', 'area', 'code', ':', 'param', 'message_text', ':', 'string', 'with', 'message', 'text', ':', 'return', ':', 'dictionary', 'with', 'details', 'of', 'response', '{', 'direction', ':', 'outbound', '-', 'api', 'to', ':', '+', '18001234567', 'status', ':', 'queued', 'dt', ':', '1479603651', '.', '0', 'body', ':', 'good', 'times', 'error_code', ':', 'None', 'from', ':', '+', '19001234567', '}']",python,X,2,True,1,train
11872,collectiveacuity/labPack,labpack/messaging/telegram.py,https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/messaging/telegram.py#L688-L765,"def send_message(self, user_id, message_text, message_style='', button_list=None, small_buttons=True, persist_buttons=False, link_preview=True):

        ''' a method to send a message using telegram api

        :param user_id: integer with id of telegram user
        :param message_text: string with message to user
        :param message_style: [optional] string with style to apply to text, only 'markdown'
        :param button_list: [optional] list of string to include as buttons in message
        :param small_buttons: [optional] boolean to resize buttons to single line
        :param persist_buttons: [optional] boolean to keep buttons around after exiting
        :param link_preview: [optional] boolean to open up a preview window of a link in message
        :return: dictionary of response details with message details in [json][result]

        {
            'headers': { ... },
            'url': 'https://api.telegram.org/bot.../sendMessage',
            'code': 200,
            'error': '',
            'json': {
                'ok': True,
                'result': {
                    'chat': {
                        'first_name': 'First',
                        'type': 'private',
                        'id': 1234567890,
                        'last_name': 'Last'
                    },
                    'text': 'text me again',
                    'from': {
                        'first_name': 'my Bot',
                        'id': 987654310,
                        'username': 'myBot'
                    },
                    'message_id': 178,
                    'date': 1478729313
                }
            }
        }
        '''

        title = '%s.send_message' % self.__class__.__name__

    # validate inputs
        input_fields = {
            'user_id': user_id,
            'message_text': message_text,
            'message_style': message_style,
            'button_list': button_list
        }
        for key, value in input_fields.items():
            if value:
                object_title = '%s(%s=%s)' % (title, key, str(value))
                self.fields.validate(value, '.%s' % key, object_title)

    # construct key word arguments
        request_kwargs = {
            'url': '%s/sendMessage' % self.api_endpoint,
            'data': {
                'chat_id': user_id,
                'text': message_text
            }
        }
        if message_style:
            if message_style == 'markdown':
                request_kwargs['data']['parse_mode'] = 'Markdown'
            elif message_style == 'html':
                request_kwargs['data']['parse_mode'] = 'HTML'
        if button_list:
            request_kwargs['data']['reply_markup'] = self._compile_buttons(button_list, small_buttons, persist_buttons)
        # elif keypad_type:
        #     request_kwargs['data']['reply_markup'] = self._compile_keypad(keypad_type, persist_buttons)
        if not link_preview:
            request_kwargs['data']['disable_web_page_preview'] = True

    # send request
        response_details = self._post_request(**request_kwargs)

        return response_details","['def', 'send_message', '(', 'self', ',', 'user_id', ',', 'message_text', ',', 'message_style', '=', ""''"", ',', 'button_list', '=', 'None', ',', 'small_buttons', '=', 'True', ',', 'persist_buttons', '=', 'False', ',', 'link_preview', '=', 'True', ')', ':', 'title', '=', ""'%s.send_message'"", '%', 'self', '.', '__class__', '.', '__name__', '# validate inputs\r', 'input_fields', '=', '{', ""'user_id'"", ':', 'user_id', ',', ""'message_text'"", ':', 'message_text', ',', ""'message_style'"", ':', 'message_style', ',', ""'button_list'"", ':', 'button_list', '}', 'for', 'key', ',', 'value', 'in', 'input_fields', '.', 'items', '(', ')', ':', 'if', 'value', ':', 'object_title', '=', ""'%s(%s=%s)'"", '%', '(', 'title', ',', 'key', ',', 'str', '(', 'value', ')', ')', 'self', '.', 'fields', '.', 'validate', '(', 'value', ',', ""'.%s'"", '%', 'key', ',', 'object_title', ')', '# construct key word arguments\r', 'request_kwargs', '=', '{', ""'url'"", ':', ""'%s/sendMessage'"", '%', 'self', '.', 'api_endpoint', ',', ""'data'"", ':', '{', ""'chat_id'"", ':', 'user_id', ',', ""'text'"", ':', 'message_text', '}', '}', 'if', 'message_style', ':', 'if', 'message_style', '==', ""'markdown'"", ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'parse_mode'"", ']', '=', ""'Markdown'"", 'elif', 'message_style', '==', ""'html'"", ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'parse_mode'"", ']', '=', ""'HTML'"", 'if', 'button_list', ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'reply_markup'"", ']', '=', 'self', '.', '_compile_buttons', '(', 'button_list', ',', 'small_buttons', ',', 'persist_buttons', ')', '# elif keypad_type:\r', ""#     request_kwargs['data']['reply_markup'] = self._compile_keypad(keypad_type, persist_buttons)\r"", 'if', 'not', 'link_preview', ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'disable_web_page_preview'"", ']', '=', 'True', '# send request\r', 'response_details', '=', 'self', '.', '_post_request', '(', '*', '*', 'request_kwargs', ')', 'return', 'response_details']","a method to send a message using telegram api

        :param user_id: integer with id of telegram user
        :param message_text: string with message to user
        :param message_style: [optional] string with style to apply to text, only 'markdown'
        :param button_list: [optional] list of string to include as buttons in message
        :param small_buttons: [optional] boolean to resize buttons to single line
        :param persist_buttons: [optional] boolean to keep buttons around after exiting
        :param link_preview: [optional] boolean to open up a preview window of a link in message
        :return: dictionary of response details with message details in [json][result]

        {
            'headers': { ... },
            'url': 'https://api.telegram.org/bot.../sendMessage',
            'code': 200,
            'error': '',
            'json': {
                'ok': True,
                'result': {
                    'chat': {
                        'first_name': 'First',
                        'type': 'private',
                        'id': 1234567890,
                        'last_name': 'Last'
                    },
                    'text': 'text me again',
                    'from': {
                        'first_name': 'my Bot',
                        'id': 987654310,
                        'username': 'myBot'
                    },
                    'message_id': 178,
                    'date': 1478729313
                }
            }
        }","['a', 'method', 'to', 'send', 'a', 'message', 'using', 'telegram', 'api', ':', 'param', 'user_id', ':', 'integer', 'with', 'id', 'of', 'telegram', 'user', ':', 'param', 'message_text', ':', 'string', 'with', 'message', 'to', 'user', ':', 'param', 'message_style', ':', '[', 'optional', ']', 'string', 'with', 'style', 'to', 'apply', 'to', 'text', 'only', 'markdown', ':', 'param', 'button_list', ':', '[', 'optional', ']', 'list', 'of', 'string', 'to', 'include', 'as', 'buttons', 'in', 'message', ':', 'param', 'small_buttons', ':', '[', 'optional', ']', 'boolean', 'to', 'resize', 'buttons', 'to', 'single', 'line', ':', 'param', 'persist_buttons', ':', '[', 'optional', ']', 'boolean', 'to', 'keep', 'buttons', 'around', 'after', 'exiting', ':', 'param', 'link_preview', ':', '[', 'optional', ']', 'boolean', 'to', 'open', 'up', 'a', 'preview', 'window', 'of', 'a', 'link', 'in', 'message', ':', 'return', ':', 'dictionary', 'of', 'response', 'details', 'with', 'message', 'details', 'in', '[', 'json', ']', '[', 'result', ']', '{', 'headers', ':', '{', '...', '}', 'url', ':', 'https', ':', '//', 'api', '.', 'telegram', '.', 'org', '/', 'bot', '...', '/', 'sendMessage', 'code', ':', '200', 'error', ':', 'json', ':', '{', 'ok', ':', 'True', 'result', ':', '{', 'chat', ':', '{', 'first_name', ':', 'First', 'type', ':', 'private', 'id', ':', '1234567890', 'last_name', ':', 'Last', '}', 'text', ':', 'text', 'me', 'again', 'from', ':', '{', 'first_name', ':', 'my', 'Bot', 'id', ':', '987654310', 'username', ':', 'myBot', '}', 'message_id', ':', '178', 'date', ':', '1478729313', '}', '}', '}']",python,X,2,True,1,train
11873,collectiveacuity/labPack,labpack/messaging/telegram.py,https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/messaging/telegram.py#L767-L881,"def send_photo(self, user_id, photo_id='', photo_path='', photo_url='', caption_text='', button_list=None, small_buttons=True, persist_buttons=False):

        ''' a method to send a photo using telegram api

        :param user_id: integer with id of telegram user
        :param photo_id: [optional] string with id of file stored with telegram api
        :param photo_path: [optional] string with local path to file
        :param photo_url: [optional] string with url of file
        :param caption_text: [optional] string with caption to add to photo
        :return: dictionary of response details with message detail in [json][result]

        {
            'headers': { ... },
            'url': 'https://api.telegram.org/bot.../sendPhoto',
            'code': 200,
            'error': '',
            'json': {
                'ok': True,
                'result': {
                    'chat': {
                        'first_name': 'First',
                        'type': 'private',
                        'id': 1234567890,
                        'last_name': 'Last'
                    },
                    'caption': 'lab logo',
                    'photo': [
                        {
                            'file_id': 'AgADAQ...EC',
                            'width': 51,
                            'file_size': 1238,
                            'height': 90
                        },
                        {
                            'file_id': 'AgADAQ...Ag',
                            'width': 180,
                            'file_size': 13151,
                            'height': 320
                        },
                        {
                            'file_id': 'AgADAQ...VC',
                            'width': 449,
                            'file_size': 51134,
                            'height': 800
                        },
                        {
                            'file_id': 'AgADAQ...AC',
                            'width': 719,
                            'file_size': 82609,
                            'height': 1280
                        }
                    ],
                    'from': {
                        'first_name': 'my Bot',
                        'id': 987654310,
                        'username': 'myBot'
                    },
                    'message_id': 179,
                    'date': 1478729413
                }
            }
        }
        '''

        title = '%s.send_photo' % self.__class__.__name__

    # validate inputs
        input_fields = {
            'user_id': user_id,
            'caption_text': caption_text,
            'photo_id': photo_id,
            'photo_path': photo_path,
            'photo_url': photo_url,
            'button_list': button_list
        }
        for key, value in input_fields.items():
            if value:
                object_title = '%s(%s=%s)' % (title, key, str(value))
                self.fields.validate(value, '.%s' % key, object_title)

    # construct extension map
        extension_map = self.fields.schema['photo_extensions']

    # construct key word arguments
        request_kwargs = {
            'url': '%s/sendPhoto' % self.api_endpoint,
            'data': {
                'chat_id': user_id
            }
        }
        if caption_text:
            request_kwargs['data']['caption'] = caption_text
        if button_list:
            request_kwargs['data']['reply_markup'] = self._compile_buttons(button_list, small_buttons, persist_buttons)

    # add photo to request keywords
        if photo_path:
            import os
            self._validate_type(photo_path, extension_map, title, 'photo_path')
            if not os.path.exists(photo_path):
                raise ValueError('%s is not a valid file path.' % photo_path)
            request_kwargs['files'] = { 'photo': open(photo_path, 'rb') }
        elif photo_id:
            request_kwargs['data']['photo'] = photo_id
        elif photo_url:
            file_extension = self._validate_type(photo_url, extension_map, title, 'photo_url')
            file_buffer = self._get_data(photo_url, 'photo%s' % file_extension, title, 'photo_url')
            request_kwargs['files'] = { 'photo': file_buffer }
        else:
            raise IndexError('%s(...) requires either a photo_path, photo_id or photo_url argument' % title)

    # send request
        response_details = self._post_request(**request_kwargs)

        return response_details","['def', 'send_photo', '(', 'self', ',', 'user_id', ',', 'photo_id', '=', ""''"", ',', 'photo_path', '=', ""''"", ',', 'photo_url', '=', ""''"", ',', 'caption_text', '=', ""''"", ',', 'button_list', '=', 'None', ',', 'small_buttons', '=', 'True', ',', 'persist_buttons', '=', 'False', ')', ':', 'title', '=', ""'%s.send_photo'"", '%', 'self', '.', '__class__', '.', '__name__', '# validate inputs\r', 'input_fields', '=', '{', ""'user_id'"", ':', 'user_id', ',', ""'caption_text'"", ':', 'caption_text', ',', ""'photo_id'"", ':', 'photo_id', ',', ""'photo_path'"", ':', 'photo_path', ',', ""'photo_url'"", ':', 'photo_url', ',', ""'button_list'"", ':', 'button_list', '}', 'for', 'key', ',', 'value', 'in', 'input_fields', '.', 'items', '(', ')', ':', 'if', 'value', ':', 'object_title', '=', ""'%s(%s=%s)'"", '%', '(', 'title', ',', 'key', ',', 'str', '(', 'value', ')', ')', 'self', '.', 'fields', '.', 'validate', '(', 'value', ',', ""'.%s'"", '%', 'key', ',', 'object_title', ')', '# construct extension map\r', 'extension_map', '=', 'self', '.', 'fields', '.', 'schema', '[', ""'photo_extensions'"", ']', '# construct key word arguments\r', 'request_kwargs', '=', '{', ""'url'"", ':', ""'%s/sendPhoto'"", '%', 'self', '.', 'api_endpoint', ',', ""'data'"", ':', '{', ""'chat_id'"", ':', 'user_id', '}', '}', 'if', 'caption_text', ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'caption'"", ']', '=', 'caption_text', 'if', 'button_list', ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'reply_markup'"", ']', '=', 'self', '.', '_compile_buttons', '(', 'button_list', ',', 'small_buttons', ',', 'persist_buttons', ')', '# add photo to request keywords\r', 'if', 'photo_path', ':', 'import', 'os', 'self', '.', '_validate_type', '(', 'photo_path', ',', 'extension_map', ',', 'title', ',', ""'photo_path'"", ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'photo_path', ')', ':', 'raise', 'ValueError', '(', ""'%s is not a valid file path.'"", '%', 'photo_path', ')', 'request_kwargs', '[', ""'files'"", ']', '=', '{', ""'photo'"", ':', 'open', '(', 'photo_path', ',', ""'rb'"", ')', '}', 'elif', 'photo_id', ':', 'request_kwargs', '[', ""'data'"", ']', '[', ""'photo'"", ']', '=', 'photo_id', 'elif', 'photo_url', ':', 'file_extension', '=', 'self', '.', '_validate_type', '(', 'photo_url', ',', 'extension_map', ',', 'title', ',', ""'photo_url'"", ')', 'file_buffer', '=', 'self', '.', '_get_data', '(', 'photo_url', ',', ""'photo%s'"", '%', 'file_extension', ',', 'title', ',', ""'photo_url'"", ')', 'request_kwargs', '[', ""'files'"", ']', '=', '{', ""'photo'"", ':', 'file_buffer', '}', 'else', ':', 'raise', 'IndexError', '(', ""'%s(...) requires either a photo_path, photo_id or photo_url argument'"", '%', 'title', ')', '# send request\r', 'response_details', '=', 'self', '.', '_post_request', '(', '*', '*', 'request_kwargs', ')', 'return', 'response_details']","a method to send a photo using telegram api

        :param user_id: integer with id of telegram user
        :param photo_id: [optional] string with id of file stored with telegram api
        :param photo_path: [optional] string with local path to file
        :param photo_url: [optional] string with url of file
        :param caption_text: [optional] string with caption to add to photo
        :return: dictionary of response details with message detail in [json][result]

        {
            'headers': { ... },
            'url': 'https://api.telegram.org/bot.../sendPhoto',
            'code': 200,
            'error': '',
            'json': {
                'ok': True,
                'result': {
                    'chat': {
                        'first_name': 'First',
                        'type': 'private',
                        'id': 1234567890,
                        'last_name': 'Last'
                    },
                    'caption': 'lab logo',
                    'photo': [
                        {
                            'file_id': 'AgADAQ...EC',
                            'width': 51,
                            'file_size': 1238,
                            'height': 90
                        },
                        {
                            'file_id': 'AgADAQ...Ag',
                            'width': 180,
                            'file_size': 13151,
                            'height': 320
                        },
                        {
                            'file_id': 'AgADAQ...VC',
                            'width': 449,
                            'file_size': 51134,
                            'height': 800
                        },
                        {
                            'file_id': 'AgADAQ...AC',
                            'width': 719,
                            'file_size': 82609,
                            'height': 1280
                        }
                    ],
                    'from': {
                        'first_name': 'my Bot',
                        'id': 987654310,
                        'username': 'myBot'
                    },
                    'message_id': 179,
                    'date': 1478729413
                }
            }
        }","['a', 'method', 'to', 'send', 'a', 'photo', 'using', 'telegram', 'api', ':', 'param', 'user_id', ':', 'integer', 'with', 'id', 'of', 'telegram', 'user', ':', 'param', 'photo_id', ':', '[', 'optional', ']', 'string', 'with', 'id', 'of', 'file', 'stored', 'with', 'telegram', 'api', ':', 'param', 'photo_path', ':', '[', 'optional', ']', 'string', 'with', 'local', 'path', 'to', 'file', ':', 'param', 'photo_url', ':', '[', 'optional', ']', 'string', 'with', 'url', 'of', 'file', ':', 'param', 'caption_text', ':', '[', 'optional', ']', 'string', 'with', 'caption', 'to', 'add', 'to', 'photo', ':', 'return', ':', 'dictionary', 'of', 'response', 'details', 'with', 'message', 'detail', 'in', '[', 'json', ']', '[', 'result', ']', '{', 'headers', ':', '{', '...', '}', 'url', ':', 'https', ':', '//', 'api', '.', 'telegram', '.', 'org', '/', 'bot', '...', '/', 'sendPhoto', 'code', ':', '200', 'error', ':', 'json', ':', '{', 'ok', ':', 'True', 'result', ':', '{', 'chat', ':', '{', 'first_name', ':', 'First', 'type', ':', 'private', 'id', ':', '1234567890', 'last_name', ':', 'Last', '}', 'caption', ':', 'lab', 'logo', 'photo', ':', '[', '{', 'file_id', ':', 'AgADAQ', '...', 'EC', 'width', ':', '51', 'file_size', ':', '1238', 'height', ':', '90', '}', '{', 'file_id', ':', 'AgADAQ', '...', 'Ag', 'width', ':', '180', 'file_size', ':', '13151', 'height', ':', '320', '}', '{', 'file_id', ':', 'AgADAQ', '...', 'VC', 'width', ':', '449', 'file_size', ':', '51134', 'height', ':', '800', '}', '{', 'file_id', ':', 'AgADAQ', '...', 'AC', 'width', ':', '719', 'file_size', ':', '82609', 'height', ':', '1280', '}', ']', 'from', ':', '{', 'first_name', ':', 'my', 'Bot', 'id', ':', '987654310', 'username', ':', 'myBot', '}', 'message_id', ':', '179', 'date', ':', '1478729413', '}', '}', '}']",python,X,2,True,1,train
12504,PyMLGame/pymlgame,pymlgame/controller.py,https://github.com/PyMLGame/pymlgame/blob/450fe77d35f9a26c107586d6954f69c3895bf504/pymlgame/controller.py#L151-L179,"def send(self, uid, event, payload=None):
        """"""
        Send an event to a connected controller. Use pymlgame event type and correct payload.
        To send a message to the controller use pymlgame.E_MESSAGE event and a string as payload.

        :param uid: Unique id of the controller
        :param event: Event type
        :param payload: Payload of the event
        :type uid: str
        :type event: Event
        :type payload: str
        :return: Number of bytes send or False
        :rtype: int
        """"""
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        if uid in self.controllers.keys():
            addr = self.controllers[uid][0]
            port = self.controllers[uid][1]
            if event == E_MESSAGE:
                #print('/message/{} => {}:{}'.format(payload, addr, port))
                return sock.sendto('/message/{}'.format(payload).encode('utf-8'), (addr, port))
            elif event == E_RUMBLE:
                #print('/rumble/{} => {}:{}'.format(payload, addr, port))
                return sock.sendto('/rumble/{}'.format(payload).encode('utf-8'), (addr, port))
            else:
                pass
        else:
            pass
        return False","['def', 'send', '(', 'self', ',', 'uid', ',', 'event', ',', 'payload', '=', 'None', ')', ':', 'sock', '=', 'socket', '.', 'socket', '(', 'socket', '.', 'AF_INET', ',', 'socket', '.', 'SOCK_DGRAM', ')', 'if', 'uid', 'in', 'self', '.', 'controllers', '.', 'keys', '(', ')', ':', 'addr', '=', 'self', '.', 'controllers', '[', 'uid', ']', '[', '0', ']', 'port', '=', 'self', '.', 'controllers', '[', 'uid', ']', '[', '1', ']', 'if', 'event', '==', 'E_MESSAGE', ':', ""#print('/message/{} => {}:{}'.format(payload, addr, port))"", 'return', 'sock', '.', 'sendto', '(', ""'/message/{}'"", '.', 'format', '(', 'payload', ')', '.', 'encode', '(', ""'utf-8'"", ')', ',', '(', 'addr', ',', 'port', ')', ')', 'elif', 'event', '==', 'E_RUMBLE', ':', ""#print('/rumble/{} => {}:{}'.format(payload, addr, port))"", 'return', 'sock', '.', 'sendto', '(', ""'/rumble/{}'"", '.', 'format', '(', 'payload', ')', '.', 'encode', '(', ""'utf-8'"", ')', ',', '(', 'addr', ',', 'port', ')', ')', 'else', ':', 'pass', 'else', ':', 'pass', 'return', 'False']","Send an event to a connected controller. Use pymlgame event type and correct payload.
        To send a message to the controller use pymlgame.E_MESSAGE event and a string as payload.

        :param uid: Unique id of the controller
        :param event: Event type
        :param payload: Payload of the event
        :type uid: str
        :type event: Event
        :type payload: str
        :return: Number of bytes send or False
        :rtype: int","['Send', 'an', 'event', 'to', 'a', 'connected', 'controller', '.', 'Use', 'pymlgame', 'event', 'type', 'and', 'correct', 'payload', '.', 'To', 'send', 'a', 'message', 'to', 'the', 'controller', 'use', 'pymlgame', '.', 'E_MESSAGE', 'event', 'and', 'a', 'string', 'as', 'payload', '.']",python,X,2,True,1,train
18372,jamieleshaw/lurklib,lurklib/sending.py,https://github.com/jamieleshaw/lurklib/blob/a861f35d880140422103dd78ec3239814e85fd7e/lurklib/sending.py#L24-L36,"def privmsg(self, target, message):
        """"""
        Sends a PRIVMSG to someone.
        Required arguments:
        * target - Who to send the message to.
        * message - Message to send.
        """"""
        with self.lock:
            self.send('PRIVMSG ' + target + ' :' + message)
            if self.readable():
                msg = self._recv(expected_replies=('301',))
                if msg[0] == '301':
                    return 'AWAY', msg[2].split(None, 1)[1].replace(':', '', 1)","['def', 'privmsg', '(', 'self', ',', 'target', ',', 'message', ')', ':', 'with', 'self', '.', 'lock', ':', 'self', '.', 'send', '(', ""'PRIVMSG '"", '+', 'target', '+', ""' :'"", '+', 'message', ')', 'if', 'self', '.', 'readable', '(', ')', ':', 'msg', '=', 'self', '.', '_recv', '(', 'expected_replies', '=', '(', ""'301'"", ',', ')', ')', 'if', 'msg', '[', '0', ']', '==', ""'301'"", ':', 'return', ""'AWAY'"", ',', 'msg', '[', '2', ']', '.', 'split', '(', 'None', ',', '1', ')', '[', '1', ']', '.', 'replace', '(', ""':'"", ',', ""''"", ',', '1', ')']","Sends a PRIVMSG to someone.
        Required arguments:
        * target - Who to send the message to.
        * message - Message to send.","['Sends', 'a', 'PRIVMSG', 'to', 'someone', '.', 'Required', 'arguments', ':', '*', 'target', '-', 'Who', 'to', 'send', 'the', 'message', 'to', '.', '*', 'message', '-', 'Message', 'to', 'send', '.']",python,X,2,True,1,train
19123,Gwildor/Pyromancer,pyromancer/objects.py,https://github.com/Gwildor/Pyromancer/blob/250a83ad4b6e87560bea8f2e0526ad0bba678f3d/pyromancer/objects.py#L313-L353,"def msg(self, message, *args, **kwargs):
        """"""Shortcut to send a message through the connection.

        This function sends the input message through the connection. A target
        can be defined, else it will send it to the channel or user from the
        input Line, effectively responding on whatever triggered the command
        which calls this function to be called. If raw has not been set to
        True, formatting will be applied using the standard Python Formatting
        Mini-Language, using the additional given args and kwargs, along with
        some additional kwargs, such as the match object to easily access Regex
        matches, color codes and other things.

        http://docs.python.org/3.3/library/string.html#format-string-syntax
        """"""

        target = kwargs.pop('target', None)
        raw = kwargs.pop('raw', False)

        if not target:
            target = self.line.sender.nick if self.line.pm else \
                self.line.target

        if not raw:
            kw = {
                'm': self,
                'b': chr(2),
                'c': chr(3),
                'u': chr(31),
            }
            kw.update(kwargs)

            try:
                message = message.format(*args, **kw)
            except IndexError:
                if len(args) == 1 and isinstance(args[0], list):
                    # Message might be: msg, [arg1, arg2], kwargs
                    message = message.format(*args[0], **kw)
                else:
                    raise

        self.connection.msg(target, message)","['def', 'msg', '(', 'self', ',', 'message', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'target', '=', 'kwargs', '.', 'pop', '(', ""'target'"", ',', 'None', ')', 'raw', '=', 'kwargs', '.', 'pop', '(', ""'raw'"", ',', 'False', ')', 'if', 'not', 'target', ':', 'target', '=', 'self', '.', 'line', '.', 'sender', '.', 'nick', 'if', 'self', '.', 'line', '.', 'pm', 'else', 'self', '.', 'line', '.', 'target', 'if', 'not', 'raw', ':', 'kw', '=', '{', ""'m'"", ':', 'self', ',', ""'b'"", ':', 'chr', '(', '2', ')', ',', ""'c'"", ':', 'chr', '(', '3', ')', ',', ""'u'"", ':', 'chr', '(', '31', ')', ',', '}', 'kw', '.', 'update', '(', 'kwargs', ')', 'try', ':', 'message', '=', 'message', '.', 'format', '(', '*', 'args', ',', '*', '*', 'kw', ')', 'except', 'IndexError', ':', 'if', 'len', '(', 'args', ')', '==', '1', 'and', 'isinstance', '(', 'args', '[', '0', ']', ',', 'list', ')', ':', '# Message might be: msg, [arg1, arg2], kwargs', 'message', '=', 'message', '.', 'format', '(', '*', 'args', '[', '0', ']', ',', '*', '*', 'kw', ')', 'else', ':', 'raise', 'self', '.', 'connection', '.', 'msg', '(', 'target', ',', 'message', ')']","Shortcut to send a message through the connection.

        This function sends the input message through the connection. A target
        can be defined, else it will send it to the channel or user from the
        input Line, effectively responding on whatever triggered the command
        which calls this function to be called. If raw has not been set to
        True, formatting will be applied using the standard Python Formatting
        Mini-Language, using the additional given args and kwargs, along with
        some additional kwargs, such as the match object to easily access Regex
        matches, color codes and other things.

        http://docs.python.org/3.3/library/string.html#format-string-syntax","['Shortcut', 'to', 'send', 'a', 'message', 'through', 'the', 'connection', '.']",python,X,2,True,1,train
19488,eeue56/PyChat.js,pychatjs/server/room.py,https://github.com/eeue56/PyChat.js/blob/45056de6f988350c90a6dbe674459a4affde8abc/pychatjs/server/room.py#L21-L26,"def disconnect(self, user):
        """""" Disconnect a user and send a message to the 
            connected clients """"""
        self.remove_user(user)
        self.send_message(create_message('RoomServer', 'Please all say goodbye to {name}!'.format(name=user.id.name)))
        self.send_message(create_disconnect(user.id.name))","['def', 'disconnect', '(', 'self', ',', 'user', ')', ':', 'self', '.', 'remove_user', '(', 'user', ')', 'self', '.', 'send_message', '(', 'create_message', '(', ""'RoomServer'"", ',', ""'Please all say goodbye to {name}!'"", '.', 'format', '(', 'name', '=', 'user', '.', 'id', '.', 'name', ')', ')', ')', 'self', '.', 'send_message', '(', 'create_disconnect', '(', 'user', '.', 'id', '.', 'name', ')', ')']","Disconnect a user and send a message to the 
            connected clients","['Disconnect', 'a', 'user', 'and', 'send', 'a', 'message', 'to', 'the', 'connected', 'clients']",python,X,2,True,1,train
19490,eeue56/PyChat.js,pychatjs/server/room.py,https://github.com/eeue56/PyChat.js/blob/45056de6f988350c90a6dbe674459a4affde8abc/pychatjs/server/room.py#L50-L54,"def send_message(self, message):
        """""" send a message to each of the users """"""
        for handler in self.users:
            logging.info('Handler: ' + str(handler))
            handler.write_message(message)","['def', 'send_message', '(', 'self', ',', 'message', ')', ':', 'for', 'handler', 'in', 'self', '.', 'users', ':', 'logging', '.', 'info', '(', ""'Handler: '"", '+', 'str', '(', 'handler', ')', ')', 'handler', '.', 'write_message', '(', 'message', ')']",send a message to each of the users,"['send', 'a', 'message', 'to', 'each', 'of', 'the', 'users']",python,X,2,True,1,train
20007,jamieleshaw/lurklib,lurklib/connection.py,https://github.com/jamieleshaw/lurklib/blob/a861f35d880140422103dd78ec3239814e85fd7e/lurklib/connection.py#L246-L257,"def _user(self, user, real_name):
        """"""
        Sends the USER message.
        Required arguments:
        * user - Username to send.
        * real_name - Real name to send.
        """"""
        with self.lock:
            self.send('USER %s 0 * :%s' % (user, real_name))
            if self.readable():
                self._recv()
                self.stepback()","['def', '_user', '(', 'self', ',', 'user', ',', 'real_name', ')', ':', 'with', 'self', '.', 'lock', ':', 'self', '.', 'send', '(', ""'USER %s 0 * :%s'"", '%', '(', 'user', ',', 'real_name', ')', ')', 'if', 'self', '.', 'readable', '(', ')', ':', 'self', '.', '_recv', '(', ')', 'self', '.', 'stepback', '(', ')']","Sends the USER message.
        Required arguments:
        * user - Username to send.
        * real_name - Real name to send.","['Sends', 'the', 'USER', 'message', '.', 'Required', 'arguments', ':', '*', 'user', '-', 'Username', 'to', 'send', '.', '*', 'real_name', '-', 'Real', 'name', 'to', 'send', '.']",python,X,2,True,1,train
4707,python-diamond/Diamond,src/diamond/handler/logentries_diamond.py,https://github.com/python-diamond/Diamond/blob/0f3eb04327d6d3ed5e53a9967d6c9d2c09714a47/src/diamond/handler/logentries_diamond.py#L70-L83,"def _send(self):
        """"""
        Convert message to a json object and send to Lognetries
        """"""
        while len(self.queue) > 0:
            metric = self.queue.popleft()
            topic, value, timestamp = str(metric).split()
            msg = json.dumps({""event"": {topic: value}})
            req = urllib2.Request(""https://js.logentries.com/v1/logs/"" +
                                  self.log_token, msg)
            try:
                urllib2.urlopen(req)
            except urllib2.URLError as e:
                logging.error(""Can't send log message to Logentries %s"", e)","['def', '_send', '(', 'self', ')', ':', 'while', 'len', '(', 'self', '.', 'queue', ')', '>', '0', ':', 'metric', '=', 'self', '.', 'queue', '.', 'popleft', '(', ')', 'topic', ',', 'value', ',', 'timestamp', '=', 'str', '(', 'metric', ')', '.', 'split', '(', ')', 'msg', '=', 'json', '.', 'dumps', '(', '{', '""event""', ':', '{', 'topic', ':', 'value', '}', '}', ')', 'req', '=', 'urllib2', '.', 'Request', '(', '""https://js.logentries.com/v1/logs/""', '+', 'self', '.', 'log_token', ',', 'msg', ')', 'try', ':', 'urllib2', '.', 'urlopen', '(', 'req', ')', 'except', 'urllib2', '.', 'URLError', 'as', 'e', ':', 'logging', '.', 'error', '(', '""Can\'t send log message to Logentries %s""', ',', 'e', ')']",Convert message to a json object and send to Lognetries,"['Convert', 'message', 'to', 'a', 'json', 'object', 'and', 'send', 'to', 'Lognetries']",python,X,2,True,1,train
7909,aiogram/aiogram,aiogram/types/message.py,https://github.com/aiogram/aiogram/blob/2af930149ce2482547721e2c8755c10307295e48/aiogram/types/message.py#L1021-L1057,"async def reply_voice(self, voice: typing.Union[base.InputFile, base.String],
                          caption: typing.Union[base.String, None] = None,
                          duration: typing.Union[base.Integer, None] = None,
                          disable_notification: typing.Union[base.Boolean, None] = None,
                          reply_markup=None,
                          reply=True) -> Message:
        """"""
        Use this method to send audio files, if you want Telegram clients to display the file
        as a playable voice message.

        For this to work, your audio must be in an .ogg file encoded with OPUS
        (other formats may be sent as Audio or Document).

        Source: https://core.telegram.org/bots/api#sendvoice

        :param voice: Audio file to send.
        :type voice: :obj:`typing.Union[base.InputFile, base.String]`
        :param caption: Voice message caption, 0-200 characters
        :type caption: :obj:`typing.Union[base.String, None]`
        :param duration: Duration of the voice message in seconds
        :type duration: :obj:`typing.Union[base.Integer, None]`
        :param disable_notification: Sends the message silently. Users will receive a notification with no sound.
        :type disable_notification: :obj:`typing.Union[base.Boolean, None]`
        :param reply_markup: Additional interface options.
        :type reply_markup: :obj:`typing.Union[types.InlineKeyboardMarkup,
            types.ReplyKeyboardMarkup, types.ReplyKeyboardRemove, types.ForceReply, None]`
        :param reply: fill 'reply_to_message_id'
        :return: On success, the sent Message is returned.
        :rtype: :obj:`types.Message`
        """"""
        return await self.bot.send_voice(chat_id=self.chat.id,
                                         voice=voice,
                                         caption=caption,
                                         duration=duration,
                                         disable_notification=disable_notification,
                                         reply_to_message_id=self.message_id if reply else None,
                                         reply_markup=reply_markup)","['async', 'def', 'reply_voice', '(', 'self', ',', 'voice', ':', 'typing', '.', 'Union', '[', 'base', '.', 'InputFile', ',', 'base', '.', 'String', ']', ',', 'caption', ':', 'typing', '.', 'Union', '[', 'base', '.', 'String', ',', 'None', ']', '=', 'None', ',', 'duration', ':', 'typing', '.', 'Union', '[', 'base', '.', 'Integer', ',', 'None', ']', '=', 'None', ',', 'disable_notification', ':', 'typing', '.', 'Union', '[', 'base', '.', 'Boolean', ',', 'None', ']', '=', 'None', ',', 'reply_markup', '=', 'None', ',', 'reply', '=', 'True', ')', '->', 'Message', ':', 'return', 'await', 'self', '.', 'bot', '.', 'send_voice', '(', 'chat_id', '=', 'self', '.', 'chat', '.', 'id', ',', 'voice', '=', 'voice', ',', 'caption', '=', 'caption', ',', 'duration', '=', 'duration', ',', 'disable_notification', '=', 'disable_notification', ',', 'reply_to_message_id', '=', 'self', '.', 'message_id', 'if', 'reply', 'else', 'None', ',', 'reply_markup', '=', 'reply_markup', ')']","Use this method to send audio files, if you want Telegram clients to display the file
        as a playable voice message.

        For this to work, your audio must be in an .ogg file encoded with OPUS
        (other formats may be sent as Audio or Document).

        Source: https://core.telegram.org/bots/api#sendvoice

        :param voice: Audio file to send.
        :type voice: :obj:`typing.Union[base.InputFile, base.String]`
        :param caption: Voice message caption, 0-200 characters
        :type caption: :obj:`typing.Union[base.String, None]`
        :param duration: Duration of the voice message in seconds
        :type duration: :obj:`typing.Union[base.Integer, None]`
        :param disable_notification: Sends the message silently. Users will receive a notification with no sound.
        :type disable_notification: :obj:`typing.Union[base.Boolean, None]`
        :param reply_markup: Additional interface options.
        :type reply_markup: :obj:`typing.Union[types.InlineKeyboardMarkup,
            types.ReplyKeyboardMarkup, types.ReplyKeyboardRemove, types.ForceReply, None]`
        :param reply: fill 'reply_to_message_id'
        :return: On success, the sent Message is returned.
        :rtype: :obj:`types.Message`","['Use', 'this', 'method', 'to', 'send', 'audio', 'files', 'if', 'you', 'want', 'Telegram', 'clients', 'to', 'display', 'the', 'file', 'as', 'a', 'playable', 'voice', 'message', '.']",python,X,2,True,1,train
8002,aiogram/aiogram,aiogram/bot/bot.py,https://github.com/aiogram/aiogram/blob/2af930149ce2482547721e2c8755c10307295e48/aiogram/bot/bot.py#L525-L575,"async def send_voice(self, chat_id: typing.Union[base.Integer, base.String],
                         voice: typing.Union[base.InputFile, base.String],
                         caption: typing.Union[base.String, None] = None,
                         parse_mode: typing.Union[base.String, None] = None,
                         duration: typing.Union[base.Integer, None] = None,
                         disable_notification: typing.Union[base.Boolean, None] = None,
                         reply_to_message_id: typing.Union[base.Integer, None] = None,
                         reply_markup: typing.Union[types.InlineKeyboardMarkup,
                                                    types.ReplyKeyboardMarkup,
                                                    types.ReplyKeyboardRemove,
                                                    types.ForceReply, None] = None) -> types.Message:
        """"""
        Use this method to send audio files, if you want Telegram clients to display the file
        as a playable voice message.

        For this to work, your audio must be in an .ogg file encoded with OPUS
        (other formats may be sent as Audio or Document).

        Source: https://core.telegram.org/bots/api#sendvoice

        :param chat_id: Unique identifier for the target chat or username of the target channel
        :type chat_id: :obj:`typing.Union[base.Integer, base.String]`
        :param voice: Audio file to send
        :type voice: :obj:`typing.Union[base.InputFile, base.String]`
        :param caption: Voice message caption, 0-1024 characters
        :type caption: :obj:`typing.Union[base.String, None]`
        :param parse_mode: Send Markdown or HTML, if you want Telegram apps to show bold, italic,
            fixed-width text or inline URLs in your bot's message.
        :type parse_mode: :obj:`typing.Union[base.String, None]`
        :param duration: Duration of the voice message in seconds
        :type duration: :obj:`typing.Union[base.Integer, None]`
        :param disable_notification: Sends the message silently. Users will receive a notification with no sound
        :type disable_notification: :obj:`typing.Union[base.Boolean, None]`
        :param reply_to_message_id: If the message is a reply, ID of the original message
        :type reply_to_message_id: :obj:`typing.Union[base.Integer, None]`
        :param reply_markup: Additional interface options
        :type reply_markup: :obj:`typing.Union[types.InlineKeyboardMarkup,
            types.ReplyKeyboardMarkup, types.ReplyKeyboardRemove, types.ForceReply, None]`
        :return: On success, the sent Message is returned
        :rtype: :obj:`types.Message`
        """"""
        reply_markup = prepare_arg(reply_markup)
        payload = generate_payload(**locals(), exclude=['voice'])
        if self.parse_mode:
            payload.setdefault('parse_mode', self.parse_mode)

        files = {}
        prepare_file(payload, files, 'voice', voice)

        result = await self.request(api.Methods.SEND_VOICE, payload, files)
        return types.Message(**result)","['async', 'def', 'send_voice', '(', 'self', ',', 'chat_id', ':', 'typing', '.', 'Union', '[', 'base', '.', 'Integer', ',', 'base', '.', 'String', ']', ',', 'voice', ':', 'typing', '.', 'Union', '[', 'base', '.', 'InputFile', ',', 'base', '.', 'String', ']', ',', 'caption', ':', 'typing', '.', 'Union', '[', 'base', '.', 'String', ',', 'None', ']', '=', 'None', ',', 'parse_mode', ':', 'typing', '.', 'Union', '[', 'base', '.', 'String', ',', 'None', ']', '=', 'None', ',', 'duration', ':', 'typing', '.', 'Union', '[', 'base', '.', 'Integer', ',', 'None', ']', '=', 'None', ',', 'disable_notification', ':', 'typing', '.', 'Union', '[', 'base', '.', 'Boolean', ',', 'None', ']', '=', 'None', ',', 'reply_to_message_id', ':', 'typing', '.', 'Union', '[', 'base', '.', 'Integer', ',', 'None', ']', '=', 'None', ',', 'reply_markup', ':', 'typing', '.', 'Union', '[', 'types', '.', 'InlineKeyboardMarkup', ',', 'types', '.', 'ReplyKeyboardMarkup', ',', 'types', '.', 'ReplyKeyboardRemove', ',', 'types', '.', 'ForceReply', ',', 'None', ']', '=', 'None', ')', '->', 'types', '.', 'Message', ':', 'reply_markup', '=', 'prepare_arg', '(', 'reply_markup', ')', 'payload', '=', 'generate_payload', '(', '*', '*', 'locals', '(', ')', ',', 'exclude', '=', '[', ""'voice'"", ']', ')', 'if', 'self', '.', 'parse_mode', ':', 'payload', '.', 'setdefault', '(', ""'parse_mode'"", ',', 'self', '.', 'parse_mode', ')', 'files', '=', '{', '}', 'prepare_file', '(', 'payload', ',', 'files', ',', ""'voice'"", ',', 'voice', ')', 'result', '=', 'await', 'self', '.', 'request', '(', 'api', '.', 'Methods', '.', 'SEND_VOICE', ',', 'payload', ',', 'files', ')', 'return', 'types', '.', 'Message', '(', '*', '*', 'result', ')']","Use this method to send audio files, if you want Telegram clients to display the file
        as a playable voice message.

        For this to work, your audio must be in an .ogg file encoded with OPUS
        (other formats may be sent as Audio or Document).

        Source: https://core.telegram.org/bots/api#sendvoice

        :param chat_id: Unique identifier for the target chat or username of the target channel
        :type chat_id: :obj:`typing.Union[base.Integer, base.String]`
        :param voice: Audio file to send
        :type voice: :obj:`typing.Union[base.InputFile, base.String]`
        :param caption: Voice message caption, 0-1024 characters
        :type caption: :obj:`typing.Union[base.String, None]`
        :param parse_mode: Send Markdown or HTML, if you want Telegram apps to show bold, italic,
            fixed-width text or inline URLs in your bot's message.
        :type parse_mode: :obj:`typing.Union[base.String, None]`
        :param duration: Duration of the voice message in seconds
        :type duration: :obj:`typing.Union[base.Integer, None]`
        :param disable_notification: Sends the message silently. Users will receive a notification with no sound
        :type disable_notification: :obj:`typing.Union[base.Boolean, None]`
        :param reply_to_message_id: If the message is a reply, ID of the original message
        :type reply_to_message_id: :obj:`typing.Union[base.Integer, None]`
        :param reply_markup: Additional interface options
        :type reply_markup: :obj:`typing.Union[types.InlineKeyboardMarkup,
            types.ReplyKeyboardMarkup, types.ReplyKeyboardRemove, types.ForceReply, None]`
        :return: On success, the sent Message is returned
        :rtype: :obj:`types.Message`","['Use', 'this', 'method', 'to', 'send', 'audio', 'files', 'if', 'you', 'want', 'Telegram', 'clients', 'to', 'display', 'the', 'file', 'as', 'a', 'playable', 'voice', 'message', '.']",python,X,2,True,1,train
11763,Bogdanp/dramatiq,dramatiq/actor.py,https://github.com/Bogdanp/dramatiq/blob/a8cc2728478e794952a5a50c3fb19ec455fe91b6/dramatiq/actor.py#L101-L111,"def send(self, *args, **kwargs):
        """"""Asynchronously send a message to this actor.

        Parameters:
          *args(tuple): Positional arguments to send to the actor.
          **kwargs(dict): Keyword arguments to send to the actor.

        Returns:
          Message: The enqueued message.
        """"""
        return self.send_with_options(args=args, kwargs=kwargs)","['def', 'send', '(', 'self', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'send_with_options', '(', 'args', '=', 'args', ',', 'kwargs', '=', 'kwargs', ')']","Asynchronously send a message to this actor.

        Parameters:
          *args(tuple): Positional arguments to send to the actor.
          **kwargs(dict): Keyword arguments to send to the actor.

        Returns:
          Message: The enqueued message.","['Asynchronously', 'send', 'a', 'message', 'to', 'this', 'actor', '.']",python,X,2,True,1,train
11764,Bogdanp/dramatiq,dramatiq/actor.py,https://github.com/Bogdanp/dramatiq/blob/a8cc2728478e794952a5a50c3fb19ec455fe91b6/dramatiq/actor.py#L113-L130,"def send_with_options(self, *, args=None, kwargs=None, delay=None, **options):
        """"""Asynchronously send a message to this actor, along with an
        arbitrary set of processing options for the broker and
        middleware.

        Parameters:
          args(tuple): Positional arguments that are passed to the actor.
          kwargs(dict): Keyword arguments that are passed to the actor.
          delay(int): The minimum amount of time, in milliseconds, the
            message should be delayed by.
          **options(dict): Arbitrary options that are passed to the
            broker and any registered middleware.

        Returns:
          Message: The enqueued message.
        """"""
        message = self.message_with_options(args=args, kwargs=kwargs, **options)
        return self.broker.enqueue(message, delay=delay)","['def', 'send_with_options', '(', 'self', ',', '*', ',', 'args', '=', 'None', ',', 'kwargs', '=', 'None', ',', 'delay', '=', 'None', ',', '*', '*', 'options', ')', ':', 'message', '=', 'self', '.', 'message_with_options', '(', 'args', '=', 'args', ',', 'kwargs', '=', 'kwargs', ',', '*', '*', 'options', ')', 'return', 'self', '.', 'broker', '.', 'enqueue', '(', 'message', ',', 'delay', '=', 'delay', ')']","Asynchronously send a message to this actor, along with an
        arbitrary set of processing options for the broker and
        middleware.

        Parameters:
          args(tuple): Positional arguments that are passed to the actor.
          kwargs(dict): Keyword arguments that are passed to the actor.
          delay(int): The minimum amount of time, in milliseconds, the
            message should be delayed by.
          **options(dict): Arbitrary options that are passed to the
            broker and any registered middleware.

        Returns:
          Message: The enqueued message.","['Asynchronously', 'send', 'a', 'message', 'to', 'this', 'actor', 'along', 'with', 'an', 'arbitrary', 'set', 'of', 'processing', 'options', 'for', 'the', 'broker', 'and', 'middleware', '.']",python,X,2,True,1,train
